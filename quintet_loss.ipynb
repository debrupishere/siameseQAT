{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quintet Loss with Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [2., 3., 4.],\n",
       "       [1., 1., 2.],\n",
       "       [1., 1., 2.],\n",
       "       [3., 3., 4.],\n",
       "       [3., 1., 2.],\n",
       "       [1., 1., 2.],\n",
       "       [2., 0., 0.],\n",
       "       [1., 1., 2.],\n",
       "       [1., 1., 2.],\n",
       "       [3., 1., 2.],\n",
       "       [3., 3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    \n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_pos = tf.concat([embeddings, centroid_embed_pos], axis=0)\n",
    "    \n",
    "    # add label for centroids\n",
    "    labels_pos = tf.concat([labels, labels], axis=0)\n",
    "    labels_pos = tf.cast(labels_pos, dtype=dtypes.float32)\n",
    "    vects_pos = tf.concat([tf.reshape(labels_pos, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "    return vects_pos\n",
    "\n",
    "    # neg\n",
    "    #return mask_negatives\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_neg = tf.concat([embeddings, centroid_embed_neg], axis=0)\n",
    "    # create the matrix of neg ids\n",
    "    repeat = tf.fill([1, batch_size], 1)[0]\n",
    "    neg_ids = tf.repeat(tf.reshape(labels, (1, -1)), repeats=[batch_size], axis=0)\n",
    "    mask_negatives_bool = tf.cast(mask_negatives, dtype=dtypes.bool)\n",
    "    neg_ids = tf.cast(neg_ids, dtype=dtypes.float32)\n",
    "    # TODO: get the most frequent neg id\n",
    "#     return tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "#     unique, _, count = tf.unique_with_counts(tf.where(mask_negatives_bool, neg_ids, mask_negatives))\n",
    "#     max_occurrences = tf.reduce_max(count)\n",
    "#     max_cond = tf.equal(count, max_occurrences)\n",
    "    #max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n",
    "    #return max_cond\n",
    "    neg_ids = tf.reduce_max(tf.where(mask_negatives_bool, neg_ids, mask_negatives), axis=1, keepdims=True)\n",
    "    #return neg_ids\n",
    "    labels_neg = tf.concat([tf.cast(labels, dtype=dtypes.float32), neg_ids], axis=0)\n",
    "    #return labels_neg\n",
    "    vects_neg = tf.concat([tf.reshape(labels_neg, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "#     return vects_neg\n",
    "    \n",
    "    #return triplet_loss(vects), triplet_loss(vects_pos), triplet_loss(vects_neg)\n",
    "    #return triplet_loss(vects)\n",
    "    return tf.reduce_mean([triplet_loss(vects), triplet_loss(vects_pos) * 0.5, triplet_loss(vects_neg) * 0.5])\n",
    "    \n",
    "\n",
    "v = tf.constant([[1.0, 1.0, 2.0], [2.0, 3.0, 4.0], [1.0, 1.0, 2.0], \n",
    "                 [1.0, 1.0, 2.0], [3.0, 3.0, 4.0], [3.0, 1.0, 2.0]]) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.52771944, 1.0, 0.25, 0.018982518, 0.52272725, 0.8764739, 0.62, 0.10454356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.48923445, 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.25      , 0.52272725, 0.15789473, 1.0263158 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import numpy as np\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # Include the anchor to positives\n",
    "#     mask_positives = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "    \n",
    "#     return mask_positives\n",
    "    \n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    \n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_pos = tf.concat([embeddings, centroid_embed_pos], axis=0)\n",
    "    \n",
    "    # add label for centroids\n",
    "    labels_pos = tf.concat([labels, labels], axis=0)\n",
    "    labels_pos = tf.cast(labels_pos, dtype=dtypes.float32)\n",
    "    vects_pos = tf.concat([tf.reshape(labels_pos, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "#     return mask_positives\n",
    "\n",
    "    # neg\n",
    "    # create the matrix of neg ids\n",
    "    repeat = tf.fill([1, batch_size], 1)[0]\n",
    "    neg_ids = tf.repeat(tf.reshape(labels, (1, -1)), repeats=[batch_size], axis=0)\n",
    "    mask_negatives_bool = tf.cast(mask_negatives, dtype=dtypes.bool)\n",
    "    neg_ids = tf.cast(neg_ids, dtype=dtypes.float32)\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    neg_matrix=tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "\n",
    "    neg_random_matrix = tf.Variable([])\n",
    "    def random_negative(i, neg_random_matrix):\n",
    "        batch = tf.gather(neg_matrix, i)\n",
    "        batch = tf.boolean_mask(batch, tf.greater(batch, 0))\n",
    "        prob = tf.random_uniform_initializer(minval=0., maxval=1.)(shape=[1])[0]\n",
    "        value = tf.cast(prob * tf.cast(tf.size(batch), dtype=dtypes.float32), dtype=dtypes.int32)\n",
    "        value = batch[value]\n",
    "        neg_random_matrix = tf.concat([neg_random_matrix, [value]], axis=0)\n",
    "        return [tf.add(i, 1), neg_random_matrix]\n",
    "    _, neg_random_matrix = tf.while_loop(lambda i, _: i<batch_size, \n",
    "                                        random_negative, \n",
    "                                        [i, neg_random_matrix],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "    \n",
    "    new_neg_ids = tf.reshape(neg_random_matrix, (batch_size, 1))\n",
    "    mask_negatives = tf.cast(tf.equal(neg_ids, new_neg_ids), dtype=dtypes.float32)\n",
    "#     return mask_negatives\n",
    "    \n",
    "    neg_ids = new_neg_ids\n",
    "    \n",
    "    # Embedding negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_neg = tf.concat([embeddings, centroid_embed_neg], axis=0)\n",
    "#     return neg_ids\n",
    "#     return tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "    # Get the biggest id\n",
    "#     neg_ids = tf.reduce_max(tf.where(mask_negatives_bool, neg_ids, mask_negatives), axis=1, keepdims=True)\n",
    "#     return neg_ids\n",
    "    labels_neg = neg_ids\n",
    "    labels_neg = tf.concat([tf.cast(labels, dtype=dtypes.float32), neg_ids], axis=0)\n",
    "#     return labels_neg\n",
    "#     return embeddings_anchor_centroid_neg\n",
    "    vects_neg = tf.concat([labels_neg, embeddings_anchor_centroid_neg], axis=1)\n",
    "#     return vects_neg\n",
    "    \n",
    "    # Centroids embed\n",
    "#     return triplet_loss(vects)\n",
    "    embeddings = tf.concat([centroid_embed_pos, centroid_embed_neg], axis=0)\n",
    "    labels = tf.cast(labels, dtype=dtypes.float32)\n",
    "    labels = tf.concat([labels, neg_ids], axis=0)\n",
    "    vects_centroids =  tf.concat([tf.reshape(labels, (-1, 1)), embeddings], axis=1)\n",
    "    #return triplet_loss(vects), triplet_loss(vects_pos), triplet_loss(vects_neg)\n",
    "    #return triplet_loss(vects)\n",
    "    \n",
    "#     return TL_w\n",
    "    TL_anchor_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = 1;0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "#     TL_centroid_w = TL_w[0][3]\n",
    "    #sum_of_w = tf.reduce_sum(TL_w)\n",
    "    #TL_w = tf.truediv(TL_w, sum_of_w)\n",
    "    # Normalization in probabilities\n",
    "    #TL_anchor_w = tf.truediv(TL_anchor_w, sum_of_w)\n",
    "    #TL_pos_w = tf.truediv(TL_pos_w, sum_of_w)\n",
    "    #TL_neg_w = tf.truediv(TL_neg_w, sum_of_w)\n",
    "#     return TL_pos_w, TL_neg_w\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = triplet_loss(vects_pos)\n",
    "    TL_neg = triplet_loss(vects_neg)\n",
    "    TL_centroid = triplet_loss(vects_centroids)\n",
    "#     TL_centroid = 0.0\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w])  \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w \n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])\n",
    "\n",
    "v = tf.constant([[1.0, 1.0, 2.0], [2.0, 3.0, 4.0], [1.0, 1.0, 2.0], \n",
    "                 [1.0, 1.0, 2.0], [3.0, 3.0, 4.0], [3.0, 1.0, 2.0]]) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.30926000e+05, 1.60398836e-02, 4.98874287e-01, ...,\n",
       "        3.14803366e-01, 5.87878146e-01, 9.02188455e-01],\n",
       "       [1.33860000e+05, 3.07970113e-01, 3.66614283e-01, ...,\n",
       "        4.84030796e-01, 5.01390221e-02, 1.66062161e-01],\n",
       "       [2.84787000e+05, 9.21228908e-01, 7.22928016e-02, ...,\n",
       "        8.65632646e-01, 3.49485702e-01, 1.81379350e-01],\n",
       "       ...,\n",
       "       [3.29419000e+05, 7.52911527e-01, 9.59667405e-01, ...,\n",
       "        7.65883767e-01, 7.75899734e-01, 2.04349029e-01],\n",
       "       [1.36890000e+05, 7.63322091e-01, 2.48772197e-01, ...,\n",
       "        9.26766755e-01, 8.16594491e-01, 1.44814338e-02],\n",
       "       [1.80840000e+04, 7.26801138e-01, 1.77437418e-01, ...,\n",
       "        3.83668102e-01, 1.70476073e-01, 8.59313968e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [130926,133860,284787,102727,275267,352297,159354,295390,381207,321088\n",
    ",351176,411703,250243,46067,190119,125376,424120,298717,291029,152442\n",
    ",301724,36328,183961,416809,3407,144150,329419,374624,269326,410734\n",
    ",374624,65934,220763,14679,13556,301593,184909,295390,76922,40227\n",
    ",169079,31064,2859,200295,374624,153304,374624,159354,332879,257318\n",
    ",424120,30610,350370,236970,168880,3407,47818,128387,188385,111626\n",
    ",374624,58431,13556,344913,194533,198282,117300,104540,301724,198282\n",
    ",240969,240969,187251,102727,231616,2644,95462,240969,168880,3407\n",
    ",295390,86523,286212,301724,353652,410919,89993,295390,102709,59503\n",
    ",130926,89899,144029,215227,73860,374624,308040,351509,139379,159354\n",
    ",364270,220763,145285,275267,329419,167087,371122,86523,341677,130926\n",
    ",139379,374624,45216,198282,243689,71124,30567,374624,194533,155740\n",
    ",109864,36328,301724,63803,112696,70655,223497,189093,139379,376285\n",
    ",109864,267661,374624,194533,325238,194533,125851,125851,345992,275267\n",
    ",313536,134427,260398,130926,295390,295390,128522,329419,128387,3407\n",
    ",47818,301724,388838,11160,304048,176043,413602,41105,174272,380477\n",
    ",190119,66629,240969,315735,16190,34991,396897,235970,295390,301724\n",
    ",159354,45013,60696,301724,49492,156529,280748,41105,295390,63803\n",
    ",104110,72820,349524,395469,302654,240969,87472,301724,295390,422419\n",
    ",236970,413602,81152,3407,294185,198282,269326,125376,235970,319605\n",
    ",152246,349524,281898,156529,24529,374624,280748,410734,301724,237570\n",
    ",122711,342186,394820,142540,80754,111480,93274,269326,368894,60696\n",
    ",236970,194533,342186,31064,45376,329419,89993,376285,166836,269326\n",
    ",348037,236970,301724,54423,3407,329419,116849,301724,220698,364510\n",
    ",255216,371122,145685,60976,301724,23617,304048,298098,337080,275267\n",
    ",48054,167758,172761,80754,424120,301927,374624,116849,245911,54423\n",
    ",73860,295390,3407,71711,2859,234829,169079,86523,424120,295390\n",
    ",81152,152442,134452,130926,383237,70655,315735,396897,367093,34991\n",
    ",99231,104540,351487,167007,284961,240969,244632,63739,152442,210004\n",
    ",356883,66629,37874,329419,380477,301724,172761,325238,291029,202337\n",
    ",321088,101168,260398,375861,199307,104110,376609,99231,325238,46067\n",
    ",128442,3407,81134,177017,80754,339137,45216,202337,128398,194533\n",
    ",340685,243689,144029,152442,112955,261495,333917,301724,67049,410928\n",
    ",175658,267661,198282,358948,333917,374624,3407,45376,76922,135763\n",
    ",130926,114197,58431,200295,348037,295390,223497,301724,325238,165031\n",
    ",194533,139379,206446,398356,301724,80754,198282,87472,329419,288151\n",
    ",261305,134427,194533,300212,49492,169032,86523,366897,71124,292091\n",
    ",160241,144150,126587,417858,167087,160241,93274,301294,187779,301724\n",
    ",301724,329419,136890,18084]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "l = np.concatenate([np.asarray(np.reshape(l, (-1, 1)), np.int32), np.random.rand(len(l), 901) ], 1)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import _pickle as pickle\n",
    "\n",
    "DIR = 'data/processed/{}/{}'.format('openoffice', 'bert')\n",
    "\n",
    "with open(os.path.join(DIR, 'train_batch'), 'rb') as f:\n",
    "    l = pickle.load(f)\n",
    "    \n",
    "l = l[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 91607],\n",
       "       [107375],\n",
       "       [ 39602],\n",
       "       [ 53755],\n",
       "       [ 56186],\n",
       "       [ 67087],\n",
       "       [ 56186],\n",
       "       [ 83937],\n",
       "       [ 93798],\n",
       "       [ 56186],\n",
       "       [ 40863],\n",
       "       [106079],\n",
       "       [ 56186],\n",
       "       [ 82164],\n",
       "       [ 73887],\n",
       "       [ 40738],\n",
       "       [ 96812],\n",
       "       [ 56186],\n",
       "       [ 98409],\n",
       "       [ 15050],\n",
       "       [ 40046],\n",
       "       [ 51115],\n",
       "       [ 86046],\n",
       "       [  9597],\n",
       "       [ 42886],\n",
       "       [ 90862],\n",
       "       [ 56186],\n",
       "       [ 76604],\n",
       "       [  9843],\n",
       "       [ 56186],\n",
       "       [ 83937],\n",
       "       [ 76604],\n",
       "       [ 56186],\n",
       "       [ 85165],\n",
       "       [ 72865],\n",
       "       [ 92807],\n",
       "       [ 79871],\n",
       "       [ 76604],\n",
       "       [ 94406],\n",
       "       [ 56186],\n",
       "       [ 90509],\n",
       "       [ 41884],\n",
       "       [ 70261],\n",
       "       [ 72865],\n",
       "       [ 56186],\n",
       "       [ 37264],\n",
       "       [ 66937],\n",
       "       [ 96812],\n",
       "       [ 69006],\n",
       "       [ 56186],\n",
       "       [ 97157],\n",
       "       [ 56186],\n",
       "       [ 45972],\n",
       "       [ 93322],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 45743],\n",
       "       [101956],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 37264],\n",
       "       [ 56186],\n",
       "       [ 72865],\n",
       "       [ 56186],\n",
       "       [103798],\n",
       "       [ 21347],\n",
       "       [ 68480],\n",
       "       [ 96812],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 43222],\n",
       "       [ 10234],\n",
       "       [106817],\n",
       "       [ 83320],\n",
       "       [ 35909],\n",
       "       [ 56984],\n",
       "       [ 86046],\n",
       "       [ 50489],\n",
       "       [115619],\n",
       "       [ 91607],\n",
       "       [ 56186],\n",
       "       [ 62950],\n",
       "       [ 75107],\n",
       "       [ 56186],\n",
       "       [ 62710],\n",
       "       [ 28065],\n",
       "       [  4794],\n",
       "       [ 45059],\n",
       "       [ 56186],\n",
       "       [ 92807],\n",
       "       [111135],\n",
       "       [ 56186],\n",
       "       [ 33492],\n",
       "       [ 72865],\n",
       "       [ 96812],\n",
       "       [ 13828],\n",
       "       [ 56186],\n",
       "       [ 37264],\n",
       "       [ 56186],\n",
       "       [ 79121],\n",
       "       [ 56186],\n",
       "       [ 39548],\n",
       "       [ 16331],\n",
       "       [105434],\n",
       "       [ 15817],\n",
       "       [ 41876],\n",
       "       [ 23912],\n",
       "       [ 56186],\n",
       "       [ 46728],\n",
       "       [ 62710],\n",
       "       [ 13519],\n",
       "       [119939],\n",
       "       [ 98409],\n",
       "       [ 18023],\n",
       "       [ 56188],\n",
       "       [ 56186],\n",
       "       [ 25279],\n",
       "       [ 73018],\n",
       "       [ 56186],\n",
       "       [ 92807],\n",
       "       [ 31189],\n",
       "       [ 91607],\n",
       "       [ 56186],\n",
       "       [ 50489],\n",
       "       [ 77995],\n",
       "       [ 85128],\n",
       "       [ 56186],\n",
       "       [ 23996],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 91607],\n",
       "       [101956],\n",
       "       [ 91607],\n",
       "       [ 76604],\n",
       "       [108088],\n",
       "       [101956],\n",
       "       [ 98247],\n",
       "       [100394],\n",
       "       [ 23283],\n",
       "       [ 91607],\n",
       "       [   819],\n",
       "       [ 56186],\n",
       "       [ 39548],\n",
       "       [ 36387],\n",
       "       [  4375],\n",
       "       [ 42012],\n",
       "       [  6949],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 21205],\n",
       "       [ 67953],\n",
       "       [ 83451],\n",
       "       [ 56186],\n",
       "       [ 83320],\n",
       "       [106817],\n",
       "       [ 67087],\n",
       "       [104134],\n",
       "       [ 96812],\n",
       "       [ 79871],\n",
       "       [ 56186],\n",
       "       [ 18023],\n",
       "       [110147],\n",
       "       [ 76604],\n",
       "       [ 79121],\n",
       "       [ 60165],\n",
       "       [  3838],\n",
       "       [ 62706],\n",
       "       [ 25279],\n",
       "       [ 41462],\n",
       "       [ 56186],\n",
       "       [ 18967],\n",
       "       [106395],\n",
       "       [ 35909],\n",
       "       [ 48842],\n",
       "       [ 47033],\n",
       "       [ 23154],\n",
       "       [106395],\n",
       "       [119592],\n",
       "       [ 94406],\n",
       "       [ 42695],\n",
       "       [ 76279],\n",
       "       [ 56186],\n",
       "       [ 91607],\n",
       "       [ 56186],\n",
       "       [ 39548],\n",
       "       [ 48469],\n",
       "       [ 61937],\n",
       "       [ 56186],\n",
       "       [ 25042],\n",
       "       [ 77034],\n",
       "       [ 16331],\n",
       "       [ 56186],\n",
       "       [104426],\n",
       "       [ 56186],\n",
       "       [ 76604],\n",
       "       [ 11783],\n",
       "       [ 86751],\n",
       "       [ 56186],\n",
       "       [ 91607],\n",
       "       [ 23912],\n",
       "       [ 92807],\n",
       "       [111135],\n",
       "       [ 56186],\n",
       "       [ 39187],\n",
       "       [ 16331],\n",
       "       [ 79871],\n",
       "       [ 67087],\n",
       "       [ 68480],\n",
       "       [ 89892],\n",
       "       [ 13828],\n",
       "       [104426],\n",
       "       [ 41876],\n",
       "       [ 56186],\n",
       "       [ 91607],\n",
       "       [ 91607],\n",
       "       [108088],\n",
       "       [ 92807],\n",
       "       [ 67932],\n",
       "       [ 41884],\n",
       "       [ 16331],\n",
       "       [ 72865],\n",
       "       [ 26997],\n",
       "       [ 56186],\n",
       "       [ 49952],\n",
       "       [ 83320],\n",
       "       [ 16395],\n",
       "       [ 56186],\n",
       "       [ 22036],\n",
       "       [ 89592],\n",
       "       [   882],\n",
       "       [ 16331],\n",
       "       [ 70983],\n",
       "       [ 67551],\n",
       "       [ 63768],\n",
       "       [ 76604],\n",
       "       [ 56186],\n",
       "       [ 15983],\n",
       "       [ 41492],\n",
       "       [ 56186],\n",
       "       [107749],\n",
       "       [ 16331],\n",
       "       [ 56186],\n",
       "       [ 16331],\n",
       "       [ 92908],\n",
       "       [ 79871],\n",
       "       [ 16331],\n",
       "       [123181],\n",
       "       [ 91607],\n",
       "       [ 37264],\n",
       "       [ 22371],\n",
       "       [ 56186],\n",
       "       [ 18454],\n",
       "       [ 56186],\n",
       "       [ 29366],\n",
       "       [117160],\n",
       "       [ 16395],\n",
       "       [ 85128],\n",
       "       [ 56186],\n",
       "       [ 89592],\n",
       "       [122296],\n",
       "       [  9296],\n",
       "       [ 56186],\n",
       "       [ 22723],\n",
       "       [ 72865],\n",
       "       [ 96812],\n",
       "       [ 56835],\n",
       "       [ 39478],\n",
       "       [ 90912],\n",
       "       [100440],\n",
       "       [ 42012],\n",
       "       [ 75829],\n",
       "       [118419],\n",
       "       [ 33492],\n",
       "       [ 13993],\n",
       "       [ 29588],\n",
       "       [ 26440],\n",
       "       [ 37264],\n",
       "       [ 56186],\n",
       "       [ 59068],\n",
       "       [ 56186],\n",
       "       [ 50103],\n",
       "       [ 83320],\n",
       "       [ 56186],\n",
       "       [ 29033],\n",
       "       [ 64062],\n",
       "       [ 56186],\n",
       "       [  4353],\n",
       "       [ 56186],\n",
       "       [ 76002],\n",
       "       [107375],\n",
       "       [ 56186],\n",
       "       [ 91607],\n",
       "       [ 56186],\n",
       "       [ 96846],\n",
       "       [101956],\n",
       "       [ 56186],\n",
       "       [117105],\n",
       "       [ 43039],\n",
       "       [121184],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 22723],\n",
       "       [ 42886],\n",
       "       [ 76604],\n",
       "       [ 13211],\n",
       "       [ 30376],\n",
       "       [ 37625],\n",
       "       [ 51115],\n",
       "       [ 56186],\n",
       "       [ 46929],\n",
       "       [ 91607],\n",
       "       [101956],\n",
       "       [117160],\n",
       "       [ 76834],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 45743],\n",
       "       [ 56186],\n",
       "       [ 70004],\n",
       "       [ 91607],\n",
       "       [106610],\n",
       "       [ 56186],\n",
       "       [ 76604],\n",
       "       [ 56186],\n",
       "       [ 23154],\n",
       "       [ 43039],\n",
       "       [  9138],\n",
       "       [122296],\n",
       "       [ 48880],\n",
       "       [ 70513],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 39548],\n",
       "       [ 67551],\n",
       "       [ 37684],\n",
       "       [ 24809],\n",
       "       [ 92807],\n",
       "       [ 56186],\n",
       "       [ 92807],\n",
       "       [ 37264],\n",
       "       [107428],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [ 52151],\n",
       "       [ 13519],\n",
       "       [ 75829],\n",
       "       [ 89892],\n",
       "       [ 45588],\n",
       "       [ 56186],\n",
       "       [ 87316],\n",
       "       [ 13077],\n",
       "       [ 93304],\n",
       "       [ 91607],\n",
       "       [ 56186],\n",
       "       [ 16331],\n",
       "       [ 11990],\n",
       "       [ 45495],\n",
       "       [ 83320],\n",
       "       [ 15817],\n",
       "       [ 58504],\n",
       "       [ 26997],\n",
       "       [ 16331],\n",
       "       [  9416],\n",
       "       [ 58250],\n",
       "       [ 83320],\n",
       "       [ 38725],\n",
       "       [ 93798],\n",
       "       [ 92807],\n",
       "       [ 56186],\n",
       "       [ 56186],\n",
       "       [101381],\n",
       "       [101956],\n",
       "       [ 24206],\n",
       "       [ 91607],\n",
       "       [ 76155],\n",
       "       [ 76604],\n",
       "       [ 56186]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.reshape(l, (-1, 1)), np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.concatenate([np.asarray(np.reshape(l, (-1, 1)), np.int32), np.random.rand(len(l), 900) ], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 901)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"while_3/concat_1:0\", shape=(?,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.667259  ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        0.9116882 ,  0.7158266 , 16.451548  ,  0.58997315], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import numpy as np\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    "\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings =  tf.cast(inputs[:, 1:], dtype='float32')\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    mask_negatives = adjacency_not\n",
    "\n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "#     return mask_positives\n",
    "\n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives_centroids, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "    # negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "\n",
    "#     return mask_positives_centroids\n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    batch_centroid_matrix_neg = tf.Variable([])\n",
    "    batch_centroid_matrix_all = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all):\n",
    "        # centroid pos\n",
    "        mask_positives_batch = tf.reshape(tf.gather(mask_positives, i), (-1, 1))\n",
    "        centroid_pos = tf.gather(centroid_embed_pos, i)\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_pos], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_pos = mask_positives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_positives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_pos, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid neg\n",
    "        centroid_neg = tf.gather(centroid_embed_neg, i)\n",
    "        mask_negatives_batch = tf.reshape(tf.gather(mask_negatives, i), (-1, 1))\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_neg], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_neg = mask_negatives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_negatives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_neg, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix_neg = tf.concat([batch_centroid_matrix_neg, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid pos and neg\n",
    "        new_batch_centroids = tf.reduce_sum([new_batch_centroid_pos, new_batch_centroid_neg], axis=0, keepdims=True)[0]\n",
    "        vects_new_batch_centroids = tf.concat([labels_pos, new_batch_centroids], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch_centroids)\n",
    "        batch_centroid_matrix_all = tf.concat([batch_centroid_matrix_all, [TL_new_batch]], axis=0)\n",
    "        \n",
    "        print(batch_centroid_matrix)\n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all]\n",
    "    _, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all = tf.while_loop(lambda i, a, b, c: i<batch_size, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])])\n",
    "\n",
    "    TL_anchor_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    \n",
    "#     tl_weights = tf.truediv(num_of_pos, tf.reduce_max(num_of_pos))\n",
    "#     tl_w = tl_weights # tf.random_uniform_initializer(minval=0.0, maxval=tl_weights)(shape=[1])\n",
    "#     TL_pos_weighted = tf.reshape(batch_centroid_matrix, (-1, 1)) * tl_w\n",
    "#     TL_pos = tf.truediv(tf.reduce_sum(TL_pos_weighted), tf.reduce_sum(tl_w))\n",
    "\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL_neg = tf.reduce_mean(batch_centroid _matrix_neg) #triplet_loss(vects_neg)\n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix_all) # triplet_loss(vects_centroids)\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w]) # \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, \n",
    "                    TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "v = tf.constant(train_batch) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
