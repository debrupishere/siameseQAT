{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modelos\\\\model_model_baseline_100epoch_16steps_(eclipse).json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-014f1a0d33a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model_baseline_100epoch_16steps_(eclipse)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msimilarity_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\PythonProjects\\bug_report_duplicated\\methods\\baseline.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(DIR, name)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mm_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'modelos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# load json and create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_{}.json\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modelos\\\\model_model_baseline_100epoch_16steps_(eclipse).json'"
     ]
    }
   ],
   "source": [
    "name = 'model_baseline_100epoch_16steps_(eclipse)'\n",
    "similarity_model = Baseline.load_model('', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in similarity_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze weights\n",
    "for layer in similarity_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "bug_t_a = Input(shape = (max_sequence_length_t, ), name = 'title_a')\n",
    "bug_t_b = Input(shape = (max_sequence_length_t, ), name = 'title_b')\n",
    "\n",
    "bug_d_a = Input(shape = (max_sequence_length_d, ), name = 'desc_a')\n",
    "bug_d_b = Input(shape = (max_sequence_length_d, ), name = 'desc_b')\n",
    "\n",
    "title_encoder = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "desc_encoder = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "\n",
    "# model = similarity_model.get_layer('merge_features_in')\n",
    "\n",
    "bugt_t_a = title_encoder(bug_t_a)\n",
    "bugt_d_a = desc_encoder(bug_d_a)\n",
    "\n",
    "bugt_t_b = title_encoder(bug_t_b)\n",
    "bugt_d_b = desc_encoder(bug_d_b)\n",
    "\n",
    "bug_a = concatenate([bugt_t_a, bugt_d_a], name = 'bug_a')\n",
    "bug_b = concatenate([bugt_t_b, bugt_d_b], name = 'bug_b')\n",
    "\n",
    "x = concatenate([bug_a, bug_b], name='bugs')\n",
    "#x = Dense(64, activation = 'relu')(x)\n",
    "#x = Dense(32, activation = 'relu')(x)\n",
    "output = Dense(2, activation = 'softmax', name = 'output')(x)\n",
    "\n",
    "model_clf = Model(inputs=[bug_t_a, bug_t_b, bug_d_a, bug_d_b], outputs=[output])\n",
    "model_clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def batch_classification(data, batch_size, n_neg):\n",
    "    encoder = LabelEncoder()\n",
    "    while True:\n",
    "        input_sample, input_pos, input_neg, sim = baseline.batch_iterator(data, batch_size, n_neg)\n",
    "        sim = encoder.fit_transform(sim)\n",
    "        sim = to_categorical(sim)\n",
    "        \n",
    "        title_a = np.concatenate([input_sample['title'], input_sample['title']])\n",
    "        title_b = np.concatenate([input_pos['title'], input_neg['title']])\n",
    "        desc_a = np.concatenate([input_sample['desc'], input_sample['desc']])\n",
    "        desc_b = np.concatenate([input_pos['desc'], input_neg['desc']])\n",
    "        \n",
    "        yield ({ 'title_a' : title_a, 'title_b': title_b, \n",
    "        'desc_a' : desc_a, 'desc_b' : desc_b }, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation = batch_classification(bug_dir, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "\n",
    "# valid_labels = encoder.fit_transform(test_gen[1])\n",
    "# valid_labels = to_categorical(valid_labels)\n",
    "# test_validation = (test_gen[0], valid_labels)\n",
    "\n",
    "h_clf = model_clf.fit_generator(batch_classification(bug_dir, 512, 1), \n",
    "                               steps_per_epoch = 10,\n",
    "                               validation_data=test_validation, # \n",
    "                                             epochs = 100,\n",
    "                                             verbose = True) # callbacks=[early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline.validation_accuracy_loss(h_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'baseline_classification_100epoch_16steps(eclipse)'\n",
    "save_model(similarity_model, name)\n",
    "save_result(h, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
