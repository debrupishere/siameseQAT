{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Qe7YuuS5zE"
   },
   "source": [
    "# Baseline \n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UaUYO0dxFqKC"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tYyondqOGCVg",
    "outputId": "7ee821a7-3ea7-4653-ab20-5ac9f50e564d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zNX32ANgGD8u",
    "outputId": "bdcea734-379c-48aa-c428-70b27b39f9a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMNWjVRqVMg5"
   },
   "outputs": [],
   "source": [
    "DIR = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aHXwIOOGGoj"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UYXKu6xmGIad",
    "outputId": "b324d19d-2623-4c1f-fbc4-6f0f24b7b0de"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaeADDEkOvAq"
   },
   "source": [
    "### Bugs train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfJWqEb3GK1e"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('mozilla_firefox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "id": "hhQiObQ2GNMo",
    "outputId": "ac936da9-1853-49a6-8b5d-31dda5d8c670"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_id</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Component</th>\n",
       "      <th>Duplicated_issue</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Status</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Version</th>\n",
       "      <th>Created_time</th>\n",
       "      <th>Resolved_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10954</td>\n",
       "      <td>P3</td>\n",
       "      <td>Preferences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dialup properties needs to be exposed in prefs</td>\n",
       "      <td>The dialup properties of the profile should be...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>WONTFIX</td>\n",
       "      <td>Trunk</td>\n",
       "      <td>1999-07-30 15:55:51 -0700</td>\n",
       "      <td>2008-05-14 11:44:15 -0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14871</td>\n",
       "      <td>--</td>\n",
       "      <td>General</td>\n",
       "      <td>269442.0</td>\n",
       "      <td>[Find] Find whole word only</td>\n",
       "      <td>Please add Match Whole Word Only option to bro...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>DUPLICATE</td>\n",
       "      <td>Trunk</td>\n",
       "      <td>1999-09-24 14:49:34 -0700</td>\n",
       "      <td>2011-10-05 16:35:31 -0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19118</td>\n",
       "      <td>--</td>\n",
       "      <td>Preferences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plug-In Manager (ui for choosing mimetype-plug...</td>\n",
       "      <td>I would really like a plug-in manager for my b...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>WONTFIX</td>\n",
       "      <td>Trunk</td>\n",
       "      <td>1999-11-17 14:58:26 -0800</td>\n",
       "      <td>2013-01-29 11:48:39 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Issue_id Priority    Component  Duplicated_issue  \\\n",
       "0     10954       P3  Preferences               NaN   \n",
       "1     14871       --      General          269442.0   \n",
       "2     19118       --  Preferences               NaN   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Dialup properties needs to be exposed in prefs   \n",
       "1                        [Find] Find whole word only   \n",
       "2  Plug-In Manager (ui for choosing mimetype-plug...   \n",
       "\n",
       "                                         Description    Status Resolution  \\\n",
       "0  The dialup properties of the profile should be...  RESOLVED    WONTFIX   \n",
       "1  Please add Match Whole Word Only option to bro...  RESOLVED  DUPLICATE   \n",
       "2  I would really like a plug-in manager for my b...  RESOLVED    WONTFIX   \n",
       "\n",
       "  Version               Created_time              Resolved_time  \n",
       "0   Trunk  1999-07-30 15:55:51 -0700  2008-05-14 11:44:15 -0700  \n",
       "1   Trunk  1999-09-24 14:49:34 -0700  2011-10-05 16:35:31 -0700  \n",
       "2   Trunk  1999-11-17 14:58:26 -0800  2013-01-29 11:48:39 -0800  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2o3vTI0YJpQ"
   },
   "outputs": [],
   "source": [
    "df_train.columns = ['issue_id', 'priority', 'component', 'duplicated_issue', 'title', 'description', 'status', 'resolution', 'version', 'creation_ts', 'delta_ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ea1CcFJzPfRE"
   },
   "source": [
    "### Pairs train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cEp5PCjHN8W"
   },
   "outputs": [],
   "source": [
    "df_train_pair = pd.read_csv('train_mozilla_firefox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mi-kgU-WHKx2",
    "outputId": "7e8a83c0-be72-44c1-c78d-125c058c80d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92651, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JlveE_jCHnww",
    "outputId": "8b34522f-2eb0-4191-dc60-f3e4ffbc5602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62625, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pair[df_train_pair['Duplicate'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "GLarKJhDHSUh",
    "outputId": "b3b1d57a-4673-4945-dcd1-71e41e29d663"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_id</th>\n",
       "      <th>Duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14871</td>\n",
       "      <td>243500;410103;505684;515027;528678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19118</td>\n",
       "      <td>326494;328227;414070;436576;443686;457861;475975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56892</td>\n",
       "      <td>191258;281233;290692;300719;307581;310641;3117...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Issue_id                                          Duplicate\n",
       "0     10954                                                NaN\n",
       "1     14871                 243500;410103;505684;515027;528678\n",
       "2     19118   326494;328227;414070;436576;443686;457861;475975\n",
       "3     54746                                                NaN\n",
       "4     56892  191258;281233;290692;300719;307581;310641;3117..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pair.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ6IT8dJQ2ND"
   },
   "source": [
    "### Reading pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIU4Pa0GQ5x7"
   },
   "outputs": [],
   "source": [
    "def read_pairs(df):\n",
    "  bug_pairs = []\n",
    "  bug_ids = set()\n",
    "  for row in df.iterrows():\n",
    "    duplicates = row[1]['Duplicate']\n",
    "    bug1 = row[1]['Issue_id']\n",
    "    duplicates = [] if (type(duplicates) == float) else np.array(duplicates.split(';'), dtype=np.float)\n",
    "    if len(duplicates) == 0: # No duplicate\n",
    "      bug_ids.add(int(bug1))\n",
    "    else: # duplicate\n",
    "      bug_ids.add(int(bug1))\n",
    "      for bug2 in duplicates:\n",
    "        bug_pairs.append((int(bug1), int(bug2)))\n",
    "        bug_ids.add(int(bug2))\n",
    "  with open(os.path.join(DIR, 'bug_pairs.txt'), 'w') as f:\n",
    "    for pair in bug_pairs:\n",
    "      f.write(\"%d %d\\n\" % pair)\n",
    "  bug_ids = sorted(bug_ids)\n",
    "  with open(os.path.join(DIR, 'bug_ids.txt'), 'w') as f:\n",
    "    for bug_id in bug_ids:\n",
    "      f.write(\"%d\\n\" % bug_id)\n",
    "  return bug_pairs, bug_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "szmmQEzXStkI",
    "outputId": "485fc02a-f3eb-4ac4-b27d-fb61647db2b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bugs: 92651\n",
      "Number of pairs: 310378\n"
     ]
    }
   ],
   "source": [
    "bug_pairs, bug_ids = read_pairs(df_train_pair)\n",
    "print(\"Number of bugs: {}\".format(len(bug_ids)))\n",
    "print(\"Number of pairs: {}\".format(len(bug_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yas-8qAjISSC"
   },
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STRLr80XIXBS"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nryISB_U7PS"
   },
   "outputs": [],
   "source": [
    "def split_train_test(bug_pairs):\n",
    "  random.shuffle(bug_pairs)\n",
    "  split_idx = int(len(bug_pairs) * VALIDATION_SPLIT)\n",
    "  with open(os.path.join(DIR, 'train.txt'), 'w') as f:\n",
    "    for pair in bug_pairs[:split_idx]:\n",
    "      f.write(\"%d %d\\n\" % pair)\n",
    "  test_data = {}\n",
    "  for pair in bug_pairs[split_idx:]:\n",
    "    bug1 = int(pair[0])\n",
    "    bug2 = int(pair[1])\n",
    "    if bug1 not in test_data:\n",
    "      test_data[bug1] = set()\n",
    "    test_data[bug1].add(bug2)\n",
    "  with open(os.path.join(DIR, 'test.txt'), 'w') as f:\n",
    "    for bug in test_data.keys():\n",
    "      f.write(\"{} {}\\n\".format(bug, ' '.join([str(x) for x in test_data[bug]])))\n",
    "  print('Train and test created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gc5mEmO3VgUd",
    "outputId": "cf9ad738-f36c-4aeb-e631-d8093a18389f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test created\n"
     ]
    }
   ],
   "source": [
    "split_train_test(bug_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgut68gPW6Zk"
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IR7RL19Zhk2"
   },
   "source": [
    "#### Normalize text (lower, remove simbols and other caracter and tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTUjzcYlZg6K"
   },
   "outputs": [],
   "source": [
    "def func_name_tokenize(text):\n",
    "  s = []\n",
    "  for i, c in enumerate(text):\n",
    "    if c.isupper() and i > 0 and text[i-1].islower():\n",
    "      s.append(' ')\n",
    "    s.append(c)\n",
    "  return ''.join(s).strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "  try:\n",
    "    tokens = re.compile(r'[\\W_]+', re.UNICODE).split(text)\n",
    "    text = ' '.join([func_name_tokenize(token) for token in tokens])\n",
    "    text = re.sub(r'\\d+((\\s\\d+)+)?', 'number', text)\n",
    "  except:\n",
    "    return 'description'\n",
    "  return ' '.join([word.lower() for word in nltk.word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZQkDPntW9Ow"
   },
   "outputs": [],
   "source": [
    "def normalized_data(bug_ids, df):\n",
    "  products = set()\n",
    "  bug_severities = set()\n",
    "  priorities = set()\n",
    "  versions = set()\n",
    "  components = set()\n",
    "  bug_statuses = set()\n",
    "  text = []\n",
    "  normalized_bugs = open(os.path.join(DIR, 'normalized_bugs.json'), 'w')\n",
    "  for row in df.iterrows():\n",
    "      \n",
    "      bug = row[1]\n",
    "      \n",
    "      bug['description'] = normalize_text(bug['description'])\n",
    "      if 'title' in bug:\n",
    "        bug['title'] = normalize_text(bug['title'])\n",
    "      else:\n",
    "        bug['title'] = ''\n",
    "      \n",
    "      normalized_bugs.write('{}\\n'.format(bug.to_json()))\n",
    "\n",
    "      text.append(bug['description'])\n",
    "      text.append(bug['title'])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ygWRrFgpZXSL",
    "outputId": "7685d9e2-2d61-45e1-f82f-3d0071fd017d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "text = normalized_data(bug_ids, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKQaPzM_dXQL"
   },
   "source": [
    "#### Visualizing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ECguTuP0axqs",
    "outputId": "1a95caf0-b8d9-43a6-b0ec-e8511d4ea9b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web pages that open new windows have browsers with default toolbars',\n",
       " 'user agent mozilla number windows u windows nt number en us rv numbera gecko number phoenix number build identifier mozilla number windows u windows nt number en us rv numbera gecko number phoenix number the get new themes link in the themes section of the new options preferences dialog has the appearance of normal text but should actually look like a hyperlink reproducible always steps to reproduce number go to tools options themes number hover mouse over get new themes number left click on get new themes actual results get new themes has the appearance of normal text and when hovering the mouse appears as a text selection cursor however the link to the themes page still works expected results get new themes should be highlighted and underlined like a normal hyperlink and when hover the mouse cursor should appear as a pointing hand']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(range(len(text))) + 2\n",
    "text[idx-2:idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh3TBdoEzKf1"
   },
   "source": [
    "### Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SK5XMvazUup"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(train_text):\n",
    "  word_freq = build_freq_dict(train_text)\n",
    "  print('word vocabulary')\n",
    "  word_vocab = save_vocab(word_freq, MAX_NB_WORDS, 'word_vocab.pkl')\n",
    "  return word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ke3n3FPPziK4"
   },
   "outputs": [],
   "source": [
    "def build_freq_dict(train_text):\n",
    "  print('building frequency dictionaries')\n",
    "  word_freq = defaultdict(int)\n",
    "  for text in tqdm(train_text):\n",
    "    for word in text.split():\n",
    "      word_freq[word] += 1\n",
    "  return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-AO64On0DZ8"
   },
   "outputs": [],
   "source": [
    "def save_vocab(freq_dict, vocab_size, filename):\n",
    "  top_tokens = sorted(freq_dict.items(), key=lambda x: -x[1])[:vocab_size - 2]\n",
    "  print('most common token is %s which appears %d times' % (top_tokens[0][0], top_tokens[0][1]))\n",
    "  print('less common token is %s which appears %d times' % (top_tokens[-1][0], top_tokens[-1][1]))\n",
    "  vocab = {}\n",
    "  i = 2  # 0-index is for padding, 1-index is for UNKNOWN\n",
    "  for j in range(len(top_tokens)):\n",
    "    vocab[top_tokens[j][0]] = i\n",
    "    i += 1\n",
    "  with open(os.path.join(DIR, filename), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Suf0R3uXzNPX",
    "outputId": "a7d162aa-367f-4f34-b982-f5e5ea9e9910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building frequency dictionaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 231628/231628 [00:07<00:00, 31271.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vocabulary\n",
      "most common token is number which appears 1290496 times\n",
      "less common token is anoyng which appears 10 times\n"
     ]
    }
   ],
   "source": [
    "word_vocab = build_vocabulary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_vocab loaded!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_vocab(filename):\n",
    "    with open(os.path.join(DIR, filename), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "word_vocab = load_vocab('word_vocab.pkl')\n",
    "\"word_vocab loaded!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKiWmCoZ0bl0"
   },
   "source": [
    "### Saving the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mnFQ2ASS0uQc"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "UNK = 1\n",
    "num_lines =  len(open(os.path.join(DIR, 'normalized_bugs.json'), 'r').read().splitlines()) * 2\n",
    "total = num_lines // 2\n",
    "\n",
    "def dump_bugs(word_vocab):\n",
    "    bug_dir = os.path.join(DIR, 'bugs')\n",
    "    if not os.path.exists(bug_dir):\n",
    "        os.mkdir(bug_dir)\n",
    "    bugs = []\n",
    "    cont = 1\n",
    "    print(\"Reading the normalized_bugs.json ...\")\n",
    "    with open(os.path.join(DIR, 'normalized_bugs.json'), 'r') as f:\n",
    "        loop = tqdm(f)\n",
    "        for line in f:\n",
    "            loop.set_description('Data dumping {}/{}'.format(cont, total))\n",
    "            bugs.append(json.loads(line))\n",
    "            cont += 1\n",
    "\n",
    "    return bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "def dump_vocabulary(bugs, bug_dir):\n",
    "    cont=0\n",
    "    total = len(bugs)\n",
    "    print(\"Starting the dump ...\")\n",
    "    loop = tqdm()\n",
    "    for bug in bugs:\n",
    "        #bug = json.loads(line)\n",
    "        #print(bug)\n",
    "        if cont % 100 == 0:\n",
    "            clear_output()\n",
    "            loop.set_description('Data dumping {}/{}'.format(cont, total))\n",
    "        cont+=1\n",
    "        bug['description_word'] = [word_vocab.get(w, UNK) for w in bug['description'].split()]\n",
    "        if len(bug['title']) == 0:\n",
    "            bug['title'] = bug['description'][:10]\n",
    "        bug['title_word'] = [word_vocab.get(w, UNK) for w in bug['title'].split()]\n",
    "        #bug.pop('description')\n",
    "        #bug.pop('title')\n",
    "        with open(os.path.join(bug_dir, str(bug['issue_id']) + '.pkl'), 'wb') as f:\n",
    "            pickle.dump(bug, f)\n",
    "\n",
    "def processing_dump(bugs):\n",
    "    #clear_output()\n",
    "    cpu = 2\n",
    "    pool = Pool(processes=cpu) # start 4 worker processes\n",
    "    bug_dir = os.path.join(DIR, 'bugs')\n",
    "    print(\"Starting the slice ...\")\n",
    "    works = []\n",
    "    n = len(bugs) // cpu\n",
    "    n = 1 if n == 0 else n\n",
    "    sliced = []\n",
    "    pos_end = n\n",
    "    end = len(bugs)\n",
    "    for i in range(cpu):\n",
    "        pos_end = end if pos_end>=end else pos_end\n",
    "        pos_end = end if (i+1) == cpu and pos_end < end else pos_end\n",
    "        sliced.append(bugs[i*n:pos_end])\n",
    "        pos_end += n\n",
    "\n",
    "    print(\"Slicing done!\")\n",
    "    for s in sliced:\n",
    "        if len(s) > 0:\n",
    "            #works.append(pool.apply_async(dump_vocabulary, (s, bug_dir, )))\n",
    "            dump_vocabulary(s, bug_dir)\n",
    "\n",
    "    #print(\"Executing the works...\")\n",
    "    #res = [w.get() for w in works]\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "KAktpUVe0Zze",
    "outputId": "08d9ce17-d3ea-4b40-c946-aa7f41a2b250",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the normalized_bugs.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data dumping 115814/115814: : 0it [05:03, ?it/s]"
     ]
    }
   ],
   "source": [
    "bugs = dump_bugs(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115814"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data dumping 57900/57907: : 0it [01:43, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "processing_dump(bugs)\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case to parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1, 2, 3, 4, 5, 6]\n",
    "# a = [1, 2, 3, 4]\n",
    "# a = [1, 2, 3]\n",
    "# a = [1]\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "cpu = multiprocessing.cpu_count()\n",
    "n = len(a) // cpu\n",
    "\n",
    "print(\"N=\", n)\n",
    "\n",
    "n = 1 if n == 0 else n\n",
    "\n",
    "sliced = []\n",
    "pos_end = n\n",
    "end = len(a)\n",
    "for i in range(cpu):\n",
    "    pos_end = end if pos_end>=end else pos_end\n",
    "    pos_end = end if (i+1) == cpu and pos_end < end else pos_end\n",
    "    sliced.append(a[i*n:pos_end])\n",
    "    pos_end += n\n",
    "sliced"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Dataset prepare - Bug Triage.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
