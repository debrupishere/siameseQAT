{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose centroid replacing the masters with BERT siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the use of GPUs\n",
    "# https://datascience.stackexchange.com/questions/23895/multi-gpu-in-keras\n",
    "# https://keras.io/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus\n",
    "# https://stackoverflow.com/questions/56316451/how-to-use-specific-gpus-in-keras-for-multi-gpu-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "# from __future__ import print_function, division\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers\n",
    "\n",
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval\n",
    "\n",
    "import os\n",
    "from keras_bert import load_vocabulary\n",
    "import random\n",
    "\n",
    "from keras.initializers import RandomUniform, RandomNormal, Ones\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import _pickle as pickle\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "    \n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: epochs=10\n",
      "env: base=eclipse\n"
     ]
    }
   ],
   "source": [
    "# %env epochs 10\n",
    "# %env base eclipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "TOPIC_SEQUENCE = 30 # Number of topics considered\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = int(os.environ['epochs'])\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = os.environ['base']\n",
    "METHOD = 'deepQL_topics_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "deepQL_topics_10 for 10 epochs in eclipse\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "print(\"*********\")\n",
    "print(\"{} for {} epochs in {}\".format(METHOD, epochs, DOMAIN))\n",
    "print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a77b73e8c64471bd3d5d17d7a93070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7281341af9c4aa18e252cd975c879e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 37.6 s, sys: 3.46 s, total: 41.1 s\n",
      "Wall time: 40.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ef6b02ed3141928dffd0387ec53cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 15.3 ms, total: 2.29 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '675\\n',\n",
       " 'creation_ts': '2009-02-23 15:50:00 -0500',\n",
       " 'delta_ts': '2009-03-05 09:55:43 -0500',\n",
       " 'description': \"[CLS] while experimenting with different remote systems through ss ##h protocol , we found a consistent failure to a certain linux server . the run ##time error ( remote ##fi ##les ##ub ##sy ##ste ##mi ##mp ##l . log ##er ##ror ( ) ) displayed is pretty cryptic and doesn ' t explain much in terms of underlying configuration problem with the target system . any other ss ##h client works pretty standard with this target linux server , and we are not sure what might be the root cause . any hints and suggestions in finding the root cause is highly appreciated . remote system : - - - - - - - - - - - - - linux pa ##x - 03 2 . 6 . 9 - 34 . el ##sm ##p # 1 sm ##p fr ##i feb 24 16 : 54 : 53 est 2006 i ##6 ##86 i ##6 ##86 i ##38 ##6 gnu / linux local client : - - - - - - - - - - - - eclipse . build ##id = m2 ##00 ##8 ##12 ##18 - 110 ##2 java . full ##version = j ##2 ##re 1 . 6 . 0 ibm j ##9 2 . 4 windows xp x ##86 - 32 j ##v ##m ##wi ##32 ##60 - 2008 ##0 ##41 ##5 _ 1876 ##2 ( ji ##t enabled , ao ##t enabled ) j ##9 ##v ##m - 2008 ##0 ##41 ##5 _ 01 ##8 ##7 ##6 ##2 _ l ##hd ##sm ##r ji ##t - r ##9 _ 2008 ##0 ##41 ##5 _ 152 ##0 g ##c - 2008 ##0 ##41 ##5 _ aa boot ##load ##er constant ##s : os = win ##32 , arch = x ##86 , w ##s = win ##32 , nl = en _ ca command - line arguments : - os win ##32 - w ##s win ##32 - arch x ##86 exception displayed : - - - - - - - - - - - - - - - - - - - - - - org . eclipse . rs ##e . services . clients ##er ##ver . messages . system ##mes ##sa ##gee ##x ##ception : an unexpected error occurred . at org . eclipse . rs ##e . sub ##systems . files . core . services ##ub ##sy ##ste ##m . files ##er ##vic ##es ##ub ##sy ##ste ##m . get ##rem ##ote ##fi ##le ##ob ##ject ( files ##er ##vic ##es ##ub ##sy ##ste ##m . java : 264 ) at org . eclipse . rs ##e . sub ##systems . files . core . sub ##systems . remote ##fi ##les ##ub ##sy ##ste ##m . internal ##res ##ol ##ve ##fi ##lter ##st ##ring ( remote ##fi ##les ##ub ##sy ##ste ##m . java : 64 ##5 ) at org . eclipse . rs ##e . sub ##systems . files . core . sub ##systems . remote ##fi ##les ##ub ##sy ##ste ##m . internal ##res ##ol ##ve ##fi ##lter ##st ##ring ##s ( remote ##fi ##les ##ub ##sy ##ste ##m . java : 46 ##8 ) at org . eclipse . rs ##e . core . sub ##systems . sub ##sy ##ste ##m . resolve ##fi ##lter ##st ##ring ##s ( sub ##sy ##ste ##m . java : 225 ##0 ) at org . eclipse . rs ##e . internal . ui . view . system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . internal ##get ##child ##ren ( system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . java : 46 ##4 ) at org . eclipse . rs ##e . internal . ui . view . system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . get ##child ##ren ( system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . java : 283 ) at org . eclipse . rs ##e . internal . ui . view . system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . get ##child ##ren ( system ##view ##fi ##lter ##re ##ference ##ada ##pt ##er . java : 291 ) at org . eclipse . rs ##e . ui . operations . system ##fe ##tch ##oper ##ation . execute ( system ##fe ##tch ##oper ##ation . java : 36 ##3 ) at org . eclipse . rs ##e . ui . operations . system ##fe ##tch ##oper ##ation . run ( system ##fe ##tch ##oper ##ation . java : 141 ) at org . eclipse . rs ##e . ui . view . abstracts ##yst ##em ##view ##ada ##pt ##er . fetch ##de ##fer ##red ##child ##ren ( abstracts ##yst ##em ##view ##ada ##pt ##er . java : 230 ##0 ) at org . eclipse . ui . progress . def ##erre ##dt ##ree ##con ##ten ##tman ##ager $ 1 . run ( def ##erre ##dt ##ree ##con ##ten ##tman ##ager . java : 234 ) at org . eclipse . core . internal . jobs . worker . run ( worker . java : 55 ) [SEP]\",\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  2096, 23781,  2007,  2367,  6556,  3001,  2083,  7020,\n",
       "         2232,  8778,  1010,  2057,  2179,  1037,  8335,  4945,  2000,\n",
       "         1037,   102]),\n",
       " 'dup_id': '227135',\n",
       " 'issue_id': 265881,\n",
       " 'priority': '1\\n',\n",
       " 'product': '109\\n',\n",
       " 'resolution': 'DUPLICATE',\n",
       " 'textual_token': array([  101,  1031,  7020,  2232,  1033,  6556,  8873,  4244, 12083,\n",
       "         6508, 13473,  4328,  8737,  2140,  1012,  8833,  2121, 29165,\n",
       "         1006,   102,   101,  2096, 23781,  2007,  2367,  6556,  3001,\n",
       "         2083,  7020,  2232,  8778,  1010,  2057,  2179,  1037,  8335,\n",
       "         4945,  2000,  1037,   102]),\n",
       " 'title': '[CLS] [ ss ##h ] remote ##fi ##les ##ub ##sy ##ste ##mi ##mp ##l . log ##er ##ror ( ) fails with unexpected error . . . [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  1031,  7020,  2232,  1033,  6556,  8873,  4244, 12083,\n",
       "         6508, 13473,  4328,  8737,  2140,  1012,  8833,  2121, 29165,\n",
       "         1006,   102]),\n",
       " 'topic': 22,\n",
       " 'topic_index': 21,\n",
       " 'topics': array([0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03,\n",
       "        0.  , 0.07, 0.  , 0.07, 0.  , 0.  , 0.15, 0.  , 0.  , 0.06, 0.18,\n",
       "        0.28, 0.  , 0.05, 0.05, 0.03, 0.  , 0.  , 0.  ]),\n",
       " 'version': '64\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : [], 'topics' : []}\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    title_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_T), 0)\n",
    "    description_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "    for i, bug_id in enumerate(batch_bugs):\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug, index=i, title_ids=title_ids, description_ids=description_ids)\n",
    "\n",
    "    batch_features['title'] = { 'token' : np.array(batch_features['title']), 'segment' : title_ids }\n",
    "    batch_features['desc'] = { 'token' : np.array(batch_features['desc']), 'segment' : description_ids }\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    batch_features['topics'] = np.array(batch_features['topics'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'],\n",
    "                            'topics' : batch_features['topics'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title']['token'], valid_input_sample['title']['segment'], \n",
    "                   valid_input_sample['description']['token'], valid_input_sample['description']['segment'],\n",
    "                   valid_input_sample['info'], valid_input_sample['topics'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 20), (384, 20), (384, 1682), (384, 30), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, \\\n",
    "valid_input_sample['description']['token'].shape, \\\n",
    "valid_input_sample['title']['segment'].shape, \\\n",
    "valid_input_sample['description']['segment'].shape, \\\n",
    "valid_input_sample['info'].shape, valid_input_sample['topics'].shape, \\\n",
    "valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 16995)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#     )\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=True,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "#     model = get_model(\n",
    "#         token_num=len(token_dict),\n",
    "#         head_num=10,\n",
    "#         transformer_num=layer_num,\n",
    "#         embed_dim=100,\n",
    "#         feed_forward_dim=100,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#         pos_num=MAX_SEQUENCE_LENGTH,\n",
    "#         dropout_rate=0.05,\n",
    "#     )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    layers = ['Encoder-{}-MultiHeadSelfAttention-Adapter', 'Encoder-{}-FeedForward-Adapter', \n",
    "     'Encoder-{}-MultiHeadSelfAttention-Norm', 'Encoder-{}-FeedForward-Norm']\n",
    "    outputs = []\n",
    "    for i in range(1, 13):\n",
    "        outputs += [ model.get_layer(layer.format(layer_num)).output for layer in layers ]\n",
    "    outputs = Average()(outputs)\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "    outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='FeatureTopic_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureTopicMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    \n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "    \n",
    "    semi_hard_negatives_mask = math_ops.equal(pdist_matrix, masked_maximum(pdist_matrix, adjacency_not))\n",
    "    \n",
    "    # Remove false negatives with similarity equal to the true negatives\n",
    "    semi_hard_negatives_mask = tf.reshape(tf.cast(semi_hard_negatives_mask, 'int32'), (-1, 1)) * tf.reshape(tf.cast(adjacency_not, 'int32'), (-1, 1))\n",
    "    semi_hard_negatives_mask = tf.cast(tf.reshape(semi_hard_negatives_mask, (batch_size, batch_size)), 'bool')\n",
    "    \n",
    "    # Recovery the bug label from semi-hard-negatives\n",
    "    label_matrix = tf.repeat(tf.reshape(labels, (1, -1)), repeats=[batch_size], axis=0)\n",
    "    semi_hard_negatives_ids = tf.reshape(label_matrix, (-1, 1)) * tf.cast(tf.reshape(semi_hard_negatives_mask, (-1, 1)), 'int32')\n",
    "    semi_hard_negatives_ids = tf.reshape(semi_hard_negatives_ids, (batch_size, batch_size))\n",
    "\n",
    "    i = tf.constant(0)\n",
    "    most_freq_matrix = tf.Variable([])\n",
    "    def most_frequent(i, most_freq_matrix):\n",
    "        batch = tf.gather(semi_hard_negatives_ids, i)\n",
    "        neg_label_default = [tf.unique(batch)[0][0]]\n",
    "        batch = tf.boolean_mask(batch, tf.greater(batch, 0))\n",
    "        unique, _, count = tf.unique_with_counts(batch)\n",
    "        max_occurrences = tf.reduce_max(count)\n",
    "        max_cond = tf.equal(count, max_occurrences)\n",
    "        max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n",
    "        max_numbers = tf.cond(tf.cast(tf.size(unique) > 1, tf.bool), lambda: unique[0], lambda: max_numbers)\n",
    "        max_numbers = tf.cond(tf.cast(tf.shape(unique) == 0, tf.bool), \n",
    "                              lambda: neg_label_default, \n",
    "                              lambda: max_numbers)\n",
    "        most_freq_matrix = tf.concat([most_freq_matrix, [max_numbers]], axis=0)\n",
    "        return [tf.add(i, 1), most_freq_matrix]\n",
    "    _, negatives_ids = tf.while_loop(lambda i, _: i<batch_size, \n",
    "                                        most_frequent, \n",
    "                                        [i, most_freq_matrix],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "    negatives_ids = tf.cast(negatives_ids, 'int32')\n",
    "    labels_neg = tf.reshape(negatives_ids, (-1, 1))\n",
    "    mask_negatives = math_ops.equal(labels_neg, semi_hard_negatives_ids)\n",
    "    mask_negatives = tf.cast(mask_negatives, 'float32')\n",
    "    labels_neg = tf.cast(labels_neg, 'float32')\n",
    "    \n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "    \n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "    # centroid pos \n",
    "    embed_pos = tf.matmul(mask_positives, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "\n",
    "    # centroid negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix):\n",
    "        # anchor\n",
    "        anchor = [tf.gather(embeddings, i)]\n",
    "        label_pos = [tf.gather(labels_pos, i)]\n",
    "        # centroid pos\n",
    "        centroid_pos = [tf.gather(centroid_embed_pos, i)]\n",
    "        # centroid neg\n",
    "        centroid_neg = [tf.gather(centroid_embed_neg, i)]\n",
    "        label_neg = [tf.gather(labels_neg, i)]\n",
    "        # new batch\n",
    "        new_batch = tf.concat([anchor, centroid_pos, centroid_neg], axis=0)\n",
    "        new_labels = tf.concat([label_pos, label_pos, label_neg], axis=0)\n",
    "        # Batch anchor + centroid_positive + centroid_negative\n",
    "        batch_anchor_centroids = tf.concat([new_labels, new_batch], axis=1)\n",
    "        TL_single_triplet = triplet_loss(batch_anchor_centroids)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_single_triplet]], axis=0)\n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix]\n",
    "    _, batch_centroid_matrix = tf.while_loop(lambda i, a: i<batch_size, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "    \n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.constant(0.0)\n",
    "    TL_neg = tf.constant(0.0) #tf.reduce_mean(batch_centroid_matrix_neg)\n",
    "   \n",
    "    return K.stack([TL, TL_pos, TL_neg, TL_centroid], axis=0)\n",
    "\n",
    "def quintet_trainable(inputs):\n",
    "    TL = inputs[0]\n",
    "    TL_pos = inputs[1]\n",
    "    TL_neg = inputs[2]\n",
    "    TL_centroid = inputs[3]\n",
    "    TL_anchor_w = inputs[4]\n",
    "    TL_pos_w = inputs[5]\n",
    "    TL_neg_w = inputs[6]\n",
    "    TL_centroid_w = inputs[7]\n",
    "    \n",
    "    TL_anchor_w = math_ops.maximum(TL_anchor_w, 0.0)\n",
    "    TL_neg_w = math_ops.maximum(TL_neg_w, 0.0)\n",
    "    TL_centroid_w = math_ops.maximum(TL_centroid_w, 0.0)\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([ TL * TL_anchor_w, TL_centroid * TL_centroid_w]) # , TL_neg * TL_neg_w,\n",
    "    sum_of_weigths = tf.reduce_sum([TL_anchor_w, TL_centroid_w])\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return K.stack([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], axis=0)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, topic_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, sequence_length_topics, name):\n",
    "  \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    # Topics\n",
    "    bug_topics = Input(shape = (sequence_length_topics, ), name='topics_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    bug_topics_feat = topic_feature_model(bug_topics)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat, bug_topics_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i, bug_topics], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuintetWeights(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(QuintetWeights, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = tf.reshape(self.add_weight(name='quintet_kernel_weight', \n",
    "                                      shape=(input_shape[0], self.output_dim),\n",
    "                                      initializer=keras.initializers.Ones(),\n",
    "#                                       initializer=keras.initializers.RandomUniform(minval=0.0, maxval=1.0, seed=None),\n",
    "                                      trainable=False), (1, 1))\n",
    "        super(QuintetWeights, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (1, 1))\n",
    "        return [K.dot(x, self.kernel), self.kernel]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape, input_shape]\n",
    "    \n",
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    feature = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    TL_loss = Lambda(quintet_loss, name='quintet_loss')(feature)\n",
    "    \n",
    "    tl_l = Lambda(lambda x:tf.reshape(x[0], (1,)), name='TL')(TL_loss)\n",
    "    tl_l_p = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_pos')(TL_loss)\n",
    "    tl_l_n = Lambda(lambda x:tf.reshape(x[2], (1,)), name='TL_neg')(TL_loss)\n",
    "    tl_l_c = Lambda(lambda x:tf.reshape(x[3], (1, )), name='TL_centroid')(TL_loss)\n",
    "    \n",
    "    TL_w = QuintetWeights(output_dim=1)(tl_l)\n",
    "    TL_pos_w = QuintetWeights(output_dim=1)(tl_l_p)\n",
    "    TL_neg_w = QuintetWeights(output_dim=1)(tl_l_n)\n",
    "    TL_centroid_w = QuintetWeights(output_dim=1)(tl_l_c)\n",
    "    \n",
    "    TL_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_weight')(TL_w)\n",
    "    TL_pos_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_pos_weight')(TL_pos_w)\n",
    "    TL_neg_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_neg_weight')(TL_neg_w)\n",
    "    TL_centroid_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_centroid_weight')(TL_centroid_w)\n",
    "    \n",
    "    output = concatenate([tl_l, tl_l_p, tl_l_n, tl_l_c, TL_weight, TL_pos_weight, TL_neg_weight, TL_centroid_weight])\n",
    "    output = Lambda(quintet_trainable, name='quintet_trainable')(output)\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                         TL, TL_pos, TL_neg, TL_centroid]) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'deepQL_topics_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-29-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-30-3bc265d0a7cc>:120: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topics_in (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureTopicMlpGenerationModel  (None, 300)          9300        topics_in[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1200)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "                                                                 FeatureTopicMlpGenerationModel[1]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1201)         0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (4,)                 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TL (Lambda)                     (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid (Lambda)            (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_1 (QuintetWeigh [(1,), (1,)]         1           TL[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_2 (QuintetWeigh [(1,), (1,)]         1           TL_pos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_3 (QuintetWeigh [(1,), (1,)]         1           TL_neg[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_4 (QuintetWeigh [(1,), (1,)]         1           TL_centroid[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TL_weight (Lambda)              (1,)                 0           quintet_weights_1[0][0]          \n",
      "                                                                 quintet_weights_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos_weight (Lambda)          (1,)                 0           quintet_weights_2[0][0]          \n",
      "                                                                 quintet_weights_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg_weight (Lambda)          (1,)                 0           quintet_weights_3[0][0]          \n",
      "                                                                 quintet_weights_3[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid_weight (Lambda)     (1,)                 0           quintet_weights_4[0][0]          \n",
      "                                                                 quintet_weights_4[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (8,)                 0           TL[0][0]                         \n",
      "                                                                 TL_pos[0][0]                     \n",
      "                                                                 TL_neg[0][0]                     \n",
      "                                                                 TL_centroid[0][0]                \n",
      "                                                                 TL_weight[0][0]                  \n",
      "                                                                 TL_pos_weight[0][0]              \n",
      "                                                                 TL_neg_weight[0][0]              \n",
      "                                                                 TL_centroid_weight[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_trainable (Lambda)      (9,)                 0           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 161,669,076\n",
      "Trainable params: 1,196,896\n",
      "Non-trainable params: 160,472,180\n",
      "__________________________________________________________________________________________________\n",
      "Total of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "=> result saved!\n",
      "Epoch: 1 Loss: 3.42, Loss_test: 1.97\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.68, TL_pos: 0.00, TL_neg: 0.00, TL_centroid: 6.16, recall@25: 0.25\n",
      "Best_epoch=0, Best_loss=1.00, Recall@25=0.25\n",
      "CPU times: user 2h 9min 3s, sys: 42.5 s, total: 2h 9min 46s\n",
      "Wall time: 19min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "topic_feature_model = topic_model(TOPIC_SEQUENCE)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, topic_feature_model,\n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, TOPIC_SEQUENCE, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_input_sample['topics'], train_sim]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert-topic')\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "                \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "              \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], \n",
    "                                          h[4], h[5], h[6], h[7], h[8], recall))\n",
    "    else:\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "# experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "# experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_deepQL_topics_1.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_deepQL_topics_1_feature_1epochs_64batch(eclipse).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topics_in (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureTopicMlpGenerationModel  (None, 300)          9300        topics_in[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1200)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "                                                                 FeatureTopicMlpGenerationModel[1]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1201)         0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (4,)                 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TL (Lambda)                     (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid (Lambda)            (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_1 (QuintetWeigh [(1,), (1,)]         1           TL[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_2 (QuintetWeigh [(1,), (1,)]         1           TL_pos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_3 (QuintetWeigh [(1,), (1,)]         1           TL_neg[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_4 (QuintetWeigh [(1,), (1,)]         1           TL_centroid[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TL_weight (Lambda)              (1,)                 0           quintet_weights_1[0][0]          \n",
      "                                                                 quintet_weights_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos_weight (Lambda)          (1,)                 0           quintet_weights_2[0][0]          \n",
      "                                                                 quintet_weights_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg_weight (Lambda)          (1,)                 0           quintet_weights_3[0][0]          \n",
      "                                                                 quintet_weights_3[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid_weight (Lambda)     (1,)                 0           quintet_weights_4[0][0]          \n",
      "                                                                 quintet_weights_4[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (8,)                 0           TL[0][0]                         \n",
      "                                                                 TL_pos[0][0]                     \n",
      "                                                                 TL_neg[0][0]                     \n",
      "                                                                 TL_centroid[0][0]                \n",
      "                                                                 TL_weight[0][0]                  \n",
      "                                                                 TL_pos_weight[0][0]              \n",
      "                                                                 TL_neg_weight[0][0]              \n",
      "                                                                 TL_centroid_weight[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_trainable (Lambda)      (9,)                 0           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 161,669,076\n",
      "Trainable params: 1,196,896\n",
      "Non-trainable params: 160,472,180\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_2')\n",
    "output = Lambda(quintet_trainable, name='quintet_trainable')(model.output)\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                     TL, TL_pos, TL_neg, TL_centroid])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepQL_topics_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.98, Loss_test: 0.65\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.66, TL_pos: 0.00, TL_neg: 0.00, TL_centroid: 1.29\n",
      "Epoch: 3 Loss: 0.54, Loss_test: 0.47\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.65, TL_pos: 0.00, TL_neg: 0.00, TL_centroid: 0.42\n",
      "Epoch: 4 Loss: 0.44, Loss_test: 0.42\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.66, TL_pos: 0.00, TL_neg: 0.00, TL_centroid: 0.22\n",
      "Epoch: 5 Loss: 0.36, Loss_test: 0.40\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.56, TL_pos: 0.00, TL_neg: 0.00, TL_centroid: 0.15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e6286ae766b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     batch_triplet_train,         train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n\u001b[1;32m      4\u001b[0m                                                        \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdup_sets_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbug_train_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                            batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m     train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n\u001b[1;32m      7\u001b[0m                    \u001b[0mtrain_input_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ec83beb84b25>\u001b[0m in \u001b[0;36mbatch_iterator\u001b[0;34m(self, retrieval, model, data, dup_sets, bug_ids, batch_size, n_neg, issues_by_buckets, TRIPLET_HARD, FLOATING_PADDING)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# global self.bug_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'desc'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'info'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topics'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/random.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;31m# pick an element in x[:i+1] with which to exchange x[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_input_sample['topics'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_deepQL_topics_10_feature10epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert-topic')\n",
    "print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "       \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "        \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "      \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2:15392,9779,94|226:0.5684879124164581,1586:0.5330164432525635,4664:0.52483731508255,21464:0.5246564149856567,88:0.5233481228351593,4724:0.5211328566074371,232:0.5165833234786987,8355:0.5160245001316071,4518:0.5101359486579895,63298:0.5040749609470367,2564:0.5033756196498871,3223:0.5007448196411133,93227:0.5001167356967926,93228:0.5001167356967926,1594:0.49875032901763916,36295:0.4974897503852844,86664:0.494792103767395,44895:0.49453598260879517,76218:0.4938657879829407,171:0.49245744943618774,143595:0.487126886844635,2884:0.4865824580192566,3500:0.4863436818122864,71975:0.4845206141471863,136455:0.48429131507873535,4805:0.48409974575042725,70697:0.4835215210914612,181418:0.4825901985168457,70620:0.4824434518814087',\n",
       " '393232:393282,390667,383388|225062:0.5244124531745911,95150:0.5215834081172943,214788:0.5215179622173309,386240:0.5133010149002075,196411:0.5125053226947784,137715:0.5123872458934784,338509:0.5062163174152374,120578:0.5051821172237396,78701:0.5045704245567322,78896:0.5042006969451904,101141:0.5041373670101166,323863:0.5039838552474976,387339:0.5024819374084473,414877:0.4988504648208618,156888:0.49874842166900635,129498:0.49866223335266113,104109:0.49780869483947754,161189:0.4972374439239502,411948:0.4972304701805115,156764:0.49641942977905273,21748:0.4963343143463135,272592:0.49580878019332886,105437:0.49545544385910034,57634:0.49536043405532837,162581:0.49395251274108887,126907:0.493710994720459,140991:0.4936593770980835,142644:0.49291306734085083,118701:0.4924558997154236',\n",
       " '30:205979,38236,99332,71263|19669:0.5548818707466125,20521:0.5504303574562073,21128:0.5487973392009735,280370:0.5446213781833649,21240:0.5376587808132172,22835:0.5374807119369507,103374:0.5354404747486115,53235:0.5282395482063293,50288:0.5270511507987976,14775:0.5259226560592651,14097:0.5254766643047333,15119:0.52545365691185,71048:0.5246472060680389,45679:0.5217809975147247,47044:0.5211527049541473,25783:0.5208667814731598,44742:0.5206941664218903,4904:0.5194151699542999,21818:0.5186838209629059,70426:0.5151148438453674,204563:0.5143200755119324,36483:0.514281302690506,67050:0.5141388177871704,22914:0.5138905644416809,201873:0.5127035081386566,78290:0.5114797353744507,28903:0.511361688375473,38236:0.5111158192157745,373796:0.5107135772705078',\n",
       " '131123:234849|10053:0.5285888016223907,128017:0.5217568278312683,30397:0.5198237001895905,31056:0.5187575221061707,59334:0.5164436995983124,96937:0.5136663913726807,66896:0.5130829811096191,26592:0.5128166079521179,63278:0.511806309223175,145448:0.5115134119987488,92818:0.5098875463008881,44764:0.5073935687541962,69807:0.5052054226398468,71762:0.5050588548183441,26307:0.5047193467617035,174625:0.5046603381633759,141801:0.5033794939517975,100266:0.5031740963459015,252393:0.5026173889636993,63366:0.5025947391986847,157516:0.5019355714321136,199752:0.5016800165176392,21537:0.5015186667442322,114335:0.501372903585434,40632:0.5010850429534912,102041:0.5008993148803711,28021:0.5000413358211517,37496:0.5000012218952179,176125:0.4997097849845886',\n",
       " '393277:393864,400436|151743:0.469266414642334,346866:0.464111328125,33971:0.46150749921798706,406621:0.44986140727996826,354593:0.44692444801330566,241313:0.44660043716430664,18515:0.4462661147117615,28385:0.4461926817893982,199303:0.445384681224823,186099:0.4429982900619507,32355:0.441947877407074,175606:0.4418739676475525,160609:0.4406309127807617,348068:0.439330518245697,18941:0.4391017556190491,223490:0.4377060532569885,83121:0.4376586675643921,62586:0.43623679876327515,9951:0.43569880723953247,238594:0.4353489279747009,287103:0.4350810647010803,355511:0.43466514348983765,241770:0.4344807267189026,265920:0.4343631863594055,179848:0.43431347608566284,6855:0.43425309658050537,49532:0.43411803245544434,68271:0.4335610866546631,65600:0.43296968936920166',\n",
       " '131136:150817,149868,145937,65841,102812|54596:0.4825981855392456,57062:0.48009490966796875,7760:0.479508638381958,21485:0.4738103151321411,31579:0.46992045640945435,307121:0.46899908781051636,16202:0.4662069082260132,131985:0.4605121612548828,71151:0.46014630794525146,108321:0.45772701501846313,193601:0.45639681816101074,41007:0.45414137840270996,139708:0.4537662863731384,55893:0.4535614848136902,139707:0.45273876190185547,78968:0.45245301723480225,224250:0.4513818025588989,65244:0.4508433938026428,33718:0.4507431983947754,45742:0.45027613639831543,168168:0.4491404891014099,133576:0.44878917932510376,310947:0.44860386848449707,101757:0.4483951926231384,61211:0.4477524757385254,27973:0.44729387760162354,92092:0.44702357053756714,409937:0.4456642270088196,104303:0.4453961253166199',\n",
       " '393282:393232,390667,383388|393232:0.5404602885246277,149441:0.5353645086288452,33446:0.5235588848590851,196411:0.5194589793682098,97617:0.5153546035289764,225062:0.5114601254463196,39611:0.5111133456230164,323863:0.5108122229576111,384147:0.5107261836528778,383951:0.5102090835571289,35311:0.5098352432250977,283332:0.5084977149963379,139136:0.5084714889526367,191094:0.5081067681312561,65189:0.5077996850013733,108276:0.5071384012699127,193833:0.5054447054862976,97163:0.5047843754291534,140991:0.5010126531124115,214788:0.5008552372455597,147608:0.5007437169551849,396463:0.49977731704711914,388038:0.49874037504196167,297888:0.4987347722053528,35307:0.4974544048309326,35310:0.4974544048309326,235174:0.4972720146179199,55208:0.4970993995666504,151165:0.49701762199401855',\n",
       " '262212:292851,292845,291839|304014:0.5463793575763702,312670:0.5284707546234131,333937:0.523463249206543,203013:0.5210556983947754,304139:0.5205047130584717,213562:0.5202429592609406,203018:0.5188705921173096,182091:0.5186963975429535,316678:0.5179342031478882,207269:0.5176612436771393,174704:0.5174402296543121,166555:0.5165421366691589,234366:0.5165276527404785,308779:0.5164231956005096,360377:0.5159237086772919,331301:0.5157090723514557,344856:0.5152451395988464,302781:0.515213817358017,221884:0.5148527920246124,193429:0.5142686665058136,317226:0.513959139585495,192787:0.5129916667938232,216015:0.5126332640647888,330474:0.5123542845249176,305054:0.5122663676738739,212813:0.5113230347633362,309414:0.5110850036144257,218812:0.5108155906200409,351387:0.5106820464134216',\n",
       " '131143:135599|313169:0.5286111235618591,104036:0.5235144197940826,159331:0.5135842263698578,385459:0.5051482617855072,298097:0.503607988357544,36654:0.5029904246330261,170441:0.4936184883117676,131097:0.49196553230285645,101574:0.4911905527114868,273478:0.4902794361114502,110699:0.4880935549736023,140991:0.4880337119102478,72191:0.48771148920059204,172338:0.48766618967056274,121630:0.4872392416000366,222588:0.48610037565231323,104981:0.48530179262161255,126067:0.48491114377975464,385830:0.4827253222465515,343754:0.48251873254776,124180:0.4819622039794922,421343:0.4814983010292053,8655:0.4814203977584839,286783:0.4805799722671509,35386:0.47951000928878784,291886:0.4784783124923706,44739:0.47692686319351196,36184:0.47634559869766235,11886:0.47589564323425293',\n",
       " '71:256,228|153011:0.4571874141693115,2500:0.45513057708740234,28033:0.45028215646743774,344444:0.44786369800567627,4470:0.44412750005722046,4570:0.43885236978530884,19360:0.4356726408004761,249846:0.43513739109039307,339774:0.4330018162727356,58142:0.4327771067619324,25783:0.43164628744125366,158475:0.42954325675964355,149441:0.42743051052093506,423582:0.42727506160736084,77932:0.42481499910354614,67537:0.4246089458465576,25621:0.42334121465682983,48467:0.42276328802108765,205923:0.42214125394821167,179414:0.4219391942024231,162330:0.4218723177909851,174207:0.42145854234695435,19744:0.4210314154624939,202220:0.42075151205062866,122:0.4204046130180359,258746:0.4202272891998291,331631:0.42003631591796875,384731:0.41962093114852905,103458:0.41961097717285156',\n",
       " '85:247,10959|3939:0.5414624810218811,3274:0.5333366096019745,387592:0.5172083973884583,3307:0.5168490409851074,58642:0.5113612711429596,13865:0.5112515687942505,3037:0.5104019343852997,94520:0.5045605003833771,306648:0.5042522251605988,4015:0.5020093023777008,4760:0.501695305109024,19584:0.4990353584289551,359525:0.49850302934646606,150478:0.49695682525634766,89375:0.4957427382469177,33619:0.4946752190589905,90924:0.49423229694366455,314569:0.4903312921524048,12934:0.48759061098098755,13239:0.4867072105407715,93978:0.4842727780342102,68422:0.4835227131843567,129915:0.483271062374115,14193:0.4832140803337097,39957:0.48287826776504517,1649:0.48264437913894653,64239:0.48228198289871216,126957:0.4821605086326599,385830:0.48214417695999146',\n",
       " '88:6099,14116|247:0.5762993097305298,4895:0.5695661902427673,232:0.5684464275836945,1594:0.5568106174468994,3761:0.547762542963028,4805:0.5458841919898987,2005:0.5455341339111328,181418:0.5427594184875488,3308:0.5405244827270508,1586:0.5401967167854309,2884:0.5362290740013123,187717:0.5352124273777008,28440:0.5330515205860138,4724:0.5327045023441315,16577:0.5299397110939026,2508:0.5295134484767914,3274:0.5289861857891083,273:0.5260854959487915,256:0.5258507430553436,2:0.5233481228351593,4823:0.52234748005867,4664:0.5221938192844391,21240:0.521892786026001,1916:0.5206352472305298,2504:0.5200923979282379,1829:0.5181586444377899,3838:0.5177973210811615,3945:0.5175618827342987,1575:0.5171397030353546',\n",
       " '131161:44438,103374,98057|85622:0.5472313165664673,99959:0.5246386528015137,246495:0.5209089815616608,44438:0.5160821676254272,82201:0.5121441781520844,51219:0.5106880962848663,45916:0.509142130613327,125851:0.508078008890152,118782:0.5061247646808624,71985:0.505588948726654,65767:0.5055226683616638,47757:0.5049946308135986,99515:0.5049033761024475,136091:0.5036605596542358,152703:0.5034149885177612,250620:0.5031896531581879,23473:0.5029657483100891,243692:0.5021712779998779,134750:0.5006744861602783,108923:0.49932098388671875,242622:0.49872589111328125,240673:0.4955311417579651,133284:0.4950953722000122,74655:0.4944955110549927,151244:0.49411648511886597,38218:0.49342578649520874,194105:0.49333518743515015,46701:0.49310290813446045,230331:0.49265867471694946',\n",
       " '262235:260098,263611|234213:0.49343860149383545,233071:0.490325391292572,207837:0.4838562607765198,194011:0.4830387830734253,222509:0.4812643527984619,246579:0.476637065410614,235117:0.47571641206741333,202812:0.47555971145629883,38111:0.4752679467201233,115544:0.4738738536834717,15826:0.47265005111694336,38368:0.4711611866950989,77374:0.4709813594818115,67993:0.47082626819610596,236495:0.4693857431411743,114674:0.468025267124176,61095:0.46800440549850464,58179:0.4677964448928833,310471:0.4674513339996338,139769:0.46691006422042847,71713:0.4665137529373169,25548:0.4641633629798889,104034:0.46314406394958496,55674:0.46191298961639404,19756:0.46116405725479126,81166:0.4606444835662842,53171:0.46064335107803345,142965:0.4591748118400574,233105:0.45830392837524414',\n",
       " '94:15392,2,9779|15392:0.6128800809383392,14674:0.5450837910175323,19333:0.5433556437492371,67948:0.5424472391605377,40067:0.5417402386665344,77479:0.5334718823432922,23108:0.5298245847225189,44895:0.528998851776123,248597:0.5260485410690308,248609:0.5260485410690308,7327:0.5221336781978607,152479:0.5193279981613159,248598:0.519027441740036,14610:0.5168770253658295,28915:0.5168507397174835,307121:0.5154721140861511,38731:0.5148729681968689,60315:0.5142929255962372,22215:0.5125495493412018,11492:0.5121563374996185,263575:0.5112170875072479,18288:0.5110628306865692,159977:0.5103286504745483,5939:0.5103063881397247,100550:0.5096665918827057,28817:0.509064257144928,8355:0.5079452991485596,98128:0.507399320602417,24029:0.5073661804199219',\n",
       " '131171:202560,96651,172374|166555:0.5423145592212677,200596:0.5333826243877411,90097:0.5283587276935577,187276:0.527621865272522,276624:0.5275058150291443,198641:0.525894284248352,197781:0.5244329869747162,185914:0.524297833442688,281069:0.5232402682304382,234849:0.5225368142127991,227454:0.5224120020866394,268177:0.521909087896347,166176:0.520645409822464,182566:0.5198505818843842,196309:0.5197241306304932,167475:0.517288476228714,282179:0.5167704522609711,258834:0.5156872570514679,272256:0.515279233455658,283127:0.5147645771503448,283051:0.5139205455780029,194471:0.5130789875984192,265592:0.5127913355827332,239374:0.5123001933097839,174252:0.5106169283390045,221884:0.5106039345264435,272823:0.5095430314540863,167601:0.5094373226165771,178601:0.5092461407184601',\n",
       " '393320:394988|161189:0.5189270377159119,399832:0.518244057893753,215201:0.5171017050743103,215202:0.5171017050743103,215203:0.5171017050743103,215206:0.5171017050743103,289163:0.5087492763996124,315007:0.5077600479125977,367323:0.5059221982955933,55724:0.5054203271865845,327940:0.5019981861114502,86016:0.5006962418556213,232:0.4990144968032837,161450:0.49896085262298584,55727:0.4969490170478821,268209:0.4963902235031128,150412:0.4952491521835327,164787:0.49524277448654175,164790:0.4934702515602112,104001:0.49336791038513184,150439:0.49326133728027344,341937:0.4929479956626892,281634:0.49232810735702515,151171:0.4922131896018982,118779:0.49169808626174927,55701:0.49052703380584717,67204:0.48868507146835327,67205:0.48868507146835327,353781:0.48862165212631226',\n",
       " '131180:134058,132491,131646|32551:0.5003930032253265,234126:0.4699082374572754,170070:0.4659992456436157,96651:0.4641938805580139,63748:0.46232181787490845,54893:0.46116793155670166,406110:0.4609829783439636,8706:0.46058690547943115,124149:0.46053510904312134,22132:0.459796667098999,305043:0.4577649235725403,105767:0.45734667778015137,130934:0.4572656750679016,51241:0.4560771584510803,6586:0.4557899236679077,40589:0.45496344566345215,19311:0.4548722505569458,28095:0.454816997051239,347602:0.4542804956436157,162978:0.4537432789802551,62526:0.45333003997802734,9951:0.4510762691497803,306765:0.450960636138916,100280:0.45086121559143066,78452:0.4500305652618408,183832:0.4497419595718384,336741:0.4493207335472107,18941:0.4491726756095886,258102:0.44812971353530884',\n",
       " '393332:405833,414629|134427:0.477496862411499,106413:0.46904557943344116,101635:0.4682660698890686,115370:0.46781229972839355,150334:0.45805346965789795,35886:0.45638197660446167,129478:0.45387357473373413,49158:0.4535120129585266,360610:0.4527769684791565,57207:0.45234137773513794,139725:0.4517017602920532,120427:0.44931304454803467,128601:0.4484568238258362,155877:0.44762200117111206,98441:0.44533807039260864,73973:0.4424200654029846,248829:0.4408368468284607,116948:0.43958139419555664,38412:0.4394649863243103,49414:0.43815702199935913,298770:0.4371298551559448,216375:0.4340057969093323,140995:0.43226152658462524,49459:0.4319401979446411,49460:0.4319401979446411,65065:0.4288865923881531,223991:0.4274744391441345,401205:0.42634451389312744,170495:0.42439329624176025',\n",
       " '262266:262871|185362:0.5019356608390808,53612:0.491621732711792,149653:0.48456013202667236,25454:0.4825977087020874,62229:0.4823426604270935,172886:0.4764947295188904,45087:0.4697612524032593,116898:0.4692663550376892,72489:0.46889376640319824,67773:0.46704983711242676,51657:0.465843141078949,121537:0.4647104740142822,170857:0.4617761969566345,71456:0.46168053150177,130939:0.4606586694717407,67145:0.4601830244064331,96352:0.45927149057388306,53396:0.4590367078781128,206790:0.4589517116546631,48182:0.45887118577957153,59008:0.457841157913208,53078:0.45672547817230225,36646:0.4566745162010193,194209:0.4563273787498474,100753:0.4561920762062073,16695:0.4561006426811218,76726:0.4560614824295044,72556:0.45590710639953613,146081:0.4552137851715088']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 16995\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_preprocessing_deepQL_topics_10_feature_10epochs_64batch(eclipse)\n"
     ]
    }
   ],
   "source": [
    "print(SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topics_in (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureTopicMlpGenerationModel  (None, 300)          9300        topics_in[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1200)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "                                                                 FeatureTopicMlpGenerationModel[1]\n",
      "==================================================================================================\n",
      "Total params: 161,669,072\n",
      "Trainable params: 1,196,896\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27321\n"
     ]
    }
   ],
   "source": [
    "print(len(exported_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/eclipse/bert/exported_rank_deepQL_topics_10.txt\n"
     ]
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "print(EXPORT_RANK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1 - recall_at_5': 0.19, '4 - recall_at_20': 0.24, '2 - recall_at_10': 0.22, '0 - recall_at_1': 0.14, '3 - recall_at_15': 0.23, '5 - recall_at_25': 0.25}\n"
     ]
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
