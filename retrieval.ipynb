{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.retrieval import Retrieval\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/212512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212512/212512 [00:56<00:00, 3781.70it/s]\n",
      "  0%|          | 510/321483 [00:00<01:02, 5099.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321483/321483 [00:15<00:00, 20220.10it/s]\n",
      "100%|██████████| 39523/39523 [00:01<00:00, 34866.78it/s]\n",
      "12798it [00:00, 64864.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval = Retrieval()\n",
    "\n",
    "path = 'data/processed/eclipse'\n",
    "path_buckets = 'data/normalized/eclipse/eclipse.csv'\n",
    "path_train = 'data/processed/eclipse/train.txt'\n",
    "path_test = 'data/processed/eclipse/test.txt'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_T = 20 # Title\n",
    "MAX_SEQUENCE_LENGTH_D = 200 # Description\n",
    "MAX_SEQUENCE_LENGTH_I = 1682 # Status, Severity, Version, Component, Module\n",
    "\n",
    "# Create the instance from baseline\n",
    "retrieval.baseline = Baseline(path, path_buckets, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "df = pd.read_csv(path_buckets)\n",
    "\n",
    "# Load bug ids\n",
    "retrieval.load_bugs(path, path_train)\n",
    "# Create the buckets\n",
    "retrieval.create_bucket(df)\n",
    "# Read and create the test queries duplicate\n",
    "retrieval.create_queries(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58d8dde14cf4b8b8d491abd8d58ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = {}\n",
    "for bucket in tqdm(retrieval.buckets):\n",
    "    issues_by_buckets[bucket] = bucket\n",
    "    for issue in np.array(retrieval.buckets[bucket]).tolist():\n",
    "        issues_by_buckets[issue] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_a (InputLayer)             (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_a (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_a (InputLayer)             (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_b (InputLayer)             (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_b (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_b (InputLayer)             (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 256)          4471853     info_a[0][0]                     \n",
      "                                                                 info_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 50)           38628600    title_a[0][0]                    \n",
      "                                                                 title_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 256)          39047944    desc_a[0][0]                     \n",
      "                                                                 desc_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 562)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "                                                                 FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "bugs (Concatenate)              (None, 1124)         0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_in[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          288000      bugs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 82,504,751\n",
      "Trainable params: 5,522,281\n",
      "Non-trainable params: 76,982,470\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# Read the siamese model\n",
    "name = 'baseline_classification_100epoch_10steps(eclipse)'\n",
    "retrieval.read_model(name, MAX_SEQUENCE_LENGTH_I, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "retrieval.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "retrieval.train_vectorized, retrieval.test_result = [], []\n",
    "# Infer vector to all train\n",
    "retrieval.read_train(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing train pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70287/70287 [00:00<00:00, 1396022.47it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieval.train_vectorized = []\n",
    "retrieval.infer_vector_train(retrieval.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44739fd9bc7f4de59283c770a3b066af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting only buckets from train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8638a05a6341329256888253a8651f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=70287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buckets_duplicates = [key for key in tqdm(retrieval.buckets) if len(retrieval.buckets[key]) > 1]\n",
    "print(\"Selecting only buckets from train...\")\n",
    "buckets_train = set()\n",
    "for row in tqdm(retrieval.train):\n",
    "    dup_a_id, dup_b_id = row\n",
    "    buckets_train.add(issues_by_buckets[dup_a_id])\n",
    "    buckets_train.add(issues_by_buckets[dup_b_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buckets with at least 2 duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets train: 20938\n",
      "Buckets test: 3475\n",
      "All Buckets: 24413\n"
     ]
    }
   ],
   "source": [
    "print(\"Buckets train:\", len(buckets_train))\n",
    "print(\"Buckets test:\", len(buckets_duplicates) - len(buckets_train))\n",
    "print(\"All Buckets:\", len(buckets_duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to vectorizer all buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "name = 'baseline_1000epoch_10steps_1024batch(eclipse)'\n",
    "similarity_model = Baseline.load_model('', name, {'l2_normalize' : Baseline.l2_normalize})\n",
    "\n",
    "bug_title = similarity_model.get_layer('title_in').input\n",
    "bug_desc = similarity_model.get_layer('desc_in').input\n",
    "bug_info = similarity_model.get_layer('info_in').input\n",
    "\n",
    "title_encoder = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "desc_encoder = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "info_encoder = similarity_model.get_layer('FeatureMlpGenerationModel')\n",
    "\n",
    "bug_t = title_encoder(bug_title)\n",
    "bug_d = desc_encoder(bug_desc)\n",
    "bug_i = info_encoder(bug_info)\n",
    "\n",
    "model = similarity_model.get_layer('merge_features_in')\n",
    "output = model([bug_i, bug_t, bug_d])\n",
    "\n",
    "model = Model(inputs=[bug_title, bug_desc, bug_info], outputs=[output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing all buckets from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bd503715a14643852f804fd89b1918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "buckets_train_vectorized = []\n",
    "for bug_id in tqdm(buckets_train):\n",
    "    bug = bug_set[bug_id]\n",
    "    bug_vector = model.predict([ [bug['title_word']], [bug['description_word']], [retrieval.get_info(bug)] ])[0]\n",
    "    buckets_train_vectorized.append({ 'bug_id' : bug_id, 'vector' : bug_vector })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulding the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting buckets duplicates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aadc000b95e41cabf23125da8753eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting only bugs did not used in the train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07163fa4e6654f6fb23d3984db7ea05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing buckets that did not appear in the train\n",
      "Adding inside the train buckets that were not used \n"
     ]
    }
   ],
   "source": [
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "queries_test = []\n",
    "print(\"Selecting buckets duplicates...\")\n",
    "buckets_duplicates = [key for key in tqdm(retrieval.buckets) if len(retrieval.buckets[key]) > 1]\n",
    "test_no_present_in_trained = []\n",
    "print(\"Selecting only bugs did not used in the train...\")\n",
    "for row in tqdm(retrieval.test):\n",
    "    dup_a_id, dup_b_id = row\n",
    "    diff = list(set(row) - retrieval.bugs_train)\n",
    "    test_no_present_in_trained += diff\n",
    "queries_test = test_no_present_in_trained\n",
    "print(\"Removing buckets that did not appear in the train\")\n",
    "queries_test = [bug_id for bug_id in test_no_present_in_trained if issues_by_buckets[bug_id] != bug_id]\n",
    "print(\"Adding inside the train buckets that were not used \")\n",
    "buckets_added_train = [bug_id for bug_id in test_no_present_in_trained if issues_by_buckets[bug_id] == bug_id]\n",
    "for bug_id in buckets_added_train:\n",
    "    bug = bug_set[bug_id]\n",
    "    bug_vector = model.predict([ [bug['title_word']], [bug['description_word']], [retrieval.get_info(bug)] ])[0]\n",
    "    buckets_train_vectorized.append({ 'bug_id' : bug_id, 'vector' : bug_vector })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of test: 4081\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of test:\", len(queries_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing all test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f43c5e5f0748ec8e1a1b1213cb3daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "queries_test_vectorized = []\n",
    "for bug_id in tqdm(queries_test):\n",
    "    bug = bug_set[bug_id]\n",
    "    bug_vector = model.predict([ [bug['title_word']], [bug['description_word']], [retrieval.get_info(bug)] ])[0]\n",
    "    queries_test_vectorized.append({ 'bug_id' : bug_id, 'vector' : bug_vector })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing all vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "# Indexing all train\n",
    "X = np.array(buckets_train_vectorized)\n",
    "annoy = AnnoyIndex(X[0]['vector'].shape[0])  # Length of item vector that will be indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e1253820be4e009599bd0ec0a04c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24722), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop = tqdm(total=len(X))\n",
    "for index, row in enumerate(X):\n",
    "    vector = row['vector']\n",
    "    annoy.add_item(index, vector)\n",
    "    loop.update(1)\n",
    "loop.close()\n",
    "annoy.build(10) # 10 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval using classication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.25 s, sys: 1.45 ms, total: 1.26 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_test = queries_test_vectorized\n",
    "distance_test, indices_test = [], []\n",
    "for index, row in enumerate(X_test):\n",
    "    vector = row['vector']\n",
    "    rank, dist = annoy.get_nns_by_vector(vector, 30, include_distances=True)\n",
    "    indices_test.append(rank)\n",
    "    distance_test.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total bucekets train vectorized: 24722'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total bucekets train vectorized: {}\".format(len(buckets_train_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using classification model to predict similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2ba99369bc458cbfa8e45762d7a2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16654561), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "loop = tqdm(total=len(indices_test) * len(X_test)) # tqdm(total=len(indices_test))\n",
    "for rank in indices_test:\n",
    "    for train_bug_id in X_test:\n",
    "        test += [(buckets_train_vectorized[index]['bug_id'], train_bug_id['bug_id']) for index in rank]\n",
    "        loop.update(1)\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total queries test: 499636830'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total queries test: {}\".format(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(retrieval, bug, info_cache):\n",
    "        if bug['issue_id'] in info_cache:\n",
    "            return info_cache[bug['issue_id']]\n",
    "        info = np.concatenate((\n",
    "            retrieval.baseline.to_one_hot(bug['bug_severity'], retrieval.baseline.info_dict['bug_severity']),\n",
    "            retrieval.baseline.to_one_hot(bug['bug_status'], retrieval.baseline.info_dict['bug_status']),\n",
    "            retrieval.baseline.to_one_hot(bug['component'], retrieval.baseline.info_dict['component']),\n",
    "            retrieval.baseline.to_one_hot(bug['priority'], retrieval.baseline.info_dict['priority']),\n",
    "            retrieval.baseline.to_one_hot(bug['product'], retrieval.baseline.info_dict['product']),\n",
    "            retrieval.baseline.to_one_hot(bug['version'], retrieval.baseline.info_dict['version']))\n",
    "        )\n",
    "        info_cache[bug['issue_id']] = info\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eafc7305e2e448f9918edd6c453ae8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=499636830), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected info_b to have shape (1682,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2541ebf32168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     sim = retrieval.model.predict([ [bug_a['title_word']], [bug_b['title_word']], \n\u001b[1;32m      9\u001b[0m                                         \u001b[0;34m[\u001b[0m\u001b[0mbug_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbug_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                         [get_info(retrieval, bug_a, info_cache)], [get_info(retrieval, bug_b, info_cache)] ])[0][1]\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdup_b\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrank_test_sorted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrank_test_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdup_b\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected info_b to have shape (1682,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "rank_test_sorted = {}\n",
    "info_cache = {}\n",
    "\n",
    "for row in tqdm(test):\n",
    "    dup_a, dup_b = row\n",
    "    bug_a = bug_set[dup_a]\n",
    "    bug_b = bug_set[dup_b]\n",
    "    sim = retrieval.model.predict([ [bug_a['title_word']], [bug_b['title_word']], \n",
    "                                        [bug_a['description_word']], [bug_b['description_word']],\n",
    "                                        [get_info(retrieval, bug_a, info_cache)], [get_info(retrieval, bug_b, info_cache)] ])[0][1]\n",
    "    if dup_b not in rank_test_sorted:\n",
    "        rank_test_sorted[dup_b] = []\n",
    "\n",
    "    rank_test_sorted[dup_b].append((dup_a, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Total of rank tested: {}\".format(len(rank_test_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering the rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4157ddc5ad49e1bba16a7ce48382e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key in tqdm(rank_test_sorted):\n",
    "    rank = rank_test_sorted[key]\n",
    "    rank_test_sorted[key] = sorted(rank, key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41607b4b4cb4a3d93cd3365b76542bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formated_rank = []\n",
    "for row in tqdm(rank_test_sorted):\n",
    "    rank = rank_test_sorted[row][:20]\n",
    "    formated_rank.append(\",\".join([\"{}:{}\".format(bug, sim) for bug, sim in rank]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b887f9c5f734a428868c8b9202d9436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the rank result\n",
    "rank_queries = []\n",
    "for index, row in tqdm(enumerate(X_test)):\n",
    "    dup_a, dup_b = row['bug_id'], issues_by_buckets[row['bug_id']]\n",
    "    rank_queries.append(\"{}:{}\".format(dup_a, dup_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271c1c5997474d48bad9d124864087fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exported_rank = []\n",
    "loop = tqdm(total=len(X_test))\n",
    "\n",
    "for query, rank in zip(rank_queries, formated_rank):\n",
    "    search, dup = query.split(\":\")\n",
    "    exported_rank.append(\"{}|{}\".format(query, rank))\n",
    "    loop.update(1)\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98309:128463|132787:0.26083698868751526,131246:0.2434113770723343,197865:0.22080236673355103,295382:0.047042302787303925,101094:0.037583835422992706,262561:0.03204227611422539,393332:0.026789871975779533,232063:0.026379309594631195,131180:0.02292243205010891,393277:0.012278357520699501,199241:0.011731269769370556,232304:0.010547742247581482,394517:0.009860608726739883,67031:0.002433638321235776,34454:0.0023417582269757986,198571:0.0022025585640221834,229377:0.0021864583250135183,101023:0.0013441552873700857,166737:0.0011927509913221002,66688:0.001113341422751546']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'exported_rank.txt'), 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "        Rank recall_rate_@k\n",
    "        rank = \"query:master|master:id:sim,master:id:sim\"\n",
    "    \"\"\"\n",
    "    def top_k_recall(self, rank, k):\n",
    "        query, rank = rank.split('|')\n",
    "        query_dup_id, query_master = query.split(\":\")\n",
    "        query_master = int(query_master)\n",
    "        hit = 0\n",
    "        for pos, item in enumerate(rank.split(\",\")[:20]):\n",
    "            if item.strip() == '': continue\n",
    "            master, sim = item.split(':')\n",
    "            master = int(master)\n",
    "            if master == query_master and (pos+1) <= k:\n",
    "                hit=1\n",
    "                return [hit]\n",
    "        return [hit]\n",
    "\n",
    "    def evaluate(self, path):\n",
    "        recall_at_5, recall_at_10, recall_at_15, recall_at_20 = [], [], [], []\n",
    "        total = 0\n",
    "        print(\"Evaluating...\")\n",
    "        with open(path, 'r') as file_input:\n",
    "            for row in file_input:\n",
    "                if row == '': continue\n",
    "                recall_at_5 += self.top_k_recall(row, k=5)\n",
    "                recall_at_10 += self.top_k_recall(row, k=10)\n",
    "                recall_at_15 += self.top_k_recall(row, k=15)\n",
    "                recall_at_20 += self.top_k_recall(row, k=20)\n",
    "                total+=1\n",
    "        \n",
    "        report = {\n",
    "            'recall_at_5' : round(sum(recall_at_5) / total, 4),\n",
    "            'recall_at_10' : round(sum(recall_at_10) / total, 4),\n",
    "            'recall_at_15' : round(sum(recall_at_15) / total, 4),\n",
    "            'recall_at_20' : round(sum(recall_at_20) / total, 4)\n",
    "        }\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'recall_at_10': 0.0,\n",
       " 'recall_at_15': 0.0,\n",
       " 'recall_at_20': 0.0,\n",
       " 'recall_at_5': 0.0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from methods.evaluation import Evaluation\n",
    "evaluation = Evaluation()\n",
    "report = evaluation.evaluate(os.path.join(path, 'exported_rank.txt'))\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_bugs_rank(index):\n",
    "    query, rank = exported_rank[index].split('|')\n",
    "    similar_ids = []\n",
    "    for row in rank.split(','):\n",
    "        master_id, bug_id, sim = row.split(':')\n",
    "        similar_ids.append(bug_id)\n",
    "    df_query = df[df['bug_id'] == int(query.split(':')[0])]\n",
    "    df_similar = df[df['bug_id'].isin(similar_ids)]\n",
    "    return df_query, df_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank(test_labels, tsne_features):\n",
    "    obj_categories = ['anchor', 'positive', 'negative']\n",
    "    groups = [0, 1, 2]\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, 3))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n",
    "        plt.scatter(tsne_features[np.where(test_labels == c_group), 0],\n",
    "                    tsne_features[np.where(test_labels == c_group), 1],\n",
    "                    marker='o',\n",
    "                    color=c_color,\n",
    "                    linewidth='1',\n",
    "                    alpha=0.8,\n",
    "                    label=c_label)\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('t-SNE on Testing Samples')\n",
    "    plt.legend(loc='best')\n",
    "    #plt.savefig('clothes-dist.png')\n",
    "    plt.show(block=False)\n",
    "\n",
    "def display_rank_at_position(index):\n",
    "    query, rank = exported_rank[index].split('|')\n",
    "    query_bug_id = int(query.split(':')[0])\n",
    "    x_test_features = []\n",
    "    x_test_features.append()\n",
    "    tsne_features = Baseline.create_features(x_test_features)\n",
    "    Baseline.plot_2d(valid_sim, tsne_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval.buckets[128463]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query, df_similar = get_similar_bugs_rank(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
