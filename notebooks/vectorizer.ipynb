{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.retrieval import Retrieval\n",
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 200 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 2000\n",
    "\n",
    "retrieval = Retrieval()\n",
    "\n",
    "DOMAIN = 'eclipse'\n",
    "path = 'data/processed/{}'.format(DOMAIN)\n",
    "path_buckets = 'data/normalized/{}/{}.csv'.format(DOMAIN, DOMAIN)\n",
    "path_train = 'data/processed/{}/train.txt'.format(DOMAIN)\n",
    "path_test = 'data/processed/{}/test.txt'.format(DOMAIN)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_T = 20 # Title\n",
    "MAX_SEQUENCE_LENGTH_D = 200 # Description\n",
    "MAX_SEQUENCE_LENGTH_I = 1682 # Status, Severity, Version, Component, Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading the test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bf9be297a4c09aba8538bd21d0bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7f0204f8e6485e9d349082ad2410c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the instance from baseline\n",
    "retrieval.baseline = Baseline(path, path_buckets, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "df = pd.read_csv(path_buckets)\n",
    "\n",
    "# Load bug ids\n",
    "retrieval.load_bugs(path, path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieval.baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(baseline, data, dup_sets, batch_size, n_neg):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets = []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        neg_bug = Baseline.get_neg_bug(dup_sets[data[offset][0]], baseline.bug_ids)\n",
    "        anchor, pos, neg = data[offset][0], data[offset][1], neg_bug\n",
    "        bug_anchor = baseline.bug_set[anchor]\n",
    "        bug_pos = baseline.bug_set[pos]\n",
    "        bug_neg = baseline.bug_set[neg]\n",
    "        baseline.read_batch_bugs(batch_input, bug_anchor)\n",
    "        baseline.read_batch_bugs(batch_pos, bug_pos)\n",
    "        baseline.read_batch_bugs(batch_neg, bug_neg)\n",
    "        batch_triplets.append([data[offset][0], data[offset][1], neg_bug])\n",
    "\n",
    "    batch_input['title'] = np.array(batch_input['title'])\n",
    "    batch_input['desc'] = np.array(batch_input['desc'])\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = np.array(batch_pos['title'])\n",
    "    batch_pos['desc'] = np.array(batch_pos['desc'])\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = np.array(batch_neg['title'])\n",
    "    batch_neg['desc'] = np.array(batch_neg['desc'])\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "\n",
    "    n_half = batch_size // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg = {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test=512\n",
    "batch_triplets, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = batch_iterator(\n",
    "                                                                                          retrieval.baseline,\n",
    "                                                                                          retrieval.baseline.train_data, \n",
    "                                                                                          retrieval.baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[96204, 85581, 15196],\n",
       " [32475, 32465, 371487],\n",
       " [14019, 13487, 17526],\n",
       " [179690, 179684, 383873],\n",
       " [197526, 122833, 417066],\n",
       " [387428, 390154, 111264],\n",
       " [361419, 367263, 225269],\n",
       " [202500, 197620, 334133],\n",
       " [248836, 256052, 75393],\n",
       " [226666, 228481, 235827]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_triplets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese model\n",
    "\n",
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07\n",
    "\n",
    "https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def load_model_disk(DIR, name, dependences):\n",
    "    m_dir = os.path.join(DIR, 'modelos')\n",
    "    # load json and create model\n",
    "    json_file = open(os.path.join(m_dir, \"model_{}.json\".format(name)), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json, dependences)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(os.path.join(m_dir, \"model_{}.h5\".format(name)), by_name=True)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(baseline, bug):\n",
    "    info = np.concatenate((\n",
    "        baseline.to_one_hot(bug['bug_severity'], baseline.info_dict['bug_severity']),\n",
    "        baseline.to_one_hot(bug['bug_status'], baseline.info_dict['bug_status']),\n",
    "        baseline.to_one_hot(bug['component'], baseline.info_dict['component']),\n",
    "        baseline.to_one_hot(bug['priority'], baseline.info_dict['priority']),\n",
    "        baseline.to_one_hot(bug['product'], baseline.info_dict['product']),\n",
    "        baseline.to_one_hot(bug['version'], baseline.info_dict['version']))\n",
    "    )\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "bug_id = [96204, 2]#[269536, 2]\n",
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "dup_a, dup_b = bug_id\n",
    "bug_a = bug_set[dup_a]\n",
    "bug_b = bug_set[dup_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "name = 'checkpoint/checkpoint_baseline_1000epoch_10steps_1024batch({}).hdf5'.format(DOMAIN)\n",
    "\n",
    "similarity_model = load_model(name, {'l2_normalize' : Baseline.l2_normalize, \n",
    "                                     'margin_loss' : Baseline.margin_loss,\n",
    "                                    'pos_distance' : Baseline.pos_distance,\n",
    "                                    'neg_distance' : Baseline.neg_distance})\n",
    "\n",
    "lstm_feature_model = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "cnn_feature_model = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "mlp_feature_model = similarity_model.get_layer('FeatureMlpGenerationModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941076040267944"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_vector_a_t = lstm_feature_model.predict(np.array([bug_a['title_word']]))\n",
    "bug_vector_b_t = lstm_feature_model.predict(np.array([bug_b['title_word']]))\n",
    "result = 1 - spatial.distance.cosine(bug_vector_a_t, bug_vector_b_t)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998556971549988"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_vector_a_d = cnn_feature_model.predict(np.array([bug_a['description_word']]))\n",
    "bug_vector_b_d = cnn_feature_model.predict(np.array([bug_b['description_word']]))\n",
    "result = 1 - spatial.distance.cosine(bug_vector_a_d, bug_vector_b_d)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18021604418754578"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = retrieval.baseline\n",
    "bug_vector_a_i = mlp_feature_model.predict(np.array([get_info(baseline, bug_a)]))\n",
    "bug_vector_b_i = mlp_feature_model.predict(np.array([get_info(baseline, bug_b)]))\n",
    "result = 1 - spatial.distance.cosine(bug_vector_a_i, bug_vector_b_i)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9728338122367859"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_vector_a = np.concatenate([ bug_vector_a_t, bug_vector_a_d, bug_vector_a_i ], -1)\n",
    "bug_vector_b = np.concatenate([ bug_vector_b_t, bug_vector_b_d, bug_vector_b_i ], -1)\n",
    "result = 1 - spatial.distance.cosine(bug_vector_a, bug_vector_b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "name = 'modelos/model_baseline_1000epoch_10steps_1024batch({}).h5'.format(DOMAIN)\n",
    "#similarity_model = load_model('', name, {'l2_normalize' : Baseline.l2_normalize})\n",
    "\n",
    "similarity_model = load_model(name, {'l2_normalize' : Baseline.l2_normalize, \n",
    "                                     'margin_loss' : Baseline.margin_loss,\n",
    "                                    'pos_distance' : Baseline.pos_distance,\n",
    "                                    'neg_distance' : Baseline.neg_distance})\n",
    "\n",
    "bug_title =  similarity_model.get_layer('title_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_T, ), name = 'title')\n",
    "bug_desc =  similarity_model.get_layer('desc_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_D, ), name = 'desc')\n",
    "bug_info = similarity_model.get_layer('info_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_I, ), name = 'info') # \n",
    "\n",
    "title_encoder = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "desc_encoder = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "info_encoder = similarity_model.get_layer('FeatureMlpGenerationModel')\n",
    "\n",
    "bug_t = title_encoder(bug_title)\n",
    "bug_d = desc_encoder(bug_desc)\n",
    "bug_i = info_encoder(bug_info)\n",
    "# Representation layer\n",
    "model = similarity_model.get_layer('merge_features_in')\n",
    "output = model([bug_i, bug_t, bug_d])\n",
    "\n",
    "model = Model(inputs=[bug_title, bug_desc, bug_info], outputs=[output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9727153778076172"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_vector_a = model.predict([ [bug_a['title_word']], [bug_a['description_word']], [retrieval.get_info(bug_a)] ])[0]\n",
    "bug_vector_b = model.predict([ [bug_b['title_word']], [bug_b['description_word']], [retrieval.get_info(bug_b)] ])[0]\n",
    "result = 1 - spatial.distance.cosine(bug_vector_a, bug_vector_b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96204 [0. 0. 0. ... 0. 0. 0.]\n",
      "2 [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36704135776492697"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dup_a, retrieval.get_info(bug_a))\n",
    "print(dup_b, retrieval.get_info(bug_b))\n",
    "1 - spatial.distance.cosine(bug_a['description_word'], bug_b['description_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate cosine between the positive and negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem https://stackoverflow.com/questions/40510703/implement-siamese-network-in-keras-issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test=512\n",
    "valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = retrieval.baseline.batch_iterator(retrieval.baseline.train_data, \n",
    "                                                                                          retrieval.baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "import math\n",
    "\n",
    "# https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
    "def cosine(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "cos_pos_median = []\n",
    "cos_neg_median = []\n",
    "\n",
    "for bug_anchor_title, bug_pos_title, bug_neg_title,\\\n",
    "    bug_anchor_desc, bug_pos_desc, bug_neg_desc,\\\n",
    "    bug_anchor_info, bug_pos_info, bug_neg_info, sim in zip(valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'],\\\n",
    "                                             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\\\n",
    "                                             valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info'], \n",
    "                                            valid_sim):\n",
    "    \n",
    "    bug_vector_anchor = model.predict([ [bug_anchor_title], [bug_anchor_desc], [bug_anchor_info] ])[0]\n",
    "    bug_vector_pos = model.predict([ [bug_pos_title], [bug_pos_desc], [bug_pos_info] ])[0]\n",
    "    bug_vector_neg = model.predict([ [bug_neg_title], [bug_neg_desc], [bug_neg_info] ])[0]\n",
    "    cosine_pos = cosine(bug_vector_anchor, bug_vector_pos)\n",
    "    cosine_neg = cosine(bug_vector_anchor, bug_vector_neg)\n",
    "    cos_pos_median.append(cosine_pos)\n",
    "    cos_neg_median.append(cosine_neg)\n",
    "    #print(\"cosine_pos\", cosine_pos, \"cosine_neg\", cosine_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9696703, 0.9609164)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(cos_pos_median), np.mean(cos_neg_median)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
