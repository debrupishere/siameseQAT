{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'propose_softmax'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41320a75a2f1423dad34da6dc85c6667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57667), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708b47a5d8854d038c6d83020f0b1359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7dd7673aad4db6b0dbd039f5fba06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48b6d33139e479890ac4076cde64975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 27.2 s, sys: 1.08 s, total: 28.3 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd671dcffce4eaa930156cc4f98cf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 19.7 s, sys: 10.3 ms, total: 19.7 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '31\\n',\n",
       " 'creation_ts': '2004-12-12 20:19:00 +0000',\n",
       " 'delta_ts': '2008-01-02 11:37:34 +0000',\n",
       " 'description': 'just run file export as pdf on the attached file on page of the pdf everything after the header is shifted drastically to the upper right',\n",
       " 'description_word': array([  189,   360,    24,   183,    27,   195,    21,     2,   123,\n",
       "           24,    21,    61,    10,     2,   195,   918,    97,     2,\n",
       "          524,     7,  3326, 16358,     4,     2,  1497,   136,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 38869,\n",
       " 'priority': '1\\n',\n",
       " 'product': '14\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'title': 'pdf alignment bug',\n",
       " 'title_word': array([ 195, 1294,  178,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " 'version': '190\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 11043)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 ms, sys: 0 ns, total: 33.3 ms\n",
      "Wall time: 32.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 500), (128, 729), (128,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: menu missing in basic ide\n",
      "***Title***: macros no menu bar for module dialog editing window\n",
      "***Description***: autotest any basic bas only verified on linux since src m the menu in the basic ide is absent\n",
      "***Description***: run soffice open the tools macros organise dialogs new dialog create a new dialog click edit the window opens but it has no menu bar but only two toolbars this can be closed by ctrl w or ctrl f backing window is shown again\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: can not insert a tab comma delimited external text csv file into a spreadsheet\n",
      "***Title***: unable to insert local file\n",
      "***Description***: there seems to be an issue in inserting any all types of external data specially a tab comma delimited data in a txt or other files insert external data click on external data source and select the csv file select filter i select text csv and go thru formating selecting the data columns and click ok get back to the external data windows the ok button is greyed out and can not proceed further to import the data into workseet this bug has been there in as well hope it gets fixed thanx\n",
      "***Description***: i tried for inserting a local text file through insert link to external data but i am unable to do the same steps open a spread sheet go to insert link to external data give local file location eg file home abdul desktop test prese enter nothing is happening ok button is disabled by default\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: copying of formulas fails\n",
      "***Title***: non working word filter for codeweaver cross office\n",
      "***Description***: when copying ctrl c a formulas in the text to another location in the document crtl v a completely different formula was put in the text it seems that the buffer used for copying does not completely get cleaned out or the copied formula is not put in the buffer\n",
      "***Description***: non working word filter for codeweaver cross office\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: can not install under suse\n",
      "***Title***: math objects are shown empty when entering text\n",
      "***Description***: trying to install ooo rc under suse i get the following error message installation starting please be patient glibc version initializing installation program tmp sv tmp setup bin error while loading shared libraries tmp sv tmp libvcl li so undefined symbol ft_activate_size installation completed i have previously installed and beta with no problems\n",
      "***Description***: math objects are shown empty when entering text\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 26.1 ms, sys: 2.1 ms, total: 28.2 ms\n",
      "Wall time: 27.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 76631'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, f.readline().split())\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading FastText\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         embed = list(map(float, tokens[1:]))\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826e30b43e87437d9ea0cb054a723709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8390487f1dba40daae843d9d49d49e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101338), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 24s, sys: 3.32 s, total: 1min 27s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    layer = Flatten()(lstm_layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    bug_anchor_pos = Concatenate()([encoded_anchor, encoded_positive])\n",
    "    bug_anchor_neg = Concatenate()([encoded_anchor, encoded_negative])\n",
    "    positive_d = Dense(2, activation='softmax')(bug_anchor_pos)\n",
    "    negative_d = Dense(2, activation='softmax')(bug_anchor_neg)\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "#     output = Lambda(\n",
    "#         lambda vects: stack_tensors(vects),\n",
    "#         name='stack-distances',\n",
    "#         output_shape=(2, 1)\n",
    "#     )([positive_d, negative_d])\n",
    "\n",
    "    #similarity_layer = Average()([positive_d, negative_d])\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [positive_d, negative_d], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=['binary_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def to_onehot(train_sim):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    train_sim = train_sim.reshape(len(train_sim), 1)\n",
    "    train_sim = onehot_encoder.fit_transform(train_sim)\n",
    "    return train_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          31992100    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          30651492    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1800)         0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1800)         0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            3602        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            3602        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 62,869,796\n",
      "Trainable params: 2,066,996\n",
      "Non-trainable params: 60,802,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 2 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 3 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 4 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 5 Loss positive: 1.40, Loss negative: 0.69\n",
      "Epoch: 6 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 7 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 8 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 9 Loss positive: 1.37, Loss negative: 0.68\n",
      "Epoch: 10 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 11 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 12 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 13 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 14 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 15 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 16 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 17 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 18 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 19 Loss positive: 1.36, Loss negative: 0.69\n",
      "Epoch: 20 Loss positive: 1.39, Loss negative: 0.71\n",
      "Epoch: 21 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 22 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 23 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 24 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 25 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 26 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 27 Loss positive: 1.36, Loss negative: 0.67\n",
      "Epoch: 28 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 29 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 30 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 31 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 32 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 33 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 34 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 35 Loss positive: 1.39, Loss negative: 0.68\n",
      "Epoch: 36 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 37 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 38 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 39 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 40 Loss positive: 1.36, Loss negative: 0.69\n",
      "Epoch: 41 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 42 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 43 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 44 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 45 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 46 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 47 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 48 Loss positive: 1.42, Loss negative: 0.72\n",
      "Epoch: 49 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 50 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 51 Loss positive: 1.36, Loss negative: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 Loss positive: 1.45, Loss negative: 0.72\n",
      "Epoch: 53 Loss positive: 1.42, Loss negative: 0.72\n",
      "Epoch: 54 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 55 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 56 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 57 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 58 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 59 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 60 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 61 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 62 Loss positive: 1.42, Loss negative: 0.71\n",
      "Epoch: 63 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 64 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 65 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 66 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 67 Loss positive: 1.37, Loss negative: 0.68\n",
      "Epoch: 68 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 69 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 70 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 71 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 72 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 73 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 74 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 75 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 76 Loss positive: 1.43, Loss negative: 0.72\n",
      "Epoch: 77 Loss positive: 1.38, Loss negative: 0.68\n",
      "Epoch: 78 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 79 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 80 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 81 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 82 Loss positive: 1.37, Loss negative: 0.68\n",
      "Epoch: 83 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 84 Loss positive: 1.40, Loss negative: 0.69\n",
      "Epoch: 85 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 86 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 87 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 88 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 89 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 90 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 91 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 92 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 93 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 94 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 95 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 96 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 97 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 98 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 99 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 100 Loss positive: 1.39, Loss negative: 0.69, recall@25: 0.55\n",
      "Saved model 'modelos/model_propose_softmax_feature_100epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=0, Best_loss=1.00, Recall@25=0.55\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, bug_train_ids, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "    # binary encode\n",
    "    #train_sim_inverse = to_onehot(train_sim[::-1])\n",
    "    train_sim = to_onehot(train_sim)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=[train_sim, train_sim])\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss positive: {:.2f}, Loss negative: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss positive: {:.2f}, Loss negative: {:.2f}\".format(epoch+1, h[0], h[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|108379:0.4709373712539673,107779:0.45197659730911255,105671:0.44646257162094116,104574:0.4323633313179016,104576:0.4323633313179016,105272:0.43221497535705566,106168:0.4292343258857727,116229:0.42878657579421997,92365:0.42761605978012085,113113:0.42485421895980835,108492:0.4248132109642029,108529:0.4241333603858948,107794:0.4237405061721802,104173:0.4235216975212097,98647:0.42267364263534546,106234:0.4225043058395386,110928:0.42099452018737793,107267:0.41950303316116333,93318:0.41931378841400146,46957:0.4191850423812866,106462:0.4180976152420044,96429:0.4180328845977783,110322:0.4166584014892578,115557:0.41455841064453125,102712:0.41451114416122437,93171:0.4144558906555176,105961:0.41392362117767334,110323:0.4122380018234253,102450:0.4118548035621643',\n",
       " '109674:108544,111059,108379,109366|95277:0.38510167598724365,108529:0.38058340549468994,93288:0.37393200397491455,107845:0.362984836101532,90533:0.3591517210006714,92543:0.3564755916595459,114153:0.356339693069458,93055:0.3460116982460022,94421:0.344473659992218,108390:0.3387978672981262,113509:0.33725112676620483,105082:0.33385562896728516,112299:0.3323691487312317,111514:0.3305119276046753,105511:0.3256669044494629,100665:0.3181632161140442,108377:0.31569379568099976,116199:0.2989009618759155,101734:0.19499021768569946,96062:0.17298495769500732,91101:0.1693912148475647,97574:0.16736412048339844,94523:0.16682064533233643,115608:0.16134274005889893,122603:0.1598171591758728,91251:0.15881752967834473,100472:0.154951274394989,118846:0.15451544523239136,111337:0.15311861038208008',\n",
       " '111059:108544,109674,108379,109366|110555:0.8043714165687561,111297:0.7828231304883957,110274:0.7539116889238358,114280:0.41381680965423584,112520:0.4107440114021301,113881:0.41014087200164795,115068:0.40988242626190186,92690:0.4075911045074463,92117:0.40681779384613037,92348:0.4060025215148926,102450:0.40533769130706787,114022:0.40317201614379883,106337:0.4019087553024292,105272:0.40092283487319946,102471:0.40076589584350586,102557:0.3983042240142822,108492:0.39818137884140015,109853:0.3976130485534668,106234:0.397103488445282,105961:0.3963890075683594,110928:0.3956308960914612,115569:0.39553123712539673,115100:0.3950492739677429,46957:0.39502763748168945,102398:0.3941674828529358,104267:0.39218831062316895,115421:0.3914638161659241,92020:0.39108508825302124,107779:0.39103543758392334',\n",
       " '108379:108544,111059,109674,109366|108492:0.8245321363210678,108544:0.4709373712539673,102712:0.45054787397384644,102450:0.449232816696167,46957:0.4467355012893677,116229:0.43392032384872437,98647:0.4333619475364685,110928:0.43016517162323,92690:0.4298165440559387,92541:0.42939358949661255,108408:0.4287790060043335,107794:0.42805594205856323,108386:0.4276357889175415,92117:0.42703425884246826,114022:0.42671239376068115,115888:0.42562252283096313,104173:0.42543327808380127,111593:0.4254257082939148,106234:0.42501676082611084,104574:0.42306822538375854,104576:0.42306822538375854,104267:0.42284178733825684,104329:0.4226677417755127,115068:0.42263317108154297,113881:0.42189574241638184,102398:0.42188531160354614,113113:0.42181795835494995,106168:0.42107176780700684,112713:0.42038440704345703',\n",
       " '109366:108544,111059,109674,108379|110044:0.7739459723234177,92537:0.4295443892478943,104266:0.4044414162635803,102497:0.4043954610824585,102082:0.4027971029281616,109431:0.4026120901107788,104956:0.40002983808517456,101622:0.39774662256240845,109592:0.39306360483169556,108311:0.3927844762802124,115491:0.39212721586227417,105235:0.3890557289123535,107044:0.3803778886795044,105086:0.3787292242050171,106191:0.37859052419662476,114770:0.3755795955657959,91310:0.3745949864387512,105993:0.3717678189277649,106574:0.37138617038726807,112617:0.3690909743309021,109535:0.3686993718147278,109298:0.3674015402793884,105482:0.36721330881118774,114719:0.3565332889556885,114671:0.3549903631210327,111295:0.34539783000946045,98821:0.3436007499694824,99191:0.32969772815704346,106904:0.2176131010055542',\n",
       " '110594:107073,108355,108453,109162,111761,111800|109162:0.7536064237356186,114529:0.41138601303100586,108453:0.3827019929885864,103471:0.37558722496032715,106479:0.36807894706726074,103827:0.36122846603393555,108084:0.36097514629364014,101462:0.3602619767189026,98310:0.36004889011383057,107073:0.36003750562667847,108439:0.3439459204673767,98331:0.3303704261779785,95494:0.33016854524612427,108240:0.3263665437698364,111761:0.3035017251968384,110445:0.21339726448059082,109165:0.21083849668502808,111235:0.19044506549835205,109861:0.18424522876739502,115685:0.18320298194885254,101366:0.18268775939941406,91337:0.1798701286315918,94284:0.17884796857833862,114956:0.17307090759277344,108932:0.1715906262397766,116590:0.17153054475784302,100328:0.16882848739624023,107802:0.1688089370727539,99097:0.1687890887260437',\n",
       " '111800:107073,110594,108355,108453,109162,111761|102068:0.4446747303009033,105931:0.4445064067840576,108569:0.42792052030563354,97681:0.42665886878967285,57538:0.42553359270095825,56891:0.4232252240180969,90449:0.42170780897140503,100537:0.419044554233551,98278:0.41900408267974854,96212:0.41384440660476685,98948:0.4102843403816223,102556:0.4076220989227295,105594:0.4061840772628784,94719:0.4049804210662842,76670:0.4009799361228943,112864:0.399223268032074,112809:0.39749670028686523,108513:0.39519214630126953,100179:0.3951237201690674,91324:0.39492255449295044,111833:0.3941837549209595,106252:0.39154863357543945,102522:0.38108569383621216,94707:0.37988483905792236,95903:0.3716021776199341,96117:0.37056398391723633,96736:0.3677058219909668,98022:0.3593010902404785,102865:0.35332322120666504',\n",
       " '111761:107073,110594,108355,108453,109162,111800|108453:0.4107043743133545,110712:0.3964788317680359,103827:0.3699682354927063,95494:0.3660372495651245,98310:0.3638303875923157,103471:0.36061275005340576,110992:0.3589794635772705,106479:0.35545140504837036,108240:0.3519015908241272,107073:0.3483654260635376,109162:0.34815406799316406,108439:0.3457835912704468,108084:0.3451420068740845,101462:0.34015852212905884,98331:0.3377288579940796,110594:0.3035017251968384,110795:0.21559780836105347,109681:0.21527260541915894,112472:0.2096347212791443,109630:0.1996748447418213,110123:0.19900184869766235,109729:0.1984984278678894,110365:0.1968982219696045,114956:0.1945369839668274,110468:0.19440722465515137,102447:0.19269710779190063,92473:0.1922367811203003,115520:0.18823492527008057,106804:0.1878466010093689',\n",
       " '109162:107073,110594,108355,108453,111761,111800|110594:0.7536064237356186,114529:0.44474297761917114,106479:0.4205735921859741,108453:0.41476792097091675,98310:0.4004182815551758,103827:0.3989129066467285,108084:0.39661240577697754,108439:0.396583616733551,108240:0.3963770270347595,107073:0.3959869146347046,101462:0.38510018587112427,98331:0.3817620873451233,95494:0.3784211277961731,103471:0.3715359568595886,111761:0.34815406799316406,109165:0.2284153699874878,100328:0.21226191520690918,110445:0.21171563863754272,109861:0.21003395318984985,91337:0.20658040046691895,116590:0.20591837167739868,115685:0.20418953895568848,100327:0.20039820671081543,108932:0.19940423965454102,109698:0.19660699367523193,104930:0.19643527269363403,92473:0.19519156217575073,107802:0.19494295120239258,95926:0.1939188838005066',\n",
       " '108355:107073,110594,108453,109162,111761,111800|107439:0.22262877225875854,114202:0.21050339937210083,106479:0.20364081859588623,105461:0.19515210390090942,106083:0.1864514946937561,109909:0.18605542182922363,115898:0.18392890691757202,110059:0.18057316541671753,109103:0.17608356475830078,98331:0.17188841104507446,109162:0.16979485750198364,108453:0.167497456073761,111761:0.15907365083694458,98310:0.15874582529067993,101462:0.15757697820663452,110938:0.1553693413734436,110594:0.15220510959625244,95494:0.15007507801055908,103471:0.14822691679000854,103827:0.13727706670761108,106817:0.12065941095352173,104930:0.06441307067871094,102766:0.05571913719177246,107802:0.053364574909210205,100678:0.05128961801528931,110118:0.0477026104927063,119633:0.04303097724914551,99097:0.03851741552352905,95926:0.03503137826919556',\n",
       " '108453:107073,110594,108355,109162,111761,111800|98331:0.4225705862045288,109162:0.41476792097091675,95494:0.4137302041053772,111761:0.4107043743133545,101462:0.40649473667144775,103827:0.4059944748878479,106479:0.40594029426574707,98310:0.4028211832046509,103471:0.3837199807167053,110594:0.3827019929885864,107073:0.35847097635269165,108084:0.356117308139801,108439:0.35145753622055054,108240:0.34930360317230225,99097:0.22930192947387695,114529:0.21895575523376465,111235:0.2169148325920105,98394:0.21048259735107422,91337:0.21024125814437866,100328:0.20719778537750244,95380:0.2057042121887207,108018:0.20500439405441284,115685:0.2049471139907837,114956:0.2026458978652954,94063:0.20203357934951782,106804:0.20161640644073486,94284:0.20066499710083008,111946:0.20062190294265747,116590:0.2000921368598938',\n",
       " '114705:114676|105047:0.4588015079498291,98659:0.4356849789619446,116991:0.4281749129295349,115404:0.4266279339790344,102518:0.42139315605163574,105747:0.41575223207473755,113470:0.41455376148223877,105727:0.4076029658317566,110915:0.4027881622314453,105822:0.3954014778137207,95152:0.39463186264038086,105368:0.3883623480796814,113535:0.3849734663963318,112187:0.3833107352256775,109640:0.3719801902770996,116139:0.36641407012939453,108423:0.3655449151992798,93230:0.29564154148101807,93231:0.29564154148101807,99885:0.28782981634140015,97668:0.27751219272613525,94539:0.2712376117706299,107215:0.26385802030563354,115403:0.26381736993789673,95333:0.26223719120025635,98979:0.2605578303337097,95040:0.26050037145614624,102735:0.25992876291275024,96654:0.25932735204696655',\n",
       " '114676:114705|101867:0.4696989059448242,111881:0.45129895210266113,101522:0.4482695460319519,107893:0.44407564401626587,102648:0.44345641136169434,101448:0.44192254543304443,111908:0.44145143032073975,91245:0.43980878591537476,96817:0.4359699487686157,103562:0.4325922131538391,103513:0.4320547580718994,115350:0.429196834564209,110879:0.42834585905075073,115237:0.4277347922325134,110900:0.42737269401550293,106268:0.42281848192214966,108401:0.4225437045097351,106269:0.4205086827278137,112930:0.41981011629104614,112931:0.41981011629104614,112843:0.4181087613105774,112844:0.4181087613105774,104359:0.41697800159454346,104360:0.41697800159454346,113669:0.4142267107963562,113492:0.24305713176727295,115349:0.241654634475708,108191:0.23539382219314575,107842:0.23251807689666748',\n",
       " '110593:110618|110397:0.825199767947197,111172:0.7753565758466721,110388:0.44460511207580566,100353:0.4281589984893799,109950:0.42256855964660645,92892:0.42229729890823364,92620:0.4215806722640991,115496:0.4138103723526001,109435:0.4134209156036377,110392:0.41324925422668457,110956:0.41269683837890625,89825:0.40955162048339844,108408:0.40954941511154175,110375:0.40546441078186035,110485:0.40425312519073486,111681:0.4033438563346863,90421:0.4027079939842224,109486:0.40185850858688354,110343:0.4015554189682007,110346:0.4010341167449951,111642:0.40072721242904663,112277:0.40021228790283203,111651:0.4001631736755371,110366:0.3999844789505005,111673:0.39962834119796753,116027:0.39898622035980225,114881:0.3983011841773987,110012:0.39818400144577026,103790:0.3977988362312317',\n",
       " '110618:110593|112181:0.41836410760879517,90511:0.4138507843017578,96223:0.41281747817993164,93746:0.4098260998725891,92000:0.40941017866134644,89338:0.4087836742401123,91390:0.4050789475440979,90953:0.4040924906730652,90196:0.4040069580078125,97263:0.40351516008377075,110922:0.4031335711479187,112234:0.4003021717071533,91826:0.3992483615875244,90215:0.39897435903549194,91710:0.3985661268234253,93639:0.39803755283355713,115168:0.39799827337265015,110329:0.39767569303512573,110343:0.3966912031173706,89920:0.39500075578689575,94544:0.3942605257034302,109486:0.3942207098007202,89188:0.39410239458084106,106741:0.394064724445343,98768:0.3936840891838074,102250:0.3915807604789734,96115:0.39096981287002563,95031:0.3894588351249695,105276:0.38924598693847656',\n",
       " '102409:102053|102106:0.7134396135807037,101652:0.44220930337905884,106072:0.4398214817047119,90949:0.4350982904434204,106171:0.4307875633239746,99306:0.42506659030914307,101124:0.42199409008026123,97179:0.42119479179382324,95414:0.42011773586273193,97211:0.4181164503097534,97178:0.4174407124519348,96894:0.41712766885757446,96888:0.4165882468223572,93443:0.4157067537307739,96452:0.41457492113113403,96880:0.41171789169311523,96947:0.407664954662323,96945:0.40679246187210083,96471:0.404466450214386,95413:0.4032949209213257,104447:0.40188825130462646,110764:0.40185439586639404,96124:0.3984692096710205,95237:0.39614999294281006,104044:0.3945111632347107,95489:0.3943744897842407,91703:0.38988053798675537,104460:0.3741617798805237,97350:0.3649485111236572',\n",
       " '102053:102409|116288:0.4057340621948242,107236:0.4022105932235718,93913:0.21536511182785034,91126:0.2109106183052063,109858:0.20695894956588745,109067:0.1920541524887085,96739:0.1905025839805603,111603:0.19049793481826782,105631:0.17795813083648682,101932:0.17758393287658691,105054:0.17459791898727417,97095:0.168343186378479,90840:0.16731232404708862,96732:0.16427624225616455,109864:0.15567153692245483,109794:0.15456616878509521,115335:0.15422195196151733,89910:0.13159728050231934,110549:0.10816675424575806,104165:0.08642017841339111,101787:0.07789850234985352,98878:0.0772821307182312,119046:0.07492440938949585,98661:0.07341980934143066,111969:0.06976920366287231,106804:0.0678301453590393,98861:0.06463313102722168,103867:0.06436777114868164,102399:0.06350761651992798',\n",
       " '110604:110612|110612:0.4510951042175293,98005:0.2290412187576294,104153:0.20936709642410278,93035:0.13270092010498047,105334:0.12882190942764282,109067:0.12723398208618164,99626:0.10120576620101929,89691:0.09929871559143066,115318:0.09020006656646729,85170:0.08920502662658691,89195:0.08746343851089478,114166:0.07887548208236694,93922:0.07876676321029663,102447:0.07511758804321289,95413:0.07259184122085571,92895:0.07209491729736328,115928:0.07121914625167847,115317:0.06718337535858154,97780:0.06713235378265381,111603:0.06581348180770874,96739:0.06559044122695923,108601:0.06528037786483765,106670:0.06330424547195435,91771:0.06237471103668213,113361:0.06177163124084473,108198:0.06170666217803955,105209:0.061081647872924805,98661:0.06046926975250244,114811:0.060178160667419434',\n",
       " '110612:110604|111354:0.07723748683929443,111388:0.07162636518478394,91279:0.06150692701339722,107764:0.056343793869018555,93607:0.03920668363571167,100087:0.036917686462402344,94385:0.019923746585845947,97049:0.01977705955505371,92890:0.01930910348892212,92679:0.01871168613433838,111065:0.01733618974685669,101486:0.0172119140625,107824:0.013762176036834717,97894:0.012993454933166504,99610:0.008406877517700195,100676:0.0032075047492980957,92798:-0.003541707992553711,93296:-0.004879474639892578,104038:-0.004932880401611328,93405:-0.010307669639587402,93404:-0.010311245918273926,94947:-0.01644110679626465,91393:-0.02285468578338623,107778:-0.03228437900543213,114049:-0.03312480449676514,102537:-0.03405940532684326,110668:-0.03744351863861084,93364:-0.06346523761749268,99217:-0.06564974784851074',\n",
       " '116738:116938,117027|113225:0.7547352910041809,115793:0.7214632630348206,89397:0.4234781265258789,113106:0.42273271083831787,91795:0.41546106338500977,112520:0.41377002000808716,112680:0.4111446142196655,112379:0.4054688811302185,107833:0.39124155044555664,115767:0.39060550928115845,108414:0.3887172341346741,104697:0.38791072368621826,100713:0.3871750235557556,100714:0.3871750235557556,100715:0.3871750235557556,100716:0.3871750235557556,112804:0.3868452310562134,93220:0.38448989391326904,96594:0.3831232190132141,114280:0.38211536407470703,110930:0.37882810831069946,94580:0.37793290615081787,86455:0.3771311640739441,109937:0.3756507635116577,100846:0.374134361743927,113395:0.3732752799987793,113881:0.37128746509552,116944:0.3709838390350342,93192:0.37035948038101196']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 2086\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_softmax_feature_100epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          31992100    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          30651492    desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 62,862,592\n",
      "Trainable params: 2,059,792\n",
      "Non-trainable params: 60,802,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/exported_rank_propose_softmax.txt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.36,\n",
       " '2 - recall_at_10': 0.44,\n",
       " '3 - recall_at_15': 0.48,\n",
       " '4 - recall_at_20': 0.52,\n",
       " '5 - recall_at_25': 0.55}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
