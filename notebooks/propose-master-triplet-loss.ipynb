{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose Master Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 50 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 50 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'propose_master_triplet_loss'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c16ecf9eb164d5ca772fbf201b611e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f28641ae54af2bf04eb80f9b0741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271822d6a6774a81b07934c0421eab4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54865fd568964ffab86ed3df412ba2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 40.8 s, sys: 4.17 s, total: 45 s\n",
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e170905bf9e478a957a141fbc399168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "\n",
    "import random\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids, issues_by_buckets):\n",
    "    neg_bug = random.choice(list(issues_by_buckets.keys()))\n",
    "    try:\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    except:\n",
    "        invalid_bugs = [invalid_bugs]\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    return neg_bug\n",
    "\n",
    "def batch_iterator(baseline, data, dup_sets, bug_train_ids, batch_size, n_neg, issues_by_buckets):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg, master_batch_input, master_batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []},\\\n",
    "                                                    {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                        {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets = []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        neg_bug = baseline.get_neg_bug(dup_sets[data[offset][0]], bug_train_ids)\n",
    "        anchor, pos, neg = data[offset][0], data[offset][1], neg_bug\n",
    "        bug_anchor = baseline.bug_set[anchor]\n",
    "        bug_pos = baseline.bug_set[pos]\n",
    "        bug_neg = baseline.bug_set[neg]\n",
    "        # master anchor and neg\n",
    "        master_anchor = baseline.bug_set[issues_by_buckets[anchor]]\n",
    "        master_neg = baseline.bug_set[issues_by_buckets[neg]]\n",
    "        \n",
    "        baseline.read_batch_bugs(batch_input, bug_anchor)\n",
    "        baseline.read_batch_bugs(batch_pos, bug_pos)\n",
    "        baseline.read_batch_bugs(batch_neg, bug_neg)\n",
    "        # master anchor and neg\n",
    "        baseline.read_batch_bugs(master_batch_input, master_anchor)\n",
    "        baseline.read_batch_bugs(master_batch_neg, master_neg)\n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([data[offset][0], data[offset][1], neg_bug, master_anchor, master_neg])\n",
    "\n",
    "    batch_input['title'] = np.array(batch_input['title'])\n",
    "    batch_input['desc'] = np.array(batch_input['desc'])\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = np.array(batch_pos['title'])\n",
    "    batch_pos['desc'] = np.array(batch_pos['desc'])\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = np.array(batch_neg['title'])\n",
    "    batch_neg['desc'] = np.array(batch_neg['desc'])\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "    \n",
    "    # master\n",
    "    master_batch_input['title'] = np.array(master_batch_input['title'])\n",
    "    master_batch_input['desc'] = np.array(master_batch_input['desc'])\n",
    "    master_batch_input['info'] = np.array(master_batch_input['info'])\n",
    "    \n",
    "    master_batch_neg['title'] = np.array(master_batch_neg['title'])\n",
    "    master_batch_neg['desc'] = np.array(master_batch_neg['desc'])\n",
    "    master_batch_neg['info'] = np.array(master_batch_neg['info'])\n",
    "\n",
    "    n_half = len(batch_triplets) // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg, master_input_sample, master_neg = {}, {}, {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "    # master \n",
    "    master_input_sample = { 'title' : master_batch_input['title'], 'description' : master_batch_input['desc'], \n",
    "                           'info' : master_batch_input['info'] }\n",
    "    master_neg = { 'title' : master_batch_neg['title'], 'description' : master_batch_neg['desc'], \n",
    "                           'info' : master_batch_neg['info'] }\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, master_input_sample, master_neg, sim #sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = batch_iterator(baseline, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, issues_by_buckets)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f5a713128f4ccca21163945d967163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=173904), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 9s, sys: 4.38 s, total: 2min 14s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 32\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(embedded_sequences)\n",
    "        l_pool = AveragePooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=n_filters * 3, kernel_size=5)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 100\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_LSTM')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    lstm_layer = LSTM(number_lstm_units, return_sequences=True, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(embedded_sequences)\n",
    "    \n",
    "    #lstm_layer = lstm_layer(embedded_sequences)\n",
    "    #lstm_layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    #lstm_layer = Dense(300, activation='tanh')(lstm_layer)\n",
    "    #lstm_layer = GRU(100, activation='tanh', return_sequences=False)(lstm_layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[lstm_layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_bilstm')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "    lstm_layer = Bidirectional(GRU(number_lstm_units, activation='tanh', \n",
    "                                   return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "                               merge_mode='ave')\n",
    "\n",
    "#     lstm_layer = LSTM(number_lstm_units, return_sequences=True, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(embedded_sequences)\n",
    "    \n",
    "    # Attention layer to title\n",
    "    #title_input = title_layer.input\n",
    "    #title_layer = title_layer.output\n",
    "    #shape_lstm = K.int_shape(lstm_layer)\n",
    "    #lstm_layer = Permute((2, 1), input_shape=shape_lstm)(lstm_layer)\n",
    "    #shape_lstm = K.int_shape(title_layer)\n",
    "    #title_layer = Permute((2, 1), input_shape=shape_lstm)(title_layer)\n",
    "    #layer = Dot(axes=1)([lstm_layer, title_layer])\n",
    "    \n",
    "#     layer = LSTM(number_lstm_units, return_sequences=False, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(layer)\n",
    "\n",
    "    layer = lstm_layer(embedded_sequences)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureBiLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        \n",
    "        residual =  block_input\n",
    "        residual = Activation('relu')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "        layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "        layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "        layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(50, activation='relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        skip_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    #bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input, \n",
    "                                 master_anchor.input, master_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = Add()([encoded_anchor, encoded_positive])\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters\n",
    "    master_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance', output_shape=[1])([master_anchor, master_positive])\n",
    "    master_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance', output_shape=[1])([master_anchor, master_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output_bug = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-bug',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    output_master = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_positive_d, master_negative_d])\n",
    "    \n",
    "    output = Average()([output_bug, output_master])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, \n",
    "                                 metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBiLstmGenerationModel (M (None, 50, 50)       52276500    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "                                                                 title_master_in[0][0]            \n",
      "                                                                 title_master_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_in (InputLayer)    (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_neg (InputLayer)   (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "                                                                 info_master_in[0][0]             \n",
      "                                                                 info_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[1][0\n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 1)            105717716   desc_in[0][0]                    \n",
      "                                                                 title_in[0][0]                   \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 desc_neg[0][0]                   \n",
      "                                                                 title_neg[0][0]                  \n",
      "                                                                 desc_master_in[0][0]             \n",
      "                                                                 title_master_in[0][0]            \n",
      "                                                                 desc_master_neg[0][0]            \n",
      "                                                                 title_master_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[2][0\n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_in (InputLayer)     (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_in (InputLayer)     (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_neg (InputLayer)    (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_neg (InputLayer)    (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 351)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 351)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[3][0\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[4][0\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[5][0\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 351)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_in (Conca (None, 351)          0           FeatureMlpGenerationModel[4][0]  \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 351)          0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_neg (Conc (None, 351)          0           FeatureMlpGenerationModel[5][0]  \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (None, 1)            0           merge_features_master_in[0][0]   \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (None, 1)            0           merge_features_master_in[0][0]   \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-bug (Lambda)    (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master (Lambda) (None, 2, 1)         0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2, 1)         0           stack-distances-bug[0][0]        \n",
      "                                                                 stack-distances-master[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 106,222,616\n",
      "Trainable params: 1,880,216\n",
      "Non-trainable params: 104,342,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.90, pos_cosine: 0.95, neg_cosine: 0.84\n",
      "Epoch: 2 Loss: 0.87, pos_cosine: 0.93, neg_cosine: 0.80\n",
      "Epoch: 3 Loss: 0.84, pos_cosine: 0.90, neg_cosine: 0.74\n",
      "Epoch: 4 Loss: 0.80, pos_cosine: 0.89, neg_cosine: 0.69\n",
      "Epoch: 5 Loss: 0.78, pos_cosine: 0.87, neg_cosine: 0.65\n",
      "Epoch: 6 Loss: 0.77, pos_cosine: 0.88, neg_cosine: 0.64\n",
      "Epoch: 7 Loss: 0.75, pos_cosine: 0.86, neg_cosine: 0.61\n",
      "Epoch: 8 Loss: 0.72, pos_cosine: 0.87, neg_cosine: 0.59\n",
      "Epoch: 9 Loss: 0.71, pos_cosine: 0.85, neg_cosine: 0.56\n",
      "Epoch: 10 Loss: 0.71, pos_cosine: 0.85, neg_cosine: 0.55\n",
      "Epoch: 11 Loss: 0.67, pos_cosine: 0.86, neg_cosine: 0.53\n",
      "Epoch: 12 Loss: 0.69, pos_cosine: 0.87, neg_cosine: 0.56\n",
      "Epoch: 13 Loss: 0.66, pos_cosine: 0.87, neg_cosine: 0.54\n",
      "Epoch: 14 Loss: 0.68, pos_cosine: 0.86, neg_cosine: 0.54\n",
      "Epoch: 15 Loss: 0.64, pos_cosine: 0.86, neg_cosine: 0.51\n",
      "Epoch: 16 Loss: 0.67, pos_cosine: 0.85, neg_cosine: 0.52\n",
      "Epoch: 17 Loss: 0.67, pos_cosine: 0.85, neg_cosine: 0.52\n",
      "Epoch: 18 Loss: 0.69, pos_cosine: 0.84, neg_cosine: 0.53\n",
      "Epoch: 19 Loss: 0.66, pos_cosine: 0.85, neg_cosine: 0.51\n",
      "Epoch: 20 Loss: 0.66, pos_cosine: 0.85, neg_cosine: 0.51\n",
      "Epoch: 21 Loss: 0.64, pos_cosine: 0.86, neg_cosine: 0.50\n",
      "Epoch: 22 Loss: 0.69, pos_cosine: 0.85, neg_cosine: 0.54\n",
      "Epoch: 23 Loss: 0.68, pos_cosine: 0.86, neg_cosine: 0.54\n",
      "Epoch: 24 Loss: 0.68, pos_cosine: 0.87, neg_cosine: 0.55\n",
      "Epoch: 25 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.51\n",
      "Epoch: 26 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.51\n",
      "Epoch: 27 Loss: 0.65, pos_cosine: 0.86, neg_cosine: 0.51\n",
      "Epoch: 28 Loss: 0.65, pos_cosine: 0.85, neg_cosine: 0.50\n",
      "Epoch: 29 Loss: 0.65, pos_cosine: 0.86, neg_cosine: 0.51\n",
      "Epoch: 30 Loss: 0.66, pos_cosine: 0.85, neg_cosine: 0.51\n",
      "Epoch: 31 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.51\n",
      "Epoch: 32 Loss: 0.61, pos_cosine: 0.87, neg_cosine: 0.49\n",
      "Epoch: 33 Loss: 0.63, pos_cosine: 0.86, neg_cosine: 0.49\n",
      "Epoch: 34 Loss: 0.62, pos_cosine: 0.87, neg_cosine: 0.49\n",
      "Epoch: 35 Loss: 0.63, pos_cosine: 0.86, neg_cosine: 0.49\n",
      "Epoch: 36 Loss: 0.66, pos_cosine: 0.86, neg_cosine: 0.52\n",
      "Epoch: 37 Loss: 0.65, pos_cosine: 0.87, neg_cosine: 0.52\n",
      "Epoch: 38 Loss: 0.65, pos_cosine: 0.87, neg_cosine: 0.52\n",
      "Epoch: 39 Loss: 0.60, pos_cosine: 0.89, neg_cosine: 0.49\n",
      "Epoch: 40 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.52\n",
      "Epoch: 41 Loss: 0.59, pos_cosine: 0.89, neg_cosine: 0.49\n",
      "Epoch: 42 Loss: 0.63, pos_cosine: 0.86, neg_cosine: 0.49\n",
      "Epoch: 43 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.52\n",
      "Epoch: 44 Loss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 45 Loss: 0.57, pos_cosine: 0.90, neg_cosine: 0.47\n",
      "Epoch: 46 Loss: 0.60, pos_cosine: 0.89, neg_cosine: 0.49\n",
      "Epoch: 47 Loss: 0.63, pos_cosine: 0.88, neg_cosine: 0.51\n",
      "Epoch: 48 Loss: 0.62, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 49 Loss: 0.63, pos_cosine: 0.90, neg_cosine: 0.54\n",
      "Epoch: 50 Loss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 51 Loss: 0.66, pos_cosine: 0.85, neg_cosine: 0.51\n",
      "Epoch: 52 Loss: 0.62, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 53 Loss: 0.59, pos_cosine: 0.89, neg_cosine: 0.48\n",
      "Epoch: 54 Loss: 0.61, pos_cosine: 0.91, neg_cosine: 0.52\n",
      "Epoch: 55 Loss: 0.57, pos_cosine: 0.92, neg_cosine: 0.49\n",
      "Epoch: 56 Loss: 0.61, pos_cosine: 0.87, neg_cosine: 0.48\n",
      "Epoch: 57 Loss: 0.60, pos_cosine: 0.90, neg_cosine: 0.50\n",
      "Epoch: 58 Loss: 0.63, pos_cosine: 0.89, neg_cosine: 0.52\n",
      "Epoch: 59 Loss: 0.58, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 60 Loss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 61 Loss: 0.59, pos_cosine: 0.91, neg_cosine: 0.50\n",
      "Epoch: 62 Loss: 0.62, pos_cosine: 0.92, neg_cosine: 0.54\n",
      "Epoch: 63 Loss: 0.60, pos_cosine: 0.90, neg_cosine: 0.49\n",
      "Epoch: 64 Loss: 0.62, pos_cosine: 0.87, neg_cosine: 0.48\n",
      "Epoch: 65 Loss: 0.62, pos_cosine: 0.89, neg_cosine: 0.50\n",
      "Epoch: 66 Loss: 0.62, pos_cosine: 0.92, neg_cosine: 0.54\n",
      "Epoch: 67 Loss: 0.61, pos_cosine: 0.90, neg_cosine: 0.51\n",
      "Epoch: 68 Loss: 0.56, pos_cosine: 0.92, neg_cosine: 0.48\n",
      "Epoch: 69 Loss: 0.65, pos_cosine: 0.87, neg_cosine: 0.53\n",
      "Epoch: 70 Loss: 0.57, pos_cosine: 0.90, neg_cosine: 0.47\n",
      "Epoch: 71 Loss: 0.64, pos_cosine: 0.86, neg_cosine: 0.51\n",
      "Epoch: 72 Loss: 0.66, pos_cosine: 0.89, neg_cosine: 0.55\n",
      "Epoch: 73 Loss: 0.59, pos_cosine: 0.92, neg_cosine: 0.50\n",
      "Epoch: 74 Loss: 0.60, pos_cosine: 0.89, neg_cosine: 0.50\n",
      "Epoch: 75 Loss: 0.62, pos_cosine: 0.90, neg_cosine: 0.52\n",
      "Epoch: 76 Loss: 0.58, pos_cosine: 0.92, neg_cosine: 0.50\n",
      "Epoch: 77 Loss: 0.64, pos_cosine: 0.87, neg_cosine: 0.51\n",
      "Epoch: 78 Loss: 0.62, pos_cosine: 0.86, neg_cosine: 0.48\n",
      "Epoch: 79 Loss: 0.59, pos_cosine: 0.90, neg_cosine: 0.49\n",
      "Epoch: 80 Loss: 0.60, pos_cosine: 0.90, neg_cosine: 0.50\n",
      "Epoch: 81 Loss: 0.57, pos_cosine: 0.92, neg_cosine: 0.49\n",
      "Epoch: 82 Loss: 0.61, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 83 Loss: 0.61, pos_cosine: 0.92, neg_cosine: 0.53\n",
      "Epoch: 84 Loss: 0.57, pos_cosine: 0.90, neg_cosine: 0.47\n",
      "Epoch: 85 Loss: 0.58, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 86 Loss: 0.63, pos_cosine: 0.90, neg_cosine: 0.53\n",
      "Epoch: 87 Loss: 0.60, pos_cosine: 0.92, neg_cosine: 0.52\n",
      "Epoch: 88 Loss: 0.55, pos_cosine: 0.91, neg_cosine: 0.46\n",
      "Epoch: 89 Loss: 0.57, pos_cosine: 0.91, neg_cosine: 0.48\n",
      "Epoch: 90 Loss: 0.60, pos_cosine: 0.91, neg_cosine: 0.51\n",
      "Epoch: 91 Loss: 0.58, pos_cosine: 0.89, neg_cosine: 0.47\n",
      "Epoch: 92 Loss: 0.61, pos_cosine: 0.89, neg_cosine: 0.50\n",
      "Epoch: 93 Loss: 0.61, pos_cosine: 0.91, neg_cosine: 0.52\n",
      "Epoch: 94 Loss: 0.56, pos_cosine: 0.91, neg_cosine: 0.47\n",
      "Epoch: 95 Loss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 96 Loss: 0.60, pos_cosine: 0.91, neg_cosine: 0.51\n",
      "Epoch: 97 Loss: 0.63, pos_cosine: 0.90, neg_cosine: 0.53\n",
      "Epoch: 98 Loss: 0.60, pos_cosine: 0.91, neg_cosine: 0.50\n",
      "Epoch: 99 Loss: 0.60, pos_cosine: 0.90, neg_cosine: 0.50\n",
      "Epoch: 100 Loss: 0.60, pos_cosine: 0.89, neg_cosine: 0.49, recall@25: 0.51\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(eclipse).h5' to disk\n",
      "Best_epoch=88, Best_loss=0.55, Recall@25=0.51\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_dilated_model(desc_embedding_layer, title_feature_model, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "master_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_in')\n",
    "master_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = batch_iterator(baseline, baseline.train_data, baseline.dup_sets_train, bug_train_ids, batch_size, 1, issues_by_buckets)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info'],\n",
    "                  train_master_input['title'], train_master_input['description'], train_master_input['info'],\n",
    "                  train_master_neg['title'], train_master_neg['description'], train_master_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['327681:324658|324658:0.27195924520492554,411777:0.22257781028747559,396773:0.2167147397994995,415435:0.21363812685012817,366816:0.2121729850769043,366854:0.20301556587219238,326856:0.2024645209312439,326553:0.20181787014007568,386639:0.2013646960258484,412693:0.2002018690109253,422917:0.19991284608840942,416382:0.19975131750106812,396454:0.19882571697235107,335074:0.19796502590179443,350622:0.19767892360687256,390614:0.19766765832901,382251:0.1975499987602234,340509:0.19745266437530518,349802:0.19684088230133057,419469:0.19666951894760132,301623:0.19603610038757324,323714:0.1957383155822754,423262:0.19561082124710083,330866:0.1954137086868286,407957:0.1933695673942566,338114:0.19254660606384277,325671:0.19210469722747803,374285:0.19185179471969604,360277:0.1918412446975708',\n",
       " '324658:327681|327681:0.27195924520492554,322824:0.22683972120285034,319609:0.20662236213684082,328507:0.19532734155654907,386069:0.19233673810958862,326293:0.18514788150787354,409278:0.18476182222366333,411777:0.1833147406578064,388983:0.18312597274780273,343363:0.18278419971466064,388844:0.18206864595413208,354469:0.18206530809402466,384709:0.18184471130371094,337890:0.17976468801498413,359500:0.17946386337280273,407844:0.17900288105010986,325643:0.17890918254852295,357027:0.17859673500061035,362629:0.17820066213607788,407939:0.17689388990402222,330866:0.17561936378479004,335073:0.17533916234970093,359504:0.17530429363250732,325381:0.1751314401626587,332425:0.1729212999343872,324502:0.1713402271270752,360373:0.17130231857299805,385231:0.16974157094955444,406399:0.16939836740493774',\n",
       " '417795:417796,403749|417796:0.9912363849580288,422902:0.5444562435150146,412623:0.5397166311740875,350396:0.5280717611312866,334455:0.5269793570041656,350323:0.5248426496982574,350521:0.5104749500751495,368390:0.4982715845108032,409086:0.47408992052078247,411794:0.4138186573982239,406277:0.4110882878303528,344729:0.38558143377304077,390915:0.3836326599121094,398965:0.38342398405075073,348867:0.38170140981674194,350523:0.37762099504470825,403749:0.37592774629592896,336390:0.3754427433013916,391117:0.3712429404258728,341209:0.37111663818359375,398995:0.3708389401435852,385101:0.3654305934906006,348435:0.3642119765281677,351915:0.3621615171432495,391433:0.35569554567337036,352547:0.35435640811920166,349798:0.3519056439399719,94755:0.3394585847854614,366014:0.3388000726699829',\n",
       " '417796:417795,403749|417795:0.9912363849580288,422902:0.5447207093238831,412623:0.5393016338348389,350396:0.5279555916786194,334455:0.526825487613678,350323:0.5247645676136017,350521:0.5105870366096497,368390:0.4981316328048706,409086:0.4741342067718506,411794:0.4137868285179138,406277:0.41120851039886475,344729:0.3855222463607788,390915:0.3837044835090637,398965:0.38332831859588623,348867:0.3816525340080261,350523:0.3776681423187256,403749:0.37559133768081665,336390:0.3755840063095093,341209:0.3713226914405823,391117:0.371198832988739,398995:0.37097781896591187,385101:0.36563730239868164,348435:0.36422640085220337,351915:0.3621557354927063,391433:0.35561835765838623,352547:0.35434651374816895,349798:0.35206615924835205,94755:0.33955174684524536,366014:0.33886414766311646',\n",
       " '403749:417795,417796|398995:0.8775350823998451,390915:0.7927192449569702,398965:0.787523552775383,411794:0.739995539188385,349798:0.5217050015926361,409086:0.5213399231433868,366014:0.5035302340984344,391433:0.49628227949142456,104395:0.49143826961517334,394467:0.4843485951423645,406277:0.4778580665588379,418692:0.47687023878097534,350396:0.47603267431259155,334455:0.4753442406654358,350521:0.473293662071228,397464:0.46668606996536255,364815:0.46030449867248535,350323:0.45104897022247314,368390:0.4363347887992859,409520:0.39826804399490356,348435:0.3892520070075989,417795:0.37592774629592896,417796:0.37559133768081665,403927:0.3714074492454529,411636:0.36971479654312134,321035:0.3497624397277832,348459:0.3458459973335266,325982:0.3370321989059448,344196:0.3361949920654297',\n",
       " '417796:417795,403749|417795:0.9912363849580288,422902:0.5447207093238831,412623:0.5393016338348389,350396:0.5279555916786194,334455:0.526825487613678,350323:0.5247645676136017,350521:0.5105870366096497,368390:0.4981316328048706,409086:0.4741342067718506,411794:0.4137868285179138,406277:0.41120851039886475,344729:0.3855222463607788,390915:0.3837044835090637,398965:0.38332831859588623,348867:0.3816525340080261,350523:0.3776681423187256,403749:0.37559133768081665,336390:0.3755840063095093,341209:0.3713226914405823,391117:0.371198832988739,398995:0.37097781896591187,385101:0.36563730239868164,348435:0.36422640085220337,351915:0.3621557354927063,391433:0.35561835765838623,352547:0.35434651374816895,349798:0.35206615924835205,94755:0.33955174684524536,366014:0.33886414766311646',\n",
       " '403749:417795,417796|398995:0.8775350823998451,390915:0.7927192449569702,398965:0.787523552775383,411794:0.739995539188385,349798:0.5217050015926361,409086:0.5213399231433868,366014:0.5035302340984344,391433:0.49628227949142456,104395:0.49143826961517334,394467:0.4843485951423645,406277:0.4778580665588379,418692:0.47687023878097534,350396:0.47603267431259155,334455:0.4753442406654358,350521:0.473293662071228,397464:0.46668606996536255,364815:0.46030449867248535,350323:0.45104897022247314,368390:0.4363347887992859,409520:0.39826804399490356,348435:0.3892520070075989,417795:0.37592774629592896,417796:0.37559133768081665,403927:0.3714074492454529,411636:0.36971479654312134,321035:0.3497624397277832,348459:0.3458459973335266,325982:0.3370321989059448,344196:0.3361949920654297',\n",
       " '319495:319752,319435,319915,319471,319895,318297,319514,319515,319517,319551|336302:0.7547609657049179,319435:0.5146615207195282,319515:0.4948878288269043,319915:0.45955514907836914,321669:0.4507623314857483,317862:0.4328082799911499,323459:0.43081939220428467,320758:0.42748498916625977,316183:0.4265896677970886,304857:0.42005670070648193,347887:0.41767972707748413,399608:0.4154086709022522,340374:0.40867334604263306,424120:0.40693461894989014,304678:0.4057878851890564,383212:0.40334880352020264,323107:0.4024336338043213,323601:0.39922118186950684,389594:0.39816439151763916,373174:0.39772945642471313,404182:0.39735811948776245,352801:0.39700162410736084,337670:0.39210283756256104,317763:0.3899695873260498,350988:0.3898268938064575,319119:0.3895289897918701,303904:0.38941752910614014,225780:0.38833189010620117,323416:0.3881378173828125',\n",
       " '319752:319495,319435,319915,319471,319895,318297,319514,319515,319517,319551|304678:0.8212937414646149,348966:0.7844752818346024,323459:0.7773275971412659,304215:0.7649765908718109,379465:0.5138460695743561,382974:0.5063422620296478,389108:0.5044244527816772,424671:0.5025413036346436,405707:0.4995812177658081,411482:0.4976234436035156,392674:0.4844890832901001,225780:0.47801119089126587,364824:0.46435004472732544,390395:0.4635547399520874,411034:0.46308112144470215,319915:0.4602052569389343,318297:0.4553716778755188,406736:0.4552016854286194,417542:0.4531155228614807,390758:0.4497253894805908,319435:0.44744497537612915,340402:0.4461052417755127,376565:0.4454193115234375,361120:0.44409430027008057,336302:0.44342488050460815,378102:0.44212913513183594,384676:0.4412679672241211,389738:0.4395517110824585,392061:0.4372944235801697',\n",
       " '319435:319495,319752,319915,319471,319895,318297,319514,319515,319517,319551|336302:0.6652241349220276,319495:0.5146615207195282,348966:0.4694698452949524,358891:0.4546375274658203,347887:0.45323216915130615,350988:0.45106565952301025,319752:0.44744497537612915,225780:0.443950355052948,309465:0.4432786703109741,389738:0.44184136390686035,392674:0.43393832445144653,411034:0.43364202976226807,315906:0.43257808685302734,322143:0.4305756688117981,382974:0.42427563667297363,352801:0.42277389764785767,323601:0.4194226861000061,390758:0.4147888422012329,390395:0.41363978385925293,319438:0.4097074866294861,310675:0.4094998240470886,301871:0.4066395163536072,411482:0.40515321493148804,364824:0.40489089488983154,321205:0.4041983485221863,310082:0.40085822343826294,316710:0.4005178213119507,379465:0.40049105882644653,405707:0.3991135358810425',\n",
       " '319915:319495,319752,319435,319471,319895,318297,319514,319515,319517,319551|336302:0.5008279383182526,389108:0.48074573278427124,323459:0.47705793380737305,225780:0.4735555052757263,379465:0.46653228998184204,319752:0.4602052569389343,319495:0.45955514907836914,304678:0.45451170206069946,373174:0.4541487693786621,406736:0.45031362771987915,389594:0.4496862292289734,383212:0.44913893938064575,404182:0.4462694525718689,364824:0.4334973692893982,399608:0.42470282316207886,411482:0.41974306106567383,389805:0.4192765951156616,424671:0.4192127585411072,271291:0.4184073805809021,392061:0.41707319021224976,417542:0.41642850637435913,390395:0.4162697196006775,340402:0.41590678691864014,361120:0.41551458835601807,382974:0.4119110107421875,376232:0.40723055601119995,376565:0.40419334173202515,348966:0.40373969078063965,384676:0.4036664366722107',\n",
       " '319471:319495,319752,319435,319915,319895,318297,319514,319515,319517,319551|319517:0.9344002306461334,343947:0.855244979262352,330765:0.822097972035408,340629:0.8205469697713852,348377:0.8168870657682419,333365:0.8019781410694122,317762:0.7572404742240906,348024:0.7559102475643158,318020:0.7547486573457718,330428:0.7544473260641098,393745:0.7495640814304352,325940:0.7464129030704498,319342:0.7462829053401947,332843:0.745143324136734,337415:0.7417415678501129,340181:0.7390958368778229,335751:0.738684743642807,330264:0.7385229468345642,346039:0.73764768242836,329593:0.7372356951236725,351498:0.7371153831481934,322371:0.7366332411766052,320046:0.7363994717597961,353535:0.7359042763710022,348507:0.7354850172996521,346038:0.7348657548427582,334121:0.7344619333744049,325342:0.7344028055667877,354536:0.7341276705265045',\n",
       " '319895:319495,319752,319435,319915,319471,318297,319514,319515,319517,319551|288063:0.608391523361206,401550:0.46100836992263794,418266:0.4527323842048645,312784:0.4507911205291748,321044:0.45061445236206055,317896:0.44750386476516724,323511:0.44540923833847046,322056:0.44530189037323,323322:0.44407033920288086,321239:0.43457168340682983,322158:0.42091792821884155,317757:0.4138578772544861,336894:0.4067862033843994,291696:0.4060155153274536,320383:0.400277316570282,328765:0.3995930552482605,315120:0.39828187227249146,376206:0.3965655565261841,319345:0.3925668001174927,328587:0.3919569253921509,321210:0.3899853825569153,323333:0.38930433988571167,325294:0.37026000022888184,327413:0.36980801820755005,344696:0.3655736446380615,352049:0.3594202995300293,320546:0.35245275497436523,322477:0.3468440771102905,319514:0.338453471660614',\n",
       " '318297:319495,319752,319435,319915,319471,319895,319514,319515,319517,319551|320931:0.8225149661302567,344849:0.7883017659187317,319131:0.7454756200313568,349387:0.6087343990802765,357956:0.6044524908065796,325607:0.6042353212833405,285749:0.6032825112342834,383539:0.6010840237140656,235090:0.5955925583839417,377246:0.5917832553386688,350002:0.5916067957878113,382486:0.5911399722099304,353907:0.5808811485767365,367675:0.5758152604103088,320590:0.5756734311580658,358891:0.5744738578796387,325402:0.5741114914417267,321929:0.5734267234802246,387851:0.5731187164783478,408680:0.5726220607757568,216784:0.5712343454360962,352801:0.5710324048995972,402661:0.5705259442329407,323601:0.5699891746044159,346666:0.5674982964992523,411922:0.5667125880718231,318655:0.5661023855209351,204709:0.5657393336296082,360623:0.565603494644165',\n",
       " '319514:319495,319752,319435,319915,319471,319895,318297,319515,319517,319551|320546:0.7756086140871048,325294:0.7630874365568161,324473:0.6385250985622406,388299:0.5333858132362366,347183:0.5281881392002106,319123:0.4976264238357544,317929:0.49495118856430054,328795:0.47959184646606445,342114:0.47938185930252075,348806:0.4720916152000427,351083:0.4692365527153015,349105:0.45955389738082886,329375:0.4479162096977234,327772:0.44235724210739136,344833:0.4408645033836365,332039:0.4396531581878662,348805:0.4383000135421753,402343:0.43415510654449463,378155:0.433866024017334,369880:0.40852129459381104,322917:0.40818315744400024,393787:0.40410053730010986,365722:0.40053558349609375,344696:0.3957371115684509,358923:0.39226120710372925,288063:0.3908824920654297,393612:0.3893499970436096,409314:0.3690001368522644,319895:0.338453471660614',\n",
       " '319515:319495,319752,319435,319915,319471,319895,318297,319514,319517,319551|347887:0.7420297861099243,336302:0.49796056747436523,319495:0.4948878288269043,356306:0.4596861004829407,334632:0.45947200059890747,325356:0.44601213932037354,349603:0.4356396198272705,411847:0.43488645553588867,321205:0.4285845160484314,320329:0.4141554832458496,395577:0.4048294425010681,413106:0.4046562910079956,368184:0.3936033248901367,347606:0.39339762926101685,380041:0.39053916931152344,319435:0.39021915197372437,382829:0.38813668489456177,368706:0.3833245038986206,390896:0.37328261137008667,380681:0.37221187353134155,375166:0.3713991641998291,357211:0.3701699376106262,359660:0.36206936836242676,396548:0.3593273162841797,403398:0.35288697481155396,321640:0.34938037395477295,385129:0.34804415702819824,349554:0.33691495656967163,378421:0.33462703227996826',\n",
       " '319517:319495,319752,319435,319915,319471,319895,318297,319514,319515,319551|319471:0.9344002306461334,343947:0.8562981486320496,340629:0.8259542882442474,330765:0.8216271549463272,333365:0.8106855750083923,348377:0.8027751296758652,330428:0.7605001628398895,348024:0.7520472407341003,317762:0.751811146736145,318020:0.7503432631492615,353535:0.7389876246452332,393745:0.7379422783851624,351498:0.737843781709671,337415:0.736518919467926,332843:0.7362421751022339,346039:0.7352443933486938,320802:0.7350017428398132,319342:0.7348661124706268,330264:0.7340802848339081,325940:0.7330138087272644,340181:0.7324670255184174,331631:0.7321808040142059,327881:0.7317987680435181,335751:0.7317872643470764,334306:0.7317096889019012,346038:0.7311984002590179,354536:0.731144905090332,338881:0.7311270833015442,325342:0.7310328781604767',\n",
       " '319551:319495,319752,319435,319915,319471,319895,318297,319514,319515,319517|314785:0.6225131154060364,318262:0.62224081158638,374590:0.608505517244339,326746:0.6069382131099701,372120:0.6037298738956451,330561:0.6032631695270538,345289:0.6031957268714905,379609:0.6031897962093353,351018:0.6030674874782562,386394:0.6027006506919861,404837:0.6026574671268463,359044:0.6026467382907867,363822:0.602493405342102,408955:0.6018829643726349,340285:0.6017150282859802,373117:0.6012544333934784,346906:0.60102179646492,330655:0.5997628569602966,323715:0.5995128750801086,321432:0.5990657806396484,371901:0.5981049537658691,373743:0.5978167057037354,359760:0.5973028540611267,348320:0.5967200398445129,333456:0.5964042842388153,398766:0.596342533826828,355317:0.5955913066864014,331901:0.5949893295764923,385103:0.5948235094547272',\n",
       " '401416:401362,401363,401461,401023|374895:0.8706392645835876,333434:0.8515602946281433,387214:0.8403740972280502,411599:0.8322986513376236,380606:0.8046362996101379,333392:0.7278147637844086,330702:0.5112372040748596,361893:0.4950183033943176,362874:0.47783738374710083,422690:0.47696495056152344,347328:0.4762173295021057,350535:0.4761120676994324,348449:0.47557228803634644,325811:0.47034502029418945,418260:0.46999430656433105,347744:0.46818435192108154,360045:0.46486544609069824,389210:0.4630611538887024,359758:0.4630611538887024,367406:0.4623597264289856,358456:0.46111518144607544,366672:0.4580271244049072,323653:0.4569406509399414,384442:0.4567960500717163,384012:0.4558863043785095,423067:0.4546893239021301,380833:0.4546250104904175,345104:0.45363181829452515,379376:0.4528343677520752',\n",
       " '401362:401416,401363,401461,401023|392494:0.5171352326869965,419841:0.4614931344985962,412266:0.43082284927368164,321166:0.39043593406677246,199110:0.37757807970046997,408099:0.36112362146377563,399608:0.3404361605644226,382112:0.2955509424209595,411847:0.2785598635673523,403339:0.2731279730796814,406736:0.2688269019126892,402661:0.262478232383728,408680:0.26203954219818115,383614:0.260272741317749,400886:0.25939875841140747,413091:0.25843483209609985,411482:0.2582159638404846,407863:0.2567892074584961,395577:0.2564576268196106,408266:0.2537934184074402,398264:0.24943852424621582,397836:0.2406601905822754,390395:0.23266226053237915,389805:0.23050707578659058,396486:0.2281208038330078,382829:0.21949142217636108,407749:0.21577191352844238,402884:0.21247327327728271,389913:0.2092779278755188']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 4641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_100epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBiLstmGenerationModel (M (None, 50, 50)       52276500    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[1][0\n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 1)            105717716   desc_in[0][0]                    \n",
      "                                                                 title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 351)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "==================================================================================================\n",
      "Total params: 158,499,116\n",
      "Trainable params: 1,985,516\n",
      "Non-trainable params: 156,513,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/exported_rank_propose_master_triplet_loss.txt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.32,\n",
       " '2 - recall_at_10': 0.4,\n",
       " '3 - recall_at_15': 0.45,\n",
       " '4 - recall_at_20': 0.49,\n",
       " '5 - recall_at_25': 0.51}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
