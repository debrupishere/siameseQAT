{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'propose_softmax'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e25ca8f371744abb3d50fd001f584d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83503), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5f1db0abd04c7ba7d074cf0cb39b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98070"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c2ffc161634797b2f6e3913e6470b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432eb8a5bb1b4015b907c0ee6f0b396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 15.4 s, sys: 1.08 s, total: 16.5 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e86444ff24746cf867ff5ef1e252b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83503), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 29.3 s, sys: 3.27 ms, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '128\\n',\n",
       " 'creation_ts': '2005-07-19 22:28:00 +0000',\n",
       " 'delta_ts': '2013-08-07 14:42:16 +0000',\n",
       " 'description': 'in the file attached there is a large table spread onto pages with the number rows selected to repeat at top when cells on the number row are merged the heading row moves somewhere in the document leaving its normal position after saving and restoring the problem is solved but its rather stressing working like this',\n",
       " 'description_word': array([   6,    2,   24,  150,   75,   10,    4,  728,   73, 2346, 1970,\n",
       "         350,   19,    2,   30,  565,  300,    5, 1395,   59,  493,   35,\n",
       "         283,   23,    2,   30,  347,   46, 1344,    2,  647,  347, 1889,\n",
       "        1471,    6,    2,   37, 1994,  420,  637,  501,   98,  374,    9,\n",
       "        5676,    2,   82,   10, 2334,   45,  420,  980,    1,  408,  140,\n",
       "          17,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 52189,\n",
       " 'priority': '3\\n',\n",
       " 'product': '0\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'title': 'merge cell desrupting view',\n",
       " 'title_word': array([596, 136,   1, 228,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " 'version': '193\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 12268)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 ms, sys: 3.81 ms, total: 31.2 ms\n",
      "Wall time: 30.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 100), (128, 738), (128,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: column to text\n",
      "***Title***: person feature under data text to columns\n",
      "***Description***: unable to do column to text conversion\n",
      "***Description***: lets you parse a column similarly to importing a csv file into a spreadsheet this feature lets you do an individual column and seperate it out i can not find this feature in calc we use this feature quite regularly thanks\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: word processor crashes while trying to insert a table\n",
      "***Title***: program crashes when user tries to enter to column size textbox in the insert table dialogue box\n",
      "***Description***: the open office crashes when a product â œnumber product â is entered in the rows field and then the product â œtab product â key is pressed followed by product â œshift tab product â keys and the product â œ delete product â key on the keyboard artwork service pack mb ram organization installed on a network drive available hard disk space total hard disk space mb start open office word processor product â a new unnamed file is created from the product â insert product â menu select product â product â option product â table product â product â option in the resulting product â insert table product â window artwork the number in the product â œ rows product â field and enter product â product â in it now hit product â œ tab product â on the key board it is observable that the product â œ header product â check box get selected hit product â œ shift tab product â on the key board it is noticeable that product â œ rows product â field gets highlighted hit the product â œ delete product â key on the keyboard a dialog box saying product â œ an unrecoverable error has occurred product â pops up press product â œ okproduct â button of the dialog box a dialog box with error message saying product â œ the exception product division by number product â pops up press product â œ okproduct â button of the dialog box a dialog box saying product â œ an unrecoverable error has occurred product â pops up press product â œ okproduct â button of the dialog box a dialog box with error message saying product â œ the instruction at product â œ x product â referenced memory at product â œ x product â the memory could not be product â œread product â product â pops up press product â œ okproduct â button of the dialog box and we can observe that the open office application closes off by itself or crashes follow up test start open office word processor product â a new unnamed file is created from the product â insert product â menu select product â product â option product â table product â product â option in the resulting product â insert table product â window artwork the number in the product â œ columns product â field and enter product â product â in it now hit product â œ tab product â on the key board it is observable that the product â œ rows product â field get selected hit product â œ shift tab product â on the key board it is noticeable that product â œ columns product â field gets highlighted hit the product â œ delete product â key on the keyboard a dialog box saying product â œ an unrecoverable error has occurred product â pops up press product â œ okproduct â button of the dialog box a dialog box with error message saying product â œ the exception product division by number product â pops up press product â œ okproduct â button of the dialog box a dialog box saying product â œ an unrecoverable error has occurred product â pops up press product â œ okproduct â button of the dialog box a dialog box with error message saying product â œ the instruction at product â œ x product â referenced memory at product â œ x product â the memory could not be product â œread product â product â pops up press product â œ okproduct â button of the dialog box and we can observe that the open office application closes off by itself or crashes\n",
      "***Description***: reduplication steps create a new text document in oo click the insert table icon on the organization floating bar this opens an insert table dialogue box clear default value and then enter from keyboard to column size text box move cursor away is automatically changed to move cursor to column size text box again and try to clear a warning message pops up an unrecoverable error has occurred all modified files have been saved and probably be recovered at program start press ok a number warning message pops up organization has caused an error in swe mi dll organization will now close if you continue to experience problems try restarting your computer close debug i reproduced this bug on both organization and organization on organization the number warning message is about division by number the exception nationality division by number x occurred in the application at location x e ed ok cancel the instruction at x reference memory at x the memory could not be read ok cancel\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: high latency of www openoffice org causes the timeout\n",
      "***Title***: password protection open edit\n",
      "***Description***: i have a business client that has a hard time reaching the openoffice org website due to very high latency in receiving replies from www openoffice org the client uses organization cable broadband and organization organization broadband organization consistently is over time on receiving any sort of response from the website i have spoken with tech support staff from both braodband companies about this issue and they assure me that the problem does not lie with them or their dns servers my own tests have confirmed this because i am able to reach other websites from the clients computers i have observed the same problems from my home organization adsl connection at times it takes number or number refreshes for the website to appear both myself and my business client are located in country zip code\n",
      "***Description***: password protection open edit\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: assertion when displaying certain product object\n",
      "***Title***: pdf export should create an index bookmarks\n",
      "***Description***: open document from issue error ungueltiger index bei const organization auf xpolygon from file o src src svx source xoutdev xpoly cxx at line\n",
      "***Description***: pdf export should create an index bookmarks\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 28.7 ms, sys: 24 µs, total: 28.7 ms\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 131563'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46661d38667242c88d67f492eb2ee847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d58cfe23114aeeb6e285db59288be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=131563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min, sys: 2.88 s, total: 2min 3s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    layer = Flatten()(lstm_layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    bug_anchor_pos = Concatenate()([encoded_anchor, encoded_positive])\n",
    "    bug_anchor_neg = Concatenate()([encoded_anchor, encoded_negative])\n",
    "    positive_d = Dense(2, activation='softmax')(bug_anchor_pos)\n",
    "    negative_d = Dense(2, activation='softmax')(bug_anchor_neg)\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "#     output = Lambda(\n",
    "#         lambda vects: stack_tensors(vects),\n",
    "#         name='stack-distances',\n",
    "#         output_shape=(2, 1)\n",
    "#     )([positive_d, negative_d])\n",
    "\n",
    "    #similarity_layer = Average()([positive_d, negative_d])\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [positive_d, negative_d], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=['binary_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def to_onehot(train_sim):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    train_sim = train_sim.reshape(len(train_sim), 1)\n",
    "    train_sim = onehot_encoder.fit_transform(train_sim)\n",
    "    return train_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          41059600    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          39718992    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1800)         0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1800)         0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            3602        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            3602        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 81,007,496\n",
      "Trainable params: 2,069,696\n",
      "Non-trainable params: 78,937,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 2 Loss positive: 4.85, Loss negative: 3.97\n",
      "Epoch: 3 Loss positive: 2.51, Loss negative: 1.22\n",
      "Epoch: 4 Loss positive: 2.31, Loss negative: 1.59\n",
      "Epoch: 5 Loss positive: 2.36, Loss negative: 1.37\n",
      "Epoch: 6 Loss positive: 1.50, Loss negative: 0.73\n",
      "Epoch: 7 Loss positive: 2.14, Loss negative: 1.31\n",
      "Epoch: 8 Loss positive: 1.86, Loss negative: 0.99\n",
      "Epoch: 9 Loss positive: 1.38, Loss negative: 0.67\n",
      "Epoch: 10 Loss positive: 1.62, Loss negative: 0.87\n",
      "Epoch: 11 Loss positive: 1.74, Loss negative: 0.95\n",
      "Epoch: 12 Loss positive: 1.60, Loss negative: 0.85\n",
      "Epoch: 13 Loss positive: 1.50, Loss negative: 0.76\n",
      "Epoch: 14 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 15 Loss positive: 1.42, Loss negative: 0.72\n",
      "Epoch: 16 Loss positive: 1.50, Loss negative: 0.79\n",
      "Epoch: 17 Loss positive: 1.52, Loss negative: 0.78\n",
      "Epoch: 18 Loss positive: 1.47, Loss negative: 0.74\n",
      "Epoch: 19 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 20 Loss positive: 1.45, Loss negative: 0.73\n",
      "Epoch: 21 Loss positive: 1.42, Loss negative: 0.70\n",
      "Epoch: 22 Loss positive: 1.51, Loss negative: 0.77\n",
      "Epoch: 23 Loss positive: 1.43, Loss negative: 0.73\n",
      "Epoch: 24 Loss positive: 1.42, Loss negative: 0.74\n",
      "Epoch: 25 Loss positive: 1.42, Loss negative: 0.73\n",
      "Epoch: 26 Loss positive: 1.46, Loss negative: 0.73\n",
      "Epoch: 27 Loss positive: 1.37, Loss negative: 0.68\n",
      "Epoch: 28 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 29 Loss positive: 1.41, Loss negative: 0.72\n",
      "Epoch: 30 Loss positive: 1.44, Loss negative: 0.73\n",
      "Epoch: 31 Loss positive: 1.43, Loss negative: 0.74\n",
      "Epoch: 32 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 33 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 34 Loss positive: 1.44, Loss negative: 0.70\n",
      "Epoch: 35 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 36 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 37 Loss positive: 1.42, Loss negative: 0.70\n",
      "Epoch: 38 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 39 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 40 Loss positive: 1.44, Loss negative: 0.73\n",
      "Epoch: 41 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 42 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 43 Loss positive: 1.37, Loss negative: 0.70\n",
      "Epoch: 44 Loss positive: 1.38, Loss negative: 0.67\n",
      "Epoch: 45 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 46 Loss positive: 1.42, Loss negative: 0.73\n",
      "Epoch: 47 Loss positive: 1.42, Loss negative: 0.71\n",
      "Epoch: 48 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 49 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 50 Loss positive: 1.47, Loss negative: 0.72\n",
      "Epoch: 51 Loss positive: 1.43, Loss negative: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 Loss positive: 1.43, Loss negative: 0.70\n",
      "Epoch: 53 Loss positive: 1.37, Loss negative: 0.69\n",
      "Epoch: 54 Loss positive: 1.43, Loss negative: 0.72\n",
      "Epoch: 55 Loss positive: 1.42, Loss negative: 0.70\n",
      "Epoch: 56 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 57 Loss positive: 1.36, Loss negative: 0.66\n",
      "Epoch: 58 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 59 Loss positive: 1.39, Loss negative: 0.69\n",
      "Epoch: 60 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 61 Loss positive: 1.42, Loss negative: 0.70\n",
      "Epoch: 62 Loss positive: 1.39, Loss negative: 0.70\n",
      "Epoch: 63 Loss positive: 1.44, Loss negative: 0.71\n",
      "Epoch: 64 Loss positive: 1.45, Loss negative: 0.73\n",
      "Epoch: 65 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 66 Loss positive: 1.41, Loss negative: 0.71\n",
      "Epoch: 67 Loss positive: 1.43, Loss negative: 0.72\n",
      "Epoch: 68 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 69 Loss positive: 1.44, Loss negative: 0.70\n",
      "Epoch: 70 Loss positive: 1.40, Loss negative: 0.72\n",
      "Epoch: 71 Loss positive: 1.42, Loss negative: 0.72\n",
      "Epoch: 72 Loss positive: 1.40, Loss negative: 0.71\n",
      "Epoch: 73 Loss positive: 1.39, Loss negative: 0.71\n",
      "Epoch: 74 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 75 Loss positive: 1.38, Loss negative: 0.68\n",
      "Epoch: 76 Loss positive: 1.44, Loss negative: 0.71\n",
      "Epoch: 77 Loss positive: 1.42, Loss negative: 0.71\n",
      "Epoch: 78 Loss positive: 1.35, Loss negative: 0.69\n",
      "Epoch: 79 Loss positive: 1.39, Loss negative: 0.71\n",
      "Epoch: 80 Loss positive: 1.40, Loss negative: 0.69\n",
      "Epoch: 81 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 82 Loss positive: 1.44, Loss negative: 0.73\n",
      "Epoch: 83 Loss positive: 1.36, Loss negative: 0.68\n",
      "Epoch: 84 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 85 Loss positive: 1.43, Loss negative: 0.71\n",
      "Epoch: 86 Loss positive: 1.44, Loss negative: 0.71\n",
      "Epoch: 87 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 88 Loss positive: 1.41, Loss negative: 0.69\n",
      "Epoch: 89 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 90 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 91 Loss positive: 1.46, Loss negative: 0.72\n",
      "Epoch: 92 Loss positive: 1.44, Loss negative: 0.71\n",
      "Epoch: 93 Loss positive: 1.40, Loss negative: 0.70\n",
      "Epoch: 94 Loss positive: 1.38, Loss negative: 0.69\n",
      "Epoch: 95 Loss positive: 1.37, Loss negative: 0.67\n",
      "Epoch: 96 Loss positive: 1.38, Loss negative: 0.70\n",
      "Epoch: 97 Loss positive: 1.41, Loss negative: 0.70\n",
      "Epoch: 98 Loss positive: 1.44, Loss negative: 0.72\n",
      "Epoch: 99 Loss positive: 1.37, Loss negative: 0.68\n",
      "Epoch: 100 Loss positive: 1.43, Loss negative: 0.72, recall@25: 0.51\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=0, Best_loss=1.00, Recall@25=0.51\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, bug_train_ids, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "    # binary encode\n",
    "    #train_sim_inverse = to_onehot(train_sim[::-1])\n",
    "    train_sim = to_onehot(train_sim)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=[train_sim, train_sim])\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss positive: {:.2f}, Loss negative: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss positive: {:.2f}, Loss negative: {:.2f}\".format(epoch+1, h[0], h[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|108379:0.720730185508728,115065:0.7185792326927185,92188:0.7152953743934631,115064:0.7152141034603119,102788:0.7149108350276947,92117:0.7136993706226349,113894:0.7120520770549774,115421:0.710508406162262,106962:0.7094748914241791,112520:0.7059905529022217,105166:0.7016399502754211,105087:0.6967573463916779,98212:0.6959536075592041,108276:0.6948375403881073,104267:0.6943702697753906,113893:0.6941440105438232,93169:0.6919395327568054,108492:0.6918684542179108,114280:0.6916755735874176,112713:0.6906169354915619,115634:0.6883853673934937,110928:0.6824096441268921,98647:0.6809911131858826,94033:0.6764324903488159,114022:0.6730243563652039,102470:0.6666096746921539,101268:0.665512353181839,111277:0.6649572253227234,102712:0.6640356183052063',\n",
       " '109674:108544,111059,108379,109366|94815:0.6929844617843628,112693:0.6634054183959961,108377:0.6578225195407867,112299:0.6565679609775543,111514:0.6532493233680725,93055:0.6531805098056793,100665:0.6490773260593414,94421:0.6331706941127777,116199:0.629844605922699,95162:0.6271050572395325,92796:0.6269939243793488,103031:0.6268646419048309,115569:0.6267204582691193,104148:0.6265116631984711,102557:0.6260197162628174,96062:0.6230756938457489,105272:0.6226874589920044,115888:0.6212084293365479,91999:0.6199069619178772,110555:0.619385153055191,110669:0.6193590462207794,118846:0.6181731522083282,102003:0.6179994940757751,99982:0.615399032831192,100490:0.6153640449047089,104155:0.6150064468383789,102471:0.6141020655632019,97026:0.6131737232208252,111990:0.6126205921173096',\n",
       " '111059:108544,109674,108379,109366|110555:0.7849155962467194,110274:0.7522030621767044,113630:0.7106382250785828,99982:0.7057762742042542,109853:0.7036482393741608,65064:0.6995401978492737,106234:0.6978227496147156,106337:0.690877765417099,97026:0.689779669046402,112693:0.6890487968921661,105671:0.6858386099338531,93562:0.6834243834018707,99981:0.6825739741325378,114310:0.6822323501110077,115068:0.6813772320747375,106462:0.6808021366596222,94815:0.6763056814670563,100394:0.6761310696601868,104173:0.6716556549072266,102471:0.671442836523056,115569:0.6713533401489258,91999:0.6701603829860687,107473:0.6699563562870026,104329:0.6668857038021088,115888:0.6643997132778168,105272:0.6629533171653748,116229:0.6612116992473602,105907:0.6597857177257538,95800:0.6594395339488983',\n",
       " '108379:108544,111059,109674,109366|108492:0.7461613714694977,108544:0.720730185508728,98212:0.7107301354408264,112713:0.7056558132171631,108276:0.7031363248825073,113894:0.6997916400432587,105166:0.6928226053714752,102788:0.6917978525161743,115421:0.6907140612602234,104267:0.6899444758892059,113893:0.6852158606052399,108386:0.6836150884628296,115634:0.68259397149086,89456:0.6747208535671234,115065:0.6742300093173981,93177:0.6734837591648102,115064:0.6717104315757751,102470:0.6713310182094574,114280:0.6677634716033936,92541:0.6677297353744507,102712:0.6676589548587799,108408:0.6654138267040253,110553:0.6651539206504822,110554:0.6651539206504822,92188:0.6598516404628754,105087:0.6598198413848877,93171:0.659604012966156,92117:0.6541300714015961,112520:0.6533893942832947',\n",
       " '109366:108544,111059,109674,108379|110044:0.687328040599823,106191:0.6708239614963531,114719:0.6705430150032043,108311:0.6665928363800049,106591:0.6576475501060486,99982:0.6515780091285706,108943:0.6443786919116974,107249:0.638420045375824,114310:0.6367447674274445,113313:0.6362331211566925,110555:0.6347844004631042,106574:0.6340473592281342,109535:0.6337754428386688,116227:0.6330715715885162,108012:0.6310722827911377,105671:0.631058931350708,105235:0.6295974552631378,106337:0.6294675469398499,65064:0.6292024254798889,110023:0.6291640102863312,96918:0.6259871125221252,111641:0.6235189735889435,111059:0.6213978826999664,118884:0.6209155023097992,99981:0.6205797493457794,101552:0.6184543967247009,105272:0.6174114346504211,100394:0.6166602671146393,94815:0.6157797873020172',\n",
       " '110594:107073,108355,108453,109162,111761,111800|109162:0.712530791759491,111761:0.6792530715465546,106479:0.6703014373779297,107073:0.6667959690093994,110256:0.6451137959957123,114529:0.6445034146308899,108439:0.627536416053772,108453:0.6176614463329315,102442:0.6135383546352386,107802:0.6061671078205109,93258:0.6052330136299133,114077:0.6044606864452362,110012:0.6034616827964783,95778:0.602309912443161,96099:0.5992827713489532,98072:0.5989887118339539,99097:0.5985269546508789,92387:0.5982502102851868,110005:0.5982325971126556,104964:0.598130613565445,90736:0.5973528921604156,99842:0.5968241989612579,107749:0.5967149436473846,110118:0.5965422093868256,98015:0.595751017332077,115419:0.5952585935592651,114811:0.593940794467926,90468:0.59286829829216,109698:0.5909684598445892',\n",
       " '111800:107073,110594,108355,108453,109162,111761|100537:0.6837661266326904,91324:0.6709328889846802,95999:0.6700814962387085,105594:0.6672542989253998,76670:0.6655164659023285,112598:0.6591587960720062,102556:0.6555158793926239,112864:0.6515327095985413,96212:0.6449097394943237,92385:0.6445729732513428,93172:0.641222357749939,96038:0.6394586563110352,120037:0.6372917294502258,94076:0.6367990374565125,78260:0.6331508755683899,110976:0.6302635371685028,98022:0.6295599639415741,94253:0.6251119077205658,89305:0.6242924332618713,95240:0.6241880357265472,108569:0.624011904001236,114322:0.6238081455230713,96142:0.62357497215271,99452:0.6233454942703247,101672:0.6217780709266663,105208:0.6214678287506104,115522:0.6205740571022034,89188:0.6197056472301483,116481:0.6183875799179077',\n",
       " '111761:107073,110594,108355,108453,109162,111800|110594:0.6792530715465546,110012:0.6734916269779205,106479:0.6716795861721039,107073:0.6716589629650116,109162:0.6601143479347229,110992:0.6483778059482574,110005:0.6461462378501892,108453:0.6339582800865173,108439:0.6262485086917877,111673:0.6237823367118835,111758:0.6198180019855499,95494:0.6157535314559937,110256:0.614653468132019,107167:0.6122828722000122,90736:0.6105430126190186,104483:0.6088299453258514,111946:0.60824054479599,96099:0.6057421267032623,96871:0.6047454476356506,98072:0.6044066250324249,97796:0.6038725674152374,109729:0.6036933660507202,111444:0.6030844748020172,93258:0.6024112701416016,104195:0.6022060811519623,106207:0.6018247604370117,98331:0.6013815999031067,102442:0.6007525026798248,108318:0.6006322801113129',\n",
       " '109162:107073,110594,108355,108453,111761,111800|110594:0.712530791759491,108453:0.6631086766719818,111761:0.6601143479347229,108439:0.6585976481437683,110256:0.6455656588077545,107073:0.6451843082904816,96099:0.6315431892871857,106479:0.6307677328586578,95494:0.6261750161647797,97796:0.6229362189769745,98072:0.6226740479469299,93258:0.6221591830253601,109698:0.6204939186573029,109647:0.620173841714859,104483:0.6174316704273224,107167:0.6150225102901459,114529:0.6138299703598022,98331:0.6133427321910858,96371:0.6083205044269562,104964:0.6081437170505524,102442:0.607333242893219,110118:0.6063508093357086,98284:0.6052741408348083,110555:0.6049181520938873,95778:0.604041337966919,97736:0.6039992868900299,90736:0.6034253239631653,90468:0.6025556921958923,98015:0.6002171039581299',\n",
       " '108355:107073,110594,108453,109162,111761,111800|103827:0.5432530045509338,108240:0.5398648083209991,114938:0.5380466282367706,108084:0.5354150235652924,107049:0.5199861526489258,90800:0.518951803445816,98310:0.511464923620224,110059:0.5111753940582275,110230:0.5102017819881439,104447:0.5098095834255219,102114:0.5094907879829407,106817:0.5041980743408203,92902:0.5038641691207886,45851:0.5038459897041321,114730:0.5038051903247833,100158:0.5035927593708038,105768:0.50339075922966,105051:0.5025038421154022,102142:0.5010916292667389,110657:0.501079797744751,92537:0.5006275475025177,112617:0.5001137554645538,110938:0.49862444400787354,106523:0.497700035572052,110146:0.4976256489753723,110147:0.4976256489753723,114123:0.4970574378967285,95421:0.4962950348854065,104698:0.49436211585998535',\n",
       " '108453:107073,110594,108355,109162,111761,111800|109162:0.6631086766719818,108439:0.6613138318061829,107073:0.645235687494278,111761:0.6339582800865173,98331:0.6254958212375641,97796:0.6211420893669128,98072:0.6186007559299469,110594:0.6176614463329315,94076:0.6146629750728607,102442:0.6122149229049683,96099:0.6099157333374023,95494:0.60812047123909,97736:0.6068098247051239,107167:0.6063226163387299,106479:0.6059590280056,110005:0.6055971384048462,96371:0.6024494767189026,104483:0.6020485758781433,107749:0.5986931025981903,99097:0.5982406139373779,93258:0.5978312790393829,106207:0.5958735942840576,103771:0.5953792333602905,115895:0.5945508778095245,104964:0.5927607715129852,70858:0.5926624834537506,110118:0.5916892290115356,95778:0.5915673971176147,115419:0.5904257297515869',\n",
       " '114705:114676|116139:0.6050190925598145,105047:0.6037074327468872,95152:0.5854474306106567,115404:0.5691211819648743,113535:0.5676661431789398,110230:0.5605564117431641,116112:0.5555312931537628,92051:0.5491798520088196,116991:0.5480341613292694,109021:0.5476915538311005,112272:0.5462309420108795,105727:0.5444128215312958,113271:0.5434417128562927,93580:0.5418623089790344,107904:0.5380421280860901,105295:0.537972629070282,85388:0.5366736352443695,102803:0.5358802080154419,100905:0.5348266065120697,116312:0.5306884050369263,99315:0.5305434465408325,102000:0.5290012359619141,106523:0.5289984345436096,108454:0.5282121002674103,89522:0.5231586992740631,107842:0.5210691392421722,109592:0.5204368829727173,116457:0.5202265381813049,107049:0.5197758078575134',\n",
       " '114676:114705|102648:0.6654895544052124,110879:0.6643738448619843,111881:0.6632338762283325,104359:0.6574057042598724,104360:0.6574057042598724,96817:0.6573605239391327,103513:0.6508295238018036,101448:0.6477690041065216,111908:0.6385349333286285,106092:0.6384798586368561,106093:0.6384798586368561,107893:0.6285749077796936,101522:0.6273061037063599,112843:0.6214171051979065,112844:0.6214171051979065,91245:0.6135344505310059,94735:0.6132277250289917,112930:0.6104025542736053,112931:0.6104025542736053,113669:0.591322511434555,91458:0.5752841830253601,106268:0.5736780762672424,99186:0.568531721830368,121385:0.5582804083824158,113850:0.5501200556755066,89517:0.548242449760437,89518:0.548242449760437,91432:0.5469606220722198,91337:0.5468112528324127',\n",
       " '110593:110618|111608:0.6585716605186462,105527:0.6371790766716003,111642:0.6349548697471619,103860:0.6324321627616882,111172:0.6322541832923889,106950:0.6298586428165436,98291:0.626277357339859,116986:0.625125378370285,109485:0.6247617900371552,106085:0.6233657002449036,109921:0.6227413713932037,103790:0.6223126649856567,89825:0.6214498579502106,116027:0.6190422475337982,90871:0.6163333058357239,112475:0.6159925162792206,112708:0.612617701292038,109513:0.6120114922523499,85283:0.6117209792137146,110601:0.6104833483695984,102836:0.6094516515731812,92697:0.6094335317611694,101922:0.6093220114707947,111583:0.6089989840984344,99982:0.6083974838256836,92342:0.6065406203269958,114440:0.6060087382793427,116347:0.6059317886829376,110555:0.6053843796253204',\n",
       " '110618:110593|110922:0.6724236905574799,90892:0.6704422831535339,91390:0.6664424538612366,108029:0.6653054058551788,106586:0.6646051108837128,106587:0.6646051108837128,106588:0.6646051108837128,89188:0.6627929508686066,112234:0.6612281501293182,94545:0.6604425609111786,106686:0.6602228879928589,108780:0.6585559844970703,90641:0.6579940021038055,94253:0.6537877321243286,96115:0.6533479690551758,100033:0.6525564193725586,104013:0.6508435308933258,89006:0.6499642133712769,109242:0.649622917175293,112598:0.6487211883068085,106046:0.6480005979537964,110306:0.6474462747573853,99797:0.6448380351066589,99798:0.6448380351066589,97163:0.6448350548744202,106741:0.6433514058589935,109422:0.6429643332958221,104111:0.642112523317337,98867:0.6419624984264374',\n",
       " '102409:102053|102003:0.7130133509635925,102557:0.7114329636096954,96452:0.668409526348114,102471:0.6672001481056213,115895:0.6654565036296844,114022:0.644123762845993,115569:0.6414540112018585,92613:0.6388942003250122,109298:0.6308741569519043,102450:0.6306903660297394,105272:0.6297422349452972,101398:0.6286133825778961,103626:0.6273002624511719,94815:0.6229248940944672,107267:0.6225146055221558,104569:0.6219808161258698,102597:0.6218150556087494,108116:0.621716171503067,115888:0.6214046776294708,105961:0.6208798885345459,92348:0.6208473742008209,104574:0.6201269030570984,104576:0.6201269030570984,96894:0.6185433268547058,115557:0.6183593571186066,102398:0.618097335100174,103217:0.6179137229919434,89620:0.6131250858306885,93295:0.6120039820671082',\n",
       " '102053:102409|115721:0.6077057719230652,99506:0.6064896285533905,105054:0.6039097309112549,91126:0.5972463190555573,109067:0.5917899310588837,98949:0.5889813303947449,98204:0.586490660905838,103217:0.5860298275947571,109513:0.5839764177799225,110482:0.5833782851696014,108532:0.5816989243030548,93042:0.5807812511920929,103951:0.5803489089012146,103572:0.5796880125999451,102142:0.5784429907798767,109393:0.5783168971538544,109491:0.5777206718921661,110123:0.5776149928569794,49413:0.5771480202674866,105337:0.5769436955451965,89699:0.5766680240631104,108918:0.5763367414474487,116944:0.5758780837059021,97981:0.575242280960083,102388:0.5746616721153259,97139:0.5742057263851166,103387:0.5737731456756592,100899:0.573636531829834,105084:0.5732340514659882',\n",
       " '110604:110612|92497:0.6563113033771515,110612:0.6020694077014923,114062:0.5875203907489777,100749:0.5822350382804871,100750:0.5822350382804871,100751:0.5822350382804871,112804:0.5815441310405731,103714:0.5789904594421387,102736:0.578179270029068,96594:0.5770543217658997,91246:0.5749825835227966,98367:0.5739302635192871,103872:0.573403924703598,102785:0.5712741911411285,112201:0.5689271986484528,95223:0.5687613189220428,103719:0.5686008334159851,109861:0.5679439604282379,74223:0.5674535930156708,99735:0.5674373805522919,91337:0.5673008561134338,108180:0.567092776298523,110740:0.5669461488723755,114133:0.5658411085605621,115385:0.5655747056007385,91358:0.5649296641349792,106670:0.5646645426750183,100675:0.5641037225723267,71739:0.5639554262161255',\n",
       " '110612:110604|110604:0.6020694077014923,92497:0.5491260886192322,92798:0.5229625105857849,70979:0.5061745345592499,102595:0.5048365592956543,91508:0.5034676194190979,102274:0.5022473633289337,114676:0.49452584981918335,93607:0.4923447370529175,82495:0.492231547832489,68942:0.491346538066864,111557:0.48896700143814087,100676:0.48638230562210083,110740:0.48531103134155273,105511:0.4831455945968628,103821:0.48285144567489624,93568:0.4825240969657898,103872:0.48120176792144775,113850:0.48081308603286743,93406:0.4803868532180786,101486:0.47971343994140625,107700:0.47959810495376587,101632:0.47821199893951416,108984:0.4777018427848816,111388:0.477053701877594,100878:0.4751558303833008,102701:0.47500061988830566,104359:0.47488582134246826,104360:0.47488582134246826',\n",
       " '116738:116938,117027|113225:0.7043426632881165,115793:0.689071923494339,107833:0.6875996291637421,115767:0.6739067733287811,112520:0.6724278032779694,113219:0.6692163646221161,89573:0.6549077332019806,86455:0.6516530811786652,90796:0.6497487127780914,99737:0.6491401493549347,100846:0.6490384042263031,111969:0.6482829451560974,114280:0.6434978246688843,108414:0.6431342959403992,90653:0.6417358815670013,89397:0.640059620141983,93990:0.636423259973526,101244:0.6288428008556366,116007:0.6284235715866089,92166:0.6270836591720581,111165:0.6254169940948486,108045:0.6232693195343018,92823:0.619508296251297,113107:0.6188201308250427,114724:0.6168493628501892,107465:0.6162888705730438,111277:0.6161879897117615,99966:0.6157547533512115,89276:0.6153136789798737']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 2299\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_100epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          41059600    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          39718992    desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 81,000,292\n",
      "Trainable params: 2,062,492\n",
      "Non-trainable params: 78,937,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/exported_rank_propose_softmax.txt'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.35,\n",
       " '2 - recall_at_10': 0.42,\n",
       " '3 - recall_at_15': 0.46,\n",
       " '4 - recall_at_20': 0.49,\n",
       " '5 - recall_at_25': 0.51}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
