{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose Master Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'netbeans'\n",
    "METHOD = 'propose_master_triplet_loss'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5183ff35ad47b4bec21587bde1b41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9766971468f94deeb9b63ac3494a3941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0498203db81e4cf996d8af5fc7c539c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a7a6b73b547f5aee8cd2e1da54580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 22s, sys: 2.37 s, total: 1min 24s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c31dd26c8143049fcbf2283d5f090d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=181971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 2min 41s, sys: 0 ns, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '1\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '412\\n',\n",
       " 'creation_ts': '2009-10-15 15:55:00 +0000',\n",
       " 'delta_ts': '2009-10-23 08:54:37 +0000',\n",
       " 'description': 'build net beans ide dev build vm java hot spot tm client vm b organization runtime environment rc b organization product x user comments jiriprox person when trying to find replace only in selection product java lang index out of bounds exception illegal start index at java util regex person find person java at org netbeans modules editor lib search organization find document finder java at org netbeans modules editor lib search organization replace impl document finder java at org netbeans modules editor lib search organization replace result document finder java at org netbeans modules editor lib search editor find support find replace in block editor find support java at org netbeans modules editor lib search editor find support find replace impl editor find support java',\n",
       " 'description_word': array([  37,   47,   51,   58,   87,   37,   77,    2,  107,  108,  104,\n",
       "         102,   77,   45,    4,  153,  189,  446,   45,    4,    8,   53,\n",
       "         114,  267, 5038,   10,   66,  415,   11,  136,  923,  219,   14,\n",
       "         468,    8,    2,   50,  207,  225,   27,  696,   64,  413,  202,\n",
       "         207,    6,    2,   41, 1490,   10,  136,   10,    2,    6,    3,\n",
       "           5,    7,   23,   36,  409,    4,  136,  290, 2295,    2,    6,\n",
       "           3,    5,    7,   23,   36,  409,    4,  923,   79,  290, 2295,\n",
       "           2,    6,    3,    5,    7,   23,   36,  409,    4,  923,  284,\n",
       "         290, 2295,    2,    6,    3,    5,    7,   23,   36,  409,   23,\n",
       "         136,  121,  136,  923,   14,  985,   23,  136,  121,    2,    6,\n",
       "           3,    5,    7,   23,   36,  409,   23,  136,  121,  136,  923,\n",
       "          79,   23,  136,  121,    2,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 174674,\n",
       " 'priority': '0\\n',\n",
       " 'product': '34\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'title': 'index out of bounds exception illegal start index',\n",
       " 'title_word': array([207, 225,  27, 696,  64, 413, 202, 207,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " 'version': '16\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 30600)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "\n",
    "import random\n",
    "\n",
    "def read_batch_bugs(self, batch, bug):\n",
    "        info = np.concatenate((\n",
    "            self.to_one_hot(bug['bug_severity'], self.info_dict['bug_severity']),\n",
    "            self.to_one_hot(bug['bug_status'], self.info_dict['bug_status']),\n",
    "            self.to_one_hot(bug['component'], self.info_dict['component']),\n",
    "            self.to_one_hot(bug['priority'], self.info_dict['priority']),\n",
    "            self.to_one_hot(bug['product'], self.info_dict['product']),\n",
    "            self.to_one_hot(bug['version'], self.info_dict['version']))\n",
    "        )\n",
    "        #info.append(info_)\n",
    "        batch['info'].append(info)\n",
    "        batch['title'].append(bug['title_word'])\n",
    "        batch['desc'].append(bug['description_word'])\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids, issues_by_buckets):\n",
    "    neg_bug = random.choice(list(issues_by_buckets.keys()))\n",
    "    try:\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    except:\n",
    "        invalid_bugs = [invalid_bugs]\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    return neg_bug\n",
    "\n",
    "def batch_iterator(baseline, data, dup_sets, bug_train_ids, batch_size, n_neg, issues_by_buckets):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg, master_batch_input, master_batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []},\\\n",
    "                                                    {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                        {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets = []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        neg_bug = baseline.get_neg_bug(dup_sets[data[offset][0]], bug_train_ids)\n",
    "        anchor, pos, neg = data[offset][0], data[offset][1], neg_bug\n",
    "        bug_anchor = baseline.bug_set[anchor]\n",
    "        bug_pos = baseline.bug_set[pos]\n",
    "        bug_neg = baseline.bug_set[neg]\n",
    "        # master anchor and neg\n",
    "        master_anchor = baseline.bug_set[issues_by_buckets[anchor]]\n",
    "        master_neg = baseline.bug_set[issues_by_buckets[neg]]\n",
    "        \n",
    "        baseline.read_batch_bugs(batch_input, bug_anchor)\n",
    "        baseline.read_batch_bugs(batch_pos, bug_pos)\n",
    "        baseline.read_batch_bugs(batch_neg, bug_neg)\n",
    "        # master anchor and neg\n",
    "        baseline.read_batch_bugs(master_batch_input, master_anchor)\n",
    "        baseline.read_batch_bugs(master_batch_neg, master_neg)\n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([data[offset][0], data[offset][1], neg_bug, master_anchor, master_neg])\n",
    "\n",
    "    batch_input['title'] = np.array(batch_input['title'])\n",
    "    batch_input['desc'] = np.array(batch_input['desc'])\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = np.array(batch_pos['title'])\n",
    "    batch_pos['desc'] = np.array(batch_pos['desc'])\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = np.array(batch_neg['title'])\n",
    "    batch_neg['desc'] = np.array(batch_neg['desc'])\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "    \n",
    "    # master\n",
    "    master_batch_input['title'] = np.array(master_batch_input['title'])\n",
    "    master_batch_input['desc'] = np.array(master_batch_input['desc'])\n",
    "    master_batch_input['info'] = np.array(master_batch_input['info'])\n",
    "    \n",
    "    master_batch_neg['title'] = np.array(master_batch_neg['title'])\n",
    "    master_batch_neg['desc'] = np.array(master_batch_neg['desc'])\n",
    "    master_batch_neg['info'] = np.array(master_batch_neg['info'])\n",
    "\n",
    "    n_half = len(batch_triplets) // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg, master_input_sample, master_neg = {}, {}, {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "    # master \n",
    "    master_input_sample = { 'title' : master_batch_input['title'], 'description' : master_batch_input['desc'], \n",
    "                           'info' : master_batch_input['info'] }\n",
    "    master_neg = { 'title' : master_batch_neg['title'], 'description' : master_batch_neg['desc'], \n",
    "                           'info' : master_batch_neg['info'] }\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, master_input_sample, master_neg, sim #sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.5 ms, sys: 0 ns, total: 54.5 ms\n",
      "Wall time: 54 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = batch_iterator(baseline, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, issues_by_buckets)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 500), (128, 544), (128,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: java lang organization appears when netbeans starts\n",
      "***Title***: organization on startup\n",
      "***Description***: person event pm gmt system info product version net beans ide borganizationld operating system windows xp version running on x organization java hot spot tm client vm b organization jdk jre organization br nb cp home dir current dir artwork person netbeans organization netbeans nb person netbeans ide person netbeans enterprise person netbeans harness person netbeans platform artwork netbeans boot ext classpath person java jdk jre lib rt jar person java jdk jre lib i n jar person java jdk jre lib sunrsasign jar person java jdk jre lib jsse jar person java jdk jre lib jce jar person java jdk jre lib charsets jar person java jdk jre classes person java jdk jre lib ext dnsns jar person java jdk jre lib ext localedata jar person java jdk jre lib ext sunjce provider jar person java jdk jre lib ext sunpkcs jar application classpath person netbeans platform lib boot jar person netbeans platform lib org openide modules jar person netbeans platform lib org openide util jar person java jdk lib dt jar person java jdk lib tools jar startup classpath person netbeans platform core core jar person netbeans platform core org openide filesystems jar person netbeans nb core org netbeans upgrader jar person netbeans nb core locale core nb jar person netbeans ide core org netbeans modules utilities cli jar organization the file default platform xml hasn t instance cookie organization the file default platform xml hasn t instance cookie org netbeans core modules product occurred at pm on may java lang null pointer product at org netbeans modules java j seplatform j organization module update source level j organization module java at org netbeans modules java j seplatform j organization module access j organization module java at org netbeans modules java j seplatform j organization module run j organization module java at org openide util product post request product java at org openide util product post write request product java at org netbeans modules java j seplatform j organization module update borganizationld properties j organization module java at org netbeans modules java j seplatform j organization module restored j organization module java at org netbeans core startup organization load code organization java catch at org netbeans core startup organization load organization java at org netbeans module manager enable module manager java at org netbeans core startup artwork java at org netbeans core startup law trigger law java at org netbeans core startup product java at org netbeans core startup product get module system product java at org netbeans core startup product start product java at org netbeans core startup organization run organization java at java lang thread run thread java turning on modules org openide util org openide modules org openide awt org openide filesystems org openide dialogs org openide nodes org openide windows org openide options org openide text org openide explorer org openide actions org openide loaders org netbeans api progress org jdesktop layout org openide io org netbeans core output org netbeans bootstrap org netbeans core startup org netbeans swing plaf org netbeans core org netbeans modules javahelp org netbeans modules db org netbeans modules db core org netbeans modules editor util org netbeans modules editor mimelookup org netbeans modules editor fold org netbeans modules editor lib org netbeans modules properties org netbeans modules queries org openidex util org netbeans modules editor settings org netbeans modules editor org netbeans modules properties syntax org netbeans modules projectapi org netbeans modules settings org netbeans tasklistapi org netbeans modules tasklist core org netbeans modules suggestions framework org netbeans modules projectorganizationapi org netbeans modules tasklist docscan org openide execution org netbeans api java org openide src org netbeans api xml org netbeans spi navigator org netbeans modules xml core org netbeans modules xml catalog org netbeans modules classfile javax jmi reflect org netbeans jmi javamodel org netbeans api mdr javax jmi model org netbeans modules jmiutils org netbeans modules mdr org netbeans modules javacore org netbeans api web webmodule org netbeans core multiview org netbeans modules xml multiview org netbeans modules schema beans org netbeans libs xerces org netbeans modules j ee dd org netbeans modules editor completion org netbeans modules java editor lib org netbeans libs formlayout org netbeans modules options api org netbeans core execution org apache tools ant module org netbeans modules java platform org netbeans modules favorites org netbeans core organization org netbeans modules masterfs org netbeans modules projectorganization org netbeans modules project ant org netbeans modules editor errorstripe api org netbeans modules java org netbeans modules project libraries org netbeans modules java project org netbeans modules editor codetemplates org netbeans modules java editor org netbeans modules j ee common org netbeans modules websvc clientapi org netbeans modules j ee dd webservice org netbeans modules websvc websvcapi org netbeans modules editor hints org netbeans spi viewmodel org netbeans api debugger org netbeans api debugger jpda org netbeans modules refactoring org netbeans modules junit org netbeans modules j eeapis org netbeans modules j eeserver org netbeans modules dbschema org netbeans modules j ee api ejbmodule org netbeans modules j ee ejbcore org netbeans modules j ee ddloaders org netbeans spi palette org netbeans modules beans org netbeans modules java hints org netbeans core ide org netbeans lib cvsclient org netbeans modules apisupport ant org netbeans modules ant browsetask org netbeans modules debugger jpda org netbeans spi debugger organization org netbeans modules debugger jpda organization org netbeans modules debugger jpda ant org netbeans modules xml tax org netbeans modules apisupport project org netbeans modules j ee blueprints org netbeans modules java j seproject org netbeans modules websvc registry org netbeans modules websvc core org netbeans modules websvc dev org netbeans libs commons logging org netbeans modules servletapi org netbeans modules web jspparser org netbeans modules editor structure org netbeans modules xml text org netbeans modules editor plain lib org netbeans modules html editor lib org netbeans modules html editor org netbeans modules web core syntax org netbeans modules web core org netbeans modules j ee refactoring org netbeans modules jmxri org netbeans modules j ee weblogic org netbeans modules javadoc org netbeans modules image org netbeans upgrader org netbeans modules derby org netbeans modules css org netbeans modules servletapi org netbeans modules httpserver org netbeans modules web monitor org netbeans modules xsl org netbeans modules xml schema org netbeans modules diff org netbeans modules editor errorstripe org netbeans modules versioning org netbeans libs jsch org netbeans modules versioning system cvss org netbeans modules editor bookmarks org netbeans modules autoupdate org netbeans modules form org netbeans modules i n org netbeans modules i n form org netbeans modules apisupport feedreader org netbeans modules usersgorganizationde org netbeans modules web debug org netbeans modules java j seplatform org netbeans modules web examples org netbeans modules web project org netbeans modules java navigation org netbeans modules html org netbeans modules j ee sun dd org netbeans modules j ee sun appsrv org netbeans modules j ee sun ddorganization org netbeans modules j ee sun appsrv org netbeans modules defaults org netbeans modules clazz org netbeans modules j ee debug org netbeans modules ant freeform org netbeans modules java freeform org netbeans modules web freeform org netbeans modules db sql editor org netbeans modules tomcat org netbeans modules tomcat bundled org netbeans modules j ee ejbjarproject org netbeans modules j ee earproject org netbeans modules j ee genericserver org netbeans modules editor settings storage org netbeans modules websvc jaxrpc org netbeans modules ant debugger org netbeans modules updatecenters org netbeans modules xml tools org netbeans modules utilities org netbeans modules utilities project org netbeans modules ant grammar org netbeans modules web struts org netbeans modules welcome org netbeans modules web jstl org netbeans modules web jsf org netbeans modules editor plain org netbeans modules java examples org netbeans swing tabcontrol org netbeans modules j ee ant org netbeans modules options editor org netbeans modules extbrowser org netbeans modules j ee ejbfreeform org netbeans modules apisupport refactoring org netbeans modules j ee jboss org netbeans modules j ee platform org netbeans core windows\n",
      "***Description***: when i start nationality it throws this exception this used to happen after i had upgraded then i deleted the netbeans directory and let it recreate it as it wanted this worked well for a day and now it is throwing this exception again when i went in and checked out nationality i found that it had lost its product platform definition all that remained was the product that i had added date i will now try to re add the product but usually after doing that it messes up again and something gets lost java lang artwork at org netbeans modules java j seplatform j organization module update source level j organization module java at org netbeans modules java j seplatform j organization module access j organization module java at org netbeans modules java j seplatform j organization module run j organization module java at org openide util product post request product java at org openide util product post write request product java at org netbeans modules java j seplatform j organization module update build properties j organization module java at org netbeans modules java j seplatform j organization module restored j organization module java at org netbeans core startup organization load code organization java at org netbeans core startup organization load organization java at org netbeans module manager enable module manager java at org netbeans core startup artwork java at org netbeans core startup law trigger law java at org netbeans core startup product java at org netbeans core startup product get module system product java at org netbeans core startup product start product java at org netbeans core startup organization run organization java catch at java lang thread run thread java\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: titled border problem with copied jpanel in jtabbed panes\n",
      "***Title***: multi selection undesirable changing of properties\n",
      "***Description***: i have person containing number jpanel call it organization inside of which there are numerous other jpanels one of these sub panels we will call organization and give a titled border i then use the organization editor to copy and paste this number organization so now i have number jpanels in the jtabbed pane called organization and organization i want to change the text on the titled border of the organization but doing so also changes the text on the titled border of organization likewise when i change organization organization custbug this issue is based on a organization customer s bug report\n",
      "***Description***: pilsen jdk rc steps to reproduce open create form add number compnumbernts e g jbuttons select both jb and jb change border push select organization code is generated for both that s fine select only jb invoke border property editor and change some property for organization e g change property type to lowered now is code generated for both jb and jb this happens only if you number changes do by multi selection and number changes in number of them make change in all previous seleced compnumbernts\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: startup of nb and m fails due to broken core jar\n",
      "***Title***: assertion error start new start jts joined token sequence context data token sequence current ts token sequence for text x php text html at token index token list contains tokens null tex\n",
      "***Description***: when starting nb date or nb m i get this exception artwork in thread main java lang reflect invocation target artwork at sun reflect nationality nationality person invoke nationality nationality at sun reflect nationality nationality person invoke nationality nationality person java at sun reflect delegating nationality person invoke delegating nationality person java at java lang reflect nationality invoke nationality java at org netbeans person main person java at org netbeans facility main facility java caused by java lang verify error class org netbeans core startup organization method start signature v incompatible object argument for function call at org netbeans core startup facility main facility java more tested on product product\n",
      "***Description***: assertion error start new start jts joined token sequence context data token sequence current ts token sequence for text x php text html at token index token list contains tokens null tex\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: a java lang null pointer exception exception has occurred\n",
      "***Title***: organization when closing editor\n",
      "***Description***: a java lang null pointer exception exception has occurred\n",
      "***Description***: organization when closing editor\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 53.5 ms, sys: 0 ns, total: 53.5 ms\n",
      "Wall time: 53.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 122604'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading Glove\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.split()\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666fc85aa0384d7199dd3cba98a7cf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5647d0f3ae5b465f8c7e6f3b73cf7628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=122604), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 2s, sys: 2.78 s, total: 2min 5s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "from keras.layers import MaxPooling1D\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        number_of_layers = K.int_shape(block_input)[2]\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Dense(number_of_layers, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Dense(number_of_layers, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(units, number_of_layers, embedding_layer, title_layer, max_sequence_length, name):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND_{}'.format(name))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # units = 128\n",
    "    #number_of_layers = 6\n",
    "    \n",
    "    if title_layer != None:\n",
    "        title_input = title_layer.input\n",
    "        title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    if title_layer != None:\n",
    "        la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title\n",
    "        if title_layer != None:\n",
    "            la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "            lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        \n",
    "        if title_layer != None:\n",
    "            attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    if title_layer != None:\n",
    "        attention_title_layes = Add()(attention_title_layes)\n",
    "        attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1, 1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    if title_layer != None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "    \n",
    "    cnn_dilated_feature_model = Model(inputs=inputs, \n",
    "                                      outputs=[layer], name = 'FeatureCNNDilatedGenerationModel_{}'.format(name)) # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D, TimeDistributed\n",
    "\n",
    "def cnn_model(embedding_layer, title_input, title_layer, max_sequence_length):\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    \n",
    "    if title_layer != None:\n",
    "        conv = Conv1D(filters=32, kernel_size=5)(l_merge)\n",
    "        #title_layer = Permute((2, 1))(title_layer)\n",
    "        #conv = Permute((2, 1))(conv)\n",
    "        #layer = Dot(axes=1)([conv, title_layer])\n",
    "        #title_layer = TimeDistributed(Dense(1))(title_layer)\n",
    "        title_layer = Flatten()(title_layer)\n",
    "        layer = GlobalAveragePooling1D()(conv)\n",
    "        layer = Concatenate()([layer, title_layer])\n",
    "    else:\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "    \n",
    "    if title_layer != None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "\n",
    "    cnn_feature_model = Model(inputs=inputs, outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    layer = Flatten()(lstm_layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model, lstm_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute, Reshape\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        #block_input = Dense(shape_size_cols)(block_input)\n",
    "        residual =  block_input\n",
    "#         residual = Activation('tanh')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "#         layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "#         layer_out = GRU(100, activation='tanh', return_sequences=True)(layer_out)\n",
    "#         #layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "#         #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "#         layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "#         #layer_out = BatchNormalization()(layer_out)\n",
    "#         layer_out = Dense(50, activation='tanh')(layer_out)\n",
    "#         #layer_out = BatchNormalization()(layer_out)\n",
    "#         layer_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "#         skip_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        #layer_out = Dense(shape_size_cols // 2, activation='tanh')(block_input)\n",
    "        layer_out = Dense(shape_size_cols)(block_input)\n",
    "        skip_out = Dense(shape_size_cols)(block_input)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average, AveragePooling1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "#     bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    #bug_t_feat = GRU(100, return_sequences=False, activation='tanh')(bug_t_feat)\n",
    "    #bug_t_feat = Flatten()(bug_t_feat)\n",
    "    #bug_t_feat = Dense(100, activation='tanh')(bug_t_feat)\n",
    "    \n",
    "    #bug_d_feat = GRU(100, return_sequences=False, activation='tanh')(bug_d_feat)\n",
    "    #bug_d_feat = Flatten()(bug_d_feat)\n",
    "    #bug_d_feat = Dense(100, activation='tanh')(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "#     for _ in range(2):\n",
    "#         bug_feature_output = Dense(150, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = Add()([bug_feature_output, bug_feature_output_1b, bug_feature_output_2b])\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, master_positive, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input, \n",
    "                                 master_anchor.input, master_positive.input, master_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = master_positive.output\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters anchor\n",
    "    master_anchor_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance', output_shape=[1])([encoded_anchor, master_positive])\n",
    "    master_anchor_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance', output_shape=[1])([encoded_anchor, master_negative])\n",
    "    \n",
    "    # Distance master positive\n",
    "    master_pos_positive_d = Lambda(cosine_distance, name='pos_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_positive])\n",
    "    master_pos_negative_d = Lambda(cosine_distance, name='neg_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_negative])\n",
    "    \n",
    "    # Distance master negative\n",
    "    master_neg_positive_d = Lambda(cosine_distance, name='pos_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_negative])\n",
    "    master_neg_negative_d = Lambda(cosine_distance, name='neg_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_positive])\n",
    "    \n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output_bug = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-bug',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    output_master = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-anchor',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_anchor_positive_d, master_anchor_negative_d])\n",
    "    \n",
    "    output_master_pos = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-pos',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_pos_positive_d, master_pos_negative_d])\n",
    "    \n",
    "    output_master_neg = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-neg',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_neg_positive_d, master_neg_negative_d])\n",
    "    \n",
    "    #output = Average()([output_bug, output_master, output_master_pos, output_master_neg])\n",
    "    \n",
    "    output_avg_master = Average()([output_master, output_master_pos, output_master_neg])\n",
    "    output = Average()([output_bug, output_avg_master])\n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, \n",
    "                                 metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_pos (InputLayer)    (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_pos (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_pos (InputLayer)    (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_neg (InputLayer)    (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_neg (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_neg (InputLayer)    (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "                                                                 info_master_pos[0][0]            \n",
      "                                                                 info_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          36901951    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "                                                                 title_master_pos[0][0]           \n",
      "                                                                 title_master_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          37031292    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "                                                                 desc_master_pos[0][0]            \n",
      "                                                                 desc_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_pos (Conc (None, 900)          0           FeatureMlpGenerationModel[5][0]  \n",
      "                                                                 FeatureLstmGenerationModel[5][0] \n",
      "                                                                 FeatureCNNGenerationModel[5][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_neg (Conc (None, 900)          0           FeatureMlpGenerationModel[6][0]  \n",
      "                                                                 FeatureLstmGenerationModel[6][0] \n",
      "                                                                 FeatureCNNGenerationModel[6][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-anchor ( (None, 2, 1)         0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-pos (Lam (None, 2, 1)         0           pos_master_pos_cosine_distance[0]\n",
      "                                                                 neg_master_pos_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-neg (Lam (None, 2, 1)         0           pos_master_neg_cosine_distance[0]\n",
      "                                                                 neg_master_neg_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-bug (Lambda)    (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2, 1)         0           stack-distances-master-anchor[0][\n",
      "                                                                 stack-distances-master-pos[0][0] \n",
      "                                                                 stack-distances-master-neg[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 2, 1)         0           stack-distances-bug[0][0]        \n",
      "                                                                 average_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 74,096,743\n",
      "Trainable params: 534,343\n",
      "Non-trainable params: 73,562,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.98, pos_cosine: 0.97, neg_cosine: 0.95, recall@25: 0.65\n",
      "Saved model 'modelos/model_propose_master_triplet_loss_feature_1epochs_64batch(netbeans).h5' to disk\n",
      "Best_epoch=1, Best_loss=0.98s, Recall@25=0.65\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import keras\n",
    "\n",
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model, title_layer = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, None, None, MAX_SEQUENCE_LENGTH_D)\n",
    "#title_feature_model = cnn_dilated_model(50, 6, title_embedding_layer, None, MAX_SEQUENCE_LENGTH_T, 'title')\n",
    "#desc_feature_model = cnn_dilated_model(128, 6, desc_embedding_layer, title_feature_model, MAX_SEQUENCE_LENGTH_D, 'desc')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "master_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_in')\n",
    "master_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_pos')\n",
    "master_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, master_positive, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = batch_iterator(baseline, baseline.train_data, baseline.dup_sets_train, bug_train_ids, \n",
    "                                       batch_size, 1, issues_by_buckets)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info'],\n",
    "                  train_master_input['title'], train_master_input['description'], train_master_input['info'],\n",
    "                  train_master_input['title'], train_master_input['description'], train_master_input['info'],\n",
    "                   train_master_neg['title'], train_master_neg['description'], train_master_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}s, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|108529:0.781901627779007,115634:0.4409627318382263,102470:0.4394568204879761,115569:0.4327548146247864,115421:0.43272697925567627,115100:0.4318072199821472,97026:0.42612969875335693,14454:0.42421960830688477,115262:0.42329686880111694,94815:0.42085176706314087,95460:0.4176369905471802,105166:0.41506171226501465,102788:0.4145801067352295,95800:0.41407573223114014,101398:0.40790480375289917,104574:0.4002752900123596,104576:0.4002752900123596,96429:0.3997526168823242,98212:0.39928901195526123,109674:0.3968345522880554,89456:0.39520323276519775,93171:0.391379714012146,89620:0.39039283990859985,82495:0.38916486501693726,112693:0.38870614767074585,46957:0.38786113262176514,107267:0.3877156972885132,98647:0.3875355124473572,95277:0.38702458143234253',\n",
       " '109674:108544,111059,108379,109366|94815:0.745557427406311,112693:0.6850931942462921,102682:0.4924393892288208,98212:0.46523183584213257,46957:0.45145171880722046,107845:0.45027047395706177,109298:0.4484788179397583,105272:0.44847333431243896,95277:0.4454473853111267,115634:0.4381582736968994,92543:0.4352060556411743,92613:0.4340786337852478,115072:0.4333900213241577,97026:0.4327549934387207,89456:0.432539165019989,90533:0.43179231882095337,92188:0.4310871362686157,105082:0.43056541681289673,94421:0.4300861954689026,107794:0.4289540648460388,102470:0.42701423168182373,108529:0.4268093705177307,14454:0.42663753032684326,111514:0.42611730098724365,115421:0.42580580711364746,105511:0.42559248208999634,115100:0.4253774881362915,95800:0.4253711700439453,115569:0.4244946241378784',\n",
       " '111059:108544,109674,108379,109366|110274:0.8167965561151505,111297:0.809360682964325,110555:0.8039961755275726,94815:0.4511948227882385,92690:0.43927764892578125,98212:0.43873727321624756,92117:0.43717432022094727,92348:0.42571699619293213,89456:0.42027831077575684,112693:0.42013031244277954,109298:0.4154874086380005,115100:0.4151841998100281,89620:0.4148818850517273,109674:0.4122299551963806,115634:0.41175711154937744,102470:0.41142594814300537,115569:0.40932273864746094,112520:0.40921688079833984,114280:0.4073321223258972,115557:0.40566110610961914,113881:0.40565717220306396,115421:0.4034702181816101,92613:0.4021027684211731,104267:0.4015188217163086,104173:0.40105247497558594,102398:0.40049833059310913,102003:0.398734986782074,115072:0.3987054228782654,115068:0.3982052803039551',\n",
       " '108379:108544,111059,109674,109366|108492:0.9044488817453384,115634:0.44015300273895264,102470:0.4393860697746277,115569:0.4352286458015442,115421:0.4349551200866699,98212:0.4326363205909729,89456:0.43252116441726685,89620:0.43191200494766235,115100:0.4317942261695862,92690:0.42657309770584106,92117:0.42574864625930786,14454:0.42298299074172974,94815:0.42013853788375854,92348:0.41621333360671997,95460:0.41614431142807007,92365:0.415261447429657,92613:0.40934640169143677,46957:0.408413827419281,101398:0.40823930501937866,92188:0.40817689895629883,93169:0.4079575538635254,93177:0.4063264727592468,92541:0.406141459941864,115262:0.4051816463470459,97026:0.40336596965789795,109862:0.40194666385650635,93295:0.4008125066757202,105166:0.400401771068573,106543:0.3985553979873657',\n",
       " '109366:108544,111059,109674,108379|110044:0.8027364015579224,109298:0.6227025985717773,91310:0.47236430644989014,92537:0.460851788520813,111295:0.45716094970703125,99191:0.45519256591796875,102497:0.4500225782394409,104266:0.4453725814819336,114719:0.44524532556533813,114671:0.44449061155319214,114770:0.443606972694397,109431:0.44338059425354004,105086:0.43861061334609985,105993:0.43537968397140503,106574:0.43467533588409424,107044:0.4337502121925354,104956:0.432706356048584,106191:0.4319252371788025,98821:0.42624568939208984,103056:0.42543625831604004,118884:0.4248577356338501,112617:0.42473679780960083,105482:0.42428046464920044,102082:0.42029374837875366,101622:0.41943711042404175,105235:0.41807693243026733,115579:0.417444109916687,108311:0.4149009585380554,100666:0.41318994760513306',\n",
       " '110594:107073,108355,108453,109162,111761,111800|109162:0.8743464350700378,95494:0.5181114375591278,111761:0.49062997102737427,106479:0.4791414141654968,108439:0.477938711643219,103827:0.4775567054748535,108240:0.4757835865020752,108084:0.4739605784416199,107073:0.4739488959312439,103471:0.4603300094604492,98331:0.45761609077453613,101545:0.45232152938842773,101462:0.45047879219055176,96633:0.4491409659385681,108453:0.42134296894073486,98310:0.4213312864303589,93569:0.33131247758865356,91337:0.3259221315383911,108355:0.31900161504745483,104278:0.3181488513946533,105251:0.30496084690093994,95007:0.30442845821380615,95853:0.3039085865020752,97736:0.30108439922332764,94775:0.30068492889404297,105739:0.2949637174606323,93258:0.2889391779899597,94063:0.28881388902664185,95968:0.28557997941970825',\n",
       " '111800:107073,110594,108355,108453,109162,111761|56891:0.4727885127067566,57538:0.465461790561676,76670:0.46520888805389404,108513:0.45429331064224243,97681:0.4518370032310486,111833:0.4500337839126587,98278:0.44972747564315796,91324:0.4494924545288086,98948:0.44846659898757935,106252:0.4356474280357361,71739:0.4310014247894287,73946:0.43094050884246826,100029:0.4296160340309143,105931:0.4275725483894348,49142:0.425508975982666,96212:0.4245758652687073,97289:0.42170488834381104,95348:0.4207789897918701,94719:0.41987335681915283,103526:0.4193538427352905,95903:0.4170383810997009,96929:0.41674500703811646,78260:0.41646814346313477,94918:0.41642218828201294,100490:0.4161882996559143,103031:0.41599273681640625,102298:0.41516637802124023,97757:0.4139291048049927,96117:0.4126051664352417',\n",
       " '111761:107073,110594,108355,108453,109162,111800|111758:0.5191176235675812,106479:0.5180929601192474,95494:0.5100433826446533,110594:0.49062997102737427,98310:0.4903179407119751,109162:0.4883079528808594,107073:0.4780905842781067,108084:0.47764819860458374,108240:0.4753236770629883,103827:0.4734313488006592,108439:0.47312426567077637,103471:0.4640563130378723,101545:0.4613403081893921,98331:0.4556284546852112,96633:0.44347691535949707,108453:0.4390057325363159,101462:0.4346509575843811,110712:0.41816210746765137,110992:0.4139670133590698,108355:0.3853711485862732,109909:0.3805462121963501,91337:0.3192148804664612,97880:0.2848057150840759,93569:0.28035515546798706,98480:0.27728980779647827,105739:0.27592992782592773,104278:0.2757527828216553,111455:0.2717885375022888,98072:0.27152055501937866',\n",
       " '109162:107073,110594,108355,108453,111761,111800|110594:0.8743464350700378,95494:0.5208576321601868,111761:0.4883079528808594,103827:0.48395687341690063,107073:0.4786742925643921,106479:0.4785863757133484,108439:0.4765496850013733,108084:0.47562307119369507,108240:0.4750100374221802,98331:0.4689902067184448,101462:0.4635728597640991,103471:0.46284884214401245,96633:0.4531104564666748,101545:0.4526289701461792,98310:0.42451226711273193,108453:0.4230917692184448,93569:0.3353498578071594,108355:0.333432137966156,91337:0.3317953944206238,104278:0.3231297731399536,95007:0.30996280908584595,97736:0.30730462074279785,105251:0.30668842792510986,94775:0.3044346570968628,95853:0.30381906032562256,105739:0.2969932556152344,94063:0.29272615909576416,93258:0.29271507263183594,95968:0.2874661684036255',\n",
       " '108355:107073,110594,108453,109162,111761,111800|95494:0.41164714097976685,106479:0.3908386826515198,103471:0.3870159983634949,111761:0.3853711485862732,101462:0.3755027651786804,96633:0.36086583137512207,107073:0.35577625036239624,108084:0.3554403781890869,108240:0.35149121284484863,108439:0.35062265396118164,103827:0.34992045164108276,108453:0.3404216170310974,98331:0.3395921587944031,109162:0.333432137966156,110594:0.31900161504745483,98310:0.31651633977890015,101545:0.31061816215515137,114123:0.2920687794685364,114938:0.29131054878234863,115456:0.2908426523208618,110861:0.2738339304924011,103278:0.2730032205581665,99423:0.26754528284072876,90800:0.25714725255966187,107049:0.2519376873970032,105739:0.24374550580978394,95007:0.24239426851272583,97736:0.23998117446899414,95853:0.23965144157409668',\n",
       " '108453:107073,110594,108355,109162,111761,111800|95494:0.5136959254741669,108439:0.4882621169090271,107073:0.4865885376930237,108240:0.4865545630455017,101545:0.4859004616737366,108084:0.4843651056289673,106479:0.4825415015220642,103827:0.47443151473999023,103471:0.4697590470314026,98310:0.4504784941673279,96633:0.44672173261642456,111761:0.4390057325363159,101462:0.43799889087677,98331:0.4325581192970276,109162:0.4230917692184448,110594:0.42134296894073486,108355:0.3404216170310974,91337:0.32516932487487793,93258:0.32402998208999634,93569:0.3037470579147339,98072:0.3026543855667114,105251:0.29756659269332886,95968:0.29394495487213135,116803:0.29114091396331787,95750:0.2903636693954468,98480:0.288210391998291,99842:0.28385138511657715,97261:0.2830125093460083,95269:0.2809838652610779',\n",
       " '114705:114676|98659:0.4176490902900696,115404:0.414186954498291,109640:0.40629446506500244,105747:0.4049895405769348,95152:0.4035341143608093,102518:0.39938896894454956,113535:0.39612990617752075,110915:0.3953036665916443,105727:0.3844336271286011,116139:0.38391995429992676,105368:0.38321274518966675,105047:0.38224220275878906,96918:0.3771287798881531,113470:0.37540531158447266,105295:0.3723655343055725,116991:0.3688030242919922,111191:0.3673333525657654,112187:0.3665626049041748,109592:0.36598992347717285,105822:0.36282438039779663,116750:0.35222679376602173,100351:0.3517153859138489,108454:0.35091912746429443,104587:0.3503718972206116,113097:0.34722983837127686,116227:0.3471224904060364,112783:0.3463147282600403,55499:0.3397672176361084,101937:0.33800411224365234',\n",
       " '114676:114705|107893:0.3653663992881775,102648:0.34228843450546265,108061:0.33568453788757324,111881:0.33196568489074707,111908:0.3262181878089905,91245:0.3235272765159607,103513:0.321588397026062,91458:0.3166755437850952,99186:0.31621527671813965,106268:0.30773383378982544,112930:0.3055393695831299,112931:0.3055393695831299,112843:0.30532705783843994,112844:0.30532705783843994,113669:0.3035144805908203,115237:0.3034926652908325,101867:0.3009084463119507,106269:0.29832637310028076,103562:0.29709792137145996,104359:0.29650038480758667,104360:0.29650038480758667,115350:0.2942924499511719,96817:0.29299843311309814,101522:0.29188013076782227,108401:0.29180097579956055,110879:0.2866842746734619,101448:0.28664177656173706,110900:0.28003376722335815,107842:0.26656073331832886',\n",
       " '110593:110618|110397:0.7736733555793762,111172:0.7172525823116302,111286:0.7171341180801392,110375:0.5978976190090179,114133:0.5898441672325134,110346:0.5769425630569458,110374:0.5734165012836456,110329:0.5729998350143433,109880:0.5725644528865814,109486:0.5674086511135101,110343:0.5667595863342285,111389:0.5646639168262482,110366:0.5642987489700317,110485:0.5416075587272644,110392:0.5337816774845123,109568:0.533682107925415,110026:0.5331227481365204,109951:0.5280656218528748,111681:0.5226175785064697,111651:0.5116416215896606,109950:0.45460081100463867,110956:0.4485798478126526,110657:0.44812947511672974,110005:0.446144700050354,109435:0.4461049437522888,110012:0.44099336862564087,111673:0.42414218187332153,111054:0.4136282801628113,111362:0.41317903995513916',\n",
       " '110618:110593|110922:0.5809245407581329,110306:0.4958187937736511,91809:0.4751378297805786,112099:0.4731297492980957,112234:0.4668561816215515,89006:0.46181362867355347,105817:0.45760148763656616,105396:0.4572884440422058,96344:0.4538611173629761,105290:0.45235395431518555,108029:0.45201313495635986,96771:0.45138853788375854,97163:0.44999992847442627,109368:0.448225200176239,87788:0.4467037320137024,116000:0.4463460445404053,105638:0.4442492127418518,90892:0.4419155716896057,95207:0.44186341762542725,105640:0.4366263747215271,102250:0.43623507022857666,94253:0.4360724687576294,106695:0.43585580587387085,108714:0.4352109432220459,97263:0.4348701238632202,96115:0.4332813620567322,116707:0.4320875406265259,116708:0.4320131540298462,102386:0.43109130859375',\n",
       " '102409:102053|104447:0.6115068793296814,102106:0.565837949514389,95413:0.5395563840866089,96124:0.5221651494503021,95414:0.49968504905700684,90949:0.49800288677215576,97178:0.48435431718826294,106072:0.4799022078514099,97179:0.473977267742157,97211:0.472895085811615,106171:0.467481791973114,101652:0.4637144207954407,95237:0.463309109210968,102557:0.4586009979248047,91703:0.4578533172607422,96471:0.456145703792572,102682:0.45088881254196167,104460:0.4483652710914612,97350:0.44753044843673706,102471:0.44735801219940186,94569:0.44720882177352905,96945:0.4455316662788391,96947:0.4454299211502075,102003:0.4436066150665283,104267:0.4428132176399231,110764:0.4391927719116211,95489:0.43381810188293457,102398:0.4325385093688965,104173:0.42794913053512573',\n",
       " '102053:102409|107236:0.36587196588516235,116288:0.3595545291900635,107101:0.3533375859260559,114049:0.2917972207069397,107778:0.2866390347480774,90840:0.23808014392852783,97095:0.1567624807357788,101932:0.155348539352417,109864:0.14095979928970337,109794:0.14048129320144653,98546:0.14045149087905884,91126:0.14009690284729004,93913:0.13993513584136963,95114:0.13300752639770508,95768:0.12215626239776611,109067:0.12152308225631714,105631:0.12000954151153564,89910:0.11541730165481567,91543:0.10793894529342651,105054:0.09768497943878174,107819:0.08334153890609741,107820:0.08334153890609741,111354:0.07636374235153198,96739:0.07628422975540161,111603:0.07586359977722168,103541:0.07360923290252686,103145:0.07085680961608887,91458:0.07070738077163696,95975:0.06964975595474243',\n",
       " '110604:110612|110612:0.6516856551170349,109067:0.15547257661819458,104153:0.06464862823486328,99626:0.06069070100784302,98005:0.042353034019470215,97095:-0.03153419494628906,110594:-0.04440009593963623,109162:-0.04622197151184082,91523:-0.04789459705352783,107236:-0.058611512184143066,97880:-0.05999624729156494,108453:-0.06119728088378906,100327:-0.06311070919036865,116288:-0.06414544582366943,98331:-0.06425392627716064,122655:-0.06449258327484131,108240:-0.06455850601196289,108439:-0.06507301330566406,102023:-0.06671488285064697,108084:-0.06676113605499268,110444:-0.0677492618560791,102251:-0.06794857978820801,107073:-0.06797945499420166,95055:-0.06830179691314697,96871:-0.06862986087799072,94076:-0.07204198837280273,95494:-0.07212305068969727,114786:-0.0730736255645752,109864:-0.07346940040588379',\n",
       " '110612:110604|110604:0.6516856551170349,109067:0.1617143154144287,97894:0.07589811086654663,98005:0.05672657489776611,99626:0.05378425121307373,104153:0.048340022563934326,105230:0.04084998369216919,97095:0.010202527046203613,93364:0.007858574390411377,108896:0.0034289956092834473,92679:0.0028635263442993164,101569:-0.00017499923706054688,113114:-0.0008324384689331055,99909:-0.0010094642639160156,90935:-0.0013755559921264648,99217:-0.0062607526779174805,101486:-0.007862448692321777,92890:-0.009628534317016602,93913:-0.011902332305908203,111388:-0.012624263763427734,102679:-0.016293048858642578,99610:-0.018111228942871094,83448:-0.019620656967163086,91393:-0.02267301082611084,95114:-0.023894429206848145,111354:-0.025257349014282227,105054:-0.02702784538269043,94385:-0.02947366237640381,108264:-0.029583334922790527',\n",
       " '116738:116938,117027|115793:0.8697408139705658,113225:0.8559306412935257,113106:0.8015594184398651,114018:0.5551966726779938,113395:0.4350464344024658,99737:0.41880691051483154,108966:0.4021626114845276,100511:0.3971799612045288,105862:0.39578181505203247,105084:0.3953322768211365,100713:0.3901553153991699,100714:0.3901553153991699,100715:0.3901553153991699,100716:0.3901553153991699,93192:0.3887842297554016,98664:0.38648873567581177,94580:0.3856459856033325,90796:0.38517940044403076,109630:0.3849143385887146,109937:0.3810451030731201,95130:0.3777136206626892,96594:0.37707656621932983,91795:0.3769702911376953,115767:0.3761618137359619,92002:0.37106889486312866,92173:0.3694190979003906,93311:0.36688750982284546,111969:0.36506539583206177,92166:0.36484813690185547']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.4,\n",
       " '2 - recall_at_10': 0.47,\n",
       " '3 - recall_at_15': 0.52,\n",
       " '4 - recall_at_20': 0.56,\n",
       " '5 - recall_at_25': 0.58}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
