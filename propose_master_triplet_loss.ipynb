{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose Master Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'propose_master_triplet_loss'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ef3321bf8843359e3a40b57ede8ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=322339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f7414b4f304f0f94f40f2931903b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b442ce5aadb347af86edeb572a942419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432b5f5400fe4680bedc696d8ceabf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 17s, sys: 4.39 s, total: 2min 21s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bcf1c8979444b99dfe3268bd9c0e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321536), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 2min 37s, sys: 29.7 ms, total: 2min 37s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '0\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '527\\n',\n",
       " 'creation_ts': '2009-03-24 04:03:00 -0400',\n",
       " 'delta_ts': '2009-05-08 10:06:15 -0400',\n",
       " 'description': 'type in any method code return this this string no highlight as error but this error correct code next return this',\n",
       " 'description_word': array([  61,   10,  255,   22,   81,  296,   23,   23,  118,  117, 1855,\n",
       "          56,   51,   92,   23,   51,  700,   81,  344,  296,   23,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 269776,\n",
       " 'priority': '0\\n',\n",
       " 'product': '125\\n',\n",
       " 'resolution': 'INVALID',\n",
       " 'title': 'operator return this without error message',\n",
       " 'title_word': array([2139,  296,   23,  509,   51,  120,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " 'version': '2\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 34882)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "\n",
    "import random\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids, issues_by_buckets):\n",
    "    neg_bug = random.choice(list(issues_by_buckets.keys()))\n",
    "    try:\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    except:\n",
    "        invalid_bugs = [invalid_bugs]\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    return neg_bug\n",
    "\n",
    "def batch_iterator(baseline, data, dup_sets, bug_train_ids, batch_size, n_neg, issues_by_buckets):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg, master_batch_input, master_batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []},\\\n",
    "                                                    {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                        {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets = []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        neg_bug = baseline.get_neg_bug(dup_sets[data[offset][0]], bug_train_ids)\n",
    "        anchor, pos, neg = data[offset][0], data[offset][1], neg_bug\n",
    "        bug_anchor = baseline.bug_set[anchor]\n",
    "        bug_pos = baseline.bug_set[pos]\n",
    "        bug_neg = baseline.bug_set[neg]\n",
    "        # master anchor and neg\n",
    "        master_anchor = baseline.bug_set[issues_by_buckets[anchor]]\n",
    "        master_neg = baseline.bug_set[issues_by_buckets[neg]]\n",
    "        \n",
    "        baseline.read_batch_bugs(batch_input, bug_anchor)\n",
    "        baseline.read_batch_bugs(batch_pos, bug_pos)\n",
    "        baseline.read_batch_bugs(batch_neg, bug_neg)\n",
    "        # master anchor and neg\n",
    "        baseline.read_batch_bugs(master_batch_input, master_anchor)\n",
    "        baseline.read_batch_bugs(master_batch_neg, master_neg)\n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([data[offset][0], data[offset][1], neg_bug, master_anchor, master_neg])\n",
    "\n",
    "    batch_input['title'] = np.array(batch_input['title'])\n",
    "    batch_input['desc'] = np.array(batch_input['desc'])\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = np.array(batch_pos['title'])\n",
    "    batch_pos['desc'] = np.array(batch_pos['desc'])\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = np.array(batch_neg['title'])\n",
    "    batch_neg['desc'] = np.array(batch_neg['desc'])\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "    \n",
    "    # master\n",
    "    master_batch_input['title'] = np.array(master_batch_input['title'])\n",
    "    master_batch_input['desc'] = np.array(master_batch_input['desc'])\n",
    "    master_batch_input['info'] = np.array(master_batch_input['info'])\n",
    "    \n",
    "    master_batch_neg['title'] = np.array(master_batch_neg['title'])\n",
    "    master_batch_neg['desc'] = np.array(master_batch_neg['desc'])\n",
    "    master_batch_neg['info'] = np.array(master_batch_neg['info'])\n",
    "\n",
    "    n_half = len(batch_triplets) // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg, master_input_sample, master_neg = {}, {}, {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "    # master \n",
    "    master_input_sample = { 'title' : master_batch_input['title'], 'description' : master_batch_input['desc'], \n",
    "                           'info' : master_batch_input['info'] }\n",
    "    master_neg = { 'title' : master_batch_neg['title'], 'description' : master_batch_neg['desc'], \n",
    "                           'info' : master_batch_neg['info'] }\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, master_input_sample, master_neg, sim #sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 ms, sys: 773 µs, total: 51.2 ms\n",
      "Wall time: 50.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = batch_iterator(baseline, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, issues_by_buckets)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 500), (128, 1682), (128,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: the add implemented methods hover should be resizeable or should have at least a scrollbar\n",
      "***Title***: no scroll bars in completion box help panel\n",
      "***Description***: the add implemented methods dialog should be resizeable or should have at least a scrollbar otherwise it would be enough to know e g i have to implement methods because currently i can not see the full signature of the methods\n",
      "***Description***: build id i steps to reproduce open completion box in the java editor by typing for example system more information there are no scroll bars in the yellow help popup beside the completion box when the javadoc description is larger than the window i am using ubuntu linux with eclipse rc i this used to work with previous versions\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: action filter providers extension has attribute based on unknown interface\n",
      "***Title***: can not select java class in log listener extension\n",
      "***Description***: the class attribute of the action filter provider element in the org eclipse gmf runtime common ui services action action filter providers extension point is based on an unknown interface org eclipse gmf runtime common ui services action filter iaction filter provider this prevents me from using the plug in manifest editor to quickly create a new class that extends or implements the required class or interface when i click on the class link in the extension element details section of the manifest editor the resulting java attribute editor is not pre populated with that required class or interface this attribute should have the following properties based on org eclipse gmf runtime common ui services action filter abstract action filter pr ovider the extension point description must also be corrected to indicate that users of the action filter providers extension point are expected to subclass the abstract provider rather than implement the internal iaction filter provider interface\n",
      "***Description***: the class attribute of the log listener element in the org eclipse gmf runtime common core log listeners extension point has a string kind this prevents me from selecting an existing ilog listener for this attribute when i use the plug in manifest editor i have to type the name of the class manually this attribute should have the following properties kind java based on org eclipse core runtime ilog listener\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: enable exporting report in various format in viewer\n",
      "***Title***: eclipse wont start\n",
      "***Description***: currently user can only export report as pdf through web viewer to export report as other format user need to modify the url manually this is not very convenient also some format may have its common or specific options for example pagination control locale etc it s also hard to edit and describe these through url so better provide a general export functionality in viewer which can support exporting report as all format that engine support also an optional ui to input extra render options would greatly improve the usability\n",
      "***Description***: eclipse wont start\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: m must be visible to detect correct bad makefiles\n",
      "***Title***: right click causes crash\n",
      "***Description***: on solaris using eclipse wswb i with cdt bits i m trying to import a project that i downloaded or extracted from a windows based server although eclipse and most compilers accept input that has m most versions of make are less tolerant create a file named makefile with carriage returns m in it one easy way to do this is to create it in windows notepad exe then move the file to a unix place remove the txt extension consider content like objs hello world all echo objs from unix run make gnu make produces the following message which is fairly helpful makefile commands commence before first target stop open with vi note the obvious presence of m on every line open with emacs note the obvious presence of m or that the buffer is in dos mode depending on your emacs open the file in eclipse by adding it to a cdt standard make project if you try to build you ll still end up with the message in step open the makefile and you have no idea what is wrong with it expected at least be able to see the problem that make sees i could not find a way for eclipse to show me the m s in the editor as a bonus provide a way to fix the problem eg save in unix mode\n",
      "***Description***: right click causes crash\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 39.7 ms, sys: 4.02 ms, total: 43.7 ms\n",
      "Wall time: 43.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 92499'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30ea59efb4a4790b1093b5dbaa849dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22898a22776b4b1286f45ddb43d6dc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92499), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 1s, sys: 3.5 s, total: 2min 5s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "from keras.layers import MaxPooling1D\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        number_of_layers = K.int_shape(block_input)[2]\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Dense(number_of_layers, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Dense(number_of_layers, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(units, number_of_layers, embedding_layer, title_layer, max_sequence_length, name):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND_{}'.format(name))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # units = 128\n",
    "    #number_of_layers = 6\n",
    "    \n",
    "    if title_layer != None:\n",
    "        title_input = title_layer.input\n",
    "        title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    if title_layer != None:\n",
    "        la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title\n",
    "        if title_layer != None:\n",
    "            la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "            lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        \n",
    "        if title_layer != None:\n",
    "            attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    if title_layer != None:\n",
    "        attention_title_layes = Add()(attention_title_layes)\n",
    "        attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1, 1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    if title_layer != None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "    \n",
    "    cnn_dilated_feature_model = Model(inputs=inputs, \n",
    "                                      outputs=[layer], name = 'FeatureCNNDilatedGenerationModel_{}'.format(name)) # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D, TimeDistributed\n",
    "\n",
    "def cnn_model(embedding_layer, title_input, title_layer, max_sequence_length):\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    \n",
    "    if title_layer != None:\n",
    "        conv = Conv1D(filters=32, kernel_size=5)(l_merge)\n",
    "        #title_layer = Permute((2, 1))(title_layer)\n",
    "        #conv = Permute((2, 1))(conv)\n",
    "        #layer = Dot(axes=1)([conv, title_layer])\n",
    "        #title_layer = TimeDistributed(Dense(1))(title_layer)\n",
    "        title_layer = Flatten()(title_layer)\n",
    "        layer = GlobalAveragePooling1D()(conv)\n",
    "        layer = Concatenate()([layer, title_layer])\n",
    "    else:\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "    \n",
    "    if title_layer != None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "\n",
    "    cnn_feature_model = Model(inputs=inputs, outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    layer = Flatten()(lstm_layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model, lstm_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute, Reshape\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        #block_input = Dense(shape_size_cols)(block_input)\n",
    "        residual =  block_input\n",
    "#         residual = Activation('tanh')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "#         layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "#         layer_out = GRU(100, activation='tanh', return_sequences=True)(layer_out)\n",
    "#         #layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "#         #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "#         layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "#         #layer_out = BatchNormalization()(layer_out)\n",
    "#         layer_out = Dense(50, activation='tanh')(layer_out)\n",
    "#         #layer_out = BatchNormalization()(layer_out)\n",
    "#         layer_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "#         skip_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        #layer_out = Dense(shape_size_cols // 2, activation='tanh')(block_input)\n",
    "        layer_out = Dense(shape_size_cols)(block_input)\n",
    "        skip_out = Dense(shape_size_cols)(block_input)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average, AveragePooling1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "#     bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    #bug_t_feat = GRU(100, return_sequences=False, activation='tanh')(bug_t_feat)\n",
    "    #bug_t_feat = Flatten()(bug_t_feat)\n",
    "    #bug_t_feat = Dense(100, activation='tanh')(bug_t_feat)\n",
    "    \n",
    "    #bug_d_feat = GRU(100, return_sequences=False, activation='tanh')(bug_d_feat)\n",
    "    #bug_d_feat = Flatten()(bug_d_feat)\n",
    "    #bug_d_feat = Dense(100, activation='tanh')(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "#     for _ in range(2):\n",
    "#         bug_feature_output = Dense(150, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = Add()([bug_feature_output, bug_feature_output_1b, bug_feature_output_2b])\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, master_positive, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input, \n",
    "                                 master_anchor.input, master_positive.input, master_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = master_positive.output\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters anchor\n",
    "    master_anchor_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance', output_shape=[1])([encoded_anchor, master_positive])\n",
    "    master_anchor_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance', output_shape=[1])([encoded_anchor, master_negative])\n",
    "    \n",
    "    # Distance master positive\n",
    "    master_pos_positive_d = Lambda(cosine_distance, name='pos_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_positive])\n",
    "    master_pos_negative_d = Lambda(cosine_distance, name='neg_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_negative])\n",
    "    \n",
    "    # Distance master negative\n",
    "    master_neg_positive_d = Lambda(cosine_distance, name='pos_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_negative])\n",
    "    master_neg_negative_d = Lambda(cosine_distance, name='neg_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_positive])\n",
    "    \n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output_bug = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-bug',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    output_master = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-anchor',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_anchor_positive_d, master_anchor_negative_d])\n",
    "    \n",
    "    output_master_pos = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-pos',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_pos_positive_d, master_pos_negative_d])\n",
    "    \n",
    "    output_master_neg = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-neg',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_neg_positive_d, master_neg_negative_d])\n",
    "    \n",
    "    #output = Average()([output_bug, output_master, output_master_pos, output_master_neg])\n",
    "    \n",
    "    output_avg_master = Average()([output_master, output_master_pos, output_master_neg])\n",
    "    output = Average()([output_bug, output_avg_master])\n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, \n",
    "                                 metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_pos (InputLayer)    (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_pos (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_pos (InputLayer)    (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_neg (InputLayer)    (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_neg (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_master_neg (InputLayer)    (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "                                                                 info_master_pos[0][0]            \n",
      "                                                                 info_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          27870451    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "                                                                 title_master_pos[0][0]           \n",
      "                                                                 title_master_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          27999792    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "                                                                 desc_master_pos[0][0]            \n",
      "                                                                 desc_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_pos (Conc (None, 900)          0           FeatureMlpGenerationModel[5][0]  \n",
      "                                                                 FeatureLstmGenerationModel[5][0] \n",
      "                                                                 FeatureCNNGenerationModel[5][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_neg (Conc (None, 900)          0           FeatureMlpGenerationModel[6][0]  \n",
      "                                                                 FeatureLstmGenerationModel[6][0] \n",
      "                                                                 FeatureCNNGenerationModel[6][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-anchor ( (None, 2, 1)         0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-pos (Lam (None, 2, 1)         0           pos_master_pos_cosine_distance[0]\n",
      "                                                                 neg_master_pos_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-neg (Lam (None, 2, 1)         0           pos_master_neg_cosine_distance[0]\n",
      "                                                                 neg_master_neg_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-bug (Lambda)    (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2, 1)         0           stack-distances-master-anchor[0][\n",
      "                                                                 stack-distances-master-pos[0][0] \n",
      "                                                                 stack-distances-master-neg[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 2, 1)         0           stack-distances-bug[0][0]        \n",
      "                                                                 average_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 56,375,143\n",
      "Trainable params: 875,743\n",
      "Non-trainable params: 55,499,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.91, pos_cosine: 0.90, neg_cosine: 0.82\n",
      "Epoch: 2 Loss: 0.93, pos_cosine: 0.89, neg_cosine: 0.82\n",
      "Epoch: 3 Loss: 0.95, pos_cosine: 0.89, neg_cosine: 0.84\n",
      "Epoch: 4 Loss: 0.96, pos_cosine: 0.90, neg_cosine: 0.86\n",
      "Epoch: 5 Loss: 0.92, pos_cosine: 0.90, neg_cosine: 0.82\n",
      "Epoch: 6 Loss: 0.94, pos_cosine: 0.90, neg_cosine: 0.84\n",
      "Epoch: 7 Loss: 0.91, pos_cosine: 0.90, neg_cosine: 0.81\n",
      "Epoch: 8 Loss: 0.93, pos_cosine: 0.90, neg_cosine: 0.83\n",
      "Epoch: 9 Loss: 0.90, pos_cosine: 0.91, neg_cosine: 0.81\n",
      "Epoch: 10 Loss: 0.91, pos_cosine: 0.92, neg_cosine: 0.83\n",
      "Epoch: 11 Loss: 0.91, pos_cosine: 0.91, neg_cosine: 0.82\n",
      "Epoch: 12 Loss: 0.94, pos_cosine: 0.90, neg_cosine: 0.84\n",
      "Epoch: 13 Loss: 0.87, pos_cosine: 0.90, neg_cosine: 0.77\n",
      "Epoch: 14 Loss: 0.89, pos_cosine: 0.89, neg_cosine: 0.78\n",
      "Epoch: 15 Loss: 0.92, pos_cosine: 0.91, neg_cosine: 0.83\n",
      "Epoch: 16 Loss: 0.91, pos_cosine: 0.91, neg_cosine: 0.82\n",
      "Epoch: 17 Loss: 0.96, pos_cosine: 0.90, neg_cosine: 0.86\n",
      "Epoch: 18 Loss: 0.93, pos_cosine: 0.92, neg_cosine: 0.85\n",
      "Epoch: 19 Loss: 0.91, pos_cosine: 0.91, neg_cosine: 0.82\n",
      "Epoch: 20 Loss: 0.91, pos_cosine: 0.92, neg_cosine: 0.83\n",
      "Epoch: 21 Loss: 0.91, pos_cosine: 0.90, neg_cosine: 0.81\n",
      "Epoch: 22 Loss: 0.92, pos_cosine: 0.91, neg_cosine: 0.83\n",
      "Epoch: 23 Loss: 0.95, pos_cosine: 0.90, neg_cosine: 0.84\n",
      "Epoch: 24 Loss: 0.92, pos_cosine: 0.89, neg_cosine: 0.81\n",
      "Epoch: 25 Loss: 0.89, pos_cosine: 0.89, neg_cosine: 0.78\n",
      "Epoch: 26 Loss: 0.93, pos_cosine: 0.91, neg_cosine: 0.84\n",
      "Epoch: 27 Loss: 0.94, pos_cosine: 0.89, neg_cosine: 0.82\n",
      "Epoch: 28 Loss: 0.92, pos_cosine: 0.91, neg_cosine: 0.82\n",
      "Epoch: 29 Loss: 0.95, pos_cosine: 0.87, neg_cosine: 0.82\n",
      "Epoch: 30 Loss: 0.92, pos_cosine: 0.91, neg_cosine: 0.83\n",
      "Epoch: 31 Loss: 0.92, pos_cosine: 0.89, neg_cosine: 0.81\n",
      "Epoch: 32 Loss: 0.93, pos_cosine: 0.88, neg_cosine: 0.81\n",
      "Epoch: 33 Loss: 0.93, pos_cosine: 0.89, neg_cosine: 0.82\n",
      "Epoch: 34 Loss: 0.91, pos_cosine: 0.89, neg_cosine: 0.80\n",
      "Epoch: 35 Loss: 0.93, pos_cosine: 0.88, neg_cosine: 0.81\n",
      "Epoch: 36 Loss: 0.87, pos_cosine: 0.89, neg_cosine: 0.77\n",
      "Epoch: 37 Loss: 0.91, pos_cosine: 0.87, neg_cosine: 0.78\n",
      "Epoch: 38 Loss: 0.89, pos_cosine: 0.86, neg_cosine: 0.75\n",
      "Epoch: 39 Loss: 0.93, pos_cosine: 0.85, neg_cosine: 0.78\n",
      "Epoch: 40 Loss: 0.94, pos_cosine: 0.86, neg_cosine: 0.81\n",
      "Epoch: 41 Loss: 0.93, pos_cosine: 0.83, neg_cosine: 0.77\n",
      "Epoch: 42 Loss: 0.91, pos_cosine: 0.82, neg_cosine: 0.73\n",
      "Epoch: 43 Loss: 0.92, pos_cosine: 0.84, neg_cosine: 0.76\n",
      "Epoch: 44 Loss: 0.92, pos_cosine: 0.86, neg_cosine: 0.78\n",
      "Epoch: 45 Loss: 0.90, pos_cosine: 0.87, neg_cosine: 0.77\n",
      "Epoch: 46 Loss: 0.92, pos_cosine: 0.86, neg_cosine: 0.78\n",
      "Epoch: 47 Loss: 0.90, pos_cosine: 0.85, neg_cosine: 0.75\n",
      "Epoch: 48 Loss: 0.94, pos_cosine: 0.81, neg_cosine: 0.75\n",
      "Epoch: 49 Loss: 0.90, pos_cosine: 0.84, neg_cosine: 0.74\n",
      "Epoch: 50 Loss: 0.89, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 51 Loss: 0.92, pos_cosine: 0.83, neg_cosine: 0.75\n",
      "Epoch: 52 Loss: 0.90, pos_cosine: 0.84, neg_cosine: 0.74\n",
      "Epoch: 53 Loss: 0.89, pos_cosine: 0.87, neg_cosine: 0.76\n",
      "Epoch: 54 Loss: 0.93, pos_cosine: 0.85, neg_cosine: 0.78\n",
      "Epoch: 55 Loss: 0.93, pos_cosine: 0.83, neg_cosine: 0.76\n",
      "Epoch: 56 Loss: 0.88, pos_cosine: 0.83, neg_cosine: 0.71\n",
      "Epoch: 57 Loss: 0.92, pos_cosine: 0.85, neg_cosine: 0.77\n",
      "Epoch: 58 Loss: 0.89, pos_cosine: 0.82, neg_cosine: 0.71\n",
      "Epoch: 59 Loss: 0.91, pos_cosine: 0.85, neg_cosine: 0.76\n",
      "Epoch: 60 Loss: 0.92, pos_cosine: 0.84, neg_cosine: 0.76\n",
      "Epoch: 61 Loss: 0.96, pos_cosine: 0.85, neg_cosine: 0.81\n",
      "Epoch: 62 Loss: 0.93, pos_cosine: 0.84, neg_cosine: 0.76\n",
      "Epoch: 63 Loss: 0.87, pos_cosine: 0.86, neg_cosine: 0.73\n",
      "Epoch: 64 Loss: 0.92, pos_cosine: 0.85, neg_cosine: 0.77\n",
      "Epoch: 65 Loss: 0.91, pos_cosine: 0.90, neg_cosine: 0.81\n",
      "Epoch: 66 Loss: 0.88, pos_cosine: 0.89, neg_cosine: 0.77\n",
      "Epoch: 67 Loss: 0.92, pos_cosine: 0.85, neg_cosine: 0.77\n",
      "Epoch: 68 Loss: 0.89, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 69 Loss: 0.92, pos_cosine: 0.83, neg_cosine: 0.75\n",
      "Epoch: 70 Loss: 0.90, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 71 Loss: 0.88, pos_cosine: 0.85, neg_cosine: 0.73\n",
      "Epoch: 72 Loss: 0.91, pos_cosine: 0.82, neg_cosine: 0.73\n",
      "Epoch: 73 Loss: 0.90, pos_cosine: 0.85, neg_cosine: 0.75\n",
      "Epoch: 74 Loss: 0.90, pos_cosine: 0.86, neg_cosine: 0.75\n",
      "Epoch: 75 Loss: 0.90, pos_cosine: 0.85, neg_cosine: 0.75\n",
      "Epoch: 76 Loss: 0.94, pos_cosine: 0.87, neg_cosine: 0.81\n",
      "Epoch: 77 Loss: 0.89, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 78 Loss: 0.88, pos_cosine: 0.85, neg_cosine: 0.73\n",
      "Epoch: 79 Loss: 0.90, pos_cosine: 0.85, neg_cosine: 0.75\n",
      "Epoch: 80 Loss: 0.88, pos_cosine: 0.83, neg_cosine: 0.72\n",
      "Epoch: 81 Loss: 0.91, pos_cosine: 0.86, neg_cosine: 0.77\n",
      "Epoch: 82 Loss: 0.89, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 83 Loss: 0.93, pos_cosine: 0.81, neg_cosine: 0.73\n",
      "Epoch: 84 Loss: 0.88, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 85 Loss: 0.85, pos_cosine: 0.84, neg_cosine: 0.69\n",
      "Epoch: 86 Loss: 0.89, pos_cosine: 0.86, neg_cosine: 0.75\n",
      "Epoch: 87 Loss: 0.91, pos_cosine: 0.83, neg_cosine: 0.74\n",
      "Epoch: 88 Loss: 0.94, pos_cosine: 0.79, neg_cosine: 0.72\n",
      "Epoch: 89 Loss: 0.89, pos_cosine: 0.85, neg_cosine: 0.74\n",
      "Epoch: 90 Loss: 0.90, pos_cosine: 0.84, neg_cosine: 0.74\n",
      "Epoch: 91 Loss: 0.91, pos_cosine: 0.84, neg_cosine: 0.74\n",
      "Epoch: 92 Loss: 0.90, pos_cosine: 0.82, neg_cosine: 0.72\n",
      "Epoch: 93 Loss: 0.93, pos_cosine: 0.82, neg_cosine: 0.74\n",
      "Epoch: 94 Loss: 0.91, pos_cosine: 0.81, neg_cosine: 0.72\n",
      "Epoch: 95 Loss: 0.87, pos_cosine: 0.80, neg_cosine: 0.68\n",
      "Epoch: 96 Loss: 0.95, pos_cosine: 0.78, neg_cosine: 0.73\n",
      "Epoch: 97 Loss: 0.92, pos_cosine: 0.82, neg_cosine: 0.74\n",
      "Epoch: 98 Loss: 0.94, pos_cosine: 0.82, neg_cosine: 0.77\n",
      "Epoch: 99 Loss: 0.92, pos_cosine: 0.82, neg_cosine: 0.74\n",
      "Epoch: 100 Loss: 0.88, pos_cosine: 0.84, neg_cosine: 0.72, recall@25: 0.20\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(eclipse).h5' to disk\n",
      "Best_epoch=85, Best_loss=0.85s, Recall@25=0.20\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import keras\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model, title_layer = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, None, None, MAX_SEQUENCE_LENGTH_D)\n",
    "#title_feature_model = cnn_dilated_model(50, 6, title_embedding_layer, None, MAX_SEQUENCE_LENGTH_T, 'title')\n",
    "#desc_feature_model = cnn_dilated_model(128, 6, desc_embedding_layer, title_feature_model, MAX_SEQUENCE_LENGTH_D, 'desc')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "master_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_in')\n",
    "master_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_pos')\n",
    "master_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, master_positive, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = batch_iterator(baseline, baseline.train_data, baseline.dup_sets_train, bug_train_ids, \n",
    "                                       batch_size, 1, issues_by_buckets)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info'],\n",
    "                  train_master_input['title'], train_master_input['description'], train_master_input['info'],\n",
    "                  train_master_input['title'], train_master_input['description'], train_master_input['info'],\n",
    "                   train_master_neg['title'], train_master_neg['description'], train_master_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}s, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['327681:324658|382274:0.9399709813296795,361301:0.9398491233587265,397407:0.938922606408596,364809:0.937847413122654,358912:0.9373695477843285,398885:0.9371131956577301,323220:0.9368166476488113,358938:0.9290433377027512,329254:0.928930789232254,296664:0.9220841601490974,421647:0.9196240156888962,351068:0.9189987406134605,401574:0.9182261228561401,363830:0.9159024655818939,321346:0.9151357784867287,340903:0.914868175983429,393566:0.9140876084566116,378868:0.9105135351419449,359609:0.908665731549263,394186:0.9084839820861816,407704:0.9084819778800011,421713:0.907542034983635,401029:0.9047767594456673,366850:0.9045613408088684,323040:0.9042341634631157,355440:0.9035104140639305,366294:0.9029841423034668,339356:0.9015799090266228,382193:0.9014610052108765',\n",
       " '324658:327681|361734:0.8739428222179413,358287:0.8705032467842102,370853:0.8679444342851639,360484:0.8657918274402618,378700:0.863791361451149,360929:0.8617623448371887,361575:0.859798014163971,355014:0.8596422523260117,357184:0.858992412686348,326868:0.8548230975866318,411919:0.8539618104696274,360430:0.8537470996379852,383212:0.8513846099376678,342027:0.8496512621641159,369612:0.8476570844650269,337781:0.8453914970159531,376457:0.844833642244339,350353:0.8435502499341965,326338:0.8424628525972366,374836:0.8423267006874084,357040:0.8420718908309937,373884:0.8418519645929337,336463:0.8406793475151062,324863:0.8393435478210449,325519:0.8392435610294342,346060:0.8384023457765579,411141:0.8383007496595383,387650:0.8380982279777527,295598:0.8377516269683838',\n",
       " '417795:417796,403749|359268:0.9098722636699677,346872:0.8999414891004562,389246:0.8923430219292641,373949:0.8898613899946213,404937:0.8809617683291435,348435:0.8796273469924927,336223:0.8786649703979492,327731:0.878453366458416,345099:0.8763239532709122,364068:0.8746145069599152,366405:0.8689065873622894,351254:0.8673515766859055,352607:0.8671999275684357,400979:0.8655269593000412,412082:0.8642118573188782,416793:0.8633967936038971,382095:0.862705871462822,331992:0.8626502305269241,402522:0.8623263388872147,409035:0.8621004670858383,350686:0.8609070628881454,372885:0.8607305288314819,390196:0.8598119765520096,345193:0.85939060151577,352991:0.8592526912689209,371393:0.8575263470411301,332454:0.8570229858160019,355148:0.8565148115158081,318499:0.8551036864519119',\n",
       " '417796:417795,403749|409086:0.8353171199560165,344729:0.8324747979640961,360384:0.8307801336050034,237638:0.8304319083690643,65876:0.8202541321516037,320246:0.8173058927059174,370831:0.8161363452672958,412309:0.8068710267543793,420811:0.8001045137643814,349411:0.79829141497612,363714:0.7955138236284256,389597:0.7951665371656418,382323:0.7943443655967712,364637:0.7940207570791245,337941:0.7938993275165558,94152:0.7931584566831589,311877:0.792796179652214,391433:0.7914751619100571,353517:0.7902916520833969,377849:0.7901642173528671,404624:0.7901623249053955,347929:0.7897105515003204,323991:0.7896927297115326,329935:0.7870728969573975,324470:0.7858709841966629,403213:0.7854511141777039,323000:0.7839564383029938,346838:0.7830673158168793,405416:0.7830485701560974',\n",
       " '403749:417795,417796|411794:0.9007540792226791,390915:0.8936283066868782,338663:0.8822337239980698,321035:0.8808987662196159,387030:0.8758343905210495,348655:0.8724110722541809,398513:0.8723047822713852,374349:0.8706292808055878,370851:0.8697354793548584,364680:0.8658439218997955,418692:0.8648641705513,321159:0.8628461956977844,57570:0.8619767129421234,328918:0.8607400208711624,413246:0.8590202778577805,387940:0.8589722961187363,380831:0.858351543545723,403022:0.8566679209470749,372789:0.8560593277215958,413618:0.8558004349470139,345286:0.8557406961917877,373135:0.8524780124425888,351442:0.8524568527936935,384443:0.8513746410608292,378052:0.849589467048645,394692:0.8494834303855896,407864:0.8488543033599854,333183:0.8484289795160294,331441:0.8480346500873566',\n",
       " '417796:417795,403749|409086:0.8353171199560165,344729:0.8324747979640961,360384:0.8307801336050034,237638:0.8304319083690643,65876:0.8202541321516037,320246:0.8173058927059174,370831:0.8161363452672958,412309:0.8068710267543793,420811:0.8001045137643814,349411:0.79829141497612,363714:0.7955138236284256,389597:0.7951665371656418,382323:0.7943443655967712,364637:0.7940207570791245,337941:0.7938993275165558,94152:0.7931584566831589,311877:0.792796179652214,391433:0.7914751619100571,353517:0.7902916520833969,377849:0.7901642173528671,404624:0.7901623249053955,347929:0.7897105515003204,323991:0.7896927297115326,329935:0.7870728969573975,324470:0.7858709841966629,403213:0.7854511141777039,323000:0.7839564383029938,346838:0.7830673158168793,405416:0.7830485701560974',\n",
       " '403749:417795,417796|411794:0.9007540792226791,390915:0.8936283066868782,338663:0.8822337239980698,321035:0.8808987662196159,387030:0.8758343905210495,348655:0.8724110722541809,398513:0.8723047822713852,374349:0.8706292808055878,370851:0.8697354793548584,364680:0.8658439218997955,418692:0.8648641705513,321159:0.8628461956977844,57570:0.8619767129421234,328918:0.8607400208711624,413246:0.8590202778577805,387940:0.8589722961187363,380831:0.858351543545723,403022:0.8566679209470749,372789:0.8560593277215958,413618:0.8558004349470139,345286:0.8557406961917877,373135:0.8524780124425888,351442:0.8524568527936935,384443:0.8513746410608292,378052:0.849589467048645,394692:0.8494834303855896,407864:0.8488543033599854,333183:0.8484289795160294,331441:0.8480346500873566',\n",
       " '319495:319752,319435,319915,319471,319895,318297,319514,319515,319517,319551|376562:0.9233387559652328,331436:0.9221077486872673,362568:0.9125593677163124,374870:0.9085189774632454,361718:0.9075465500354767,327073:0.903048649430275,403900:0.898420013487339,389199:0.8944748565554619,357760:0.8924636244773865,386640:0.8921522125601768,352801:0.8908432945609093,388482:0.8887021467089653,367592:0.8880107998847961,380864:0.8875910714268684,404802:0.8861261531710625,394812:0.8860557526350021,374411:0.8840031325817108,393320:0.8834315538406372,375920:0.8828914165496826,392414:0.8817092478275299,391583:0.880836233496666,369540:0.8803456723690033,398152:0.8781099393963814,405707:0.8774791210889816,374081:0.8751815184950829,384999:0.8749421238899231,332088:0.8728924542665482,418838:0.8712702840566635,336302:0.8707605302333832',\n",
       " '319752:319495,319435,319915,319471,319895,318297,319514,319515,319517,319551|381149:0.9214159473776817,358240:0.9093112424015999,318655:0.8862574398517609,392674:0.8777771145105362,324454:0.8680839836597443,396128:0.8669408708810806,389840:0.8607377260923386,342591:0.8544125556945801,421946:0.8524810969829559,389491:0.8515222817659378,372299:0.8460009098052979,410252:0.8450626730918884,386301:0.8448324948549271,405341:0.8400352150201797,353781:0.82729172706604,379162:0.8267047107219696,304215:0.8252705931663513,412793:0.8224277496337891,353905:0.821664109826088,338250:0.8186091333627701,387377:0.8183096498250961,385675:0.8170656263828278,389335:0.8166922330856323,395528:0.8160144984722137,326298:0.8153060227632523,320640:0.815135195851326,386001:0.8126434087753296,349478:0.8125049918889999,357890:0.8124606013298035',\n",
       " '319435:319495,319752,319915,319471,319895,318297,319514,319515,319517,319551|356184:0.6331499218940735,317833:0.5995396673679352,332891:0.5551489591598511,332892:0.5551489591598511,350122:0.5281863212585449,358267:0.4989761710166931,406745:0.47866493463516235,345251:0.47528547048568726,313327:0.464322030544281,360214:0.46072614192962646,379606:0.45543694496154785,350900:0.45235568284988403,407783:0.441225528717041,326328:0.43889880180358887,356446:0.4372254014015198,326454:0.42804157733917236,347768:0.42742741107940674,352470:0.41720640659332275,416267:0.41707366704940796,377654:0.4149009585380554,356295:0.41481685638427734,356717:0.4133981466293335,415161:0.40913647413253784,271953:0.40744519233703613,422917:0.39961034059524536,370556:0.39930373430252075,330395:0.3992488384246826,377486:0.39595282077789307,366651:0.3903777003288269',\n",
       " '319915:319495,319752,319435,319471,319895,318297,319514,319515,319517,319551|310675:0.8016991764307022,314163:0.7988088876008987,374058:0.7890567928552628,328737:0.7821466624736786,387253:0.7713912129402161,395005:0.7707082778215408,359237:0.7567945122718811,397927:0.7495487332344055,407375:0.7450714707374573,359116:0.7416233420372009,387865:0.7415667772293091,334537:0.7412482798099518,303904:0.7412423491477966,327801:0.74024698138237,351474:0.7363761365413666,350247:0.735698789358139,320669:0.735605001449585,360266:0.734953373670578,330373:0.7324156761169434,401746:0.7261068820953369,388340:0.7260391712188721,354810:0.7212159335613251,342973:0.720753014087677,383303:0.7204328775405884,378519:0.7198841571807861,376143:0.7193425297737122,343932:0.7185070812702179,390963:0.7173254191875458,405135:0.7173137068748474',\n",
       " '319471:319495,319752,319435,319915,319895,318297,319514,319515,319517,319551|385307:0.8394344449043274,324378:0.8265649676322937,379885:0.8263829946517944,346782:0.8261914104223251,382440:0.8202480524778366,388224:0.8199030458927155,386868:0.8130693733692169,389810:0.8107167184352875,254704:0.8017064929008484,382781:0.7904938757419586,338931:0.7899178564548492,357399:0.7888084650039673,321335:0.7865059673786163,360386:0.7816707640886307,323715:0.7814190238714218,345106:0.7805507630109787,369971:0.7792423218488693,356249:0.7768418192863464,361643:0.7763417065143585,350563:0.7758194953203201,405513:0.7755525261163712,323957:0.7747556269168854,337064:0.7744308114051819,324647:0.7743179500102997,376657:0.7737672030925751,393981:0.772711306810379,334538:0.7712653428316116,361780:0.771125078201294,417287:0.7710954397916794',\n",
       " '319895:319495,319752,319435,319915,319471,318297,319514,319515,319517,319551|350225:0.8935670033097267,334187:0.8722990304231644,366179:0.8711462318897247,293689:0.8703936487436295,352049:0.8607424944639206,336696:0.85159532725811,371758:0.8511549681425095,354901:0.8500785082578659,273411:0.8458370566368103,384250:0.8404218852519989,351832:0.8398153632879257,359857:0.8391069173812866,371346:0.837944433093071,373351:0.8378067165613174,333466:0.8377933651208878,326688:0.8375619947910309,396478:0.8368732929229736,378968:0.8367714583873749,373911:0.8356136828660965,401393:0.8354862183332443,356199:0.8350160121917725,366206:0.8336406052112579,411673:0.8329740166664124,326600:0.8324678242206573,375916:0.8320435732603073,366199:0.8319247663021088,366248:0.8315743058919907,349524:0.8271848112344742,332418:0.8268798738718033',\n",
       " '318297:319495,319752,319435,319915,319471,319895,319514,319515,319517,319551|315670:0.940732654184103,348966:0.9312311857938766,383717:0.9189017042517662,389108:0.9180148392915726,357248:0.9153631329536438,377246:0.9081418737769127,413091:0.9002747312188148,407076:0.8940433040261269,395752:0.8893860355019569,358011:0.8882791250944138,364824:0.8871932104229927,396486:0.8863296955823898,403428:0.8846025466918945,357117:0.883559063076973,258182:0.8833992332220078,320758:0.8830805197358131,373174:0.882975697517395,199110:0.8768559023737907,382486:0.8754465207457542,310082:0.875164233148098,358243:0.8672483563423157,330122:0.8638697117567062,391902:0.8636779934167862,322312:0.8623626083135605,389066:0.8609025329351425,389085:0.8609025329351425,389093:0.8609025329351425,389096:0.8609025329351425,317862:0.860736608505249',\n",
       " '319514:319495,319752,319435,319915,319471,319895,318297,319515,319517,319551|411257:0.7950222045183182,337927:0.7903079986572266,408204:0.7869221121072769,362311:0.7864773124456406,402393:0.7833280712366104,390860:0.7807396054267883,370168:0.7773001790046692,325997:0.7679217904806137,347194:0.7665331065654755,368943:0.7634259611368179,346882:0.7566786408424377,386905:0.754986509680748,381495:0.7549169659614563,385592:0.7548826783895493,325811:0.7507301270961761,320243:0.7504115253686905,349252:0.7502692639827728,239589:0.7498957216739655,314255:0.7486307919025421,351126:0.7486098110675812,338490:0.7485820055007935,376229:0.7485354840755463,336761:0.7474949955940247,376236:0.7472561001777649,419503:0.7454721331596375,388108:0.7433304488658905,409732:0.7408874332904816,336812:0.7386117279529572,303168:0.7374295294284821',\n",
       " '319515:319495,319752,319435,319915,319471,319895,318297,319514,319517,319551|316183:0.9346782565116882,373912:0.9311404004693031,397836:0.92395980656147,325100:0.9188214614987373,353837:0.9125785529613495,376562:0.902651883661747,391078:0.9022087901830673,357676:0.9006826877593994,394705:0.8988158330321312,362399:0.8960880786180496,319438:0.8943660780787468,319119:0.8940570279955864,382247:0.8927965015172958,395157:0.8918891400098801,331436:0.8912007659673691,389199:0.8902794122695923,357760:0.8875003531575203,327073:0.8864993080496788,319495:0.8844132050871849,384022:0.8834988102316856,374870:0.8799435272812843,377981:0.8790874406695366,356415:0.878518246114254,384999:0.8780030161142349,322714:0.8763102740049362,358912:0.8759719282388687,361718:0.8755732700228691,361301:0.875035397708416,369540:0.874869167804718',\n",
       " '319517:319495,319752,319435,319915,319471,319895,318297,319514,319515,319551|370058:0.8128759413957596,339376:0.8025497943162918,353521:0.8023419827222824,354346:0.7922675162553787,350873:0.7913002073764801,337553:0.786966547369957,390570:0.7696880251169205,400095:0.7589938193559647,340405:0.7576474100351334,346590:0.7566661089658737,361893:0.7551096230745316,345518:0.7510915845632553,330907:0.7506878972053528,357152:0.7497180998325348,351778:0.7477532625198364,345190:0.7474369406700134,364134:0.7447140514850616,353227:0.7440918684005737,318108:0.7428935468196869,319609:0.7399651110172272,385162:0.7384259700775146,334455:0.7369334399700165,327392:0.7364144623279572,327394:0.7361336946487427,335204:0.7352023124694824,351576:0.7341454923152924,369252:0.7319456040859222,368676:0.729483425617218,372319:0.7288727462291718',\n",
       " '319551:319495,319752,319435,319915,319471,319895,318297,319514,319515,319517|348320:0.860817164182663,347855:0.8429833650588989,369281:0.8342874050140381,345394:0.8325023949146271,293774:0.8259346932172775,393957:0.8249505758285522,345249:0.8243003934621811,333186:0.8208758980035782,384987:0.8190633803606033,345289:0.8108405917882919,362206:0.8108249008655548,357483:0.8103523105382919,345603:0.8058900684118271,392136:0.8054494112730026,335755:0.8001355528831482,363262:0.7984838336706161,360383:0.7946823537349701,403813:0.7919880002737045,386050:0.7898531407117844,340714:0.7896636724472046,372847:0.7883334010839462,339952:0.787569522857666,376613:0.7859527468681335,334355:0.7842362225055695,419892:0.7832430899143219,355384:0.7815461754798889,323187:0.7801982015371323,384433:0.777673676609993,389770:0.7766793072223663',\n",
       " '401416:401362,401363,401461,401023|359593:0.8768323808908463,337365:0.8763005062937737,396400:0.8755559772253036,379861:0.8750750720500946,403401:0.8594694435596466,385965:0.8562025427818298,383361:0.8545727729797363,322407:0.8537381589412689,386132:0.8534481078386307,345402:0.8509608209133148,339300:0.8495109528303146,375254:0.8481437414884567,399771:0.8426241427659988,333822:0.8387839049100876,344373:0.8381825983524323,349023:0.8378995656967163,318187:0.8377015739679337,366094:0.8362077474594116,361739:0.8356816917657852,371799:0.8355754017829895,404764:0.8354011476039886,350401:0.8345212489366531,342591:0.8341722637414932,369837:0.8338996320962906,339788:0.8334876000881195,421315:0.8328191190958023,350161:0.8324961364269257,353781:0.8323271423578262,368682:0.831656739115715',\n",
       " '401362:401416,401363,401461,401023|345680:0.9162292182445526,389594:0.9132651686668396,271291:0.9118975177407265,383181:0.9036721810698509,383539:0.9033061489462852,365392:0.901294119656086,336302:0.8961618468165398,408680:0.893602542579174,407455:0.8857942447066307,340304:0.8844056874513626,341825:0.8823594003915787,220098:0.882284365594387,374411:0.8822347968816757,387113:0.8809384107589722,378201:0.8807857260107994,410898:0.8797876685857773,352837:0.8794011026620865,344909:0.8756198957562447,379336:0.8747157007455826,411467:0.8743221461772919,347371:0.8734118491411209,387851:0.8713917136192322,411453:0.8701486438512802,416286:0.8700160980224609,399434:0.869780421257019,419095:0.8686933666467667,384999:0.8637591600418091,410705:0.8635642826557159,358169:0.8628701120615005']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 4641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_100epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          27870451    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          27999792    desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 56,375,143\n",
      "Trainable params: 875,743\n",
      "Non-trainable params: 55,499,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/exported_rank_propose_master_triplet_loss.txt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.14,\n",
       " '2 - recall_at_10': 0.16,\n",
       " '3 - recall_at_15': 0.17,\n",
       " '4 - recall_at_20': 0.19,\n",
       " '5 - recall_at_25': 0.2}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
