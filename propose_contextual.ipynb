{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - PROPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'netbeans'\n",
    "METHOD = 'propose_{}'.format(epochs)\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8465964a6d1c4acaab2a7f92d064cc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e11dc83ab7e4d7f87373587d87a6225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796596ee470249608485ff8e0dfc47a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e2d9cebec401f8c35589fed16cd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 16.8 s, sys: 2.18 s, total: 19 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af08912599ed4feb85e1d0c7a1fb0291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=181971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n"
     ]
    }
   ],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '0\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '139\\n',\n",
       " 'creation_ts': '2007-11-14 14:14:00 +0000',\n",
       " 'delta_ts': '2008-02-07 14:26:30 +0000',\n",
       " 'description': '[CLS] product version : net ##be ##ans id ##e 6 . 0 rc ##1 ( build 2007 ##11 ##14 ##00 ##00 ) java : 1 . 6 . 0 _ 05 - ea ; java hot ##sp ##ot ( t ##m ) client v ##m 1 . 6 . 0 _ 05 - ea - b ##0 ##6 system : linux version 2 . 6 . 5 - 1 . 35 ##8 running on i ##38 ##6 ; ut ##f - 8 ; en _ us ( n ##b ) reference to outer class is replaced by null when access ##ing static members steps to reproduce : 1 ) have a class : public class new ##class { public static int field ; static class inner { public void m ( ) { field = 2 ; } } } 2 ) move inner to outer - > package p ; class inner { public void m ( ) { null . field = 2 ; / / < - - - - - - - - here should be new ##class . filed = 2 ; } } [SEP]',\n",
       " 'description_bert': '[CLS] product version : net ##be ##ans id ##e 6 . 0 rc ##1 ( build 2007 ##11 ##14 ##00 ##00 ) java : 1 . 6 . 0 _ 05 - ea ; java hot ##sp ##ot ( t ##m ) client v ##m 1 . 6 . 0 _ 05 - ea - b ##0 ##6 system : linux version 2 . 6 . 5 - 1 . 35 ##8 running on i ##38 ##6 ; ut ##f - 8 ; en _ us ( n ##b ) reference to outer class is replaced by null when access ##ing static members steps to reproduce : 1 ) have a class : public class new ##class { public static int field ; static class inner { public void m ( ) { field = 2 ; } } } 2 ) move inner to outer - > package p ; class inner { public void m ( ) { null . field = 2 ; / / < - - - - - - - - here should be new ##class . filed = 2 ; } } [SEP]',\n",
       " 'description_word': array([  101,  4031,  2544,  1024,  5658,  4783,  6962,  8909,  2063,\n",
       "         1020,  1012,  1014, 22110,  2487,  1006,  3857,  2289, 14526,\n",
       "        16932,  8889]),\n",
       " 'description_word_bert': [101,\n",
       "  4031,\n",
       "  2544,\n",
       "  1024,\n",
       "  5658,\n",
       "  4783,\n",
       "  6962,\n",
       "  8909,\n",
       "  2063,\n",
       "  1020,\n",
       "  1012,\n",
       "  1014,\n",
       "  22110,\n",
       "  2487,\n",
       "  1006,\n",
       "  3857,\n",
       "  2289,\n",
       "  14526,\n",
       "  16932,\n",
       "  8889,\n",
       "  8889,\n",
       "  1007,\n",
       "  9262,\n",
       "  1024,\n",
       "  1015,\n",
       "  1012,\n",
       "  1020,\n",
       "  1012,\n",
       "  1014,\n",
       "  1035,\n",
       "  5709,\n",
       "  1011,\n",
       "  19413,\n",
       "  1025,\n",
       "  9262,\n",
       "  2980,\n",
       "  13102,\n",
       "  4140,\n",
       "  1006,\n",
       "  1056,\n",
       "  2213,\n",
       "  1007,\n",
       "  7396,\n",
       "  1058,\n",
       "  2213,\n",
       "  1015,\n",
       "  1012,\n",
       "  1020,\n",
       "  1012,\n",
       "  1014,\n",
       "  1035,\n",
       "  5709,\n",
       "  1011,\n",
       "  19413,\n",
       "  1011,\n",
       "  1038,\n",
       "  2692,\n",
       "  2575,\n",
       "  2291,\n",
       "  1024,\n",
       "  11603,\n",
       "  2544,\n",
       "  1016,\n",
       "  1012,\n",
       "  1020,\n",
       "  1012,\n",
       "  1019,\n",
       "  1011,\n",
       "  1015,\n",
       "  1012,\n",
       "  3486,\n",
       "  2620,\n",
       "  2770,\n",
       "  2006,\n",
       "  1045,\n",
       "  22025,\n",
       "  2575,\n",
       "  1025,\n",
       "  21183,\n",
       "  2546,\n",
       "  1011,\n",
       "  1022,\n",
       "  1025,\n",
       "  4372,\n",
       "  1035,\n",
       "  2149,\n",
       "  1006,\n",
       "  1050,\n",
       "  2497,\n",
       "  1007,\n",
       "  4431,\n",
       "  2000,\n",
       "  6058,\n",
       "  2465,\n",
       "  2003,\n",
       "  2999,\n",
       "  2011,\n",
       "  19701,\n",
       "  2043,\n",
       "  3229,\n",
       "  2075,\n",
       "  10763,\n",
       "  2372,\n",
       "  4084,\n",
       "  2000,\n",
       "  21376,\n",
       "  1024,\n",
       "  1015,\n",
       "  1007,\n",
       "  2031,\n",
       "  1037,\n",
       "  2465,\n",
       "  1024,\n",
       "  2270,\n",
       "  2465,\n",
       "  2047,\n",
       "  26266,\n",
       "  1063,\n",
       "  2270,\n",
       "  10763,\n",
       "  20014,\n",
       "  2492,\n",
       "  1025,\n",
       "  10763,\n",
       "  2465,\n",
       "  5110,\n",
       "  1063,\n",
       "  2270,\n",
       "  11675,\n",
       "  1049,\n",
       "  1006,\n",
       "  1007,\n",
       "  1063,\n",
       "  2492,\n",
       "  1027,\n",
       "  1016,\n",
       "  1025,\n",
       "  1065,\n",
       "  1065,\n",
       "  1065,\n",
       "  1016,\n",
       "  1007,\n",
       "  2693,\n",
       "  5110,\n",
       "  2000,\n",
       "  6058,\n",
       "  1011,\n",
       "  1028,\n",
       "  7427,\n",
       "  102],\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 121906,\n",
       " 'priority': '3\\n',\n",
       " 'product': '26\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_word': array([  101,  1031,  2693,  5110,  2000,  6058,  1033,  4431,  2000,\n",
       "         6058,  2465,  2003,  2999,  2007, 19701,   102,     0,     0,\n",
       "            0,     0,   101,  4031,  2544,  1024,  5658,  4783,  6962,\n",
       "         8909,  2063,  1020,  1012,  1014, 22110,  2487,  1006,  3857,\n",
       "         2289, 14526, 16932,  8889]),\n",
       " 'title': '[CLS] [ move inner to outer ] reference to outer class is replaced with null [SEP]',\n",
       " 'title_bert': '[CLS] [ move inner to outer ] reference to outer class is replaced with null [SEP]',\n",
       " 'title_word': array([  101,  1031,  2693,  5110,  2000,  6058,  1033,  4431,  2000,\n",
       "         6058,  2465,  2003,  2999,  2007, 19701,   102,     0,     0,\n",
       "            0,     0]),\n",
       " 'title_word_bert': [101,\n",
       "  1031,\n",
       "  2693,\n",
       "  5110,\n",
       "  2000,\n",
       "  6058,\n",
       "  1033,\n",
       "  4431,\n",
       "  2000,\n",
       "  6058,\n",
       "  2465,\n",
       "  2003,\n",
       "  2999,\n",
       "  2007,\n",
       "  19701,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'version': '7\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 30600)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 ms, sys: 16 µs, total: 52.8 ms\n",
      "Wall time: 52.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = experiment.batch_iterator(None,\n",
    "                                                                                          baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1,\n",
    "                                                                                          issues_by_buckets)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (128, 544), (128,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 19061'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815b1d5be1304554993767820f82d685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1020484fd61f4687a9894cdab088aa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 21s, sys: 2.66 s, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def residual_context():\n",
    "    def block(block_input):\n",
    "        residual = block_input\n",
    "        \n",
    "        layer = block_input\n",
    "        for filter_size in [32, 64]:\n",
    "            #conv = Conv1D(filters=32, kernel_size=filter_size, activation='tanh')(conv)\n",
    "            layer = Dense(filter_size, activation='tanh')(layer)\n",
    "        shape_size = K.int_shape(block_input)[1]\n",
    "        skip_out = Dense(shape_size, activation='linear', use_bias=False)(layer)\n",
    "        block_out = Dense(shape_size, activation='linear', use_bias=False)(layer)\n",
    "        \n",
    "        block_out = Add()([residual, block_out])\n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_model(embedding_layer, title_input, title_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 128\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=32, kernel_size=5)(l_merge)\n",
    "    if title_layer == None:\n",
    "        #title_layer = Permute((2, 1))(title_layer)\n",
    "        #conv = Permute((2, 1))(conv)\n",
    "        #layer = Dot(axes=1)([conv, title_layer])\n",
    "        #title_layer = TimeDistributed(Dense(1))(title_layer)\n",
    "        title_layer = GlobalAveragePooling1D()(title_layer)\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "        layer = Concatenate()([layer, title_layer])\n",
    "        #layer = Dropout(0.50)(layer)\n",
    "        #layer = Activation('tanh')(layer)\n",
    "        layer, layer_b  = residual_context()(layer)\n",
    "        layer = Dropout(0.50)(layer)\n",
    "        layer, layer_c  = residual_context()(layer)\n",
    "        layer = Dropout(0.50)(layer)\n",
    "        layer = Add()([layer_b, layer_c])\n",
    "        layer = Activation('tanh')(layer)\n",
    "    else:\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    #layer = Activation('tanh')(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "    \n",
    "    if title_layer == None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "\n",
    "    cnn_feature_model = Model(inputs=inputs, outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model, lstm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute, Reshape\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        \n",
    "        residual =  block_input\n",
    "        residual = Activation('relu')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "        layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "        layer_out = GRU(100, activation='tanh', return_sequences=True)(layer_out)\n",
    "        #right_layer_out = GRU(100, activation='tanh', return_sequences=True, go_backwards=True)(left_layer_out)\n",
    "        #layer_out = Add()([left_layer_out, right_layer_out])\n",
    "        #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "        layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(50, activation='tanh')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        skip_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_margin_loss, metrics=[pos_distance, neg_distance, custom_margin_loss])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          5824000     title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          6218184     desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 900, 1)       0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 900, 1)       0           merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 900, 1)       0           merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 900, 100)     30600       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 900, 100)     30600       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 900, 100)     30600       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 100)          0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 100)          0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 100)          0           gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           5050        global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 50)           5050        global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 50)           5050        global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 900)          0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 900)          45900       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 900)          0           merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 900)          45900       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 900)          0           merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 900)          45900       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 900)          0           activation_1[0][0]               \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 900)          0           activation_2[0][0]               \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 900)          0           activation_3[0][0]               \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           add_2[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           add_2[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 12,450,334\n",
      "Trainable params: 1,013,734\n",
      "Non-trainable params: 11,436,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.03, CustomLoss: 1.03, pos_cosine: 0.92, neg_cosine: 0.95\n",
      "Epoch: 2 Loss: 1.01, CustomLoss: 1.01, pos_cosine: 0.95, neg_cosine: 0.96\n",
      "Epoch: 3 Loss: 1.01, CustomLoss: 1.01, pos_cosine: 0.95, neg_cosine: 0.96\n",
      "Epoch: 4 Loss: 1.02, CustomLoss: 1.02, pos_cosine: 0.96, neg_cosine: 0.97\n",
      "Epoch: 5 Loss: 1.00, CustomLoss: 1.00, pos_cosine: 0.97, neg_cosine: 0.97\n",
      "Epoch: 6 Loss: 1.01, CustomLoss: 1.01, pos_cosine: 0.97, neg_cosine: 0.98\n",
      "Epoch: 7 Loss: 1.00, CustomLoss: 1.00, pos_cosine: 0.98, neg_cosine: 0.98\n",
      "Epoch: 8 Loss: 1.00, CustomLoss: 1.00, pos_cosine: 0.98, neg_cosine: 0.98\n",
      "Epoch: 9 Loss: 1.00, CustomLoss: 1.00, pos_cosine: 0.98, neg_cosine: 0.98\n",
      "Epoch: 10 Loss: 0.99, CustomLoss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 11 Loss: 0.99, CustomLoss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 12 Loss: 0.98, CustomLoss: 0.98, pos_cosine: 0.98, neg_cosine: 0.96\n",
      "Epoch: 13 Loss: 0.97, CustomLoss: 0.97, pos_cosine: 0.98, neg_cosine: 0.95\n",
      "Epoch: 14 Loss: 0.95, CustomLoss: 0.95, pos_cosine: 0.98, neg_cosine: 0.92\n",
      "Epoch: 15 Loss: 0.90, CustomLoss: 0.90, pos_cosine: 0.97, neg_cosine: 0.87\n",
      "Epoch: 16 Loss: 0.83, CustomLoss: 0.83, pos_cosine: 0.97, neg_cosine: 0.79\n",
      "Epoch: 17 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.95, neg_cosine: 0.65\n",
      "Epoch: 18 Loss: 0.55, CustomLoss: 0.55, pos_cosine: 0.93, neg_cosine: 0.47\n",
      "Epoch: 19 Loss: 0.39, CustomLoss: 0.39, pos_cosine: 0.91, neg_cosine: 0.30\n",
      "Epoch: 20 Loss: 0.28, CustomLoss: 0.28, pos_cosine: 0.90, neg_cosine: 0.18\n",
      "Epoch: 21 Loss: 0.22, CustomLoss: 0.22, pos_cosine: 0.90, neg_cosine: 0.11\n",
      "Epoch: 22 Loss: 0.16, CustomLoss: 0.16, pos_cosine: 0.91, neg_cosine: 0.07\n",
      "Epoch: 23 Loss: 0.13, CustomLoss: 0.13, pos_cosine: 0.93, neg_cosine: 0.05\n",
      "Epoch: 24 Loss: 0.10, CustomLoss: 0.10, pos_cosine: 0.94, neg_cosine: 0.04\n",
      "Epoch: 25 Loss: 0.07, CustomLoss: 0.07, pos_cosine: 0.96, neg_cosine: 0.03\n",
      "Epoch: 26 Loss: 0.06, CustomLoss: 0.06, pos_cosine: 0.97, neg_cosine: 0.03\n",
      "Epoch: 27 Loss: 0.05, CustomLoss: 0.05, pos_cosine: 0.98, neg_cosine: 0.02\n",
      "Epoch: 28 Loss: 0.04, CustomLoss: 0.04, pos_cosine: 0.98, neg_cosine: 0.02\n",
      "Epoch: 29 Loss: 0.03, CustomLoss: 0.03, pos_cosine: 0.99, neg_cosine: 0.02\n",
      "Epoch: 30 Loss: 0.03, CustomLoss: 0.03, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 31 Loss: 0.02, CustomLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 32 Loss: 0.02, CustomLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 33 Loss: 0.02, CustomLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 34 Loss: 0.02, CustomLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 35 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 36 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 37 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 38 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.00\n",
      "Epoch: 39 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.00\n",
      "Epoch: 40 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 41 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 42 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 43 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 44 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 45 Loss: 0.01, CustomLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 46 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 47 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 48 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 49 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 50 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 51 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 52 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 53 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 54 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 55 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 56 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 57 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 58 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 59 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 60 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 61 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 62 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 63 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 64 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 65 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 66 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 67 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 68 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 69 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 70 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 71 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 72 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 73 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 74 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 75 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 76 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 77 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 78 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 79 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 80 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 81 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 82 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 83 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 84 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 85 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 86 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 87 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 88 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 89 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 90 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 91 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 92 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 93 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 94 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 95 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 96 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 97 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 98 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 99 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 100 Loss: 0.00, CustomLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00, recall@25: 0.70\n",
      "Saved model 'modelos/model_propose_100_feature_100epochs_64batch(netbeans).h5' to disk\n",
      "Best_epoch=100, Best_loss=0.00, Recall@25=0.70\n",
      "CPU times: user 14h 6min 25s, sys: 1h 53min 21s, total: 15h 59min 47s\n",
      "Wall time: 6h 13min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model, title_layer = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, title_feature_model.input, \n",
    "                               title_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(encoded_anchor, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[3]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['233472:230872|234329:0.9855268690735102,231856:0.9848450804129243,230092:0.9845450390130281,230090:0.9842839352786541,234917:0.9837376661598682,234918:0.9837376661598682,231804:0.9831626061350107,229428:0.9829104468226433,235223:0.9752777554094791,235224:0.9749243054538965,238521:0.9747538566589355,233848:0.9745042137801647,231419:0.9744695760309696,233852:0.9743472021073103,232979:0.9740253929048777,235782:0.973811898380518,234525:0.9732354823499918,230929:0.9731643628329039,234218:0.972349502146244,233727:0.9722915794700384,233709:0.9721514564007521,235129:0.9718491621315479,236741:0.9717494733631611,232615:0.9716611187905073,236716:0.9715937785804272,227180:0.9714547265321016,239024:0.9714519809931517,232621:0.9714026991277933,234631:0.9709947388619184',\n",
       " '230872:233472|235293:0.9826061353087425,229581:0.9811638556420803,229528:0.9724440947175026,235463:0.9674784354865551,234525:0.9666150026023388,235128:0.9655389040708542,234962:0.9645094200968742,237171:0.9644129388034344,230152:0.9641860723495483,232692:0.9641508087515831,226927:0.9639851152896881,229372:0.9639707431197166,228377:0.9638597555458546,231607:0.9636866971850395,228527:0.9633203335106373,234329:0.9631711877882481,229038:0.9630463644862175,230822:0.9630068950355053,231003:0.962952695786953,235736:0.9628743454813957,228834:0.9628173932433128,231933:0.9628008976578712,230760:0.962740670889616,238331:0.9626723267138004,236305:0.9624682553112507,230879:0.9624605588614941,230678:0.9624386951327324,230328:0.9623804427683353,230136:0.9623796381056309',\n",
       " '221186:220416,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '226947:220416,221186,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '220553:220416,221186,226947,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '220554:220416,221186,226947,220553,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '218635:220416,221186,226947,220553,220554,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|218622:0.9739003293216228,220578:0.9738217908889055,220515:0.9736572671681643,220643:0.9736287109553814,220756:0.9734994936734438,220757:0.9734994936734438,220624:0.9734553191810846,220552:0.9734379481524229,222466:0.9734045136719942,218383:0.9732028041034937,223741:0.9731312226504087,225259:0.9730174355208874,236017:0.9729322176426649,238023:0.9728620853275061,225141:0.9727513939142227,223785:0.9727230034768581,218251:0.9727070592343807,221689:0.9727024044841528,229490:0.9726240672171116,219671:0.972592756152153,223047:0.9725814573466778,225209:0.9725690111517906,230623:0.9725571107119322,224422:0.9725490882992744,227093:0.9724824298173189,222180:0.9724483396857977,227335:0.9723589234054089,227839:0.9722884744405746,221485:0.9722447469830513',\n",
       " '220432:220416,221186,226947,220553,220554,218635,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '226961:220416,221186,226947,220553,220554,218635,220432,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|215671:0.9915987187996507,217010:0.9915987187996507,214389:0.9908335171639919,213329:0.9846955928951502,213349:0.9793401192873716,214706:0.9790057688951492,215382:0.9790057688951492,218638:0.9784616362303495,215372:0.9782237447798252,215156:0.9781000185757875,216106:0.9781000185757875,218747:0.9781000185757875,219301:0.9781000185757875,219390:0.9781000185757875,219641:0.9781000185757875,219659:0.9781000185757875,219914:0.9781000185757875,219918:0.9781000185757875,220509:0.9781000185757875,209343:0.9779829569160938,218202:0.9779271278530359,210956:0.9779177363961935,216618:0.9778417926281691,220419:0.9778417926281691,213319:0.97782227024436,214742:0.97782227024436,213522:0.9777801502496004,213659:0.9777270462363958,217678:0.9776743520051241',\n",
       " '219028:220416,221186,226947,220553,220554,218635,220432,226961,219412,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|219170:0.9999218668162939,221433:0.9909746432676911,218859:0.9907741509377956,217536:0.9907050346955657,217729:0.9907050346955657,233598:0.9900304488837719,216514:0.9899603947997093,231791:0.9898583767935634,228020:0.9893422638997436,222722:0.9885791735723615,223869:0.988527380861342,219220:0.9881165279075503,223867:0.987942548468709,221186:0.987863477319479,221291:0.987863477319479,229492:0.987863477319479,221404:0.987863477319479,222052:0.987863477319479,222141:0.987863477319479,222306:0.987863477319479,224049:0.987863477319479,224070:0.987863477319479,224218:0.987863477319479,224376:0.987863477319479,224412:0.987863477319479,224686:0.987863477319479,224797:0.987863477319479,225378:0.987863477319479,225660:0.987863477319479',\n",
       " '219412:220416,221186,226947,220553,220554,218635,220432,226961,219028,220692,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '220692:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220948,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '220948:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,224412,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '224412:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224797,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '224797:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,219550,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '219550:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219170,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '219170:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,220452,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|219028:0.9999218668162939,221433:0.9909746432676911,218859:0.9907741509377956,217536:0.9907050346955657,217729:0.9907050346955657,233598:0.9900304488837719,216514:0.9899603947997093,231791:0.9898583767935634,228020:0.9893422638997436,222722:0.9885791735723615,223869:0.988527380861342,219220:0.9881165279075503,223867:0.987942548468709,221186:0.987863477319479,221291:0.987863477319479,229492:0.987863477319479,221404:0.987863477319479,222052:0.987863477319479,222141:0.987863477319479,222306:0.987863477319479,224049:0.987863477319479,224070:0.987863477319479,224218:0.987863477319479,224376:0.987863477319479,224412:0.987863477319479,224686:0.987863477319479,224797:0.987863477319479,225378:0.987863477319479,225660:0.987863477319479',\n",
       " '220452:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,224686,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '224686:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,219311,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219311:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026',\n",
       " '219311:220416,221186,226947,220553,220554,218635,220432,226961,219028,219412,220692,220948,224412,224797,219550,219170,220452,224686,219312,220465,224049,227122,220472,220987,222141,220353,221121,220358,224070,226374,220618,220238,219599,220497,221138,219220,219866,220379,221404,221021,224218,220384,222306,225378,222052,226405,220647,220648,218475,221291,220653,219630,229492,224376,225660|221186:0.9998030304705026,221291:0.9998030304705026,229492:0.9998030304705026,221404:0.9998030304705026,222052:0.9998030304705026,222141:0.9998030304705026,222306:0.9998030304705026,224049:0.9998030304705026,224070:0.9998030304705026,224218:0.9998030304705026,224376:0.9998030304705026,224412:0.9998030304705026,224686:0.9998030304705026,224797:0.9998030304705026,225378:0.9998030304705026,225660:0.9998030304705026,226374:0.9998030304705026,226405:0.9998030304705026,226947:0.9998030304705026,227122:0.9998030304705026,219312:0.9998030304705026,219412:0.9998030304705026,219550:0.9998030304705026,219599:0.9998030304705026,219630:0.9998030304705026,219866:0.9998030304705026,220238:0.9998030304705026,220353:0.9998030304705026,220358:0.9998030304705026']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_propose_100_feature_100epochs_64batch(netbeans).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 3162\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_100_feature_100epochs_64batch(netbeans)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          5824000     title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          6218184     desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 900, 1)       0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 900, 100)     30600       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 100)          0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           5050        global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 900)          0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 900)          45900       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 900)          0           activation_1[0][0]               \n",
      "                                                                 dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,287,234\n",
      "Trainable params: 850,634\n",
      "Non-trainable params: 11,436,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/netbeans/exported_rank_propose_100.txt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.58,\n",
       " '2 - recall_at_10': 0.63,\n",
       " '3 - recall_at_15': 0.66,\n",
       " '4 - recall_at_20': 0.69,\n",
       " '5 - recall_at_25': 0.7}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
