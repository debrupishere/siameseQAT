{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - PROPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'propose'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1e5f64d8804b2ca15390c38ebbb36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57667), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47286bff67b847cbbffe35f9862a2484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdc460a65574952a3f361db9240b570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b766e5997643d9a79a2631187a8f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 11.4 s, sys: 942 ms, total: 12.3 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771944937a3c49a19722a7cf71be8035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n"
     ]
    }
   ],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '31\\n',\n",
       " 'creation_ts': '2005-08-24 10:24:00 +0000',\n",
       " 'delta_ts': '2006-05-31 14:29:06 +0000',\n",
       " 'description': 'when opening my base document if the absolute path of the location of my file do not contain a accentuated character by example i mean a ãƒâ or ãƒâ i loose my tables and the data in it if there is a accentuated character in the path i found the tables with their data my configuration w k sp ooo langpack_fr but confirmed by two others persons at least',\n",
       " 'description_word': array([  31,  346,  107,  435,   29,   35,    2, 2422,  184,   10,    2,\n",
       "         867,   10,  107,   24,  114,   13,  831,    3, 9101,  336,   54,\n",
       "         205,    6, 1657,    3, 1093,   46, 1093,    6, 3513,  107,  429,\n",
       "           8,    2,   90,    5,   12,   35,   66,    7,    3, 9101,  336,\n",
       "           5,    2,  184,    6,  159,    2,  429,   18,  673,   90,  107,\n",
       "         768,  112,  533,  956,   33,    1,   41, 2209,   54,  197, 1210,\n",
       "        9035,   52,  563,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " 'dup_id': '52334',\n",
       " 'issue_id': 53706,\n",
       " 'priority': '0\\n',\n",
       " 'product': '38\\n',\n",
       " 'resolution': 'DUPLICATE',\n",
       " 'title': 'data loss that depend of the absolute path of the document',\n",
       " 'title_word': array([  90, 2012,   26, 2508,   10,    2, 2422,  184,   10,    2,   29,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " 'version': '233\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 11043)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.5 ms, sys: 5 µs, total: 30.5 ms\n",
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 100), (128, 729), (128,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: spaces in sxw\n",
      "***Title***: can t add spaces in the text\n",
      "***Description***: rc czech hi the attached document was created by cut paste from the document that contained the same issue the only visible letter in the document is h but when you use cursor right key you can t simply go back the document contains spaces after letter h text span text style name t h text s text c text span but spaces are not visible even when you turn spaces tabs and paragraphs sign on\n",
      "***Description***: you won t be able to type any space at the end of the line in the attached document you won t be able to type backspace too it s very strange this document was created using writer on windows and this is where the bug appears then i try with the same document on linux with writer and there was the same problem\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: bold and italic don t work in solaris and linux platform\n",
      "***Title***: bitstream vera serif does not display as italic\n",
      "***Description***: reproduce steps open a writer document input something like abcd select them press bold or italic button in toolbar nothing happens btw there is no bug in windows platform\n",
      "***Description***: i tried to set a word to italic using the default font bitstream vera serif but it did not show as italic however changing the font to something else did show the word in italic note that this was also a problem with rc\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: table rows don t break across pages\n",
      "***Title***: data transfert from sheet to database\n",
      "***Description***: i desperately need for table rows to break across pages this feature is included as an option in ms word under table table properties row if this feature is available in ooo i ve been unable to locate it there is no mention of this capability in the help file\n",
      "***Description***: data transfert from sheet to database\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: macro recording help file missing\n",
      "***Title***: date function returns incorrect date if year is a cel ref\n",
      "***Description***: open help for spreadsheet on index tab search for macros recording click display receive this message in right pane d oh you found a bug text shared guide macro_recording xhp macrorecorder not found\n",
      "***Description***: date function returns incorrect date if year is a cel ref\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 36.5 ms, sys: 3.7 ms, total: 40.2 ms\n",
      "Wall time: 39.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 76631'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, f.readline().split())\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading FastText\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         embed = list(map(float, tokens[1:]))\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abee8aa154cf4dd3b7485c6df39159ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b787f825720942b8b009eb1fafe66016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101338), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 25s, sys: 3.11 s, total: 1min 28s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def residual_context():\n",
    "    def block(block_input):\n",
    "        residual = block_input\n",
    "        \n",
    "        layer = block_input\n",
    "        for filter_size in [32, 64]:\n",
    "            #conv = Conv1D(filters=32, kernel_size=filter_size, activation='tanh')(conv)\n",
    "            layer = Dense(filter_size, activation='tanh')(layer)\n",
    "        shape_size = K.int_shape(block_input)[1]\n",
    "        skip_out = Dense(shape_size, activation='linear', use_bias=False)(layer)\n",
    "        block_out = Dense(shape_size, activation='linear', use_bias=False)(layer)\n",
    "        \n",
    "        block_out = Add()([residual, block_out])\n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_model(embedding_layer, title_input, title_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 128\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=32, kernel_size=5)(l_merge)\n",
    "    if title_layer == None:\n",
    "        #title_layer = Permute((2, 1))(title_layer)\n",
    "        #conv = Permute((2, 1))(conv)\n",
    "        #layer = Dot(axes=1)([conv, title_layer])\n",
    "        #title_layer = TimeDistributed(Dense(1))(title_layer)\n",
    "        title_layer = GlobalAveragePooling1D()(title_layer)\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "        layer = Concatenate()([layer, title_layer])\n",
    "        #layer = Dropout(0.50)(layer)\n",
    "        #layer = Activation('tanh')(layer)\n",
    "        layer, layer_b  = residual_context()(layer)\n",
    "        layer = Dropout(0.50)(layer)\n",
    "        layer, layer_c  = residual_context()(layer)\n",
    "        layer = Dropout(0.50)(layer)\n",
    "        layer = Add()([layer_b, layer_c])\n",
    "        layer = Activation('tanh')(layer)\n",
    "    else:\n",
    "        layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    #layer = Activation('tanh')(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "    \n",
    "    if title_layer == None:\n",
    "        inputs = [sequence_input, title_input]\n",
    "    else:\n",
    "        inputs = [sequence_input]\n",
    "\n",
    "    cnn_feature_model = Model(inputs=inputs, outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(1))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model, lstm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute, Reshape\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        \n",
    "        residual =  block_input\n",
    "        residual = Activation('relu')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "        layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "        layer_out = GRU(100, activation='tanh', return_sequences=True)(layer_out)\n",
    "        #right_layer_out = GRU(100, activation='tanh', return_sequences=True, go_backwards=True)(left_layer_out)\n",
    "        #layer_out = Add()([left_layer_out, right_layer_out])\n",
    "        #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "        layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(50, activation='tanh')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        skip_out = Dense(shape_size_cols, activation='tanh', use_bias=True)(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    #bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, metrics=[pos_distance, neg_distance, custom_margin_loss])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          30507100    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          30901284    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 900, 1)       0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 900, 1)       0           merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 900, 1)       0           merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 900, 100)     30600       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 900, 100)     30600       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 900, 100)     30600       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 100)          0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 100)          0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 100)          0           gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           5050        global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 50)           5050        global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 50)           5050        global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 900)          0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 900)          45900       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 900)          0           merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 900)          45900       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 900)          0           merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 900)          45900       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 900)          0           activation_1[0][0]               \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 900)          0           activation_2[0][0]               \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 900)          0           activation_3[0][0]               \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           add_2[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           add_2[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 61,872,034\n",
      "Trainable params: 1,069,234\n",
      "Non-trainable params: 60,802,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.93, CustomLoss: 0.93, pos_cosine: 0.86, neg_cosine: 0.80\n",
      "Epoch: 2 Loss: 0.91, CustomLoss: 0.91, pos_cosine: 0.87, neg_cosine: 0.79\n",
      "Epoch: 3 Loss: 0.92, CustomLoss: 0.92, pos_cosine: 0.87, neg_cosine: 0.79\n",
      "Epoch: 4 Loss: 0.92, CustomLoss: 0.92, pos_cosine: 0.86, neg_cosine: 0.78\n",
      "Epoch: 5 Loss: 0.91, CustomLoss: 0.91, pos_cosine: 0.85, neg_cosine: 0.76\n",
      "Epoch: 6 Loss: 0.88, CustomLoss: 0.88, pos_cosine: 0.86, neg_cosine: 0.73\n",
      "Epoch: 7 Loss: 0.87, CustomLoss: 0.87, pos_cosine: 0.84, neg_cosine: 0.71\n",
      "Epoch: 8 Loss: 0.82, CustomLoss: 0.82, pos_cosine: 0.85, neg_cosine: 0.67\n",
      "Epoch: 9 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.83, neg_cosine: 0.62\n",
      "Epoch: 10 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.85, neg_cosine: 0.55\n",
      "Epoch: 11 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.83, neg_cosine: 0.46\n",
      "Epoch: 12 Loss: 0.54, CustomLoss: 0.54, pos_cosine: 0.82, neg_cosine: 0.37\n",
      "Epoch: 13 Loss: 0.43, CustomLoss: 0.43, pos_cosine: 0.84, neg_cosine: 0.26\n",
      "Epoch: 14 Loss: 0.34, CustomLoss: 0.34, pos_cosine: 0.84, neg_cosine: 0.18\n",
      "Epoch: 15 Loss: 0.27, CustomLoss: 0.27, pos_cosine: 0.86, neg_cosine: 0.13\n",
      "Epoch: 16 Loss: 0.22, CustomLoss: 0.22, pos_cosine: 0.88, neg_cosine: 0.10\n",
      "Epoch: 17 Loss: 0.17, CustomLoss: 0.17, pos_cosine: 0.90, neg_cosine: 0.07\n",
      "Epoch: 18 Loss: 0.13, CustomLoss: 0.13, pos_cosine: 0.93, neg_cosine: 0.06\n",
      "Epoch: 19 Loss: 0.10, CustomLoss: 0.10, pos_cosine: 0.95, neg_cosine: 0.05\n",
      "Epoch: 20 Loss: 0.08, CustomLoss: 0.08, pos_cosine: 0.96, neg_cosine: 0.04, recall@25: 0.55\n",
      "Saved model 'modelos/model_propose_feature_20epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=20, Best_loss=0.08, Recall@25=0.55\n",
      "CPU times: user 5min 48s, sys: 34.2 s, total: 6min 22s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model, title_layer = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, title_feature_model.input, \n",
    "                               title_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 20\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, bug_train_ids, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[3]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|108529:0.8390902280807495,115895:0.8238644450902939,107794:0.8224564790725708,102712:0.8208542615175247,115421:0.8166808933019638,115634:0.815300703048706,93169:0.8147186040878296,107267:0.8141856789588928,115569:0.8141744136810303,115557:0.8139078170061111,105272:0.8137033581733704,102470:0.8136318773031235,115262:0.8132148534059525,102788:0.8131716549396515,93295:0.8128395080566406,92202:0.8126061856746674,114022:0.8120289146900177,93177:0.8112746626138687,105166:0.8094734996557236,98647:0.8093265891075134,101398:0.8081967234611511,92117:0.8072523325681686,109298:0.8072514086961746,110322:0.8066780716180801,110323:0.8066780716180801,92348:0.8062454760074615,92541:0.8062252551317215,92690:0.8055613934993744,92613:0.8053735047578812',\n",
       " '109674:108544,111059,108379,109366|112693:0.8384692370891571,94815:0.8363152593374252,112299:0.8125752508640289,113509:0.8125180751085281,111514:0.8116788268089294,100665:0.8083781450986862,108390:0.8074962049722672,116199:0.8064496368169785,93288:0.8059770315885544,108377:0.8011561185121536,94421:0.7998245656490326,105082:0.7998053878545761,93055:0.7977947890758514,90533:0.7977081686258316,95277:0.7973857223987579,92543:0.7954658418893814,102682:0.7877182811498642,114153:0.7852978557348251,107845:0.785248726606369,108529:0.7833207100629807,14454:0.7717782258987427,104574:0.7637007534503937,104576:0.7637007534503937,105907:0.7632213830947876,109081:0.7631552815437317,101278:0.7626961916685104,105511:0.7626866400241852,95460:0.7570411711931229,94780:0.7569207102060318',\n",
       " '111059:108544,109674,108379,109366|110274:0.9659782461822033,110555:0.954746674746275,111297:0.9018333554267883,95460:0.819635808467865,97026:0.8190134018659592,96429:0.8186289668083191,107779:0.8169018626213074,95800:0.816636249423027,115100:0.8153084069490433,110322:0.8145059198141098,110323:0.8145059198141098,91999:0.8135866969823837,92188:0.8114659041166306,111593:0.8113337606191635,105907:0.8107422739267349,115569:0.8075267821550369,92613:0.807301864027977,113113:0.8072333037853241,92348:0.80692458152771,92690:0.8068917095661163,109853:0.806861013174057,92020:0.8063814491033554,99981:0.805940255522728,99982:0.8059323281049728,102398:0.805724173784256,105272:0.8052283674478531,115888:0.8044378757476807,114310:0.8021713048219681,92365:0.80207459628582',\n",
       " '108379:108544,111059,109674,109366|108492:0.9546565487980843,89456:0.8266166746616364,108276:0.8223662078380585,115262:0.8179373294115067,105166:0.8171079903841019,102788:0.8169932514429092,104267:0.8166768699884415,111297:0.8126357346773148,98647:0.8113063871860504,92117:0.810439795255661,89620:0.8102473616600037,108408:0.8079725950956345,102557:0.8079705387353897,102003:0.8062607645988464,92348:0.8051059544086456,92690:0.8047793358564377,97168:0.8047375231981277,115634:0.8041348159313202,102470:0.8039755374193192,94033:0.8028571456670761,115895:0.8027365803718567,115421:0.8027052134275436,114022:0.8023601472377777,104173:0.8011737763881683,114280:0.8009318262338638,109298:0.8005973398685455,107794:0.80038021504879,112520:0.8002873212099075,92541:0.800089955329895',\n",
       " '109366:108544,111059,109674,108379|110044:0.9452259838581085,109298:0.8229324668645859,108311:0.8181327134370804,105482:0.8091916143894196,102497:0.8089908808469772,91310:0.8061856329441071,104266:0.8044233322143555,115491:0.8040363043546677,109431:0.8006260544061661,106191:0.799698144197464,106574:0.7994653731584549,92537:0.7950032502412796,105235:0.7946195900440216,107044:0.7929997593164444,111681:0.7911321371793747,114719:0.7908059656620026,111295:0.7890082150697708,105993:0.7882138937711716,105086:0.7861240655183792,89620:0.7841513007879257,98821:0.7832213193178177,114022:0.7804859131574631,104956:0.7800149619579315,114671:0.7769785076379776,99191:0.7756872177124023,102082:0.7755909264087677,97860:0.7750358581542969,107267:0.7730676531791687,115068:0.7727803289890289',\n",
       " '110594:107073,108355,108453,109162,111761,111800|109162:0.967847004532814,106479:0.8274513185024261,108453:0.8225544840097427,107073:0.8054308295249939,111761:0.8045443445444107,103827:0.8034491837024689,108439:0.8030185252428055,103471:0.7985355257987976,108084:0.798494964838028,98331:0.7974977046251297,108240:0.7939251810312271,101462:0.7857584655284882,110555:0.7725334465503693,110274:0.7724638432264328,111059:0.7715122252702713,110843:0.7705584317445755,111297:0.7638770043849945,105739:0.7556206732988358,109104:0.7543409764766693,92387:0.7542306035757065,96099:0.7524474710226059,109853:0.7519013583660126,101074:0.7516768574714661,103447:0.7514456063508987,103771:0.7483735382556915,106207:0.7465565800666809,104930:0.7460603415966034,107802:0.74569171667099,110118:0.7456416189670563',\n",
       " '111800:107073,110594,108355,108453,109162,111761|91324:0.8194706290960312,108513:0.8135567605495453,76670:0.8117404878139496,111833:0.8097527325153351,102556:0.80973020195961,100179:0.8081748783588409,108569:0.8057677447795868,100537:0.8048827648162842,98770:0.801085889339447,102522:0.8006390184164047,98278:0.7987987697124481,97681:0.7985920608043671,112809:0.7984656095504761,98948:0.7983870059251785,102068:0.797650620341301,112864:0.7976217567920685,94719:0.7968678772449493,57538:0.7967884391546249,98966:0.7961844950914383,98022:0.7961260676383972,96117:0.7959859222173691,95999:0.7945428937673569,78260:0.7937746942043304,92385:0.7933260500431061,102865:0.7928572744131088,96736:0.7925687730312347,56891:0.7920298129320145,90449:0.7899883538484573,94707:0.7887828201055527',\n",
       " '111761:107073,110594,108355,108453,109162,111800|108453:0.8227558732032776,98331:0.819633811712265,98310:0.8193158805370331,108439:0.8184669762849808,108084:0.8174899220466614,101462:0.8165879994630814,107073:0.8162856847047806,108240:0.8145893961191177,111758:0.8125022202730179,106479:0.8124453723430634,109162:0.8047726899385452,95494:0.804554358124733,110594:0.8045443445444107,103827:0.7984150350093842,110992:0.7963160872459412,103471:0.7912494093179703,114440:0.7747762501239777,109513:0.7737777382135391,93952:0.7680587619543076,92951:0.7676046788692474,96099:0.767128050327301,92387:0.7660938203334808,112708:0.7653686106204987,111946:0.7650924474000931,112276:0.7637823671102524,100234:0.7631291449069977,109550:0.7608329504728317,111165:0.7604936510324478,115520:0.7600338160991669',\n",
       " '109162:107073,110594,108355,108453,111761,111800|110594:0.967847004532814,108453:0.8224968761205673,106479:0.8207607418298721,103827:0.809312030673027,103471:0.8061503171920776,111761:0.8047726899385452,108439:0.803124338388443,108084:0.8009418845176697,107073:0.8008823990821838,108240:0.7974121123552322,101462:0.7910796701908112,98310:0.7887142449617386,95494:0.7878741472959518,110843:0.7664055675268173,93952:0.762492224574089,110889:0.7602077424526215,105739:0.7580877393484116,103810:0.7516217976808548,97784:0.7510440498590469,104068:0.7509275525808334,109104:0.7491373121738434,103447:0.7488967776298523,96633:0.7484833300113678,92387:0.7483571171760559,109853:0.7479608654975891,103771:0.7441655099391937,95448:0.7425068020820618,102142:0.7423377633094788,107802:0.74233078956604',\n",
       " '108355:107073,110594,108453,109162,111761,111800|101462:0.7652990967035294,108453:0.7639539688825607,103471:0.7591778486967087,108240:0.7550256848335266,103827:0.7539129704236984,108084:0.7531949877738953,111761:0.7521218210458755,108439:0.7499138414859772,114938:0.7476542592048645,98310:0.7457616925239563,107073:0.7438221871852875,109162:0.7416193783283234,99423:0.7415666580200195,106817:0.7410835325717926,90800:0.7384526133537292,110594:0.7383765578269958,103810:0.7294268012046814,106479:0.7270553112030029,108440:0.724437028169632,104212:0.7227418124675751,95750:0.7214172184467316,105739:0.7211250066757202,104068:0.7200980186462402,96581:0.718888908624649,95494:0.7178292870521545,103278:0.7176370024681091,95926:0.7167708873748779,101552:0.7159017622470856,112819:0.7157784104347229',\n",
       " '108453:107073,110594,108355,109162,111761,111800|111761:0.8227558732032776,110594:0.8225544840097427,109162:0.8224968761205673,103827:0.8209372311830521,98331:0.8140971064567566,106479:0.8085121065378189,108439:0.8084937334060669,107073:0.8076349794864655,98310:0.8072120100259781,108084:0.8070644587278366,108240:0.8033741116523743,103471:0.8006969392299652,95494:0.7965364456176758,101462:0.7954277247190475,93952:0.7731681913137436,100234:0.770974650979042,92387:0.7709397822618484,92951:0.7701437622308731,114227:0.7693118900060654,108492:0.7684050798416138,109104:0.7663626223802567,96633:0.762057363986969,103810:0.7611686140298843,104068:0.7609869092702866,111758:0.760966494679451,90612:0.760679692029953,103771:0.7595916390419006,90697:0.7592137902975082,94775:0.7584784775972366',\n",
       " '114705:114676|105047:0.8128659874200821,115404:0.8112825155258179,116991:0.8064946383237839,116139:0.8028274923563004,112187:0.801069051027298,113470:0.7981949895620346,113535:0.7980011105537415,95152:0.795913502573967,102518:0.7945331037044525,108423:0.7920725494623184,109640:0.7916025370359421,105822:0.7892065644264221,105368:0.7838181257247925,105727:0.782716765999794,98659:0.7817135155200958,105747:0.7797636687755585,99254:0.7710494846105576,116112:0.7704764902591705,108200:0.7684525698423386,108454:0.767101377248764,114525:0.7670629918575287,111191:0.7654073089361191,93388:0.7652019113302231,88476:0.7645001709461212,101337:0.7644836157560349,101336:0.7641017287969589,97342:0.7629818916320801,92591:0.7628789991140366,87935:0.7621090263128281',\n",
       " '114676:114705|91245:0.7979004085063934,102648:0.7923361212015152,111908:0.7906051278114319,107893:0.7903521060943604,103513:0.788869172334671,101448:0.787641167640686,101522:0.7873793542385101,111881:0.7857319265604019,110879:0.7851581275463104,96817:0.7814365923404694,110900:0.7811999768018723,112930:0.7805085182189941,112931:0.7805085182189941,113669:0.7802202850580215,112843:0.7789743840694427,112844:0.7789743840694427,106268:0.7777959704399109,108401:0.7770462483167648,101867:0.7765130400657654,104359:0.7762184292078018,104360:0.7762184292078018,115350:0.7718222290277481,106269:0.76956607401371,113492:0.7630018442869186,107842:0.7527361512184143,115237:0.7497938275337219,103562:0.7460736036300659,108061:0.7372425198554993,110866:0.7315642237663269',\n",
       " '110593:110618|111172:0.9229594469070435,110397:0.8855976983904839,112475:0.8399765193462372,110485:0.8344404250383377,110026:0.8338085561990738,106085:0.8311171531677246,110366:0.8310658186674118,110346:0.830036386847496,111583:0.8262974470853806,110343:0.8258275091648102,103860:0.8253413289785385,89825:0.824333131313324,109992:0.8235011845827103,110601:0.8192602396011353,85283:0.8177886456251144,91811:0.8177022784948349,116347:0.8174441009759903,85782:0.816766083240509,106950:0.8154319077730179,111651:0.8137330263853073,109921:0.8136554658412933,108045:0.8127531856298447,105527:0.8122455775737762,111673:0.8120039850473404,110012:0.8107868731021881,110005:0.8100435882806778,111681:0.8092057853937149,107466:0.8080251961946487,89379:0.8075661659240723',\n",
       " '110618:110593|112234:0.8353256285190582,96115:0.8333642929792404,110306:0.8317738622426987,90892:0.8309473842382431,94253:0.8301478028297424,106741:0.8273218721151352,95207:0.8271217495203018,91552:0.8248007297515869,109221:0.8243937343358994,109422:0.822909414768219,102759:0.8216395378112793,99452:0.8210470080375671,105640:0.8210451155900955,106939:0.8178343027830124,73288:0.8175117373466492,110442:0.8172801285982132,94884:0.8170232474803925,97263:0.8164599239826202,110922:0.8163847178220749,112548:0.8163285553455353,108780:0.8162256181240082,89006:0.8152967840433121,100033:0.8150206804275513,98867:0.8147515654563904,109202:0.8146954327821732,104013:0.8146857917308807,99133:0.8143740296363831,97163:0.812098354101181,96344:0.8111672252416611',\n",
       " '102409:102053|102106:0.958762776106596,104447:0.8312907665967941,96452:0.819182962179184,97211:0.8178170472383499,96894:0.8172076791524887,104460:0.8153179585933685,95489:0.8139428049325943,90949:0.8138041794300079,102003:0.8120228499174118,102557:0.8113967180252075,104173:0.811380997300148,96888:0.8104718625545502,102471:0.8102522194385529,102398:0.8089452236890793,95237:0.8077713996171951,96947:0.8076065629720688,97350:0.8065523952245712,106072:0.8060918152332306,96945:0.8060893714427948,97178:0.805768758058548,96124:0.8049915134906769,96471:0.80408975481987,96880:0.8027130961418152,101124:0.8024183362722397,104267:0.8008420467376709,95413:0.8006556630134583,101652:0.7990492731332779,97179:0.7971699982881546,110764:0.7962917238473892',\n",
       " '102053:102409|107236:0.8167770802974701,116288:0.8163518905639648,90840:0.7595093846321106,105054:0.747938871383667,106005:0.7409848570823669,97672:0.7404996752738953,92270:0.7296094000339508,103510:0.7292475402355194,90424:0.7277839481830597,100234:0.7272379398345947,97029:0.7256350219249725,101012:0.7220514118671417,103824:0.7217608988285065,103481:0.7215884923934937,102803:0.721561849117279,88476:0.7212642431259155,93996:0.7211495041847229,102465:0.7209517061710358,93884:0.7209256589412689,87935:0.7206585109233856,93454:0.720622330904007,105272:0.7204797863960266,98827:0.7203425765037537,95061:0.7198034822940826,107779:0.7197462022304535,89699:0.7195136845111847,93959:0.7192980945110321,102059:0.7192563116550446,89899:0.7192017734050751',\n",
       " '110604:110612|90881:0.7385820150375366,110712:0.7377819418907166,94554:0.737016499042511,105682:0.7356390058994293,100478:0.735348254442215,115715:0.735124409198761,92951:0.7350322008132935,109067:0.7344117760658264,116500:0.7343908548355103,110992:0.7331950962543488,100980:0.7316679954528809,93178:0.7310321033000946,93769:0.7298134565353394,110482:0.729595959186554,99966:0.729225754737854,102553:0.7290865182876587,98173:0.7290714979171753,89716:0.7284448742866516,91495:0.7282545268535614,94918:0.7281417846679688,97593:0.7278985977172852,100327:0.7268104255199432,110740:0.7267354130744934,92547:0.7262924611568451,89276:0.7261393666267395,86866:0.725922554731369,93307:0.7258158922195435,95528:0.7257468998432159,91907:0.7256864011287689',\n",
       " '110612:110604|110604:0.7845032662153244,107514:0.6818591058254242,102595:0.6788264214992523,101486:0.6782186627388,91508:0.6780050694942474,93406:0.6751556992530823,110866:0.6691360473632812,102274:0.6684207022190094,93289:0.6669415235519409,101480:0.6665231883525848,108290:0.6640093326568604,95414:0.6636857390403748,108840:0.6630111932754517,93171:0.6621426641941071,108190:0.6619371473789215,93296:0.6605232954025269,89456:0.6595058441162109,108379:0.6588032245635986,102679:0.6576983630657196,98168:0.6575275659561157,104267:0.6572215855121613,102879:0.6566586792469025,102489:0.6560811102390289,105682:0.6559949517250061,110844:0.6550905108451843,107058:0.6549053490161896,102491:0.6547510027885437,108492:0.6547275483608246,110879:0.6543106138706207',\n",
       " '116738:116938,117027|113225:0.9751435592770576,115793:0.9569746032357216,99737:0.8403828889131546,92166:0.8300937116146088,92002:0.8226947635412216,90796:0.8197460025548935,114018:0.8156659007072449,91795:0.8146228641271591,92173:0.8129475116729736,100713:0.812199741601944,100714:0.812199741601944,100715:0.812199741601944,100716:0.812199741601944,108414:0.8121134638786316,107833:0.8086231499910355,99106:0.8085191696882248,113395:0.807524710893631,94580:0.805709034204483,112379:0.8048903346061707,115767:0.8046852499246597,89397:0.8043143302202225,110740:0.8024675995111465,96594:0.8020201921463013,93220:0.8011527806520462,116944:0.8009618818759918,110930:0.8004574924707413,100511:0.7980749160051346,109937:0.7972573041915894,93192:0.7970589846372604']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_propose_feature_20epochs_64batch(openoffice).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 2086\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_20epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          30507100    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          30901284    desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 900, 1)       0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 900, 100)     30600       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 100)          0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           5050        global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 900)          0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 900)          45900       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 900)          0           activation_1[0][0]               \n",
      "                                                                 dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 61,708,934\n",
      "Trainable params: 906,134\n",
      "Non-trainable params: 60,802,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, bug_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
