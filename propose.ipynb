{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - PROPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'netbeans'\n",
    "METHOD = 'propose'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78feb30c500c455a8e88ebc2fe9fc85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9dc9c700eb8426698768049ca5f5c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12431f167f424bc48ec05a2550807486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44da3cff864941c0b8cd414ab3c3bb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 20s, sys: 2.59 s, total: 1min 23s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e8e155e33440e84a86c9e34d94f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n"
     ]
    }
   ],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets)\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '1\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '433\\n',\n",
       " 'creation_ts': '2009-07-02 15:41:00 +0000',\n",
       " 'delta_ts': '2009-11-10 01:50:12 +0000',\n",
       " 'description': 'during generation of entity classes from database using mysql for instance on tables that contains several times duplicated foreign person arnumberrk table arnumberrk and fk arnumberrk table arnumberrk the generated code in product includes several fields that references the same data and indicates at runtime the problem of number fields that are writable and only one should be writable',\n",
       " 'description_word': array([ 473, 1642,   27,  722,  230,   61,  509,  183, 1315,   29,  124,\n",
       "          30, 1765,   65,  606,  903,  843, 2610, 3791,   10, 1746,  342,\n",
       "        1746,   17, 8407, 1746,  342, 1746,    9,  405,   98,   14,    8,\n",
       "        1443,  903,  941,   65, 1120,    9,  254,  174,   17, 3482,    6,\n",
       "         153,    9,  250,   27,   46,  941,   65,   91, 3757,   17,  219,\n",
       "         266,  112,   49, 3757,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 168048,\n",
       " 'priority': '0\\n',\n",
       " 'product': '19\\n',\n",
       " 'resolution': 'INCOMPLETE',\n",
       " 'title': 'duplicated foreign keys in a table in the organization generate wrong entity classes',\n",
       " 'title_word': array([2610, 3791, 1200,   14,   16,  342,   14,    9,    4,  659,  458,\n",
       "         722,  230,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'version': '16\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 32276)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.2 ms, sys: 0 ns, total: 98.2 ms\n",
      "Wall time: 98.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 86), (128, 500), (128, 544), (128,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: infinite loop of class cast exceptions in org netbeans modules javacore jmiimpl javamodel assignment person\n",
      "***Title***: unexpected exception\n",
      "***Description***: i got an infinite loop of class cast exceptions in the class org netbeans modules javacore jmiimpl javamodel assignment person i did not know what i did to reproduce this i just selected some code inside a java file when it started i use nb build\n",
      "***Description***: the editor encoutered a java lang class cast exception exception a popup window asked me to report this so that s what i am doing here are the details java lang class cast exception at org netbeans modules javacore jmiimpl javamodel assignment person init children assignment person java at org netbeans modules javacore jmiimpl javamodel assignment person get left side assignment person java at org netbeans jmi javamodel assignment person get left side unknown source at org netbeans modules javacore jmiimpl javamodel assignment person get children assignment person java at org netbeans modules javacore jmiimpl javamodel element country get element by organization t element country java at org netbeans modules javacore jmiimpl javamodel resource person get element by organizationt resource person java at org netbeans modules java hints organization provider get element by organizationt organization provider java at org netbeans modules java hints organization provider handle unresolved hints organization provider java at org netbeans modules java hints organization provider access organization provider java at org netbeans modules java hints organization provider unresolved symbol creator creat e hint organization provider java at org netbeans modules java hints organization provider get hints organization provider java at org netbeans modules editor hints hints operator hint popup task person run hints operator java at org openide util task run task java at org openide product run request processor java catch at org openide util request processor processor run request processor java the log file contains the same information so i am not attaching that separately\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: organization after trying to diff renamed file in organization\n",
      "***Title***: java lang assertion error unable to get repository home local tester organization branches testicek src testicek new class java is probably unmanaged\n",
      "***Description***: i have a project where i renamed artwork java to artwork new java i tested the whole svn on it and in the end when i was testing organization i was getting organization everytime i tried to invoke diff to artwork on on of the renamed files however now i don t get it anymore instead i am getting it on another file maybe because it was added in branch product java java hot spot tm client vm b product version running on x\n",
      "***Description***: build net beans ide dev build vm java hot spot tm client vm b os organization os sparc user comments i checked out an older project and was browsing its history after switching to product toggle button i clicked on number revision and the organization showed up\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: error icons on all nodes but with no error\n",
      "***Title***: abstract method error org netbeans modules bugtracking spi issue finder get issue person i\n",
      "***Description***: all the hierarchy from ugly ant icon down to packages is in error but all the source code is clean indeed everything compiles properly see image that s better than words\n",
      "***Description***: abstract method error org netbeans modules bugtracking spi issue finder get issue person i\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: cat a bug in server manager netbeans beta uninstalling sun apps cause all registered server to be gone\n",
      "***Title***: open organization file throws exception and open without coloring tags\n",
      "***Description***: product version hi i think it is a bug organization product b nationality beta create a person java application server runtime create a product runtime or bea weblogic runtime close netbeans now goto where you installed person apps rename or uninstall that directory run netbeans you will see that all servers that you defined in your netbeans are gone\n",
      "***Description***: open organization file throws exception and open without coloring tags\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 97.4 ms, sys: 0 ns, total: 97.4 ms\n",
      "Wall time: 97.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 122604'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69581ecaf9a14377ac17d7b427bcedab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a216ec5427ac434db8d13fae9bbd1967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=122604), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3min 18s, sys: 2.89 s, total: 3min 21s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def arcii_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    filters_1d=max_sequence_length\n",
    "    kernel_size_1d=3\n",
    "    num_conv2d_layers=2\n",
    "    filters_2d=[32,16]\n",
    "    kernel_size_2d=[[3,3], [3,3]]\n",
    "    mpool_size_2d=[[2,2], [2,2]]\n",
    "    dropout_rate=0.5\n",
    "    \n",
    "    layer1_conv=Conv1D(filters=filters_1d, kernel_size=kernel_size_1d, padding='same')(embedded_sequences)\n",
    "    layer1_activation=Activation('relu')(layer1_conv)\n",
    "    layer1_reshaped=Reshape((max_sequence_length, max_sequence_length, -1))(layer1_activation)\n",
    "    z=MaxPooling2D(pool_size=(2,2))(layer1_reshaped)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z=Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same')(z)\n",
    "        z=Activation('tanh')(z)\n",
    "        z=MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "\n",
    "    pool1_flat=Flatten()(z)\n",
    "    pool1_flat_drop=Dropout(rate=dropout_rate)(pool1_flat)\n",
    "    pool1_norm=BatchNormalization()(pool1_flat_drop)\n",
    "    mlp1=Dense(300)(pool1_norm)\n",
    "    output=Activation('tanh')(mlp1)\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[output], name = 'FeatureARCIIGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    attention_layes = []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "\n",
    "#     l1a, l1b = DC_CNN_Block(units, 3, 2, 0.01)(la)\n",
    "#     l2a, l2b = DC_CNN_Block(units, 3, 4, 0.01)(l1a)\n",
    "#     l3a, l3b = DC_CNN_Block(units, 3, 8, 0.01)(l2a)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    # attention_layer =   Add()([l1a, l2a])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 32\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(embedded_sequences)\n",
    "        l_pool = AveragePooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=n_filters * 3, kernel_size=5)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "    lstm_layer = Bidirectional(GRU(number_lstm_units, activation='tanh', \n",
    "                                   return_sequences=True, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "                               merge_mode='ave')\n",
    "\n",
    "#     lstm_layer = LSTM(number_lstm_units, return_sequences=True, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(embedded_sequences)\n",
    "#     layer = LSTM(number_lstm_units, return_sequences=False, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(lstm_layer)\n",
    "\n",
    "    layer = lstm_layer(embedded_sequences)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh', kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        \n",
    "        residual =  block_input\n",
    "        residual = Activation('relu')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "        layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "        layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "        layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(50, activation='relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        skip_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, metrics=[pos_distance, neg_distance, custom_margin_loss])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 86)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 86)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 86)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 50)           36886500    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 150)          38003008    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 500)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 500)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 500)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 75,053,008\n",
      "Trainable params: 1,490,608\n",
      "Non-trainable params: 73,562,400\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss: 0.86, CustomLoss: 0.86, pos_cosine: 0.92, neg_cosine: 0.79\n",
      "Epoch: 2 Loss: 0.87, CustomLoss: 0.87, pos_cosine: 0.89, neg_cosine: 0.76\n",
      "Epoch: 3 Loss: 0.84, CustomLoss: 0.84, pos_cosine: 0.87, neg_cosine: 0.70\n",
      "Epoch: 4 Loss: 0.84, CustomLoss: 0.84, pos_cosine: 0.85, neg_cosine: 0.69\n",
      "Epoch: 5 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.88, neg_cosine: 0.67\n",
      "Epoch: 6 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.89, neg_cosine: 0.68\n",
      "Epoch: 7 Loss: 0.76, CustomLoss: 0.76, pos_cosine: 0.87, neg_cosine: 0.64\n",
      "Epoch: 8 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.86, neg_cosine: 0.63\n",
      "Epoch: 9 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.87, neg_cosine: 0.64\n",
      "Epoch: 10 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.90, neg_cosine: 0.60\n",
      "Epoch: 11 Loss: 0.75, CustomLoss: 0.75, pos_cosine: 0.86, neg_cosine: 0.62\n",
      "Epoch: 12 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.86, neg_cosine: 0.59\n",
      "Epoch: 13 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.87, neg_cosine: 0.56\n",
      "Epoch: 14 Loss: 0.71, CustomLoss: 0.71, pos_cosine: 0.87, neg_cosine: 0.58\n",
      "Epoch: 15 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.86, neg_cosine: 0.59\n",
      "Epoch: 16 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.88, neg_cosine: 0.53\n",
      "Epoch: 17 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.88, neg_cosine: 0.56\n",
      "Epoch: 18 Loss: 0.75, CustomLoss: 0.75, pos_cosine: 0.83, neg_cosine: 0.58\n",
      "Epoch: 19 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.86, neg_cosine: 0.53\n",
      "Epoch: 20 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.87, neg_cosine: 0.56\n",
      "Epoch: 21 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.88, neg_cosine: 0.53\n",
      "Epoch: 22 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.87, neg_cosine: 0.52\n",
      "Epoch: 23 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.85, neg_cosine: 0.52\n",
      "Epoch: 24 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.87, neg_cosine: 0.50\n",
      "Epoch: 25 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.83, neg_cosine: 0.50\n",
      "Epoch: 26 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 27 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 28 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 29 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.88, neg_cosine: 0.53\n",
      "Epoch: 30 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 31 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.88, neg_cosine: 0.52\n",
      "Epoch: 32 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.86, neg_cosine: 0.52\n",
      "Epoch: 33 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.89, neg_cosine: 0.50\n",
      "Epoch: 34 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.89, neg_cosine: 0.53\n",
      "Epoch: 35 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.89, neg_cosine: 0.51\n",
      "Epoch: 36 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.88, neg_cosine: 0.51\n",
      "Epoch: 38 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.89, neg_cosine: 0.48\n",
      "Epoch: 39 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 40 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.90, neg_cosine: 0.53\n",
      "Epoch: 41 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.91, neg_cosine: 0.51\n",
      "Epoch: 42 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.89, neg_cosine: 0.49\n",
      "Epoch: 43 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.88, neg_cosine: 0.52\n",
      "Epoch: 44 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.92, neg_cosine: 0.49\n",
      "Epoch: 45 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.49\n",
      "Epoch: 46 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.85, neg_cosine: 0.53\n",
      "Epoch: 47 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.90, neg_cosine: 0.53\n",
      "Epoch: 48 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.89, neg_cosine: 0.53\n",
      "Epoch: 49 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.87, neg_cosine: 0.48\n",
      "Epoch: 50 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.92, neg_cosine: 0.54\n",
      "Epoch: 51 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.92, neg_cosine: 0.50\n",
      "Epoch: 52 Loss: 0.55, CustomLoss: 0.55, pos_cosine: 0.91, neg_cosine: 0.46\n",
      "Epoch: 53 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 54 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.90, neg_cosine: 0.51\n",
      "Epoch: 55 Loss: 0.55, CustomLoss: 0.55, pos_cosine: 0.91, neg_cosine: 0.47\n",
      "Epoch: 56 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.91, neg_cosine: 0.48\n",
      "Epoch: 57 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.90, neg_cosine: 0.52\n",
      "Epoch: 58 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 59 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.89, neg_cosine: 0.52\n",
      "Epoch: 60 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.92, neg_cosine: 0.54\n",
      "Epoch: 61 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.90, neg_cosine: 0.49\n",
      "Epoch: 62 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.91, neg_cosine: 0.53\n",
      "Epoch: 63 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 64 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.89, neg_cosine: 0.46\n",
      "Epoch: 65 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.92, neg_cosine: 0.53\n",
      "Epoch: 66 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.90, neg_cosine: 0.51\n",
      "Epoch: 67 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 68 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.93, neg_cosine: 0.51\n",
      "Epoch: 69 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.89, neg_cosine: 0.45\n",
      "Epoch: 70 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.90, neg_cosine: 0.53\n",
      "Epoch: 71 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.90, neg_cosine: 0.48\n",
      "Epoch: 72 Loss: 0.55, CustomLoss: 0.55, pos_cosine: 0.93, neg_cosine: 0.48\n",
      "Epoch: 73 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.89, neg_cosine: 0.51\n",
      "Epoch: 74 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.92, neg_cosine: 0.51\n",
      "Epoch: 75 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.89, neg_cosine: 0.52\n",
      "Epoch: 76 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 77 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.92, neg_cosine: 0.50\n",
      "Epoch: 78 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.94, neg_cosine: 0.51\n",
      "Epoch: 79 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 80 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.90, neg_cosine: 0.49\n",
      "Epoch: 81 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.90, neg_cosine: 0.50\n",
      "Epoch: 82 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.92, neg_cosine: 0.49\n",
      "Epoch: 83 Loss: 0.56, CustomLoss: 0.56, pos_cosine: 0.91, neg_cosine: 0.47\n",
      "Epoch: 84 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.92, neg_cosine: 0.54\n",
      "Epoch: 85 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.93, neg_cosine: 0.53\n",
      "Epoch: 86 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.95, neg_cosine: 0.52\n",
      "Epoch: 87 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.91, neg_cosine: 0.50\n",
      "Epoch: 88 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.92, neg_cosine: 0.49\n",
      "Epoch: 89 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.89, neg_cosine: 0.57\n",
      "Epoch: 90 Loss: 0.54, CustomLoss: 0.54, pos_cosine: 0.95, neg_cosine: 0.49\n",
      "Epoch: 91 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.93, neg_cosine: 0.50\n",
      "Epoch: 92 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.91, neg_cosine: 0.52\n",
      "Epoch: 93 Loss: 0.54, CustomLoss: 0.54, pos_cosine: 0.92, neg_cosine: 0.46\n",
      "Epoch: 94 Loss: 0.57, CustomLoss: 0.57, pos_cosine: 0.93, neg_cosine: 0.50\n",
      "Epoch: 95 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.91, neg_cosine: 0.49\n",
      "Epoch: 96 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.91, neg_cosine: 0.51\n",
      "Epoch: 97 Loss: 0.55, CustomLoss: 0.55, pos_cosine: 0.92, neg_cosine: 0.47\n",
      "Epoch: 98 Loss: 0.58, CustomLoss: 0.58, pos_cosine: 0.93, neg_cosine: 0.51\n",
      "Epoch: 99 Loss: 0.56, CustomLoss: 0.56, pos_cosine: 0.95, neg_cosine: 0.51\n",
      "Epoch: 100 Loss: 0.59, CustomLoss: 0.59, pos_cosine: 0.92, neg_cosine: 0.51, recall@25: 0.53\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(netbeans).h5' to disk\n",
      "Best_epoch=90, Best_loss=0.54, Recall@25=0.53\n",
      "CPU times: user 10min 28s, sys: 49.8 s, total: 11min 18s\n",
      "Wall time: 9min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "'''\n",
    "desc_feature_model = cnn_dilated_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _ = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[3]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['198406:196608|196608:0.9705294799059629,210772:0.7587774842977524,214696:0.7265595197677612,234283:0.6705836355686188,201132:0.6565663516521454,195730:0.6196960210800171,198789:0.5974116027355194,192930:0.557256281375885,192486:0.5567671060562134,192247:0.5457091331481934,193869:0.5438486933708191,210909:0.5414571762084961,210781:0.541119247674942,222788:0.5405116081237793,200512:0.5401974022388458,192222:0.5392743349075317,195921:0.5391230285167694,191308:0.5390538573265076,192504:0.5388160347938538,198955:0.5374323129653931,177820:0.5351570546627045,193340:0.5350338816642761,201021:0.5349790751934052,190319:0.5349413156509399,196075:0.5345537662506104',\n",
       " '189728:198719|183595:0.9917346499860287,184214:0.981567457318306,182643:0.9811531342566013,183718:0.9810106102377176,194488:0.9792743939906359,178486:0.9490474127233028,183709:0.9488052688539028,182900:0.947974219918251,182504:0.946221511811018,182875:0.9400903731584549,192558:0.9345903545618057,182625:0.9342104122042656,182660:0.9278307929635048,182648:0.8995156064629555,197083:0.8954117596149445,196549:0.8953697606921196,180920:0.8121188282966614,190113:0.811161682009697,182728:0.7237465977668762,186557:0.7212932407855988,180222:0.7106001377105713,197717:0.7082052528858185,177914:0.7069063186645508,189233:0.6823714971542358,184669:0.6650291979312897',\n",
       " '196611:198719|183595:0.9852474117651582,182643:0.9825928509235382,183718:0.9804830998182297,194488:0.9791973028331995,184214:0.978615578263998,178486:0.9493037723004818,182900:0.9475449733436108,183709:0.9473558813333511,182504:0.946592316031456,182875:0.9389066286385059,192558:0.9339005947113037,182625:0.9334762766957283,182660:0.9266131296753883,182648:0.8993314057588577,196549:0.8974910080432892,197083:0.8943868800997734,180920:0.8123368471860886,190113:0.8112424612045288,182728:0.7233124673366547,186557:0.7210939824581146,180222:0.7106536328792572,197717:0.7079330086708069,177914:0.706988662481308,189233:0.682349294424057,184669:0.6650891900062561',\n",
       " '226337:228991|237850:0.9808258470147848,236213:0.974359042942524,235319:0.9268547892570496,228991:0.7206202149391174,236827:0.6902297735214233,227232:0.6838478744029999,230281:0.6817590296268463,236101:0.6797124147415161,209189:0.6709323227405548,230061:0.6596774160861969,232564:0.6544120609760284,229135:0.6387743949890137,233602:0.6353594362735748,236703:0.6307910680770874,234685:0.6307781338691711,230452:0.629437118768692,232134:0.628227025270462,217598:0.6242662966251373,233330:0.6218694448471069,222307:0.6174519062042236,225525:0.6092451214790344,231382:0.6092111468315125,219864:0.6091534197330475,216251:0.6088868081569672,208681:0.6065908372402191',\n",
       " '230596:228991|237850:0.9798274412751198,236213:0.9765237886458635,235319:0.9285027086734772,228991:0.7209553718566895,236827:0.6903426945209503,227232:0.6840859651565552,230281:0.6817850172519684,236101:0.6800258457660675,209189:0.671092301607132,230061:0.6596276760101318,232564:0.6543607115745544,229135:0.6388402879238129,233602:0.6355532705783844,236703:0.6307656168937683,234685:0.6307457983493805,230452:0.6293002068996429,232134:0.6285201907157898,217598:0.6241534948348999,233330:0.6219107806682587,222307:0.6176126301288605,225525:0.6093975901603699,231382:0.609371155500412,219864:0.6092471778392792,216251:0.6091242730617523,208681:0.6068127453327179',\n",
       " '229380:228991|236213:0.9782830122858286,237850:0.9777604844421148,235319:0.9282040446996689,228991:0.7209659814834595,236827:0.6903092861175537,227232:0.6839821338653564,230281:0.6814903914928436,236101:0.6799707114696503,209189:0.6711616516113281,230061:0.6595917344093323,232564:0.6543333530426025,229135:0.6386939287185669,233602:0.6354701817035675,236703:0.6306851208209991,234685:0.6306734383106232,230452:0.6291691064834595,232134:0.628558337688446,217598:0.6239942312240601,233330:0.6219791769981384,222307:0.6175373792648315,231382:0.6093377768993378,225525:0.6091770529747009,219864:0.6091479957103729,216251:0.609011709690094,208681:0.6066572070121765',\n",
       " '230625:228991|220454:0.7498593628406525,218447:0.7497817873954773,217817:0.7468945980072021,208732:0.733843982219696,203549:0.7335652112960815,207937:0.7331153750419617,216005:0.7322904169559479,206525:0.7322851717472076,190226:0.7124276161193848,232564:0.7028727233409882,232167:0.7005728781223297,197781:0.6888369917869568,233443:0.6493265628814697,221778:0.643559604883194,222307:0.6415217220783234,217598:0.6350307166576385,212938:0.6325847506523132,211655:0.632347822189331,220753:0.6318249106407166,230559:0.6299919188022614,207576:0.6293609142303467,224556:0.6283106207847595,210583:0.6272085905075073,211454:0.6262751221656799,215141:0.6220515370368958',\n",
       " '229719:228991|228991:0.9573058225214481,217598:0.759072482585907,222307:0.754703477025032,209189:0.7334294319152832,225525:0.7207511365413666,219864:0.720555990934372,216251:0.7205123901367188,231382:0.7205063998699188,219507:0.7191799283027649,230342:0.7189628183841705,222890:0.7189387083053589,235319:0.7182989716529846,237850:0.717061847448349,236213:0.7169867753982544,233557:0.7135773301124573,218475:0.7086281478404999,219709:0.701657235622406,220454:0.6978250443935394,218447:0.697430819272995,217817:0.6894042789936066,217344:0.6873156428337097,225222:0.6872799396514893,218719:0.6869813203811646,220765:0.6853833496570587,226248:0.6814013123512268',\n",
       " '142004:180230|142133:0.9158926904201508,140816:0.8032848387956619,158246:0.6038313508033752,155021:0.6026815474033356,163156:0.5849635899066925,154516:0.5745540857315063,142032:0.5690511763095856,138353:0.5685918033123016,123678:0.552353024482727,167431:0.5481657981872559,100815:0.3080088496208191,104802:0.30592411756515503,113114:0.3059006929397583,114406:0.3058616518974304,115101:0.3057541251182556,171183:0.3054516315460205,168407:0.3050001263618469,167968:0.2962753176689148,129304:0.2961384654045105,114417:0.29426634311676025,104224:0.29274678230285645,112290:0.2926889657974243,84869:0.29268336296081543,140081:0.2926675081253052,98130:0.29264283180236816',\n",
       " '223173:180230|226688:0.983588207513094,222837:0.7773776948451996,233601:0.7548027336597443,224089:0.5611754655838013,219203:0.5611422657966614,225228:0.5558987557888031,192486:0.553875744342804,192930:0.5538653433322906,219450:0.5527855455875397,217061:0.5497467219829559,225088:0.5423393845558167,225259:0.5409277975559235,217670:0.5408269762992859,218922:0.5404255092144012,210909:0.5380943119525909,210781:0.5379491746425629,229931:0.5361666977405548,236882:0.5353901088237762,207991:0.5311668217182159,207764:0.5306797027587891,208266:0.5306651890277863,225381:0.5296767950057983,226568:0.5293762981891632,222059:0.5289469361305237,218247:0.5288905799388885',\n",
       " '196616:196549|202385:0.9728011041879654,201587:0.9721489269286394,197534:0.9495340846478939,203827:0.9489880576729774,191620:0.9401118271052837,192548:0.9385467283427715,198141:0.9374627098441124,196734:0.9002574235200882,197370:0.8996586799621582,195432:0.8971373736858368,198719:0.8956526145339012,194366:0.8551482856273651,204432:0.8043393790721893,202588:0.8034780025482178,206401:0.8010415285825729,198957:0.8007340133190155,208524:0.7705693542957306,213239:0.7699011564254761,191941:0.7635140269994736,200469:0.7611335963010788,195153:0.7606398612260818,219979:0.7529274970293045,222191:0.7524981200695038,205917:0.7505933940410614,233311:0.7238447666168213',\n",
       " '196615:196549|196734:0.9733875840902328,197370:0.9709492474794388,195432:0.9315948635339737,198719:0.9291670545935631,202385:0.8995098397135735,201587:0.8994847983121872,191620:0.8927010521292686,192548:0.8917935490608215,198141:0.8911199122667313,197534:0.882514014840126,203827:0.88227928429842,204432:0.8248176425695419,194366:0.8223534375429153,206401:0.8027101904153824,198957:0.8023968487977982,202588:0.7970855832099915,205917:0.7899430841207504,208524:0.7722766399383545,213239:0.7716022580862045,219979:0.7546292692422867,222191:0.7542789131402969,211377:0.7501626014709473,191941:0.744913250207901,218601:0.7419866919517517,225451:0.7404900789260864',\n",
       " '129785:132247|132247:1.0,147776:0.9225176125764847,125694:0.7810778617858887,158716:0.41874635219573975,115461:0.41834479570388794,92542:0.411406934261322,129578:0.41134870052337646,122950:0.4113196134567261,132841:0.4112256169319153,116022:0.3999440670013428,109500:0.39993923902511597,123067:0.3999032974243164,101119:0.3998293876647949,167459:0.3997938632965088,153150:0.3997851014137268,113282:0.3997376561164856,135568:0.39971572160720825,111546:0.39963752031326294,139087:0.39941537380218506,99555:0.39327675104141235,145546:0.39299720525741577,110102:0.39203256368637085,169863:0.3900926113128662,111072:0.3896872401237488,93960:0.38966280221939087',\n",
       " '131083:132247|147776:0.9645553193986416,132247:0.9308124855160713,125694:0.7885967195034027,158716:0.3955778479576111,92542:0.3952389359474182,115461:0.39517486095428467,129578:0.39517003297805786,122950:0.3951414227485657,132841:0.3950539827346802,109500:0.3790069818496704,116022:0.3789941668510437,123067:0.3789469599723816,101119:0.37887322902679443,167459:0.3788483142852783,153150:0.37883734703063965,113282:0.37879741191864014,135568:0.37876439094543457,111546:0.37868547439575195,139087:0.3784709572792053,169863:0.3761247992515564,111072:0.37571829557418823,93960:0.3756979703903198,103091:0.3753761053085327,110102:0.37521249055862427,123362:0.37099266052246094',\n",
       " '131083:132247|147776:0.9645553193986416,132247:0.9308124855160713,125694:0.7885967195034027,158716:0.3955778479576111,92542:0.3952389359474182,115461:0.39517486095428467,129578:0.39517003297805786,122950:0.3951414227485657,132841:0.3950539827346802,109500:0.3790069818496704,116022:0.3789941668510437,123067:0.3789469599723816,101119:0.37887322902679443,167459:0.3788483142852783,153150:0.37883734703063965,113282:0.37879741191864014,135568:0.37876439094543457,111546:0.37868547439575195,139087:0.3784709572792053,169863:0.3761247992515564,111072:0.37571829557418823,93960:0.3756979703903198,103091:0.3753761053085327,110102:0.37521249055862427,123362:0.37099266052246094',\n",
       " '131084:132247|147776:0.9645553193986416,132247:0.9308124855160713,125694:0.7885967195034027,158716:0.3955778479576111,92542:0.3952389359474182,115461:0.39517486095428467,129578:0.39517003297805786,122950:0.3951414227485657,132841:0.3950539827346802,109500:0.3790069818496704,116022:0.3789941668510437,123067:0.3789469599723816,101119:0.37887322902679443,167459:0.3788483142852783,153150:0.37883734703063965,113282:0.37879741191864014,135568:0.37876439094543457,111546:0.37868547439575195,139087:0.3784709572792053,169863:0.3761247992515564,111072:0.37571829557418823,93960:0.3756979703903198,103091:0.3753761053085327,110102:0.37521249055862427,123362:0.37099266052246094',\n",
       " '131085:131082|131082:0.9998189858015394,131490:0.9737144336104393,113138:0.9668852426111698,76311:0.9662070423364639,135742:0.9273717477917671,105342:0.884021207690239,128104:0.4706706404685974,122657:0.47015368938446045,141926:0.4659598469734192,82394:0.44463998079299927,139505:0.4310106039047241,112790:0.4154949188232422,148401:0.3931316137313843,139861:0.37360531091690063,140907:0.35544341802597046,126927:0.11073881387710571,113294:0.10778564214706421,96237:0.10776883363723755,127585:0.10446637868881226,91993:0.1011425256729126,162133:0.10108828544616699,110458:0.10102689266204834,141578:0.10080474615097046,105695:0.09937387704849243,126135:0.09865593910217285',\n",
       " '65910:63195|67718:0.9854309074580669,73690:0.9851709231734276,69479:0.9845481440424919,70190:0.9844710202887654,68695:0.9825206510722637,65463:0.9817097075283527,64937:0.9813640844076872,63594:0.9813231416046619,73956:0.979201951995492,64364:0.9781072344630957,63195:0.9776620287448168,97487:0.976699685677886,66324:0.9753842353820801,70060:0.975367795675993,65285:0.9743408709764481,94858:0.9680718034505844,73679:0.9050893485546112,73805:0.8252412378787994,75032:0.8250253200531006,62743:0.8199616372585297,59706:0.8199530094861984,66646:0.8193351179361343,61784:0.8191708475351334,66304:0.8188512176275253,73804:0.8184088319540024',\n",
       " '67772:63195|70190:0.9308425411581993,73690:0.9307840541005135,63594:0.9302255883812904,67718:0.9301323294639587,97487:0.9300537705421448,69479:0.9297996163368225,73956:0.9296158403158188,64937:0.9293261840939522,63195:0.929087720811367,70060:0.9289255440235138,64364:0.9288934022188187,68695:0.928448922932148,65285:0.9282512590289116,65463:0.9274462535977364,66324:0.9266747534275055,94858:0.9255155548453331,73679:0.8981718420982361,64879:0.8212487697601318,70827:0.8209371715784073,77872:0.8208111077547073,65815:0.8204214870929718,69208:0.8203421831130981,61759:0.8203338384628296,59747:0.8193688541650772,73805:0.8135538697242737',\n",
       " '65550:63195|69479:0.9271348416805267,73690:0.9270516186952591,70190:0.9269336462020874,64937:0.9266221672296524,63594:0.9264962375164032,63195:0.9260257184505463,67718:0.9256500080227852,70060:0.9254102036356926,73956:0.925389364361763,64364:0.925318568944931,97487:0.9251034185290337,68695:0.9242067188024521,65463:0.9230676367878914,65285:0.9229060709476471,94858:0.9222799092531204,66324:0.9214466065168381,73679:0.8943854197859764,64879:0.8200346827507019,65815:0.8192836046218872,77872:0.8192605078220367,70827:0.8191003352403641,69208:0.8190432637929916,59747:0.8178558200597763,61759:0.817770704627037,75032:0.8117846548557281']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recall@25 last epoch:', 0.53)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall, exported_rank = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "\n",
    "\"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 7164\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_100epochs_64batch(netbeans)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 86)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 50)           36886500    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 150)          38003008    desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 500)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "==================================================================================================\n",
      "Total params: 75,053,008\n",
      "Trainable params: 1,490,608\n",
      "Non-trainable params: 73,562,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/netbeans/exported_rank_propose.txt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.3,\n",
       " '2 - recall_at_10': 0.41,\n",
       " '3 - recall_at_15': 0.47,\n",
       " '4 - recall_at_20': 0.51,\n",
       " '5 - recall_at_25': 0.53}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
