{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - PROPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "212512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.load_ids(DIR)\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(data, max_seq_length):\n",
    "    seq_lengths = [len(seq) for seq in data]\n",
    "    seq_lengths.append(6)\n",
    "    max_seq_length = min(max(seq_lengths), max_seq_length)\n",
    "    padded_data = np.zeros(shape=[len(data), max_seq_length])\n",
    "    for i, seq in enumerate(data):\n",
    "        seq = seq[:max_seq_length]\n",
    "        for j, token in enumerate(seq):\n",
    "            padded_data[i, j] = int(token)\n",
    "    return padded_data.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "def load_bugs(baseline):   \n",
    "    removed = []\n",
    "    baseline.corpus = []\n",
    "    baseline.sentence_dict = {}\n",
    "    baseline.bug_set = {}\n",
    "    title_padding, desc_padding = [], []\n",
    "    for bug_id in tqdm(baseline.bug_ids):\n",
    "        try:\n",
    "            bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "            title_padding.append(bug['title_word'])\n",
    "            desc_padding.append(bug['description_word'])\n",
    "            baseline.bug_set[bug_id] = bug\n",
    "            #break\n",
    "        except:\n",
    "            removed.append(bug_id)\n",
    "    \n",
    "    # Padding\n",
    "    title_padding = data_padding(title_padding, 100)\n",
    "    desc_padding = data_padding(desc_padding, 500)\n",
    "    \n",
    "    for bug_id, bug_title, bug_desc in tqdm(zip(baseline.bug_ids, title_padding, desc_padding)):\n",
    "        baseline.bug_set[bug_id]['title_word'] = bug_title\n",
    "        baseline.bug_set[bug_id]['description_word'] = bug_desc\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        baseline.sentence_dict[\",\".join(bug_title.astype(str))] = bug['title']\n",
    "        baseline.sentence_dict[\",\".join(bug_desc.astype(str))] = bug['description']\n",
    "    \n",
    "    if len(removed) > 0:\n",
    "        for x in removed:\n",
    "            baseline.bug_ids.remove(x)\n",
    "        baseline.removed = removed\n",
    "        print(\"{} were removed. To see the list call self.removed\".format(len(removed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00d76a663f4edd9aa298d5ab0a461f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d391689836ba4c9dbfed125efe2a3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 13s, sys: 2.51 s, total: 1min 16s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_bugs(baseline)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0WZNngemNM8"
   },
   "source": [
    "## Geração de batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "CPU times: user 363 ms, sys: 21 µs, total: 363 ms\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': '125\\n', 'creation_ts': '2001-10-10 22:38:00 -0400', 'bug_severity': '4\\n', 'component': '566\\n', 'resolution': 'FIXED', 'bug_status': '0\\n', 'description_word': array([ 241, 3070,   86,  548,  297,  394,    9,  196,   95,   16,  131,\n",
      "          8,   17,  213, 1931,  196,   95,    2,   27,   22,  130,  783,\n",
      "        276,   16,   20, 3484,   23,  610,   11,   28,   44,  563,    2,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 'version': '522\\n', 'issue_id': 2521, 'dup_id': '[]', 'title': 'selecting window in the window menu does not maximize window gfitic', 'priority': '0\\n', 'delta_ts': '2005-05-10 14:55:51 -0400', 'title_word': array([1036,   95,   11,    8,   95,  213,  101,   20, 3086,   95,    1,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0]), 'description': 'steps minimize all your windows go to any window and select the nationality menu pick any window organization that it only gets selected and not maximized this happens in country as well organization'}\n"
     ]
    }
   ],
   "source": [
    "if 2521 in baseline.bug_set:\n",
    "    print(baseline.bug_set[2521])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 ms, sys: 0 ns, total: 51.7 ms\n",
      "Wall time: 51.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "train_gen = baseline.siam_gen(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 43), (128, 500), (128, 1682), (128,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: many organization doesn have content\n",
      "***Title***: organization editor window scrolling area doesn match real size after eclipse restarts\n",
      "***Description***: created attachment empty debug dialog starting with organization many of the dialogs the run or debug dialog import or export dialog doesn have content after restart everything works fine see the attached file for an example debug dialog already delete the workspace but it didn help\n",
      "***Description***: created attachment editing window before exiting organization steps to reproduce open large enough source file for scrolling close eclipse start eclipse try to scroll editor window contents note there is only small rectangle scrolling not entire editing area more information organization ultimate to make it work correctly have to double click source tab twice maximize restore\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: organization organization filter as part of the import wizard which will be sent to the organization runtime\n",
      "***Title***: organization filter blade support\n",
      "***Description***: provide organization filter as part of the import wizard which will be sent to the organization runtime by any criteria including things like the most recent entries by products severity by any organization properties etc should be implemented at the back end organization agent and controlled programatically or by the front end lta\n",
      "***Description***: filtering blade support in the organization extension point for supporting pluggable filter engine organization log wizard provides pluggable filter organization and filter engine which be passed to organization on import filtering\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: uninstalling feature using updates existing plugins\n",
      "***Title***: organization crashes if link to organization is clicked twice\n",
      "***Description***: personi please see below person to reproduce install eclipse and add jdt cvs install pydev plugin from http pydev org updates ensure that the organization all update sites during install to find required software check box is unchecked restart eclipse when prompted to do so uninstall pydev using law restart when prompted note that jdt has been updated to cvs has been updated to what is happening is that is doing an update of xisting plugins during uninstall\n",
      "***Description***: organization crashes if link to organization is clicked twice\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: task restoration fails at startup\n",
      "***Title***: forms null pointer exception hovering over person control created from contstructor\n",
      "***Description***: error details date mon jan eet message could not open editor for organization src main java package classname java classname methodname qparametertype severity person org eclipse mylyn java ui organization eclipse build id java version java vendor organization constants organization linux organization organization gtk organization en organization line arguments os linux ws gtk arch exception person org eclipse core runtime organization assertion failed at org eclipse core runtime person is true person java at org eclipse core runtime person is true person java at org eclipse jface text source product region annotation iterator annotation model java at org eclipse jface text source product annotation iterator annotation model java at org eclipse jface text source artwork do paint artwork java at org eclipse jface text source artwork double buffer paint artwork java at org eclipse jface text source artwork redraw artwork java at org eclipse jface text source artwork internal listener viewport changed artwork java at org eclipse jface text organization update viewport listeners organization java at org eclipse jface text organization internal reveal range organization java at org eclipse jface text organization reveal range organization java at org eclipse jdt internal ui javaeditor person set selection person java at org eclipse jdt internal ui javaeditor person set selection person java at org eclipse jdt internal ui javaeditor organization reveal in editor organization java at org eclipse jdt ui person open in editor person java at org eclipse jdt ui person open in editor person java at org eclipse mylyn internal java ui person open person java at org eclipse mylyn internal context ui person manager context activated person manager java at org eclipse mylyn internal context core organization context manager internal activate context organization context manager java at org eclipse mylyn internal context core organization context manager activate context organization context manager java at org eclipse mylyn internal tasks ui organization task activated organization java at org eclipse mylyn internal tasks core task activity manager activate task task activity manager java at org eclipse mylyn internal tasks ui actions product run product java at org eclipse mylyn internal tasks ui views task list cell modifier toggle organization task list cell modifier java at org eclipse mylyn internal tasks ui views facilitymouse facility java at org eclipse swt widgets person handle event person java at org eclipse swt widgets artwork send event artwork java at org eclipse swt widgets person send event person java at org eclipse swt widgets person run deferred events person java at org eclipse swt widgets person read and dispatch person java at org eclipse ui internal nationality run event loop nationality java at org eclipse ui internal nationality run organization nationality java at org eclipse ui internal nationality access nationality java at org eclipse ui internal nationality run nationality java at org eclipse core databinding observable country run with default country java at org eclipse ui internal nationality create and run nationality nationality java at org eclipse ui platform organization create and run nationality platform organization java at org eclipse ui internal ide application ideapplication start ideapplication java at org eclipse equinox internal app person run person java at org eclipse core runtime internal adaptor person run application person java at org eclipse core runtime internal adaptor person start person java at org eclipse core runtime adaptor person run person java at org eclipse core runtime adaptor person run person java at sun reflect nationality person person invoke nationality person at sun reflect nationality person person invoke nationality person person java at sun reflect delegating person person invoke delegating person person java at java lang reflect person invoke person java at org eclipse equinox launcher person invoke framework person java at org eclipse equinox launcher person basic run person java at org eclipse equinox launcher person run person java\n",
      "***Description***: forms null pointer exception hovering over person control created from contstructor\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 47.3 ms, sys: 0 ns, total: 47.3 ms\n",
      "Wall time: 46.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 137745'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading Glove\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.split()\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784735b90e8d409f9b9feb996c7a50a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd825ccf75d440f9a478ad078784d1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=137745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min, sys: 3.07 s, total: 2min 3s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating for each epoch at same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods train experiment siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.retrieval import Retrieval\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_queries(retrieval, path_test):\n",
    "    print(\"Creating the queries...\")\n",
    "    test = []\n",
    "    with open(path_test, 'r') as file_test:\n",
    "        for row in tqdm(file_test):\n",
    "            tokens = row.strip().split()\n",
    "            test.append([int(tokens[0]), [int(bug) for bug in tokens[1:]]])\n",
    "    retrieval.test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce366862883b4e77abddde7ab1124883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b9200148ef4a1ca5be55d6a825b324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval = Retrieval()\n",
    "\n",
    "path = 'data/processed/{}'.format(DOMAIN)\n",
    "path_buckets = 'data/normalized/{}/{}.csv'.format(DOMAIN, DOMAIN)\n",
    "path_train = 'data/processed/{}/train.txt'.format(DOMAIN)\n",
    "path_test = 'data/processed/{}/test.txt'.format(DOMAIN)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_I = number_of_columns_info # Status, Severity, Version, Component, Module\n",
    "\n",
    "# Create the instance from baseline\n",
    "#retrieval.baseline = Baseline(path, path_buckets, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "retrieval.baseline = baseline\n",
    "\n",
    "df = pd.read_csv(path_buckets)\n",
    "\n",
    "# Load bug ids\n",
    "#retrieval.load_bugs(path, path_train)\n",
    "# Create the buckets\n",
    "retrieval.create_bucket(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9dc2e682a94c1ea6081a9df8119bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and create the test queries duplicate\n",
    "create_queries(retrieval, path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb3f97b8b942c09e238457df37c6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = {}\n",
    "for bucket in tqdm(retrieval.buckets):\n",
    "    issues_by_buckets[bucket] = bucket\n",
    "    for issue in np.array(retrieval.buckets[bucket]).tolist():\n",
    "        issues_by_buckets[issue] = bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "def get_model_vectorizer(path=None, loaded_model=None):\n",
    "    if(path):\n",
    "        loaded_model = load_model(os.path.join(\"modelos\", \"model_{}.h5\".format(path)))\n",
    "        \n",
    "        '''\n",
    "            {'l2_normalize' : l2_normalize, \n",
    "                                     'margin_loss' : margin_loss,\n",
    "                                     'pos_distance' : pos_distance,\n",
    "                                     'neg_distance' : neg_distance,\n",
    "                                     'stack_tensors': stack_tensors}\n",
    "        '''\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the list of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_query(annoy, queries_test_vectorized, verbose=1):\n",
    "    X_test = queries_test_vectorized\n",
    "    distance_test, indices_test = [], []\n",
    "    loop = enumerate(X_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(enumerate(X_test))\n",
    "        loop.set_description('Getting the list of candidates from queries')\n",
    "    for index, row in loop:\n",
    "        vector = row['vector']\n",
    "        rank, dist = annoy.get_nns_by_vector(vector, 30, include_distances=True)\n",
    "        indices_test.append(rank)\n",
    "        distance_test.append(1 - np.array(dist)) # normalize the similarity between 0 and 1\n",
    "    if(verbose): loop.close()\n",
    "    return X_test, distance_test, indices_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing all train\n",
    "def indexing_test(buckets_train_vectorized, verbose=1):\n",
    "    X = np.array(buckets_train_vectorized)\n",
    "    annoy = AnnoyIndex(X[0]['vector'].shape[0])  # Length of item vector that will be indexed\n",
    "\n",
    "    loop = total=len(X)\n",
    "    if(verbose):\n",
    "        loop = tqdm(total=len(X))\n",
    "        loop.set_description(\"Indexing test in annoy\")\n",
    "    for index, row in enumerate(X):\n",
    "        vector = row['vector']\n",
    "        annoy.add_item(index, vector)\n",
    "        if(verbose): loop.update(1)\n",
    "    if(verbose): loop.close()\n",
    "    annoy.build(10) # 10 trees\n",
    "    return annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_result(test_vectorized, indices_test, distance_test, verbose=1):\n",
    "    formated_rank = []\n",
    "    loop = zip(indices_test, distance_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(zip(indices_test, distance_test))\n",
    "        loop.set_description('Generating the rank')\n",
    "    for row_index, row_sim in loop:\n",
    "        row_index, row_sim = row_index[:25], row_sim[:25]\n",
    "        formated_rank.append(\",\".join([\"{}:{}\".format(test_vectorized[index]['bug_id'], sim) \n",
    "                                       for index, sim in zip(row_index, row_sim)]))\n",
    "    if(verbose): loop.close()\n",
    "    return formated_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_test(bug_set, model, test, issues_by_buckets, verbose=1):\n",
    "    test_vectorized = []\n",
    "    title_data, desc_data, info_data = [], [], []\n",
    "    loop = test\n",
    "    if(verbose):\n",
    "        loop = tqdm(test)\n",
    "        loop.set_description('Vectorizing buckets')\n",
    "    buckets = set()\n",
    "    for row in loop: # retrieval.bugs_train\n",
    "        query, ground_truth = row\n",
    "        bugs = [query]\n",
    "        bugs += ground_truth\n",
    "        for bug_id in bugs:\n",
    "            buckets.add(issues_by_buckets[bug_id])\n",
    "    for bucket_id in buckets:\n",
    "        bug = bug_set[bucket_id]\n",
    "        title_data.append(bug['title_word'])\n",
    "        desc_data.append(bug['description_word'])\n",
    "        info_data.append(retrieval.get_info(bug))\n",
    "        test_vectorized.append({ 'bug_id' : bucket_id })\n",
    "    if(verbose):\n",
    "        loop.close()\n",
    "    # Get embedding of all buckets\n",
    "    embed_test = model.predict([ np.array(title_data), np.array(desc_data), np.array(info_data) ])\n",
    "    # Fill the buckets array\n",
    "    for index, vector in enumerate(embed_test):\n",
    "        test_vectorized[index]['vector'] = vector\n",
    "    \n",
    "    return test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_queries(bug_set, model, test, issues_by_buckets, verbose=1):\n",
    "    queries_test_vectorized = []\n",
    "    title_data, desc_data, info_data = [], [], []\n",
    "    loop = test\n",
    "    if(verbose):\n",
    "        loop = tqdm(test)\n",
    "    for row in loop:\n",
    "        test_bug_id, ground_truth = row\n",
    "        if issues_by_buckets[test_bug_id] == test_bug_id: # if the bug is the master\n",
    "            test_bug_id = np.random.choice(ground_truth, 1)[0]\n",
    "        queries = set()\n",
    "        queries.add(test_bug_id)\n",
    "        if test_bug_id in ground_truth:\n",
    "            ground_truth = list(set(ground_truth) - set([test_bug_id])) # Remove the same bug random choice to change the master\n",
    "        if len(ground_truth) > 0:\n",
    "            for bug in ground_truth:\n",
    "                if issues_by_buckets[bug] != bug: # if the bug is the master\n",
    "                    queries.add(bug)\n",
    "                \n",
    "        for bug_id in queries:\n",
    "            bug = bug_set[bug_id]\n",
    "            title_data.append(bug['title_word'])\n",
    "            desc_data.append(bug['description_word'])\n",
    "            info_data.append(retrieval.get_info(bug))\n",
    "            queries_test_vectorized.append({ 'bug_id' : bug_id, 'ground_truth': issues_by_buckets[bug_id] })\n",
    "\n",
    "    # Get embedding of all buckets\n",
    "    embed_queries = model.predict([ np.array(title_data), np.array(desc_data), np.array(info_data) ])\n",
    "    # Fill the queries array    \n",
    "    for index, vector in enumerate(embed_queries):\n",
    "        queries_test_vectorized[index]['vector'] = vector\n",
    "    \n",
    "    return queries_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the rank result\n",
    "def formating_rank(X_test, verbose=1):\n",
    "    rank_queries = []\n",
    "    loop = enumerate(X_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(enumerate(X_test))\n",
    "        loop.set_description('Generating the queries from rank')\n",
    "    for index, row in loop:\n",
    "        dup_a, ground_truth = row['bug_id'], row['ground_truth']\n",
    "        rank_queries.append(\"{}:{}\".format(dup_a, ground_truth))\n",
    "    if(verbose): loop.close()\n",
    "    return rank_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_rank(rank_queries, formated_rank, verbose=1):\n",
    "    exported_rank = []\n",
    "    loop = len(rank_queries)\n",
    "    if(verbose):\n",
    "        loop = tqdm(total=len(rank_queries))\n",
    "        loop.set_description('Exporting the rank')\n",
    "    for query, rank in zip(rank_queries, formated_rank):\n",
    "        exported_rank.append(\"{}|{}\".format(query, rank))\n",
    "        if(verbose): loop.update(1)\n",
    "    if(verbose): loop.close()\n",
    "    return exported_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_test(retrieval, verbose, loaded_model, issues_by_buckets):\n",
    "    \n",
    "    # Load test set\n",
    "    test = retrieval.test\n",
    "    bug_set = retrieval.baseline.get_bug_set()\n",
    "    \n",
    "    # Get model\n",
    "    model = get_model_vectorizer(loaded_model=loaded_model)\n",
    "    \n",
    "    # Test \n",
    "    test_vectorized = vectorizer_test(bug_set, model, test, issues_by_buckets, verbose)\n",
    "    queries_test_vectorized = vectorize_queries(bug_set, model, test, issues_by_buckets, verbose)\n",
    "    annoy = indexing_test(test_vectorized, verbose)\n",
    "    X_test, distance_test, indices_test = indexing_query(annoy, queries_test_vectorized, verbose)\n",
    "    formated_rank = rank_result(test_vectorized, indices_test, distance_test, verbose)\n",
    "    rank_queries = formating_rank(X_test, verbose)\n",
    "    exported_rank = export_rank(rank_queries, formated_rank, verbose)\n",
    "    evaluation = Evaluation(verbose)\n",
    "    recall = evaluation.evaluate(exported_rank)['5 - recall_at_25']\n",
    "    \n",
    "    # recall@25, loss, cosine_positive, cosine_negative\n",
    "    return recall, exported_rank\n",
    "    #return report['5 - recall_at_25'], evaluation_test_batch[0], evaluation_test_batch[1], evaluation_test_batch[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    def __init__(self, verbose=1):\n",
    "        self.verbose = verbose\n",
    "        self.MAX_RANK = 25\n",
    "    \n",
    "    \"\"\"\n",
    "        Rank recall_rate_@k\n",
    "        rank = \"query:master|master:id:sim,master:id:sim\"\n",
    "    \"\"\"\n",
    "    def top_k_recall(self, row, k):\n",
    "        query, rank = row.split('|')\n",
    "        query_dup_id, ground_truth = query.split(\":\")\n",
    "        candidates = [int(item.split(':')[0]) for pos, item in enumerate(rank.split(\",\")[:self.MAX_RANK])]\n",
    "        corrects = len(set([int(ground_truth)]) & set(candidates[:k]))\n",
    "        total = len([ground_truth]) # only one master from query\n",
    "        return float(corrects), total\n",
    "\n",
    "    def evaluate(self, path):\n",
    "        self.recall_at_5_corrects_sum, self.recall_at_10_corrects_sum, \\\n",
    "        self.recall_at_15_corrects_sum, self.recall_at_20_corrects_sum, self.recall_at_25_corrects_sum = 0, 0, 0, 0, 0\n",
    "        self.recall_at_5_total_sum, self.recall_at_10_total_sum, self.recall_at_15_total_sum, \\\n",
    "        self.recall_at_20_total_sum, self.recall_at_25_total_sum = 0, 0, 0, 0, 0 \n",
    "        if(self.verbose):\n",
    "            print(\"Evaluating...\")\n",
    "        if type(path) == str:\n",
    "            with open(path, 'r') as file_input:\n",
    "                for row in file_input:\n",
    "                    self.recall(row)\n",
    "        else:\n",
    "            for row in path:\n",
    "                self.recall(row)\n",
    "        \n",
    "        report = {\n",
    "            '1 - recall_at_5' : round(self.recall_at_5_corrects_sum / self.recall_at_5_total_sum, 2),\n",
    "            '2 - recall_at_10' : round(self.recall_at_10_corrects_sum / self.recall_at_10_total_sum, 2),\n",
    "            '3 - recall_at_15' : round(self.recall_at_15_corrects_sum / self.recall_at_15_total_sum, 2),\n",
    "            '4 - recall_at_20' : round(self.recall_at_20_corrects_sum / self.recall_at_20_total_sum, 2),\n",
    "            '5 - recall_at_25' : round(self.recall_at_25_corrects_sum / self.recall_at_25_total_sum, 2)\n",
    "        }\n",
    "\n",
    "        return report\n",
    "    def recall(self, row):\n",
    "        #if row == '': continue\n",
    "        self.recall_at_5_corrects, self.recall_at_5_total = self.top_k_recall(row, k=5)\n",
    "        self.recall_at_10_corrects, self.recall_at_10_total = self.top_k_recall(row, k=10)\n",
    "        self.recall_at_15_corrects, self.recall_at_15_total = self.top_k_recall(row, k=15)\n",
    "        self.recall_at_20_corrects, self.recall_at_20_total = self.top_k_recall(row, k=20)\n",
    "        self.recall_at_25_corrects, self.recall_at_25_total = self.top_k_recall(row, k=25)\n",
    "\n",
    "        self.recall_at_5_corrects_sum += self.recall_at_5_corrects\n",
    "        self.recall_at_10_corrects_sum += self.recall_at_10_corrects\n",
    "        self.recall_at_15_corrects_sum += self.recall_at_15_corrects\n",
    "        self.recall_at_20_corrects_sum += self.recall_at_20_corrects\n",
    "        self.recall_at_25_corrects_sum += self.recall_at_25_corrects\n",
    "\n",
    "        self.recall_at_5_total_sum += self.recall_at_5_total\n",
    "        self.recall_at_10_total_sum += self.recall_at_10_total\n",
    "        self.recall_at_15_total_sum += self.recall_at_15_total\n",
    "        self.recall_at_20_total_sum += self.recall_at_20_total\n",
    "        self.recall_at_25_total_sum += self.recall_at_25_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, verbose=0):\n",
    "    m_dir = os.path.join('modelos')\n",
    "    if not os.path.exists(m_dir):\n",
    "        os.mkdir(m_dir)\n",
    "    export = os.path.join(m_dir, \"model_{}.h5\".format(name))\n",
    "    model.save(export)\n",
    "    if(verbose):\n",
    "        print(\"Saved model '{}' to disk\".format(export))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def arcii_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    filters_1d=max_sequence_length\n",
    "    kernel_size_1d=3\n",
    "    num_conv2d_layers=2\n",
    "    filters_2d=[32,16]\n",
    "    kernel_size_2d=[[3,3], [3,3]]\n",
    "    mpool_size_2d=[[2,2], [2,2]]\n",
    "    dropout_rate=0.5\n",
    "    \n",
    "    layer1_conv=Conv1D(filters=filters_1d, kernel_size=kernel_size_1d, padding='same')(embedded_sequences)\n",
    "    layer1_activation=Activation('relu')(layer1_conv)\n",
    "    layer1_reshaped=Reshape((max_sequence_length, max_sequence_length, -1))(layer1_activation)\n",
    "    z=MaxPooling2D(pool_size=(2,2))(layer1_reshaped)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z=Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same')(z)\n",
    "        z=Activation('tanh')(z)\n",
    "        z=MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "\n",
    "    pool1_flat=Flatten()(z)\n",
    "    pool1_flat_drop=Dropout(rate=dropout_rate)(pool1_flat)\n",
    "    pool1_norm=BatchNormalization()(pool1_flat_drop)\n",
    "    mlp1=Dense(300)(pool1_norm)\n",
    "    output=Activation('tanh')(mlp1)\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[output], name = 'FeatureARCIIGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False, kernel_constraint=max_norm(1.), kernel_regularizer=l2(l2_layer_reg))(block_input)                    \n",
    "        selu_out =    Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False, kernel_constraint=max_norm(1.))(selu_out)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False, kernel_constraint=max_norm(1.))(selu_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    \n",
    "    attention_layes = [lb]\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        la, lb = DC_CNN_Block(units, 3, 2 * index, 0.01)(la)\n",
    "        attention_layes.append(lb)\n",
    "\n",
    "    attention_layer =   Add()(attention_layes)\n",
    "    #l9 =   Add()([l1a, l2a, l3a, l4a, l5a, l6a, l7a])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    x =  Conv1D(1,1, activation='linear', use_bias=False, kernel_constraint=max_norm(1.) )(layer)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dropout(0.50)(x)\n",
    "    layer = Dense(300, activation='tanh')(x)\n",
    "\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    lstm_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    layer = LSTM(number_lstm_units)(lstm_layer)\n",
    "\n",
    "    #layer = lstm_layer(embedded_sequences)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "    \n",
    "class MarginLoss(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        pos = inputs[0]\n",
    "        neg = inputs[1]\n",
    "        loss = self.margin_loss(pos, neg)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return inputs\n",
    "        \n",
    "    def margin_loss(self, pos, neg):\n",
    "        margin = K.constant(1.0)\n",
    "        return K.sum(K.maximum(0.0, margin - pos + neg))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.sum(K.maximum(0.0, margin - pos + neg))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size = K.int_shape(block_input)[1]\n",
    "        \n",
    "        residual =  block_input\n",
    "        \n",
    "        layer_out = Dense(shape_size // 2, activation='tanh')(block_input)\n",
    "        \n",
    "        skip_out =  Dense(shape_size, activation='linear', use_bias=False, kernel_constraint=max_norm(1.))(layer_out)\n",
    "        \n",
    "        dense_out =  Dense(shape_size, activation='linear', use_bias=False, kernel_constraint=max_norm(1.))(layer_out)\n",
    "        \n",
    "        block_out =   Add()([residual, dense_out])\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    textual_feat = Add(name='textual_features_{}'.format(name))([bug_t_feat, bug_d_feat])\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, textual_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    # encoded_1a, encoded_1b  = residual_bug()(bug_feature_output)\n",
    "    # bug_feature_output = encoded_1a\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Cosine\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, metrics=[pos_distance, neg_distance, custom_margin_loss])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          41429200    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 300)          42093765    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 150)          45150       FeatureLstmGenerationModel[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 150)          45150       FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 150)          45150       FeatureLstmGenerationModel[2][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 150)          45150       FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 150)          45150       FeatureLstmGenerationModel[3][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 150)          45150       FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 300)          45000       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 300)          45000       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 300)          45000       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 300)          45000       dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 300)          45000       dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 300)          45000       dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 300)          0           FeatureLstmGenerationModel[1][0] \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 300)          0           FeatureCNNDilatedGenerationModel[\n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 300)          0           FeatureLstmGenerationModel[2][0] \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 300)          0           FeatureCNNDilatedGenerationModel[\n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 300)          0           FeatureLstmGenerationModel[3][0] \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 300)          0           FeatureCNNDilatedGenerationModel[\n",
      "                                                                 dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "textual_features_in (Add)       (None, 300)          0           add_9[0][0]                      \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "textual_features_pos (Add)      (None, 300)          0           add_11[0][0]                     \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "textual_features_neg (Add)      (None, 300)          0           add_13[0][0]                     \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 600)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 textual_features_in[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 600)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 textual_features_pos[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 600)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 textual_features_neg[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 84,568,765\n",
      "Trainable params: 1,921,765\n",
      "Non-trainable params: 82,647,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 13.48, MarginLoss: 0.93, pos_cosine: 0.80, neg_cosine: 0.73\n",
      "Epoch: 2 Loss: 13.05, MarginLoss: 0.84, pos_cosine: 0.83, neg_cosine: 0.67\n",
      "Epoch: 3 Loss: 12.51, MarginLoss: 0.71, pos_cosine: 0.86, neg_cosine: 0.57\n",
      "Epoch: 4 Loss: 11.93, MarginLoss: 0.56, pos_cosine: 0.87, neg_cosine: 0.43\n",
      "Epoch: 5 Loss: 11.35, MarginLoss: 0.42, pos_cosine: 0.89, neg_cosine: 0.31\n",
      "Epoch: 6 Loss: 10.79, MarginLoss: 0.31, pos_cosine: 0.89, neg_cosine: 0.20\n",
      "Epoch: 7 Loss: 10.26, MarginLoss: 0.21, pos_cosine: 0.91, neg_cosine: 0.12\n",
      "Epoch: 8 Loss: 9.77, MarginLoss: 0.15, pos_cosine: 0.93, neg_cosine: 0.07\n",
      "Epoch: 9 Loss: 9.31, MarginLoss: 0.10, pos_cosine: 0.95, neg_cosine: 0.05\n",
      "Epoch: 10 Loss: 8.87, MarginLoss: 0.07, pos_cosine: 0.96, neg_cosine: 0.03\n",
      "Epoch: 11 Loss: 8.45, MarginLoss: 0.05, pos_cosine: 0.98, neg_cosine: 0.02\n",
      "Epoch: 12 Loss: 8.05, MarginLoss: 0.04, pos_cosine: 0.98, neg_cosine: 0.02\n",
      "Epoch: 13 Loss: 7.67, MarginLoss: 0.03, pos_cosine: 0.99, neg_cosine: 0.02\n",
      "Epoch: 14 Loss: 7.31, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 15 Loss: 6.96, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 16 Loss: 6.62, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 17 Loss: 6.29, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 18 Loss: 5.98, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 19 Loss: 5.68, MarginLoss: 0.02, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 20 Loss: 5.39, MarginLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 21 Loss: 5.11, MarginLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 22 Loss: 4.85, MarginLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.01\n",
      "Epoch: 23 Loss: 4.59, MarginLoss: 0.01, pos_cosine: 0.99, neg_cosine: 0.00\n",
      "Epoch: 24 Loss: 4.35, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 25 Loss: 4.12, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 26 Loss: 3.90, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 27 Loss: 3.68, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 28 Loss: 3.48, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 29 Loss: 3.29, MarginLoss: 0.01, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 30 Loss: 3.11, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 31 Loss: 2.93, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 32 Loss: 2.77, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 33 Loss: 2.61, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 34 Loss: 2.46, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 35 Loss: 2.32, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 36 Loss: 2.18, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 37 Loss: 2.05, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 38 Loss: 1.93, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 39 Loss: 1.82, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 40 Loss: 1.71, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 41 Loss: 1.60, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 42 Loss: 1.51, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 43 Loss: 1.41, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 44 Loss: 1.33, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 45 Loss: 1.24, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 46 Loss: 1.17, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 47 Loss: 1.09, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 48 Loss: 1.02, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 49 Loss: 0.96, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 50 Loss: 0.90, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 51 Loss: 0.84, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 52 Loss: 0.78, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 53 Loss: 0.73, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 54 Loss: 0.68, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 55 Loss: 0.64, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 56 Loss: 0.60, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 57 Loss: 0.56, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 58 Loss: 0.52, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 59 Loss: 0.48, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 60 Loss: 0.45, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 61 Loss: 0.42, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 62 Loss: 0.39, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 63 Loss: 0.36, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 64 Loss: 0.34, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 65 Loss: 0.31, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 66 Loss: 0.29, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 67 Loss: 0.27, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 68 Loss: 0.25, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 69 Loss: 0.23, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 70 Loss: 0.22, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 71 Loss: 0.20, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 72 Loss: 0.19, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 73 Loss: 0.17, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 74 Loss: 0.16, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 75 Loss: 0.15, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 76 Loss: 0.14, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 77 Loss: 0.13, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 78 Loss: 0.12, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 79 Loss: 0.11, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 80 Loss: 0.10, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 81 Loss: 0.09, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 82 Loss: 0.09, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 83 Loss: 0.08, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 84 Loss: 0.07, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 85 Loss: 0.07, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 86 Loss: 0.06, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 87 Loss: 0.06, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 88 Loss: 0.05, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 89 Loss: 0.05, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 90 Loss: 0.05, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 91 Loss: 0.04, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 92 Loss: 0.04, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 93 Loss: 0.04, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 94 Loss: 0.03, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 95 Loss: 0.03, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 96 Loss: 0.03, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 97 Loss: 0.03, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 98 Loss: 0.02, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 99 Loss: 0.02, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00\n",
      "Epoch: 100 Loss: 0.02, MarginLoss: 0.00, pos_cosine: 1.00, neg_cosine: 0.00, recall@25: 0.32\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(eclipse).h5' to disk\n",
      "Best_epoch=97, Best_loss=0.00, Recall@25=0.32\n",
      "CPU times: user 1min 40s, sys: 12.8 s, total: 1min 52s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "'''\n",
    "desc_feature_model = cnn_dilated_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _ = evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, MarginLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, MarginLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[3]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196609:241619|241619:0.38720422983169556,129538:0.2880561947822571,269792:0.2663504481315613,130357:0.2629655599594116,173392:0.22602033615112305,205810:0.20740270614624023,144811:0.18740785121917725,104033:0.1714646816253662,132905:0.1692110300064087,227934:0.16867035627365112,146707:0.16306132078170776,159458:0.15082502365112305,136922:0.1502733826637268,129239:0.14674526453018188,139830:0.1466323733329773,149425:0.14462929964065552,82748:0.13654518127441406,180804:0.13541895151138306,138743:0.13101840019226074,319714:0.11330252885818481,250560:0.11092448234558105,136420:0.10773563385009766,85779:0.10686999559402466,101189:0.10645955801010132,168988:0.10322725772857666',\n",
       " '35946:31941|31525:0.9709604922682047,34648:0.9593393839895725,35657:0.922690212726593,32045:0.9193853735923767,34612:0.9155542552471161,30073:0.914227768778801,40460:0.9136621206998825,34159:0.9128481894731522,34365:0.9124507457017899,28812:0.8901591822504997,34826:0.8886719197034836,34940:0.8872854188084602,38843:0.8864264413714409,24493:0.8846512883901596,28712:0.884565643966198,36386:0.8815409764647484,32204:0.8714475780725479,26563:0.8709514439105988,33828:0.8699119240045547,36698:0.868546336889267,30983:0.8684385269880295,29302:0.8680602461099625,30021:0.865355595946312,31941:0.8652142733335495,31931:0.8646900504827499',\n",
       " '38230:31941|34940:0.996724511962384,36386:0.9602057337760925,32204:0.9187770858407021,26563:0.9169151037931442,33828:0.9150406494736671,30983:0.9125224873423576,30021:0.9075166061520576,36698:0.8902658075094223,29302:0.8894851133227348,31941:0.8855833783745766,31931:0.8849112093448639,36308:0.8846535608172417,43910:0.8843423053622246,28132:0.8838643208146095,31267:0.8837378472089767,31525:0.8827365860342979,38012:0.8816311284899712,36822:0.8810924366116524,26395:0.8810036107897758,32537:0.8808070197701454,36137:0.8807268366217613,32992:0.880426324903965,35040:0.8797672390937805,34648:0.8790198042988777,38948:0.8788158744573593',\n",
       " '245769:279566|269182:0.8453547358512878,275486:0.8434513211250305,206747:0.6537902653217316,244920:0.653314620256424,277574:0.6521055400371552,212622:0.6520576775074005,237709:0.6517552733421326,179111:0.6279304027557373,266651:0.6277574300765991,341232:0.6275393068790436,174020:0.6273941993713379,184925:0.6267055571079254,235623:0.6251554191112518,199084:0.621198445558548,162987:0.6136925220489502,384928:0.6040903031826019,297248:0.603177011013031,353914:0.6020334959030151,138877:0.5982545912265778,127232:0.5982412993907928,111079:0.5978853702545166,166113:0.5978479087352753,134746:0.5976692736148834,151996:0.5976577997207642,132795:0.5976475477218628',\n",
       " '49163:44876|52916:1.0,89739:0.9923960557207465,69713:0.992067813873291,64051:0.9789584185928106,65870:0.9784272238612175,70398:0.9750697258859873,68981:0.9559322595596313,52868:0.8811863735318184,47620:0.8811645731329918,73226:0.8730290085077286,66836:0.8636642396450043,50989:0.8552663326263428,40322:0.8552592694759369,57743:0.8551666736602783,57871:0.8483961820602417,46976:0.8451475501060486,54968:0.8451428264379501,52619:0.845142662525177,69471:0.8451404720544815,68506:0.8451402932405472,70986:0.8451365679502487,48335:0.8450934886932373,41534:0.845084622502327,75134:0.8450830727815628,80221:0.8450208455324173',\n",
       " '32783:32777|35940:1.0,32777:0.9999037125526229,33070:0.996763538569212,33239:0.9962064230348915,31422:0.9960803342983127,38750:0.9916455810889602,36389:0.972384687513113,29012:0.9640301205217838,30002:0.9397578276693821,33237:0.9156490713357925,27190:0.9154924005270004,27978:0.882476381957531,27767:0.882153183221817,31700:0.8737252652645111,30450:0.8736697882413864,28955:0.8660861104726791,28417:0.8660809248685837,28056:0.8660782873630524,32147:0.8660275638103485,38829:0.8658135086297989,25892:0.8465331643819809,33111:0.8463937342166901,34142:0.8426084667444229,31885:0.8402388691902161,33634:0.836881697177887',\n",
       " '49608:47296|68129:0.9860303914174438,44685:0.9793870206922293,64277:0.977865805849433,67048:0.9735401012003422,62293:0.9726703893393278,42080:0.9719771165400743,68702:0.9716920610517263,42511:0.9704126231372356,51294:0.966569609940052,63841:0.9176529720425606,57746:0.877506710588932,68696:0.8758693709969521,47525:0.8752418383955956,62046:0.8699750006198883,46889:0.8674865514039993,70226:0.8586755096912384,59595:0.8557072579860687,57656:0.8530858308076859,59899:0.8467251658439636,46894:0.8452675938606262,49979:0.8433399647474289,60114:0.8354371637105942,70810:0.804538682103157,73695:0.804021418094635,60803:0.8016131967306137',\n",
       " '49170:47296|68129:0.9898584512993693,44685:0.9832238405942917,64277:0.9817069713026285,67048:0.977368388324976,62293:0.9765168912708759,42080:0.9758111536502838,68702:0.9755269233137369,42511:0.974256869405508,51294:0.9703799523413181,63841:0.9193485826253891,57746:0.877864308655262,68696:0.8765514492988586,47525:0.8750167042016983,62046:0.8707598000764847,46889:0.8685461580753326,70226:0.8588860332965851,59595:0.8564684391021729,57656:0.8530397266149521,59899:0.8472026884555817,46894:0.844914898276329,49979:0.843959391117096,60114:0.8355444967746735,70810:0.8046079874038696,73695:0.8042827248573303,60803:0.8022405207157135',\n",
       " '51886:5813|48069:1.0,49265:0.9992516485508531,67327:0.9991408897330984,42593:0.9991245829151012,45352:0.998867058660835,67963:0.9988549194531515,48433:0.998692671302706,41189:0.9986481019295752,65841:0.99859501759056,64749:0.9984969033394009,65317:0.9968697926960886,57895:0.9951402931474149,49527:0.9951090542599559,56009:0.9949883981607854,60385:0.9949113493785262,62612:0.9931424898095429,49686:0.9929480813443661,70410:0.9889747984707355,51170:0.9440351948142052,54943:0.9104784429073334,71872:0.9104529917240143,63366:0.9102394208312035,45095:0.9099541157484055,41849:0.8978147134184837,61095:0.8977928981184959',\n",
       " '219343:245782|209347:0.9977536865044385,229073:0.9964322247542441,220425:0.995969501323998,241099:0.9951978116296232,201681:0.9909271886572242,221852:0.9904053155332804,217752:0.9887031177058816,225644:0.9880092395469546,219229:0.8312732577323914,233095:0.6884030401706696,238802:0.6871392130851746,245565:0.6412109732627869,249310:0.6025052964687347,264119:0.6024793982505798,266183:0.602053165435791,276326:0.6010542809963226,269001:0.6010328829288483,256910:0.5993284285068512,263108:0.5966660678386688,268616:0.5886664390563965,179125:0.5704162418842316,171897:0.5704140067100525,192863:0.5703921318054199,185582:0.5703628361225128,180791:0.5702210962772369',\n",
       " '147479:128424|128424:0.7936154752969742,74841:0.7063290476799011,140783:0.5559087693691254,129228:0.5334466099739075,111373:0.5236954391002655,147859:0.5087078809738159,130144:0.4733338952064514,142865:0.46653109788894653,165526:0.454164981842041,135132:0.4383684992790222,76694:0.43539124727249146,74926:0.43508195877075195,95586:0.4340254068374634,64786:0.41976892948150635,123124:0.4172346591949463,112781:0.4093787670135498,137016:0.40319448709487915,59905:0.4020459055900574,99215:0.3979838490486145,135343:0.3852013945579529,135320:0.38518327474594116,307609:0.3822590112686157,162407:0.377801775932312,157089:0.37511080503463745,281877:0.37375038862228394',\n",
       " '245784:214092|170890:0.6394567787647247,92250:0.5768787562847137,311863:0.5681103765964508,162097:0.5563483536243439,236724:0.5549575984477997,207827:0.5536270141601562,44554:0.5525982677936554,90255:0.5490102171897888,207927:0.5449618101119995,240033:0.5447965264320374,241490:0.5441433787345886,236513:0.544028103351593,211075:0.5434033572673798,262604:0.5416120290756226,89428:0.5401561558246613,133912:0.5400410294532776,82146:0.5396294593811035,320329:0.530935138463974,224997:0.5174894332885742,97906:0.5174882709980011,88824:0.5174875557422638,91291:0.5174801647663116,80718:0.5174792408943176,90724:0.5174695253372192,87604:0.5174538791179657',\n",
       " '238557:214092|224997:0.8403407633304596,243906:0.7120509743690491,207927:0.6429019272327423,240033:0.6428888142108917,228695:0.6428318917751312,230854:0.6426784098148346,234649:0.6426214575767517,239109:0.642436146736145,241490:0.6423782408237457,236513:0.6422665417194366,220057:0.6422268152236938,277438:0.6404180824756622,215252:0.6358975470066071,229377:0.6338725388050079,236524:0.6337226927280426,236724:0.6104633510112762,207827:0.6090587079524994,205731:0.5787256360054016,205736:0.5759756565093994,220646:0.5589892268180847,231951:0.5589758157730103,283503:0.5513573288917542,269010:0.5513367652893066,262604:0.5461178123950958,276887:0.5431296527385712',\n",
       " '229882:214092|220646:0.41192626953125,231951:0.41189539432525635,226832:0.4091993570327759,320329:0.36963993310928345,296045:0.3672940135002136,277618:0.3574255704879761,224997:0.35361331701278687,214092:0.35341185331344604,311863:0.34808480739593506,393277:0.3337429165840149,297769:0.32311004400253296,248010:0.31459081172943115,224423:0.29871875047683716,202316:0.28399670124053955,200031:0.28388357162475586,245441:0.28304725885391235,278402:0.26988857984542847,210688:0.26025086641311646,250416:0.2595687508583069,221583:0.2580745816230774,250505:0.25111281871795654,272953:0.2509695887565613,221608:0.24692058563232422,236512:0.23754912614822388,303372:0.23012304306030273',\n",
       " '281586:214092|205731:0.7451000511646271,278402:0.717389315366745,213194:0.675862580537796,269010:0.6733855903148651,283503:0.6732229888439178,262970:0.6550544202327728,252232:0.6550530195236206,277314:0.6550490260124207,255168:0.6550486087799072,260585:0.6550170481204987,276887:0.6549698412418365,251460:0.654891163110733,259181:0.6548515856266022,294986:0.6547225117683411,274387:0.6544930636882782,279103:0.6543833613395691,277195:0.6532590687274933,255000:0.6523515284061432,290395:0.6496891975402832,277356:0.6428110003471375,262985:0.6405141353607178,275589:0.6400422751903534,258877:0.638702005147934,28766:0.6283111572265625,65796:0.624455600976944',\n",
       " '242965:214092|210681:0.8346992284059525,250211:0.8346083462238312,239130:0.828243687748909,238090:0.8282177895307541,245435:0.8274457007646561,251504:0.8268761783838272,207890:0.8266944140195847,222900:0.8264720886945724,264112:0.8254574537277222,229480:0.824392095208168,243692:0.8239148259162903,224458:0.8208489418029785,205235:0.8207786530256271,247666:0.820574089884758,238380:0.8159739524126053,239607:0.8036820590496063,238484:0.7817420065402985,223878:0.7776539921760559,220928:0.7099963426589966,217233:0.6344352960586548,221723:0.5661540329456329,186547:0.5532211661338806,277643:0.5397434234619141,287833:0.5350186824798584,268837:0.5346580147743225',\n",
       " '76737:71268|52964:0.8025848865509033,70629:0.8025848120450974,75408:0.8025245070457458,54072:0.8024794608354568,45328:0.8021568655967712,74217:0.8004038333892822,54885:0.8003822565078735,71268:0.7997068017721176,73475:0.7895759344100952,76272:0.7597316652536392,62281:0.7555690854787827,51809:0.7555550634860992,73077:0.7554553300142288,73528:0.7551529258489609,45469:0.7549334466457367,128074:0.7546050548553467,67395:0.7539062798023224,52474:0.7485445141792297,50693:0.7467950582504272,45068:0.7467946410179138,74612:0.746793806552887,73992:0.7467936277389526,44759:0.7467839419841766,77737:0.7467585206031799,50515:0.7465943396091461',\n",
       " '196634:71268|67562:0.9888641750440001,76272:0.7471836805343628,128074:0.7378380298614502,163444:0.7372244894504547,135378:0.7348704636096954,89178:0.7300501465797424,111079:0.7298499345779419,166113:0.729821115732193,102270:0.7297453880310059,134746:0.7296724617481232,151996:0.729660153388977,132795:0.7296517491340637,140561:0.7294954061508179,138877:0.7290489673614502,127232:0.7290132343769073,151808:0.7155469357967377,153796:0.7155449688434601,82850:0.7121120691299438,122737:0.7103434205055237,95679:0.706828385591507,83937:0.70525723695755,45328:0.7000317871570587,54072:0.7000138163566589,75408:0.6999898850917816,52964:0.699825793504715',\n",
       " '200017:196635|199733:0.9506455138325691,171988:0.9480808340013027,185214:0.9420025385916233,154851:0.9331875145435333,196140:0.8328182250261307,188485:0.8327730298042297,195834:0.832643523812294,184829:0.8228853046894073,196635:0.8185316473245621,184502:0.818065345287323,150009:0.8153617084026337,192638:0.8116700947284698,185102:0.810362696647644,201878:0.7393700182437897,39064:0.701934278011322,157798:0.7018878757953644,119849:0.7001829445362091,154238:0.700114905834198,163543:0.6997962594032288,125455:0.6993559002876282,181788:0.6985360980033875,111573:0.6977309882640839,131969:0.6974729895591736,185457:0.6972447335720062,140392:0.6961474716663361',\n",
       " '229405:226052|239492:0.9984152974793687,267853:0.9935953561216593,265973:0.9935224195942283,267282:0.9917576052248478,261104:0.9911574982106686,251400:0.9818102531135082,235008:0.9768036250025034,275274:0.9767885599285364,262065:0.9759487472474575,324200:0.9592255204916,262656:0.9104433059692383,226748:0.9104386493563652,317885:0.9104194566607475,235654:0.9103065356612206,317785:0.9074406251311302,273776:0.8910228163003922,236358:0.8909605592489243,320494:0.8905273377895355,258117:0.8904142156243324,271993:0.8379004448652267,243375:0.8097998350858688,224472:0.7889002859592438,342369:0.7863091379404068,285438:0.7856078445911407,289777:0.7761198580265045']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recall@25 last epoch:', 0.23)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Between 0-10 epochs recall@25 = 0.28\n",
    "    Between 0-20 epochs recall@25 = 0.32\n",
    "    Between 0-70 epochs recall@25 = ?\n",
    "    Between 0-100 epochs recall@25 = 0.43\n",
    "'''\n",
    "recall, exported_rank = evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "\n",
    "\"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loss=h.history['loss']\n",
    "# val_loss=h.history['val_loss']\n",
    "\n",
    "# plt.plot(loss, label='loss')\n",
    "# plt.plot(val_loss, label='val_loss')\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 7253\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_feature_100epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          6105100     title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          6249492     desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,859,492\n",
      "Trainable params: 12,859,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank = evaluate_validation_test(0, model, retrieval.test, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(path, 'exported_rank_propose.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.33,\n",
       " '2 - recall_at_10': 0.39,\n",
       " '3 - recall_at_15': 0.43,\n",
       " '4 - recall_at_20': 0.47,\n",
       " '5 - recall_at_25': 0.5}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = Evaluation()\n",
    "report = evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
