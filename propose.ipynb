{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - PROPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 50 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 50 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'propose'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'propose_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'propose_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075c5886a4904f29aecead4308471437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83503), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38255c82b68439eb554556dbd613117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98070"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac2fab1ca754458a59cb0a39d47a83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad22899412f427eb81b092afee4abc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 11.6 s, sys: 1.36 s, total: 12.9 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba697bdd62445ca934c8225e81668f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83503), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n"
     ]
    }
   ],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets)\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '129\\n',\n",
       " 'creation_ts': '2003-09-29 16:09:00 +0000',\n",
       " 'delta_ts': '2005-08-10 14:56:04 +0000',\n",
       " 'description': 'source press category ui look product requirement a lot of extra modules listed in the startup tree navigation are rarely used organization would prefer to have instead of law list organization org writer for example and infrequently used items not listed there for user to scroll over every time they start ooo such as organization also reported as confusing what is a master document few people use master docs customer need problem startup tree navigation confusing comment cf ease of use ease of use for changes in ooo eng owner person prove product concept ooo will provide the user experience of being separate applications the start menu will reflect this in calling the items organization org writer organization org calc organization org impress organization org draw etc the options dialog and the file and window menu reflact this change too functional specification separate options http specs openoffice org appwide separate modules organization sxw',\n",
       " 'description_word': array([  70,  419, 1370, 1602,  454,   13, 2629,    4,  864,   11, 1029,\n",
       "         683, 1253,    6,    2, 1185,  885, 2056,   46, 6851,  173,    3,\n",
       "         133, 2977,    5,   49,  252,   11,  153,  147,    3,   22,   81,\n",
       "          18,  214,    9,    1,  173,  800,   16, 1253,   75,   18,  120,\n",
       "           5,  829,  472,  519,  129,  217]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 20337,\n",
       " 'priority': '1\\n',\n",
       " 'product': '33\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'title': 'q pcd ease of use separate ooo modules',\n",
       " 'title_word': array([ 715, 4201, 3137,   11,   87, 1336,   32,  683,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]),\n",
       " 'version': '215\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 13904)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.9 ms, sys: 0 ns, total: 52.9 ms\n",
      "Wall time: 52.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 50), (128, 50), (128, 738), (128,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: dragging text box is hard impossible\n",
      "***Title***: can not drag text box\n",
      "***Description***: start new blanc presentation click text icon on drawing tool bar click on slide type type type escape text box is selected has selection blocks position mouse pointer so that it becomes a cross try to drag fails save the presentation open in x dragging no problem\n",
      "***Description***: on open su se click on a text box organization and handles show correctly icon changes to gripping hand correctly on organization over click and drag does not work organization and shape handles work as expected\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: improve find replace to handle leading spaces in regular expressions\n",
      "***Title***: function sumif not the same response that organization\n",
      "***Description***: star calc allows using find replace to search for without the quotes as a regular expression to find leading spaces in a paragraph this ability is not present in o o calc and the help for regular expressions states that special contents e g blank spaces and character anchored frames at the beginning of a paragraph will be ignored it would be great for find replace to allow for this kind of search\n",
      "***Description***: if i put cells alfanumeric that work with the function organization if the number is product withn alfabetic its work but if the number is a floatpoint number with alfabetic the fuction no work in startoffice work fine\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: after organization organization are huge\n",
      "***Title***: header footer is lost exporting msword doc rtf\n",
      "***Description***: i installed oo org as part of red hat linux shrike upon attempting to either open a file or open a blank document with writer the application is unusable due to the fact that it seems to have been formatted to use a point font i don t have enough space on the screen to do anything to fix this problem\n",
      "***Description***: header footer is lost exporting msword doc rtf\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: contents of text field variable not displayed in footer\n",
      "***Title***: organization open terminate with no such element exception\n",
      "***Description***: setting a text variable in the contents part of a document and wanting to show it in the footer of the document after saving closing and then reopening the document the value of the variable is not shown anymore instead it shows\n",
      "***Description***: organization open terminate with no such element exception\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 51.1 ms, sys: 0 ns, total: 51.1 ms\n",
      "Wall time: 50.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 131563'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "    f = open(embed_path, 'rb')\n",
    "    f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f.readline().split())\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading FastText\")\n",
    "    for line in loop:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embed = list(map(float, tokens[1:]))\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89f47be0e14421491f527afc622d10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1999995 word vectors in FastText 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1f63bf6aa94fe5b65c7c376748f997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=131563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 9s, sys: 4.71 s, total: 2min 13s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def arcii_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    filters_1d=max_sequence_length\n",
    "    kernel_size_1d=3\n",
    "    num_conv2d_layers=2\n",
    "    filters_2d=[32,16]\n",
    "    kernel_size_2d=[[3,3], [3,3]]\n",
    "    mpool_size_2d=[[2,2], [2,2]]\n",
    "    dropout_rate=0.5\n",
    "    \n",
    "    layer1_conv=Conv1D(filters=filters_1d, kernel_size=kernel_size_1d, padding='same')(embedded_sequences)\n",
    "    layer1_activation=Activation('tanh')(layer1_conv)\n",
    "    layer1_reshaped=Reshape((max_sequence_length, max_sequence_length, -1))(layer1_activation)\n",
    "    z=MaxPooling2D(pool_size=(2,2))(layer1_reshaped)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z=Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same')(z)\n",
    "        z=Activation('tanh')(z)\n",
    "        z=MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "\n",
    "    pool1_flat=Flatten()(z)\n",
    "    pool1_flat_drop=Dropout(rate=dropout_rate)(pool1_flat)\n",
    "    pool1_norm=BatchNormalization()(pool1_flat_drop)\n",
    "    mlp1=Dense(300)(pool1_norm)\n",
    "    output=Activation('tanh')(mlp1)\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[output], name = 'FeatureARCIIGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "import math\n",
    "\n",
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False)(block_input) #kernel_regularizer=l2(l2_layer_reg)                    \n",
    "        \n",
    "        activation_out = Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out) # use_bias=False, kernel_constraint=max_norm(1.)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False)(activation_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, title_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_CNND')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 128\n",
    "    number_of_layers = 6\n",
    "    \n",
    "    title_input = title_layer.input\n",
    "    title_layer = title_layer.output\n",
    "\n",
    "    # Embedding layer with CNN dilated\n",
    "    #la, lb = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)\n",
    "    la = embedded_sequences\n",
    "    la_title = title_layer\n",
    "    attention_layes, attention_title_layes = [], []\n",
    "    filters_size = [3, 4, 5]\n",
    "    number_of_filters = len(filters_size)\n",
    "    for index in range(1, number_of_layers + 1):\n",
    "        # Desc\n",
    "        la, lb = DC_CNN_Block(units, 5, int(math.pow(2, index)), 0.01)(la)\n",
    "        # Title \n",
    "        la_title, lb_title = DC_CNN_Block(units, 3, int(math.pow(2, index)), 0.01)(la_title)\n",
    "        lb = Add()([lb_title, lb])\n",
    "        #la = Dropout(.90)(la)\n",
    "        #lb = Dropout(.90)(lb)\n",
    "        attention_layes.append(lb)\n",
    "        attention_title_layes.append(lb_title)\n",
    "\n",
    "    attention_layer = Add()(attention_layes)\n",
    "    attention_title_layes = Add()(attention_title_layes)\n",
    "    attention_layer =   Add()([attention_layer, attention_title_layes])\n",
    "    \n",
    "    #layer = Add()([attention_layer, l9])\n",
    "    \n",
    "    layer =   Activation('tanh')(attention_layer)\n",
    "\n",
    "    #layer =  Conv1D(1,1, activation='linear', use_bias=False)(layer)\n",
    "    \n",
    "    #layer = Flatten()(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dropout(0.50)(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(150, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    cnn_dilated_feature_model = Model(inputs=[sequence_input, title_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return cnn_dilated_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput_CNN')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 32\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(embedded_sequences)\n",
    "        l_pool = AveragePooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=n_filters * 3, kernel_size=5)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(l_merge)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 100\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_LSTM')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    lstm_layer = LSTM(number_lstm_units, return_sequences=True, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(embedded_sequences)\n",
    "    \n",
    "    #lstm_layer = lstm_layer(embedded_sequences)\n",
    "    #lstm_layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    #lstm_layer = Dense(300, activation='tanh')(lstm_layer)\n",
    "    #lstm_layer = GRU(100, activation='tanh', return_sequences=False)(lstm_layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[lstm_layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, Permute, Dot\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput_bilstm')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "    lstm_layer = Bidirectional(GRU(number_lstm_units, activation='tanh', \n",
    "                                   return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "                               merge_mode='ave')\n",
    "\n",
    "#     lstm_layer = LSTM(number_lstm_units, return_sequences=True, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(embedded_sequences)\n",
    "    \n",
    "    # Attention layer to title\n",
    "    #title_input = title_layer.input\n",
    "    #title_layer = title_layer.output\n",
    "    #shape_lstm = K.int_shape(lstm_layer)\n",
    "    #lstm_layer = Permute((2, 1), input_shape=shape_lstm)(lstm_layer)\n",
    "    #shape_lstm = K.int_shape(title_layer)\n",
    "    #title_layer = Permute((2, 1), input_shape=shape_lstm)(title_layer)\n",
    "    #layer = Dot(axes=1)([lstm_layer, title_layer])\n",
    "    \n",
    "#     layer = LSTM(number_lstm_units, return_sequences=False, kernel_initializer='random_uniform',\n",
    "#                 bias_initializer='zeros')(layer)\n",
    "\n",
    "    layer = lstm_layer(embedded_sequences)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    #layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = GRU(100, activation='tanh', return_sequences=False)(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureBiLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Average, Dot, Maximum, Permute\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size_cols = K.int_shape(block_input)[1]\n",
    "        shape_size_rows = 1\n",
    "        \n",
    "        residual =  block_input\n",
    "        residual = Activation('relu')(residual)\n",
    "        #residual = BatchNormalization()(residual)\n",
    "        \n",
    "        layer_out = Reshape((shape_size_cols, shape_size_rows))(block_input)\n",
    "        layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = GRU(100, activation='relu', return_sequences=True)(layer_out)\n",
    "        #layer_out = Reshape((shape_size_cols, ))(layer_out)\n",
    "        layer_out = GlobalAveragePooling1D()(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(50, activation='relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        layer_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        skip_out = Dense(shape_size_cols, activation='relu', use_bias=True, kernel_initializer='random_uniform')(layer_out)\n",
    "        #layer_out = Activation('relu')(layer_out)\n",
    "        #layer_out = BatchNormalization()(layer_out)\n",
    "        \n",
    "        block_out = Add()([residual, layer_out])\n",
    "        #block_out = Activation('relu')(block_out)\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    #bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_d_feat = desc_feature_model([bug_d, bug_t])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    \n",
    "#     encoded_t_1a, encoded_t_1b  = residual_bug()(bug_t_feat)\n",
    "#     encoded_d_1a, encoded_d_1b  = residual_bug()(bug_d_feat)\n",
    "#     bug_t_feat = encoded_t_1a\n",
    "#     bug_d_feat = encoded_d_1a\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #bug_feature_output, bug_feature_output_1b = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output_1a = Dropout(.5)(bug_feature_output_1a)\n",
    "    #bug_feature_output, bug_feature_output_2b = residual_bug()(bug_feature_output_1a)\n",
    "    \n",
    "    #bug_feature_output = Add()([bug_feature_output_1b, bug_feature_output_2b])\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.75)(bug_feature_output)\n",
    "#     shape_size = K.int_shape(bug_feature_output)[1]\n",
    "#     bug_feature_output = Dense(shape_size, activation='linear', use_bias=False)(bug_feature_output)\n",
    "#     bug_feature_output = Dropout(.33)(bug_feature_output)\n",
    "#     bug_feature_output = Dense(100)(bug_feature_output)\n",
    "    \n",
    "    #bug_feature_output  = residual_bug()(bug_feature_output)\n",
    "    #bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Distance\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, metrics=[pos_distance, neg_distance, custom_margin_loss])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBiLstmGenerationModel (M (None, 50, 50)       39574200    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[1][0\n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 1)            80313116    desc_in[0][0]                    \n",
      "                                                                 title_in[0][0]                   \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 desc_neg[0][0]                   \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[2][0\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[3][0\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 351)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 351)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 351)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 80,534,816\n",
      "Trainable params: 1,597,016\n",
      "Non-trainable params: 78,937,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss: 0.94, CustomLoss: 0.94, pos_cosine: 0.89, neg_cosine: 0.83\n",
      "Epoch: 2 Loss: 0.91, CustomLoss: 0.91, pos_cosine: 0.89, neg_cosine: 0.80\n",
      "Epoch: 3 Loss: 0.93, CustomLoss: 0.93, pos_cosine: 0.86, neg_cosine: 0.78\n",
      "Epoch: 4 Loss: 0.89, CustomLoss: 0.89, pos_cosine: 0.84, neg_cosine: 0.73\n",
      "Epoch: 5 Loss: 0.88, CustomLoss: 0.88, pos_cosine: 0.82, neg_cosine: 0.70\n",
      "Epoch: 6 Loss: 0.87, CustomLoss: 0.87, pos_cosine: 0.81, neg_cosine: 0.69\n",
      "Epoch: 7 Loss: 0.88, CustomLoss: 0.88, pos_cosine: 0.78, neg_cosine: 0.67\n",
      "Epoch: 8 Loss: 0.86, CustomLoss: 0.86, pos_cosine: 0.82, neg_cosine: 0.68\n",
      "Epoch: 9 Loss: 0.82, CustomLoss: 0.82, pos_cosine: 0.83, neg_cosine: 0.65\n",
      "Epoch: 10 Loss: 0.81, CustomLoss: 0.81, pos_cosine: 0.81, neg_cosine: 0.62\n",
      "Epoch: 11 Loss: 0.82, CustomLoss: 0.82, pos_cosine: 0.80, neg_cosine: 0.63\n",
      "Epoch: 12 Loss: 0.83, CustomLoss: 0.83, pos_cosine: 0.79, neg_cosine: 0.62\n",
      "Epoch: 13 Loss: 0.81, CustomLoss: 0.81, pos_cosine: 0.79, neg_cosine: 0.61\n",
      "Epoch: 14 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.81, neg_cosine: 0.59\n",
      "Epoch: 15 Loss: 0.81, CustomLoss: 0.81, pos_cosine: 0.78, neg_cosine: 0.59\n",
      "Epoch: 16 Loss: 0.78, CustomLoss: 0.78, pos_cosine: 0.79, neg_cosine: 0.56\n",
      "Epoch: 17 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.76, neg_cosine: 0.54\n",
      "Epoch: 18 Loss: 0.79, CustomLoss: 0.79, pos_cosine: 0.76, neg_cosine: 0.55\n",
      "Epoch: 19 Loss: 0.78, CustomLoss: 0.78, pos_cosine: 0.78, neg_cosine: 0.56\n",
      "Epoch: 20 Loss: 0.82, CustomLoss: 0.82, pos_cosine: 0.74, neg_cosine: 0.56\n",
      "Epoch: 21 Loss: 0.76, CustomLoss: 0.76, pos_cosine: 0.78, neg_cosine: 0.54\n",
      "Epoch: 22 Loss: 0.78, CustomLoss: 0.78, pos_cosine: 0.76, neg_cosine: 0.55\n",
      "Epoch: 23 Loss: 0.81, CustomLoss: 0.81, pos_cosine: 0.71, neg_cosine: 0.51\n",
      "Epoch: 24 Loss: 0.71, CustomLoss: 0.71, pos_cosine: 0.81, neg_cosine: 0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.77, neg_cosine: 0.51\n",
      "Epoch: 26 Loss: 0.75, CustomLoss: 0.75, pos_cosine: 0.77, neg_cosine: 0.52\n",
      "Epoch: 27 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.76, neg_cosine: 0.52\n",
      "Epoch: 28 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.77, neg_cosine: 0.55\n",
      "Epoch: 29 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.76, neg_cosine: 0.53\n",
      "Epoch: 30 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.77, neg_cosine: 0.50\n",
      "Epoch: 31 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.76, neg_cosine: 0.53\n",
      "Epoch: 32 Loss: 0.74, CustomLoss: 0.74, pos_cosine: 0.77, neg_cosine: 0.51\n",
      "Epoch: 33 Loss: 0.77, CustomLoss: 0.77, pos_cosine: 0.77, neg_cosine: 0.54\n",
      "Epoch: 34 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.81, neg_cosine: 0.53\n",
      "Epoch: 35 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.77, neg_cosine: 0.47\n",
      "Epoch: 36 Loss: 0.74, CustomLoss: 0.74, pos_cosine: 0.77, neg_cosine: 0.51\n",
      "Epoch: 37 Loss: 0.76, CustomLoss: 0.76, pos_cosine: 0.78, neg_cosine: 0.54\n",
      "Epoch: 38 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.78, neg_cosine: 0.47\n",
      "Epoch: 39 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.79, neg_cosine: 0.52\n",
      "Epoch: 40 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.83, neg_cosine: 0.51\n",
      "Epoch: 41 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.78, neg_cosine: 0.50\n",
      "Epoch: 42 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.82, neg_cosine: 0.48\n",
      "Epoch: 43 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.77, neg_cosine: 0.49\n",
      "Epoch: 44 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.77, neg_cosine: 0.49\n",
      "Epoch: 45 Loss: 0.74, CustomLoss: 0.74, pos_cosine: 0.78, neg_cosine: 0.52\n",
      "Epoch: 46 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.76, neg_cosine: 0.48\n",
      "Epoch: 47 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.79, neg_cosine: 0.51\n",
      "Epoch: 48 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.82, neg_cosine: 0.46\n",
      "Epoch: 49 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.77, neg_cosine: 0.49\n",
      "Epoch: 50 Loss: 0.75, CustomLoss: 0.75, pos_cosine: 0.76, neg_cosine: 0.50\n",
      "Epoch: 51 Loss: 0.74, CustomLoss: 0.74, pos_cosine: 0.76, neg_cosine: 0.50\n",
      "Epoch: 52 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.81, neg_cosine: 0.48\n",
      "Epoch: 53 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.80, neg_cosine: 0.49\n",
      "Epoch: 54 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.81, neg_cosine: 0.48\n",
      "Epoch: 55 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.80, neg_cosine: 0.48\n",
      "Epoch: 56 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.80, neg_cosine: 0.49\n",
      "Epoch: 57 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.81, neg_cosine: 0.47\n",
      "Epoch: 58 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.79, neg_cosine: 0.52\n",
      "Epoch: 59 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.84, neg_cosine: 0.51\n",
      "Epoch: 60 Loss: 0.71, CustomLoss: 0.71, pos_cosine: 0.81, neg_cosine: 0.52\n",
      "Epoch: 61 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.83, neg_cosine: 0.51\n",
      "Epoch: 62 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.86, neg_cosine: 0.52\n",
      "Epoch: 63 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.79, neg_cosine: 0.51\n",
      "Epoch: 64 Loss: 0.68, CustomLoss: 0.68, pos_cosine: 0.83, neg_cosine: 0.50\n",
      "Epoch: 65 Loss: 0.72, CustomLoss: 0.72, pos_cosine: 0.75, neg_cosine: 0.47\n",
      "Epoch: 66 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.81, neg_cosine: 0.51\n",
      "Epoch: 67 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.83, neg_cosine: 0.46\n",
      "Epoch: 68 Loss: 0.73, CustomLoss: 0.73, pos_cosine: 0.77, neg_cosine: 0.50\n",
      "Epoch: 69 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.83, neg_cosine: 0.50\n",
      "Epoch: 70 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.83, neg_cosine: 0.52\n",
      "Epoch: 71 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.81, neg_cosine: 0.48\n",
      "Epoch: 72 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.84, neg_cosine: 0.54\n",
      "Epoch: 73 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.82, neg_cosine: 0.52\n",
      "Epoch: 74 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.81, neg_cosine: 0.50\n",
      "Epoch: 75 Loss: 0.60, CustomLoss: 0.60, pos_cosine: 0.84, neg_cosine: 0.44\n",
      "Epoch: 76 Loss: 0.70, CustomLoss: 0.70, pos_cosine: 0.84, neg_cosine: 0.54\n",
      "Epoch: 77 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.86, neg_cosine: 0.48\n",
      "Epoch: 78 Loss: 0.62, CustomLoss: 0.62, pos_cosine: 0.88, neg_cosine: 0.50\n",
      "Epoch: 79 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.83, neg_cosine: 0.48\n",
      "Epoch: 80 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.82, neg_cosine: 0.50\n",
      "Epoch: 81 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.83, neg_cosine: 0.48\n",
      "Epoch: 82 Loss: 0.71, CustomLoss: 0.71, pos_cosine: 0.84, neg_cosine: 0.55\n",
      "Epoch: 83 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.86, neg_cosine: 0.49\n",
      "Epoch: 84 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.87, neg_cosine: 0.47\n",
      "Epoch: 85 Loss: 0.67, CustomLoss: 0.67, pos_cosine: 0.82, neg_cosine: 0.49\n",
      "Epoch: 86 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.84, neg_cosine: 0.47\n",
      "Epoch: 87 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.81, neg_cosine: 0.46\n",
      "Epoch: 88 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.89, neg_cosine: 0.52\n",
      "Epoch: 89 Loss: 0.69, CustomLoss: 0.69, pos_cosine: 0.84, neg_cosine: 0.52\n",
      "Epoch: 90 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.83, neg_cosine: 0.48\n",
      "Epoch: 91 Loss: 0.64, CustomLoss: 0.64, pos_cosine: 0.85, neg_cosine: 0.48\n",
      "Epoch: 92 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.88, neg_cosine: 0.53\n",
      "Epoch: 93 Loss: 0.65, CustomLoss: 0.65, pos_cosine: 0.85, neg_cosine: 0.50\n",
      "Epoch: 94 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.79, neg_cosine: 0.45\n",
      "Epoch: 95 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.83, neg_cosine: 0.49\n",
      "Epoch: 96 Loss: 0.66, CustomLoss: 0.66, pos_cosine: 0.84, neg_cosine: 0.51\n",
      "Epoch: 97 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.85, neg_cosine: 0.48\n",
      "Epoch: 98 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.88, neg_cosine: 0.48\n",
      "Epoch: 99 Loss: 0.63, CustomLoss: 0.63, pos_cosine: 0.83, neg_cosine: 0.46\n",
      "Epoch: 100 Loss: 0.61, CustomLoss: 0.61, pos_cosine: 0.89, neg_cosine: 0.50, recall@25: 0.29\n",
      "Saved model 'modelos/model_propose_feature_100epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=75, Best_loss=0.60, Recall@25=0.29\n",
      "CPU times: user 2min 36s, sys: 18.7 s, total: 2min 55s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = cnn_dilated_model(desc_embedding_layer, title_feature_model, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _ = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, CustomLoss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],  h[3],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[3]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16384:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '14604:15363|21034:0.4327089786529541,23220:0.42771345376968384,28625:0.4262864589691162,7213:0.393585205078125,19906:0.39244920015335083,20269:0.3888004422187805,5362:0.387737512588501,20119:0.3873816728591919,20485:0.3860454559326172,5091:0.38451075553894043,5149:0.38380032777786255,20883:0.3831349015235901,20352:0.38112008571624756,22235:0.3797825574874878,6263:0.3797810673713684,6431:0.37362515926361084,20193:0.36901843547821045,20876:0.36868250370025635,20125:0.3685894012451172,20477:0.3672598600387573,20880:0.3659793734550476,20055:0.3658294677734375,14729:0.3537505269050598,54188:0.3499295115470886,14775:0.34724265336990356',\n",
       " '16388:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '16389:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '14054:15363|14156:0.6277346611022949,4397:0.5781526267528534,23251:0.5749085545539856,78756:0.5735899806022644,4700:0.5711423754692078,4477:0.568684309720993,13015:0.5284098386764526,9278:0.5283169746398926,8698:0.5238481163978577,12207:0.5135535299777985,19911:0.48772144317626953,15006:0.47255563735961914,33449:0.466172993183136,15220:0.46516841650009155,2574:0.4523268938064575,14647:0.44833534955978394,13824:0.44397950172424316,5011:0.44102662801742554,31735:0.4409145712852478,3930:0.4366651773452759,16805:0.4223385453224182,12788:0.42018353939056396,16120:0.41779035329818726,32197:0.3962436318397522,3690:0.3813501000404358',\n",
       " '16385:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '16386:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '39820:32772|49301:0.6753091812133789,35959:0.663448691368103,51258:0.6449267864227295,15666:0.639664351940155,34343:0.6364322304725647,56806:0.6241319179534912,73844:0.6065719425678253,71542:0.5944837331771851,47372:0.5931834578514099,40717:0.5920317471027374,45952:0.5913784205913544,40550:0.5907365679740906,53885:0.5889813899993896,44924:0.5879084467887878,50135:0.5828438997268677,82544:0.5795561671257019,43215:0.5782193839550018,1596:0.5731866657733917,48748:0.5676251351833344,1761:0.566645473241806,91552:0.5647207796573639,38534:0.5639299750328064,46683:0.5635560154914856,25072:0.5567716062068939,46062:0.5554867386817932',\n",
       " '14604:15363|21034:0.4327089786529541,23220:0.42771345376968384,28625:0.4262864589691162,7213:0.393585205078125,19906:0.39244920015335083,20269:0.3888004422187805,5362:0.387737512588501,20119:0.3873816728591919,20485:0.3860454559326172,5091:0.38451075553894043,5149:0.38380032777786255,20883:0.3831349015235901,20352:0.38112008571624756,22235:0.3797825574874878,6263:0.3797810673713684,6431:0.37362515926361084,20193:0.36901843547821045,20876:0.36868250370025635,20125:0.3685894012451172,20477:0.3672598600387573,20880:0.3659793734550476,20055:0.3658294677734375,14729:0.3537505269050598,54188:0.3499295115470886,14775:0.34724265336990356',\n",
       " '16390:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '12274:31697|11371:0.8269044905900955,11040:0.8235284686088562,12228:0.8181557953357697,11493:0.7753467410802841,4568:0.7543493211269379,33307:0.7480400204658508,4508:0.7446393072605133,11491:0.7408623099327087,13592:0.7331099808216095,4502:0.7286813259124756,23728:0.728012353181839,73173:0.7278756499290466,3839:0.725584864616394,25676:0.7249465882778168,14822:0.7240451872348785,7616:0.722457081079483,21359:0.7220234870910645,2109:0.7211125791072845,11591:0.7210868299007416,4446:0.7202799320220947,19323:0.7199566960334778,31480:0.719692200422287,7530:0.7192811965942383,6879:0.718295156955719,7280:0.7165323495864868',\n",
       " '12275:31697|11371:0.8324878960847855,11040:0.8285647630691528,12228:0.8225192725658417,11493:0.7757977396249771,4568:0.7567880749702454,33307:0.7534835189580917,4508:0.7495380640029907,11491:0.7455698847770691,13592:0.7365677654743195,73173:0.7316390872001648,4502:0.7312722206115723,23728:0.7307140529155731,25676:0.7289499938488007,3839:0.7288922667503357,14822:0.7238108217716217,2109:0.7237518131732941,25680:0.723609447479248,6879:0.7230148315429688,7616:0.7227514982223511,31480:0.7225036025047302,4446:0.722093254327774,19323:0.7218759059906006,7530:0.7218278050422668,21359:0.7215016782283783,11591:0.721364289522171',\n",
       " '16393:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '14054:15363|14156:0.6277346611022949,4397:0.5781526267528534,23251:0.5749085545539856,78756:0.5735899806022644,4700:0.5711423754692078,4477:0.568684309720993,13015:0.5284098386764526,9278:0.5283169746398926,8698:0.5238481163978577,12207:0.5135535299777985,19911:0.48772144317626953,15006:0.47255563735961914,33449:0.466172993183136,15220:0.46516841650009155,2574:0.4523268938064575,14647:0.44833534955978394,13824:0.44397950172424316,5011:0.44102662801742554,31735:0.4409145712852478,3930:0.4366651773452759,16805:0.4223385453224182,12788:0.42018353939056396,16120:0.41779035329818726,32197:0.3962436318397522,3690:0.3813501000404358',\n",
       " '16394:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '16379:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '14604:15363|21034:0.4327089786529541,23220:0.42771345376968384,28625:0.4262864589691162,7213:0.393585205078125,19906:0.39244920015335083,20269:0.3888004422187805,5362:0.387737512588501,20119:0.3873816728591919,20485:0.3860454559326172,5091:0.38451075553894043,5149:0.38380032777786255,20883:0.3831349015235901,20352:0.38112008571624756,22235:0.3797825574874878,6263:0.3797810673713684,6431:0.37362515926361084,20193:0.36901843547821045,20876:0.36868250370025635,20125:0.3685894012451172,20477:0.3672598600387573,20880:0.3659793734550476,20055:0.3658294677734375,14729:0.3537505269050598,54188:0.3499295115470886,14775:0.34724265336990356',\n",
       " '16395:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '16390:15363|13015:0.7027417123317719,4700:0.6248065829277039,23251:0.590492457151413,4397:0.5700189471244812,78756:0.5666651427745819,4477:0.5534844696521759,15006:0.5226936638355255,8698:0.5121999979019165,9278:0.5113411545753479,19911:0.5048864185810089,12207:0.4958500266075134,5011:0.4899901747703552,13824:0.4725543260574341,2574:0.46981561183929443,15220:0.4617360234260559,14647:0.4574674963951111,33449:0.4351729154586792,19339:0.4231802821159363,3930:0.41424357891082764,16805:0.3922209143638611,12788:0.391169011592865,31522:0.3904825448989868,56638:0.3885725736618042,14156:0.3860929012298584,16120:0.38574880361557007',\n",
       " '8184:3724|14658:0.917235791683197,14718:0.9091258943080902,7180:0.9031982645392418,8808:0.9006338864564896,18735:0.7920824885368347,22306:0.791691929101944,4874:0.786677822470665,4925:0.7857631891965866,23476:0.7846655547618866,4904:0.7795064747333527,7560:0.7786111533641815,4695:0.7785360515117645,10621:0.7770487368106842,7020:0.771708756685257,8302:0.7628233134746552,6087:0.7419628202915192,21716:0.7403147220611572,22811:0.7391395568847656,6705:0.7347237765789032,33851:0.7282213866710663,19079:0.7244204580783844,9709:0.7236531972885132,12710:0.7218603491783142,12129:0.7216143012046814,15972:0.6774713099002838']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recall@25 last epoch:', 0.29)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall, exported_rank = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "\n",
    "\"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 3861\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose_feature_100epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBiLstmGenerationModel (M (None, 50, 50)       39574200    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 50)           0           FeatureBiLstmGenerationModel[1][0\n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNDilatedGenerationMode (None, 1)            80313116    desc_in[0][0]                    \n",
      "                                                                 title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 351)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 FeatureCNNDilatedGenerationModel[\n",
      "==================================================================================================\n",
      "Total params: 120,109,016\n",
      "Trainable params: 1,702,316\n",
      "Non-trainable params: 118,406,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/exported_rank_propose.txt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.14,\n",
       " '2 - recall_at_10': 0.19,\n",
       " '3 - recall_at_15': 0.22,\n",
       " '4 - recall_at_20': 0.26,\n",
       " '5 - recall_at_25': 0.29}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
