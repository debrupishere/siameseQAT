{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# deepCOREL_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the use of GPUs\n",
    "# https://datascience.stackexchange.com/questions/23895/multi-gpu-in-keras\n",
    "# https://keras.io/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus\n",
    "# https://stackoverflow.com/questions/56316451/how-to-use-specific-gpus-in-keras-for-multi-gpu-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "# from __future__ import print_function, division\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers\n",
    "\n",
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval\n",
    "\n",
    "import os\n",
    "from keras_bert import load_vocabulary\n",
    "import random\n",
    "\n",
    "from keras.initializers import RandomUniform, RandomNormal, Ones\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Dot\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import _pickle as pickle\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "    \n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: epochs=1000\n",
      "env: base=eclipse\n"
     ]
    }
   ],
   "source": [
    "%env epochs 1000\n",
    "%env base eclipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = int(os.environ['epochs'])\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = os.environ['base']\n",
    "METHOD = 'deepCOREL_cosine_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "deepCOREL_cosine_1000 for 1000 epochs in eclipse\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "print(\"*********\")\n",
    "print(\"{} for {} epochs in {}\".format(METHOD, epochs, DOMAIN))\n",
    "print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07f14f00e184a7bae42eaef8f25b12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879110ecf7fd46a5b2bbbfe35fb5af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 34.3 s, sys: 3.13 s, total: 37.4 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee1d707fb824678abf1bc581aa1ed81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 43.6 ms, total: 2.18 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '337\\n',\n",
       " 'creation_ts': '2008-08-20 05:34:00 -0400',\n",
       " 'delta_ts': '2008-09-16 04:49:45 -0400',\n",
       " 'description': '[CLS] ep ##f 1 . 5 build 2008 ##0 ##8 ##18 - 1653 steps : 1 . publish a library such as open ##up , include options \" publish gloss ##ary \" and \" publish index \" 2 . brows ##e the published website and open gloss ##ary window 3 . click gloss ##ary index link problems : ( 1 ) no response [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  4958,  2546,  1015,  1012,  1019,  3857,  2263,  2692,\n",
       "         2620, 15136,  1011, 29309,  4084,  1024,  1015,  1012, 10172,\n",
       "         1037,   102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 244669,\n",
       " 'priority': '1\\n',\n",
       " 'product': '18\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_token': array([  101, 10172,  1024,  2053,  3433,  2043, 11562, 27068,  5649,\n",
       "         5950,  4957,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  4958,  2546,  1015,  1012,  1019,  3857,\n",
       "         2263,  2692,  2620, 15136,  1011, 29309,  4084,  1024,  1015,\n",
       "         1012, 10172,  1037,   102]),\n",
       " 'title': '[CLS] publish : no response when click gloss ##ary index link [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101, 10172,  1024,  2053,  3433,  2043, 11562, 27068,  5649,\n",
       "         5950,  4957,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '514\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    title_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_T), 0)\n",
    "    description_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "    for i, bug_id in enumerate(batch_bugs):\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug, index=i, title_ids=title_ids, description_ids=description_ids)\n",
    "\n",
    "    batch_features['title'] = { 'token' : np.array(batch_features['title']), 'segment' : title_ids }\n",
    "    batch_features['desc'] = { 'token' : np.array(batch_features['desc']), 'segment' : description_ids }\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title']['token'], valid_input_sample['title']['segment'], \n",
    "                   valid_input_sample['description']['token'], valid_input_sample['description']['segment'],\n",
    "                   valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 20), (384, 20), (384, 1682), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, \\\n",
    "valid_input_sample['description']['token'].shape, \\\n",
    "valid_input_sample['title']['segment'].shape, \\\n",
    "valid_input_sample['description']['segment'].shape, \\\n",
    "valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 16995)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=True,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    layers = ['Encoder-{}-MultiHeadSelfAttention-Adapter', 'Encoder-{}-FeedForward-Adapter', \n",
    "     'Encoder-{}-MultiHeadSelfAttention-Norm', 'Encoder-{}-FeedForward-Norm']\n",
    "    outputs = []\n",
    "    for i in range(1, 13):\n",
    "        outputs += [ model.get_layer(layer.format(layer_num)).output for layer in layers ]\n",
    "    outputs = Average()(outputs)\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "    outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for dealing with AR Loss\n",
    "def get_armask(shape, labels):\n",
    "    if labels.dtype != tf.int32:\n",
    "        raise Exception(\"Labels must be a LongTensor with dtype=int32!\")\n",
    "\n",
    "    #mask = tf.zeros(shape)\n",
    "    #arr = tf.range(0, shape[0], 1)\n",
    "    mask = tf.identity(labels)\n",
    "    mask *= -1\n",
    "    # want to maximize similarity to the correct classes, so this is negative.\n",
    "    #mask = mask + - 1.\n",
    "    mask_arr = tf.cast(tf.math.equal(tf.reshape(arr, (-1, 1)), labels), \"float32\")\n",
    "    mask = mask + (mask_arr * -1.)\n",
    "    return mask\n",
    "\n",
    "def arloss(attraction_tensor, repulsion_tensor, lam):\n",
    "    # combine up everything to accumulate across the entire batch\n",
    "    loss_attraction = tf.reduce_sum(attraction_tensor)\n",
    "    loss_repulsion = tf.reduce_sum(repulsion_tensor)\n",
    "    arloss = (lam * loss_attraction) + ((1. - lam) * loss_repulsion)\n",
    "    return arloss / tf.cast(tf.shape(attraction_tensor)[0], 'float32')\n",
    "\n",
    "def CosineARLoss(y_true, y_pred):\n",
    "    \n",
    "    inputs = y_pred\n",
    "    labels = inputs[:, :1]\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "    embeddings =  tf.cast(inputs[:, 1:], dtype='float32')\n",
    "    lam=0.5\n",
    "    \n",
    "    mask = get_armask(tf.shape(embeddings), labels)\n",
    "\n",
    "    # make the attractor and repulsor, mask them!\n",
    "    attraction_tensor = mask * embeddings\n",
    "    repulsion_tensor = (mask + 1.0) * embeddings\n",
    "\n",
    "    # now, apply the special cosine-COREL rules, taking the argmax and squaring the repulsion\n",
    "    repulsion_tensor = tf.reduce_max(repulsion_tensor, axis=1)\n",
    "    repulsion_tensor = repulsion_tensor ** 2\n",
    "\n",
    "    return arloss(attraction_tensor, repulsion_tensor, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x_norm = tf.norm(x)\n",
    "    y_norm = tf.norm(y)\n",
    "    return K.dot(x, y) / ( x_norm * y_norm)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def cosine_similarity(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[::, 1])\n",
    "\n",
    "class CosineLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(CosineLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='cosine_weight', \n",
    "                                      shape=(self.output_dim, input_shape[-1]),\n",
    "                                      initializer=\"random_normal\",\n",
    "                                      trainable=True)\n",
    "        super(CosineLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "        \n",
    "    def call(self, x):\n",
    "        return Lambda(cosine_distance, output_shape=cos_dist_output_shape)([x, tf.transpose(self.W)])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1)\n",
    "\n",
    "def max_margin_objective(encoded_anchor, batch_size, n_classes, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_anchor = CosineLayer(1)(encoded_anchor)\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=CosineARLoss, metrics=[cosine_similarity]) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'deepCOREL_cosine_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size  64\n",
      "Number of clusters  63936\n"
     ]
    }
   ],
   "source": [
    "n_classes = len([bug for bug in retrieval.buckets if len(retrieval.buckets[bug]) >= 2])\n",
    "print(\"Batch size \", batch_size)\n",
    "print(\"Number of clusters \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 32)           53856       info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 632)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cosine_layer_1 (CosineLayer)    (None, 1)            632         merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           input_label[0][0]                \n",
      "                                                                 cosine_layer_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 161,209,360\n",
      "Trainable params: 737,184\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "Epoch: 1 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 2 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 3 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 4 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 5 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 6 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 7 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 8 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 9 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 11 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 12 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 13 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 14 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 15 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 16 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 17 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 18 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 19 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 21 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 22 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 23 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 24 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 25 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 26 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 27 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 28 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 29 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 31 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 32 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 33 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 34 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 35 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 36 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 37 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 38 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 39 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 41 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 42 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 43 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 44 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 45 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 46 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 47 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 48 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 49 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 51 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 52 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 53 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 54 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 55 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 56 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 57 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 58 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 59 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 61 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 62 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 63 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 64 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 65 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 66 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 67 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 68 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 69 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 71 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 72 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 74 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 75 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 76 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 77 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 78 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 79 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 81 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 82 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 83 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 84 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 85 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 86 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 87 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 88 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 89 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 91 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 92 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 93 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 94 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 95 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00\n",
      "Epoch: 96 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 97 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 98 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: 0.00\n",
      "Epoch: 99 Loss: 0.00, Loss_test: 0.00, cos: 0.00, cos_test: -0.00\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.00, Loss_test: -0.00, cos: 0.00, cos_test: -0.00, recall@25: 0.18\n",
      "Best_epoch=98, Best_loss=0.00, Recall@25=0.18\n"
     ]
    }
   ],
   "source": [
    "### %%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, batch_size, n_classes, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, cos: {:.2f}, cos_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h[1], h_validation[0], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, cos: {:.2f}, cos_test: {:.2f}\".format(epoch+1, h[0], h[1], h_validation[0], h_validation[1]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "# experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "# experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 100 Loss: 7.45, Loss_test: 6.23, recall@25: 0.19\n",
    "Best_epoch=100, Best_loss=0.00, Recall@25=0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_deepCOREL_cosine_100.txt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_deepCOREL_cosine_100_feature_100epochs_64batch(eclipse).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 161,659,772\n",
      "Trainable params: 1,187,596\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_1')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=CosineARLoss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepCOREL_cosine_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 102 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 103 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 104 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 105 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 106 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 107 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 108 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 109 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 111 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 112 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 113 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 114 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 115 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 116 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 117 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 118 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 119 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 121 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 122 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 123 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 124 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 125 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 126 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 127 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 128 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 129 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 131 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 132 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 133 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 134 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 135 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 136 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 137 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 138 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 139 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 141 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 142 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 143 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 144 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 145 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 146 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 147 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 148 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 149 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 151 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 152 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 153 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 154 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 155 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 156 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 157 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 158 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 159 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 161 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 162 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 163 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 164 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 165 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 166 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 167 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 168 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 169 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 171 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 172 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 173 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 174 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 175 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 176 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 177 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 178 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 179 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 181 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 182 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 183 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 184 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 185 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 186 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 187 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 188 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 189 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 191 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 192 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 193 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 194 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 195 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 196 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 197 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 198 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 199 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 201 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 202 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 203 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 204 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 205 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 206 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 207 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 208 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 209 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 211 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 212 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 213 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 214 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 215 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 216 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 217 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 218 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 219 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 221 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 222 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 223 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 224 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 225 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 226 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 227 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 228 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 229 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 231 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 232 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 233 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 234 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 235 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 236 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 237 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 238 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 239 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 241 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 242 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 243 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 244 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 245 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 246 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 247 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 248 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 249 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 251 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 252 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 253 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 254 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 255 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 256 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 257 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 258 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 259 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 261 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 262 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 263 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 264 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 265 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 266 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 267 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 268 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 269 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 271 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 272 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 273 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 274 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 275 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 276 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 277 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 278 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 279 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 281 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 282 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 283 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 284 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 285 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 286 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 287 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 288 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 289 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 291 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 292 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 293 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 294 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 295 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 296 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 297 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 298 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 299 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 301 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 302 Loss: 0.00, Loss_test: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 304 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 305 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 306 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 307 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 308 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 309 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 311 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 312 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 313 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 314 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 315 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 316 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 317 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 318 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 319 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 321 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 322 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 323 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 324 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 325 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 326 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 327 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 328 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 329 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 331 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 332 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 333 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 334 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 335 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 336 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 337 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 338 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 339 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 341 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 342 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 343 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 344 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 345 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 346 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 347 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 348 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 349 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 351 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 352 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 353 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 354 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 355 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 356 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 357 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 358 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 359 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 361 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 362 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 363 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 364 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 365 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 366 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 367 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 368 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 369 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 371 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 372 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 373 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 374 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 375 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 376 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 377 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 378 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 379 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 381 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 382 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 383 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 384 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 385 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 386 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 387 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 388 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 389 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 391 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 392 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 393 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 394 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 395 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 396 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 397 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 398 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 399 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 401 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 402 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 403 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 404 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 405 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 406 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 407 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 408 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 409 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 411 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 412 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 413 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 414 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 415 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 416 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 417 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 418 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 419 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 421 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 422 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 423 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 424 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 425 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 426 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 427 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 428 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 429 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 431 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 432 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 433 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 434 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 435 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 436 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 437 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 438 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 439 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 441 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 442 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 443 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 444 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 445 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 446 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 447 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 448 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 449 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 451 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 452 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 453 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 454 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 455 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 456 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 457 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 458 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 459 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 461 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 462 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 463 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 464 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 465 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 466 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 467 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 468 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 469 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 471 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 472 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 473 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 474 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 475 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 476 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 477 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 478 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 479 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 481 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 482 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 483 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 484 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 485 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 486 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 487 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 488 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 489 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 491 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 492 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 493 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 494 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 495 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 496 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 497 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 498 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 499 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 501 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 502 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 503 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 504 Loss: 0.00, Loss_test: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 505 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 506 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 507 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 508 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 509 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 511 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 512 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 513 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 514 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 515 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 516 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 517 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 518 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 519 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 521 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 522 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 523 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 524 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 525 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 526 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 527 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 528 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 529 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 531 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 532 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 533 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 534 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 535 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 536 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 537 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 538 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 539 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 541 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 542 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 543 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 544 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 545 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 546 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 547 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 548 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 549 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 551 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 552 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 553 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 554 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 555 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 556 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 557 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 558 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 559 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 561 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 562 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 563 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 564 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 565 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 566 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 567 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 568 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 569 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 571 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 572 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 573 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 574 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 575 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 576 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 577 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 578 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 579 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 581 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 582 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 583 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 584 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 585 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 586 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 587 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 588 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 589 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 591 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 592 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 593 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 594 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 595 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 596 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 597 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 598 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 599 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 601 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 602 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 603 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 604 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 605 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 606 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 607 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 608 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 609 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 611 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 612 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 613 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 614 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 615 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 616 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 617 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 618 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 619 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 621 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 622 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 623 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 624 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 625 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 626 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 627 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 628 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 629 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 631 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 632 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 633 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 634 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 635 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 636 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 637 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 638 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 639 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 641 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 642 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 643 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 644 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 645 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 646 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 647 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 648 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 649 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 651 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 652 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 653 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 654 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 655 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 656 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 657 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 658 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 659 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 661 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 662 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 663 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 664 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 665 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 666 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 667 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 668 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 669 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 671 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 672 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 673 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 674 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 675 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 676 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 677 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 678 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 679 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 681 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 682 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 683 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 684 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 685 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 686 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 687 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 688 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 689 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 691 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 692 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 693 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 694 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 695 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 696 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 697 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 698 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 699 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 701 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 702 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 703 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 704 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 705 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 706 Loss: 0.00, Loss_test: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 707 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 708 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 709 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 711 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 712 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 713 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 714 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 715 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 716 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 717 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 718 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 719 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 721 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 722 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 723 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 724 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 725 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 726 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 727 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 728 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 729 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 731 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 732 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 733 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 734 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 735 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 736 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 737 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 738 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 739 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 741 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 742 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 743 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 744 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 745 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 746 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 747 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 748 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 749 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 751 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 752 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 753 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 754 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 755 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 756 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 757 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 758 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 759 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 761 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 762 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 763 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 764 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 765 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 766 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 767 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 768 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 769 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 771 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 772 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 773 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 774 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 775 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 776 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 777 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 778 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 779 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 781 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 782 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 783 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 784 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 785 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 786 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 787 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 788 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 789 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 791 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 792 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 793 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 794 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 795 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 796 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 797 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 798 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 799 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 801 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 802 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 803 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 804 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 805 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 806 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 807 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 808 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 809 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 811 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 812 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 813 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 814 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 815 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 816 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 817 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 818 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 819 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 821 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 822 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 823 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 824 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 825 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 826 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 827 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 828 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 829 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 831 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 832 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 833 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 834 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 835 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 836 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 837 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 838 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 839 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 841 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 842 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 843 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 844 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 845 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 846 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 847 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 848 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 849 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 851 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 852 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 853 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 854 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 855 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 856 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 857 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 858 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 859 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 861 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 862 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 863 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 864 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 865 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 866 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 867 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 868 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 869 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 871 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 872 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 873 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 874 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 875 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 876 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 877 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 878 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 879 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 881 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 882 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 883 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 884 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 885 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 886 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 887 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 888 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 889 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 891 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 892 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 893 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 894 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 895 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 896 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 897 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 898 Loss: 0.00, Loss_test: 0.00\n",
      "Epoch: 899 Loss: 0.00, Loss_test: 0.00\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.00, Loss_test: 0.00\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append([h])\n",
    "    result['test'].append([h_validation])\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_deepCOREL_cosine_1000_feature1000epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_deepCOREL_cosine_1000_feature_1000epochs_64batch(eclipse).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4c5987234f467ba07578eebd3697fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16995), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb493364b574b648054bb9420ca0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ec61c4fd11b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, \n\u001b[0;32m----> 2\u001b[0;31m                                                                bug_train_ids, method='bert')\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hd/thiago/bug_report_duplicated/methods/experiments.py\u001b[0m in \u001b[0;36mevaluate_validation_test\u001b[0;34m(self, retrieval, verbose, loaded_model, issues_by_buckets, bug_train_ids, method, only_buckets)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mtest_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbug_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missues_by_buckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_buckets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_buckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mqueries_test_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbug_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missues_by_buckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbug_train_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_buckets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_buckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mannoy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexing_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexing_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannoy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_test_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hd/thiago/bug_report_duplicated/methods/experiments.py\u001b[0m in \u001b[0;36mvectorize_queries\u001b[0;34m(self, bug_set, model, test, issues_by_buckets, bug_train_ids, method, verbose, only_buckets)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0membed_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bert'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0membed_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dwen'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0membed_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(exported_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "print(EXPORT_RANK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
