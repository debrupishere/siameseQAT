{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DMS with QL\n",
    "\n",
    "https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 10\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'DMS_QL_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00f59de15564b9485e6563b2853f8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3c0701fd814c2daec3c3491d775035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 34.1 s, sys: 3.32 s, total: 37.4 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8aabc67b0e47c4adfc505b58e66fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_train='train_chronological', path_test='test_chronological'\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '780\\n',\n",
       " 'creation_ts': '2006-06-29 15:24:00 -0400',\n",
       " 'delta_ts': '2006-08-28 15:37:14 -0400',\n",
       " 'description': '[CLS] opened for ibm : 1 . import web ##ser ##vic ##ess ##amp ##le . ear or other ear . 2 . the import wizard does not go away until the build process is complete . it should go away as soon as it is done import ##ing . also the progress bar does not work correctly on import of ear . [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  2441,  2005,  9980,  1024,  1015,  1012, 12324,  4773,\n",
       "         8043,  7903,  7971, 16613,  2571,  1012,  4540,  2030,  2060,\n",
       "         4540,   102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 149222,\n",
       " 'priority': '1\\n',\n",
       " 'product': '164\\n',\n",
       " 'resolution': 'WORKSFORME',\n",
       " 'textual_token': array([  101,  1031, 12324,  1013,  9167,  1033,  3915,  8553,  3314,\n",
       "         2007, 12324, 10276,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  2441,  2005,  9980,  1024,  1015,  1012,\n",
       "        12324,  4773,  8043,  7903,  7971, 16613,  2571,  1012,  4540,\n",
       "         2030,  2060,  4540,   102]),\n",
       " 'title': '[CLS] [ import / export ] usa ##bility issues with import wizard [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  1031, 12324,  1013,  9167,  1033,  3915,  8553,  3314,\n",
       "         2007, 12324, 10276,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '514\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    \n",
    "    for bug_id in batch_bugs:\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug)\n",
    "\n",
    "    batch_features['title'] = np.array(batch_features['title'])\n",
    "    batch_features['desc'] = np.array(batch_features['desc'])\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 453 ms, sys: 0 ns, total: 453 ms\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title'], \n",
    "             valid_input_sample['description'],\n",
    "            valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 1682), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 16995)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 21175'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    \n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb6f8600ad47129de20c40f4016639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaf9d37fb8b4c699397f096537025ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 36s, sys: 4.1 s, total: 1min 40s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938669"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline.embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "#                                   input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, TimeDistributed\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 75\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Concatenate()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(50))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "#     return mask_positives\n",
    "\n",
    "   # pos \n",
    "    embed_pos = tf.matmul(mask_positives_centroids, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "    # negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "\n",
    "#     return mask_positives_centroids\n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    batch_centroid_matrix_neg = tf.Variable([])\n",
    "    batch_centroid_matrix_all = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all):\n",
    "        # centroid pos\n",
    "        mask_positives_batch = tf.reshape(tf.gather(mask_positives, i), (-1, 1))\n",
    "        centroid_pos = tf.gather(centroid_embed_pos, i)\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_pos], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_pos = mask_positives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_positives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_pos, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid neg\n",
    "        centroid_neg = tf.gather(centroid_embed_neg, i)\n",
    "        mask_negatives_batch = tf.reshape(tf.gather(mask_negatives, i), (-1, 1))\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_neg], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_neg = mask_negatives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_negatives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_neg, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix_neg = tf.concat([batch_centroid_matrix_neg, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid pos and neg\n",
    "        new_batch_centroids = tf.reduce_sum([new_batch_centroid_pos, new_batch_centroid_neg], axis=0, keepdims=True)[0]\n",
    "        vects_new_batch_centroids = tf.concat([labels_pos, new_batch_centroids], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch_centroids)\n",
    "        batch_centroid_matrix_all = tf.concat([batch_centroid_matrix_all, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all]\n",
    "    _, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all = tf.while_loop(lambda i, a, b, c: i<batch_size // 3, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])])\n",
    "\n",
    "\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL_neg = tf.reduce_mean(batch_centroid_matrix_neg) #triplet_loss(vects_neg)\n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix_all) # triplet_loss(vects_centroids)\n",
    "   \n",
    "    return K.stack([TL, TL_pos, TL_neg, TL_centroid], axis=0)\n",
    "\n",
    "def quintet_trainable(inputs):\n",
    "    TL = inputs[0]\n",
    "    TL_pos = inputs[1]\n",
    "    TL_neg = inputs[2]\n",
    "    TL_centroid = inputs[3]\n",
    "    TL_anchor_w = inputs[4]\n",
    "    TL_pos_w = inputs[5]\n",
    "    TL_neg_w = inputs[6]\n",
    "    TL_centroid_w = inputs[7]\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w]) # \n",
    "    sum_of_weigths = tf.reduce_sum([TL_anchor_w, TL_pos_w, TL_neg_w, TL_centroid_w])\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return K.stack([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], axis=0)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class QuintetWeights(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(QuintetWeights, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = tf.reshape(self.add_weight(name='quintet_kernel_weight', \n",
    "                                      shape=(input_shape[0], self.output_dim),\n",
    "                                      initializer=keras.initializers.Ones(),\n",
    "#                                       initializer=keras.initializers.RandomUniform(minval=0.0, maxval=1.0, seed=None),\n",
    "                                      trainable=True), (1, 1))\n",
    "        super(QuintetWeights, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (1, 1))\n",
    "        return [K.dot(x, self.kernel), self.kernel]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape, input_shape]\n",
    "    \n",
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    feature = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    TL_loss = Lambda(quintet_loss, name='quintet_loss')(feature)\n",
    "    \n",
    "    tl_l = Lambda(lambda x:tf.reshape(x[0], (1,)), name='TL')(TL_loss)\n",
    "    tl_l_p = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_pos')(TL_loss)\n",
    "    tl_l_n = Lambda(lambda x:tf.reshape(x[2], (1,)), name='TL_neg')(TL_loss)\n",
    "    tl_l_c = Lambda(lambda x:tf.reshape(x[3], (1, )), name='TL_centroid')(TL_loss)\n",
    "    \n",
    "    TL_w = QuintetWeights(output_dim=1)(tl_l)\n",
    "    TL_pos_w = QuintetWeights(output_dim=1)(tl_l_p)\n",
    "    TL_neg_w = QuintetWeights(output_dim=1)(tl_l_n)\n",
    "    TL_centroid_w = QuintetWeights(output_dim=1)(tl_l_c)\n",
    "    \n",
    "    TL_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_weight')(TL_w)\n",
    "    TL_pos_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_pos_weight')(TL_pos_w)\n",
    "    TL_neg_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_neg_weight')(TL_neg_w)\n",
    "    TL_centroid_weight = Lambda(lambda x:tf.reshape(x[1], (1,)), name='TL_centroid_weight')(TL_centroid_w)\n",
    "    \n",
    "    output = concatenate([tl_l, tl_l_p, tl_l_n, tl_l_c, TL_weight, TL_pos_weight, TL_neg_weight, TL_centroid_weight])\n",
    "    output = Lambda(quintet_trainable, name='quintet_trainable')(output)\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                         TL, TL_pos, TL_neg, TL_centroid]) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'DMS_QL_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-35-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581804100   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581850792   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (4,)                 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TL (Lambda)                     (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid (Lambda)            (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_1 (QuintetWeigh [(1,), (1,)]         1           TL[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_2 (QuintetWeigh [(1,), (1,)]         1           TL_pos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_3 (QuintetWeigh [(1,), (1,)]         1           TL_neg[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_4 (QuintetWeigh [(1,), (1,)]         1           TL_centroid[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TL_weight (Lambda)              (1,)                 0           quintet_weights_1[0][0]          \n",
      "                                                                 quintet_weights_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos_weight (Lambda)          (1,)                 0           quintet_weights_2[0][0]          \n",
      "                                                                 quintet_weights_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg_weight (Lambda)          (1,)                 0           quintet_weights_3[0][0]          \n",
      "                                                                 quintet_weights_3[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid_weight (Lambda)     (1,)                 0           quintet_weights_4[0][0]          \n",
      "                                                                 quintet_weights_4[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (8,)                 0           TL[0][0]                         \n",
      "                                                                 TL_pos[0][0]                     \n",
      "                                                                 TL_neg[0][0]                     \n",
      "                                                                 TL_centroid[0][0]                \n",
      "                                                                 TL_weight[0][0]                  \n",
      "                                                                 TL_pos_weight[0][0]              \n",
      "                                                                 TL_neg_weight[0][0]              \n",
      "                                                                 TL_centroid_weight[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_trainable (Lambda)      (9,)                 0           concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,164,159,796\n",
      "Trainable params: 958,396\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n",
      "Total of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "=> result saved!\n",
      "Epoch: 1 Loss: 0.34, Loss_test: 0.43\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.55, TL_pos: 0.54, TL_neg: 0.05, TL_centroid: 0.21, recall@25: 0.33\n",
      "Best_epoch=1, Best_loss=0.34, Recall@25=0.33\n",
      "CPU times: user 6min 48s, sys: 1min 42s, total: 8min 31s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "                \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "              \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], \n",
    "                                          h[4], h[5], h[6], h[7], h[8], recall))\n",
    "    else:\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_DMS_QL_1.txt'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_DMS_QL_1_feature_1epochs_64batch(eclipse).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581804100   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581850792   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (4,)                 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TL (Lambda)                     (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg (Lambda)                 (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid (Lambda)            (1,)                 0           quintet_loss[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_1 (QuintetWeigh [(1,), (1,)]         1           TL[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_2 (QuintetWeigh [(1,), (1,)]         1           TL_pos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_3 (QuintetWeigh [(1,), (1,)]         1           TL_neg[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quintet_weights_4 (QuintetWeigh [(1,), (1,)]         1           TL_centroid[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TL_weight (Lambda)              (1,)                 0           quintet_weights_1[0][0]          \n",
      "                                                                 quintet_weights_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_pos_weight (Lambda)          (1,)                 0           quintet_weights_2[0][0]          \n",
      "                                                                 quintet_weights_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_neg_weight (Lambda)          (1,)                 0           quintet_weights_3[0][0]          \n",
      "                                                                 quintet_weights_3[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "TL_centroid_weight (Lambda)     (1,)                 0           quintet_weights_4[0][0]          \n",
      "                                                                 quintet_weights_4[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (8,)                 0           TL[0][0]                         \n",
      "                                                                 TL_pos[0][0]                     \n",
      "                                                                 TL_neg[0][0]                     \n",
      "                                                                 TL_centroid[0][0]                \n",
      "                                                                 TL_weight[0][0]                  \n",
      "                                                                 TL_pos_weight[0][0]              \n",
      "                                                                 TL_neg_weight[0][0]              \n",
      "                                                                 TL_centroid_weight[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "quintet_trainable (Lambda)      (9,)                 0           concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,164,159,796\n",
      "Trainable params: 958,396\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_4')\n",
    "output = Lambda(quintet_trainable, name='quintet_trainable')(model.output)\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                         TL, TL_pos, TL_neg, TL_centroid])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'DMS_QL_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.41, Loss_test: 0.42\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.72, TL_pos: 0.70, TL_neg: 0.08, TL_centroid: 0.15\n",
      "Epoch: 3 Loss: 0.46, Loss_test: 0.42\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.81, TL_pos: 0.78, TL_neg: 0.09, TL_centroid: 0.15\n",
      "Epoch: 4 Loss: 0.35, Loss_test: 0.41\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.61, TL_pos: 0.59, TL_neg: 0.05, TL_centroid: 0.14\n",
      "Epoch: 5 Loss: 0.42, Loss_test: 0.40\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.76, TL_pos: 0.74, TL_neg: 0.08, TL_centroid: 0.11\n",
      "Epoch: 6 Loss: 0.36, Loss_test: 0.39\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.01, TL_centroid_w: 1.00\n",
      "TL: 0.65, TL_pos: 0.64, TL_neg: 0.05, TL_centroid: 0.11\n",
      "Epoch: 7 Loss: 0.38, Loss_test: 0.39\n",
      "TL_w: 0.99, TL_pos_w: 0.99, TL_neg_w: 1.01, TL_centroid_w: 1.01\n",
      "TL: 0.69, TL_pos: 0.67, TL_neg: 0.07, TL_centroid: 0.09\n",
      "Epoch: 8 Loss: 0.41, Loss_test: 0.38\n",
      "TL_w: 0.99, TL_pos_w: 0.99, TL_neg_w: 1.01, TL_centroid_w: 1.01\n",
      "TL: 0.76, TL_pos: 0.74, TL_neg: 0.05, TL_centroid: 0.09\n",
      "Epoch: 9 Loss: 0.37, Loss_test: 0.37\n",
      "TL_w: 0.99, TL_pos_w: 0.99, TL_neg_w: 1.01, TL_centroid_w: 1.01\n",
      "TL: 0.69, TL_pos: 0.68, TL_neg: 0.04, TL_centroid: 0.09\n",
      "Best_epoch=1, Best_loss=0.34, Recall@25=0.33\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "                \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "              \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], \n",
    "                                          h[4], h[5], h[6], h[7], h[8], recall))\n",
    "    else:\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_DMS_QL_10_feature10epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_DMS_QL_10_feature_10epochs_64batch(eclipse).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d4bff822014b829afc992d388c1a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16995), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f6119ce3c4306a397fef12916128d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2124a108d1476dbfa576df6733990c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30481), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99f338fb4114d6e82b5b8d972a92789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d63d5f4ed93410c91d6ba821b6f5201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70edb0337e04c80b8581787114d8996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bb33d4ec0b45129e9312f3d66a975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9 Loss: 0.37, Loss_test: 0.37\n",
      "TL_w: 0.99, TL_pos_w: 0.99, TL_neg_w: 1.01, TL_centroid_w: 1.01\n",
      "TL: 0.69, TL_pos: 0.68, TL_neg: 0.04, TL_centroid: 0.09, recall@25: 0.37\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "       \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "        \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "      \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2:15392,9779,94|9295:0.5874873995780945,5743:0.5809405744075775,5937:0.5797840058803558,8467:0.5769121050834656,252:0.5714425146579742,11545:0.5674876272678375,13422:0.5671060085296631,10456:0.5660412907600403,16864:0.5611875355243683,7282:0.5609087347984314,20771:0.560257762670517,19739:0.5564038455486298,16161:0.5538699626922607,6739:0.549011617898941,9437:0.5481896102428436,11615:0.5469440221786499,19650:0.5463651716709137,13616:0.5403421819210052,17518:0.5388053357601166,20521:0.5359648168087006,94:0.5357970297336578,14959:0.5317193567752838,256:0.5300610661506653,4895:0.5299100875854492,19979:0.5285190343856812,13255:0.5279819071292877,41637:0.5275957882404327,13094:0.5275154709815979,19246:0.5260620415210724',\n",
       " '393232:393282,390667,383388|405563:0.5796588659286499,392565:0.5721054971218109,412105:0.5720390975475311,410955:0.5468209087848663,392564:0.5454622805118561,421036:0.5372216105461121,410468:0.5361137688159943,367823:0.5327838957309723,395921:0.529886394739151,223993:0.5263519287109375,387650:0.5261357128620148,306933:0.5236194431781769,399971:0.5231853723526001,405416:0.5201824307441711,406503:0.5201574862003326,115556:0.517959862947464,371135:0.5168977379798889,387658:0.5149809420108795,109609:0.5119719505310059,55526:0.5106950402259827,319991:0.50999516248703,416464:0.5094134211540222,403900:0.5093299150466919,384841:0.5073530673980713,59151:0.5069050192832947,299901:0.5064720213413239,33881:0.5062435269355774,399354:0.5061285197734833,305625:0.5054488778114319',\n",
       " '30:205979,38236,99332,71263|5743:0.6047035753726959,9295:0.6027032136917114,10456:0.5964255630970001,4895:0.5933257639408112,6739:0.591200202703476,5937:0.5867509841918945,13521:0.5860152244567871,21276:0.5780087411403656,22923:0.5716531276702881,5931:0.5641763210296631,15119:0.5539878308773041,94:0.5524620115756989,8355:0.5498971045017242,23758:0.5482859015464783,31053:0.5473859906196594,7894:0.5466322600841522,23060:0.5462623536586761,7563:0.5444128215312958,20750:0.544188380241394,20729:0.5426324903964996,21979:0.5421930551528931,16778:0.5407612025737762,24609:0.5399124026298523,16204:0.5392246544361115,19739:0.5385686755180359,11633:0.5384738445281982,19669:0.5371897220611572,11545:0.5371523201465607,13422:0.5370735824108124',\n",
       " '131123:234849|15296:0.5603932738304138,145448:0.5429285168647766,19784:0.5422861576080322,134226:0.5400058329105377,10053:0.5381833910942078,15501:0.5361770987510681,70922:0.5360184907913208,32447:0.5348025262355804,11083:0.5341465473175049,21396:0.5339409112930298,21537:0.5318141281604767,79016:0.5317617654800415,390915:0.531067281961441,45240:0.5278642475605011,31056:0.5248697996139526,14946:0.524374783039093,68742:0.5218227803707123,59903:0.5216194987297058,284260:0.5214081704616547,23397:0.5196056067943573,21576:0.5195911526679993,107000:0.5193005204200745,18080:0.5188012421131134,39942:0.5174988806247711,22741:0.5157729685306549,25261:0.5152300298213959,202276:0.5151491463184357,31728:0.5143086314201355,105609:0.5134660005569458',\n",
       " '393277:393864,400436|77900:0.5230063199996948,160310:0.5071326792240143,162097:0.4907395839691162,267365:0.48567140102386475,81862:0.4685099720954895,88623:0.4627499580383301,159435:0.46227502822875977,81996:0.46097326278686523,101670:0.45774906873703003,76562:0.4559059143066406,153485:0.454561710357666,159616:0.4544649124145508,163174:0.4544214606285095,134672:0.4516568183898926,138031:0.4487461447715759,94488:0.4475228786468506,95543:0.44433581829071045,161701:0.4442896246910095,154301:0.4403029680252075,103077:0.4397991895675659,112125:0.43863940238952637,12979:0.43806707859039307,136058:0.43700259923934937,193119:0.435735821723938,403040:0.4355267286300659,254811:0.4350275993347168,162000:0.43436068296432495,162001:0.43436068296432495,76994:0.4337024688720703',\n",
       " '131136:150817,149868,145937,65841,102812|131985:0.5978856086730957,44571:0.5897655785083771,149868:0.5873299241065979,10215:0.5823464691638947,124938:0.5798681676387787,139461:0.5789207220077515,87789:0.5788058638572693,106476:0.5756054818630219,99027:0.5750907361507416,92222:0.5743288993835449,123898:0.5726707875728607,38409:0.5719490945339203,82710:0.5718498229980469,16952:0.5711143910884857,49210:0.5677304863929749,95699:0.5671674907207489,39867:0.5671619176864624,111506:0.5660200417041779,118760:0.5656970143318176,81985:0.5648000836372375,223060:0.5621747672557831,222197:0.5580609440803528,90558:0.5567915141582489,17573:0.5567379593849182,62491:0.5553057491779327,24898:0.5549020767211914,101195:0.554759293794632,82421:0.5545268058776855,111792:0.554317831993103',\n",
       " '393282:393232,390667,383388|158564:0.5000138878822327,147939:0.4720832109451294,243896:0.45693492889404297,145185:0.4442591071128845,159235:0.4435109496116638,136238:0.44212448596954346,265240:0.43775033950805664,126720:0.43620210886001587,154185:0.4357982277870178,104413:0.43524789810180664,120694:0.4341333508491516,101321:0.4332684278488159,101325:0.4332684278488159,100309:0.43305158615112305,134026:0.43209993839263916,397437:0.4294450283050537,159186:0.42902976274490356,120044:0.42610955238342285,115416:0.4260297417640686,181665:0.426002562046051,346849:0.4258534908294678,90381:0.4257967472076416,163702:0.4248307943344116,405049:0.4239068031311035,119223:0.4238145351409912,126693:0.4232195019721985,187302:0.4198839068412781,111285:0.41971737146377563,224138:0.4196491837501526',\n",
       " '262212:292851,292845,291839|304014:0.5123738050460815,178063:0.48778557777404785,271352:0.4683115482330322,308023:0.46698659658432007,113568:0.456722617149353,207855:0.45406174659729004,347400:0.45185452699661255,260394:0.4516240954399109,159052:0.4508650302886963,238097:0.44658535718917847,338968:0.4457229971885681,58765:0.4415748715400696,309844:0.441511869430542,384125:0.43957680463790894,342368:0.43892359733581543,304139:0.4370652437210083,163652:0.4370484948158264,343126:0.4366893172264099,316370:0.4350930452346802,316678:0.43488365411758423,192313:0.4297139048576355,192314:0.4297139048576355,299812:0.42967140674591064,343685:0.42859500646591187,259737:0.42814403772354126,96812:0.42781031131744385,321704:0.4278041124343872,329667:0.42743372917175293,198815:0.42716163396835327',\n",
       " '131143:135599|389024:0.4763922095298767,196496:0.4699810743331909,100288:0.45578157901763916,130371:0.4542439579963684,144650:0.4523625373840332,148476:0.4495643377304077,153589:0.44671744108200073,296387:0.4454100728034973,144089:0.4446539282798767,415491:0.44407403469085693,107725:0.44193994998931885,188787:0.44088876247406006,357272:0.4387401342391968,157011:0.4386153221130371,367795:0.43572747707366943,139777:0.43474137783050537,374966:0.43342286348342896,317062:0.432552695274353,82836:0.431443452835083,111318:0.42983388900756836,371739:0.428849458694458,71944:0.4288117289543152,56207:0.4283943176269531,398891:0.4282708168029785,135287:0.4276546239852905,196499:0.42659300565719604,187457:0.4261518716812134,172156:0.4258893132209778,137902:0.42488282918930054',\n",
       " '71:256,228|4904:0.5803988873958588,15392:0.5545362234115601,228:0.5447626113891602,7288:0.5445143282413483,12369:0.5422224998474121,7564:0.5420173704624176,7143:0.5399807095527649,24443:0.5386718213558197,11941:0.5380762815475464,21277:0.5343860685825348,170:0.5332424640655518,12545:0.531825065612793,23060:0.5278628170490265,13116:0.5233865976333618,94:0.522652268409729,4911:0.5222502648830414,8202:0.5221864283084869,23921:0.5188712775707245,24133:0.5172905921936035,20521:0.5171373784542084,5242:0.5159014165401459,17518:0.5141157507896423,18653:0.5140026211738586,226:0.5132229030132294,14959:0.5127241909503937,24614:0.5106455683708191,9437:0.5105578005313873,122:0.5089369118213654,13616:0.508793294429779',\n",
       " '85:247,10959|17809:0.6631135046482086,17518:0.6565446257591248,24133:0.6339174211025238,122:0.6295490264892578,20521:0.624881237745285,21277:0.6210329532623291,19744:0.6206690073013306,171:0.6185971796512604,4894:0.6160942614078522,9312:0.6103369295597076,11858:0.6096361577510834,14581:0.6088391244411469,23430:0.6088195145130157,13616:0.6074393093585968,9437:0.6057385206222534,22172:0.6027379333972931,14339:0.5993532836437225,19739:0.5983834564685822,19233:0.596786379814148,8202:0.5940414071083069,13224:0.5937204957008362,13521:0.5917085111141205,14671:0.5898575782775879,18653:0.5890901386737823,10740:0.5873445570468903,21263:0.5866304039955139,150:0.5863327980041504,19650:0.5854110717773438,23414:0.5852616131305695',\n",
       " '88:6099,14116|23921:0.608296126127243,13570:0.5956431031227112,15392:0.5924164950847626,7618:0.5891760587692261,13094:0.5840490758419037,226:0.5764147639274597,228:0.5745658278465271,7522:0.5716567635536194,18653:0.5713057518005371,16864:0.5711406469345093,13616:0.5701367259025574,7327:0.5673989951610565,11859:0.5655266046524048,24609:0.5644814670085907,5931:0.5635470449924469,15022:0.5634545087814331,19233:0.5627953708171844,10909:0.5614694952964783,94:0.5606342256069183,10095:0.5553314089775085,7901:0.5544329285621643,9779:0.5518765449523926,24614:0.5452570617198944,252:0.5449592471122742,14052:0.5432393252849579,12386:0.5382034480571747,15562:0.5357335805892944,14959:0.5338823199272156,16202:0.5331453680992126',\n",
       " '131161:44438,103374,98057|97110:0.5560374855995178,137720:0.5359285771846771,180991:0.5335107147693634,83519:0.5334965884685516,112641:0.5315727889537811,135399:0.5241874158382416,106074:0.5149548351764679,79731:0.5145744979381561,255413:0.509835809469223,135521:0.5083349347114563,43386:0.5042820870876312,113428:0.5036950707435608,125850:0.5028365254402161,82127:0.5021791458129883,80903:0.501688539981842,117909:0.5004661381244659,82787:0.49997979402542114,86476:0.49769479036331177,145644:0.49417293071746826,125920:0.4929831624031067,251567:0.49207645654678345,116401:0.4886159896850586,117907:0.48836255073547363,178580:0.48824554681777954,36292:0.4865104556083679,7289:0.48636263608932495,88718:0.48570573329925537,183525:0.4820520281791687,166551:0.4810682535171509',\n",
       " '262235:260098,263611|276562:0.593431830406189,274069:0.5595794916152954,266592:0.554660975933075,276049:0.5476515591144562,267921:0.5462048351764679,285667:0.5374308526515961,252517:0.5307641625404358,279091:0.5286591053009033,268192:0.5232864618301392,277244:0.5229692161083221,267246:0.519115686416626,272250:0.5179536640644073,274181:0.5160122215747833,252510:0.5155702233314514,262331:0.5107890367507935,262548:0.5102830231189728,290207:0.5102701485157013,246461:0.5047965943813324,265592:0.5038141310214996,259113:0.5018888413906097,258178:0.501654177904129,252449:0.5007345378398895,252455:0.5007345378398895,236124:0.497755765914917,267743:0.49701863527297974,261331:0.49445122480392456,250987:0.4940463900566101,258896:0.49241602420806885,273664:0.4906243085861206',\n",
       " '94:15392,2,9779|15392:0.6391006410121918,23060:0.6377319693565369,22806:0.6367084980010986,5931:0.6354849636554718,13616:0.6271636784076691,11545:0.6254582703113556,9437:0.6180435419082642,23921:0.613332599401474,19648:0.6105379462242126,13521:0.6085098087787628,17023:0.6084149777889252,7282:0.6061187982559204,13422:0.6043053865432739,14959:0.6036664545536041,228:0.6028863489627838,13116:0.6023274958133698,150:0.6015630960464478,24443:0.5999102294445038,12742:0.5990290641784668,9779:0.5985834896564484,7522:0.5965784788131714,11580:0.5965051054954529,16204:0.5945064425468445,18653:0.5943614840507507,8202:0.5943596363067627,13094:0.593889594078064,11615:0.5914623737335205,14052:0.59126016497612,19739:0.5912573635578156',\n",
       " '131171:202560,96651,172374|132916:0.6171375513076782,133821:0.5888268649578094,128955:0.5773214101791382,137565:0.5731303095817566,133948:0.5709561109542847,154703:0.5704362988471985,139831:0.569977194070816,131477:0.5694995224475861,152891:0.5691976547241211,121750:0.5685597062110901,110282:0.5615410506725311,188149:0.5605923533439636,135557:0.5600466728210449,124542:0.55995112657547,136962:0.5595880448818207,145579:0.5585843622684479,123451:0.5540816485881805,98185:0.553997129201889,141094:0.5539673268795013,108561:0.5529356598854065,111201:0.5526307225227356,157567:0.5506176650524139,174100:0.5499458611011505,146913:0.54993537068367,142587:0.5494647026062012,141777:0.5487615764141083,160241:0.5475992560386658,152866:0.5469219088554382,152870:0.5469219088554382',\n",
       " '393320:394988|296961:0.503181129693985,330580:0.4953446388244629,333476:0.47003358602523804,296613:0.4685153365135193,296388:0.4633293151855469,103363:0.4529709219932556,296389:0.45209646224975586,364208:0.45144230127334595,99267:0.44745951890945435,143213:0.44737905263900757,104958:0.4419395923614502,296387:0.44043970108032227,117348:0.4381827712059021,113958:0.43715900182724,367323:0.4355306029319763,381528:0.43120551109313965,161189:0.4284454584121704,302547:0.4256693720817566,197804:0.4240608811378479,400365:0.42398756742477417,296628:0.42334944009780884,111318:0.42217981815338135,100303:0.4215128421783447,85431:0.42149633169174194,128275:0.4214152693748474,341937:0.42140334844589233,159763:0.4212893843650818,285636:0.42111068964004517,126720:0.42110180854797363',\n",
       " '131180:134058,132491,131646|389387:0.5494600534439087,92451:0.5204586684703827,414846:0.5179340839385986,379794:0.515590101480484,23318:0.508796751499176,317046:0.505768358707428,100982:0.4998461604118347,99928:0.49852877855300903,159452:0.4980520009994507,211485:0.49732738733291626,223124:0.4973245859146118,101456:0.49666309356689453,317836:0.4957680106163025,266500:0.49521404504776,102422:0.49514007568359375,150837:0.4947373867034912,352934:0.49428462982177734,346928:0.49218201637268066,182179:0.49210846424102783,34002:0.4913175106048584,116295:0.49122148752212524,3336:0.49108922481536865,170249:0.4904109239578247,387924:0.490328848361969,171504:0.48972535133361816,62854:0.48693764209747314,167743:0.48462748527526855,114257:0.4826188087463379,49809:0.4813939332962036',\n",
       " '393332:405833,414629|36060:0.5038655400276184,31497:0.4928740859031677,32579:0.4911210536956787,34706:0.4885982275009155,401588:0.4862171411514282,370792:0.4857158064842224,28760:0.47875696420669556,25398:0.47333669662475586,29430:0.47216153144836426,299115:0.46663546562194824,37804:0.46564120054244995,37805:0.46564120054244995,90417:0.4647371768951416,40631:0.4624752998352051,37426:0.460594117641449,337660:0.45731276273727417,69860:0.4561191201210022,117018:0.45422089099884033,34772:0.45363664627075195,101186:0.4534769654273987,360335:0.4532599449157715,133101:0.45239192247390747,32042:0.45174145698547363,36854:0.45062243938446045,29029:0.4506121277809143,98611:0.4506014585494995,38105:0.45054811239242554,53597:0.4496455788612366,50894:0.44773340225219727',\n",
       " '262266:262871|147464:0.5657615065574646,257970:0.5453512966632843,146081:0.5283131897449493,137694:0.5151655673980713,42020:0.5125294029712677,140660:0.5085793137550354,137212:0.5080976784229279,74680:0.5040303468704224,164218:0.5024927854537964,182657:0.5009563267230988,140370:0.5005832314491272,148006:0.49918705224990845,112412:0.4969046711921692,18062:0.49678367376327515,151066:0.49470144510269165,228267:0.4928296208381653,78338:0.4925341010093689,32335:0.491003155708313,167667:0.49073827266693115,146880:0.4860532283782959,163305:0.48601454496383667,257202:0.4837220311164856,208062:0.48219746351242065,337791:0.48217833042144775,269320:0.4817681908607483,135542:0.48123711347579956,43388:0.479261577129364,152583:0.47906649112701416,185127:0.47772473096847534']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 16995\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_DMS_QL_10_feature_10epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581804100   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581850792   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,164,159,792\n",
      "Trainable params: 958,392\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27321"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exported_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_DMS_QL_10.txt'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.24,\n",
       " '2 - recall_at_10': 0.29,\n",
       " '3 - recall_at_15': 0.32,\n",
       " '4 - recall_at_20': 0.34,\n",
       " '5 - recall_at_25': 0.37}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some ideas to visualizate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
