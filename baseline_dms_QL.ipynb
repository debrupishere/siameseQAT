{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DMS with QL\n",
    "\n",
    "https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 10\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'DMS_QL_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1336065db5064920b0390c1664d335dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f2ad3fd4bd41fabdec254ffea82c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 34.5 s, sys: 3.21 s, total: 37.8 s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d71eb09547450ab78965327dbb7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_train='train_chronological', path_test='test_chronological'\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '5\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '831\\n',\n",
       " 'creation_ts': '2003-10-21 16:39:00 -0400',\n",
       " 'delta_ts': '2003-10-22 07:04:26 -0400',\n",
       " 'description': '[CLS] build i ##200 ##31 ##0 ##21 - kicked off a ref ##resh in the live sync view - selected one of my outgoing changes ( on contribution ##ite ##m . java ) and chose replace with / latest from head - it came up with the dial ##og indicating the fore ##ground operation was waiting on a background operation - a short while later , it came up with the dial ##og asking to over ##write my local changes - however , the dial ##og was un ##res ##pon ##sive - - neither of the buttons would accept clicks - the background sync also stopped making progress - the ui is not completely un ##res ##pon ##sive though : it still rep ##aint ##s if obscured then brought to front thread dump from the console : full thread dump java hot ##sp ##ot ( t ##m ) client v ##m ( 1 . 4 . 2 _ 01 - b ##0 ##6 mixed mode ) : \" mod ##al ##con ##text \" pri ##o = 7 ti ##d = 0 ##x ##20 ##bbe ##64 ##0 ni ##d = 0 ##x ##8 ##48 in object . wait ( ) [ 231 ##4 ##f ##00 ##0 . . 231 ##4 ##f ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##10 ##23 ##38 ##48 > ( a org . eclipse . ui . internal . se ##ma ##ph ##ore ) at org . eclipse . ui . internal . se ##ma ##ph ##ore . acquire ( se ##ma ##ph ##ore . java : 37 ) - locked < 0 ##x ##10 ##23 ##38 ##48 > ( a org . eclipse . ui . internal . se ##ma ##ph ##ore ) at org . eclipse . ui . internal . ui ##sy ##nch ##ron ##izer . sync ##ex ##ec ( ui ##sy ##nch ##ron ##izer . java : 42 ) at org . eclipse . sw ##t . wi ##dgets . display . sync ##ex ##ec ( display . java : 249 ##0 ) at org . eclipse . team . internal . ui . dial ##og ##s . prompting ##dial ##og . confirm ##over ##write ( prompting ##dial ##og . java : 100 ) at org . eclipse . team . internal . ui . dial ##og ##s . prompting ##dial ##og . prompt ##form ##ult ##ip ##le ( prompting ##dial ##og . java : 78 ) at org . eclipse . team . internal . cc ##vs . ui . actions . works ##pace ##act ##ion . check ##over ##write ##of ##di ##rt ##yr es ##our ##ces ( works ##pace ##act ##ion . java : 60 ##6 ) at org . eclipse . team . internal . cc ##vs . ui . actions . replace ##with ##rem ##ote ##act ##ion $ 1 . execute ( replace ##with ##rem ##ote ##act ##ion . java : 41 ) at org . eclipse . ui . actions . works ##pace ##mo ##di ##fy ##oper ##ation $ 1 . run ( works ##pace ##mo ##di ##fy ##oper ##ation . java : 71 ) at org . eclipse . core . internal . resources . works ##pace . run ( works ##pace . java : 155 ##5 ) at org . eclipse . core . internal . resources . works ##pace . run ( works ##pace . java : 157 ##2 ) at org . eclipse . ui . actions . works ##pace ##mo ##di ##fy ##oper ##ation . run ( works ##pace ##mo ##di ##fy ##oper ##ation . java : 85 ) - locked < 0 ##x ##12 ##500 ##33 ##8 > ( a org . eclipse . team . internal . cc ##vs . ui . actions . replace ##with ##rem ##ote ##act ##ion $ 1 ) at org . eclipse . team . internal . cc ##vs . ui . rep ##o . repository ##mana ##ger . run ( repository ##mana ##ger . java : 80 ##9 ) at org . eclipse . team . internal . cc ##vs . ui . actions . cv ##sa ##ction $ 1 . run ( cv ##sa ##ction . java : 242 ) at org . eclipse . j ##face . operation . mod ##al ##con ##text $ mod ##al ##con ##text ##th ##rea ##d . run ( mod ##al ##con ##text . java : 101 ) \" worker - 5 \" pri ##o = 5 ti ##d = 0 ##x ##20 ##b ##d ##4 ##d ##60 ni ##d = 0 ##x ##30 ##8 in object . wait ( ) [ 22 ##cd ##f ##00 ##0 . . 22 ##cd ##f ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##11 ##7 ##cc ##7 ##7 ##8 > ( a org . eclipse . core . internal . jobs . implicit ##jo ##bs $ thread ##jo ##b ) at org . eclipse . core . internal . jobs . implicit ##jo ##bs $ thread ##jo ##b . join ##run ( implicit ##jo ##bs . java : 68 ) - locked < 0 ##x ##11 ##7 ##cc ##7 ##7 ##8 > ( a org . eclipse . core . internal . jobs . implicit ##jo ##bs $ thread ##jo ##b ) at org . eclipse . core . internal . jobs . implicit ##jo ##bs . begin ( implicit ##jo ##bs . java : 198 ) at org . eclipse . core . internal . jobs . job ##mana ##ger . begin ##ru ##le ( job ##mana ##ger . java : 119 ) at org . eclipse . team . internal . cc ##vs . core . sync ##in ##fo . remote ##tag ##sy ##nch ##ron ##izer . ref ##resh ( remote ##tag ##sy ##nch ##ron ##izer . java : 236 ) at org . eclipse . team . internal . cc ##vs . core . cv ##ss ##yn ##ct ##ree ##su ##bs ##cr ##ibe ##r . ref ##resh ##rem ##ote ( cv ##ss ##yn ##ct ##ree ##su ##bs ##cr ##ibe ##r . java : 235 ) at org . eclipse . team . internal . cc ##vs . core . cv ##ss ##yn ##ct ##ree ##su ##bs ##cr ##ibe ##r . ref ##resh ( cv ##ss ##yn ##ct ##ree ##su ##bs ##cr ##ibe ##r . java : 218 ) at org . eclipse . team . internal . ui . jobs . ref ##resh ##su ##bs ##cr ##ibe ##r ##jo ##b . run ##in ##works ##pace ( ref ##resh ##su ##bs ##cr ##ibe ##r ##jo ##b . java : 131 ) - locked < 0 ##x ##11 ##9 ##4 ##e ##7 ##70 > ( a java . lang . object ) at org . eclipse . core . internal . resources . internal ##works ##pace ##jo ##b . run ( internal ##works ##pace ##jo ##b . java : 37 ) at org . eclipse . core . internal . jobs . worker . run ( worker . java : 62 ) \" worker - 4 \" pri ##o = 5 ti ##d = 0 ##x ##20 ##e ##53 ##e ##30 ni ##d = 0 ##x ##7 ##8 ##8 waiting on condition [ 22 ##c ##9 ##f ##00 ##0 . . 22 ##c ##9 ##f ##d ##9 ##4 ] at java . lang . thread . sleep ( native method ) at org . eclipse . ui . internal . progress . animation ##mana ##ger . an ##imate ##lo ##op ( animation ##mana ##ger . java : 318 ) at org . eclipse . ui . internal . progress . animation ##mana ##ger $ 3 . run ( animation ##mana ##ger . java : 46 ##7 ) at org . eclipse . core . internal . jobs . worker . run ( worker . java : 62 ) \" worker - 2 \" pri ##o = 5 ti ##d = 0 ##x ##20 ##b ##d ##1 ##ef ##0 ni ##d = 0 ##x ##6 ##b ##0 in object . wait ( ) [ 21 ##f ##6 ##f ##00 ##0 . . 21 ##f ##6 ##f ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##10 ##f ##7 ##7 ##9 ##40 > ( a org . eclipse . core . internal . jobs . worker ##pool ) at org . eclipse . core . internal . jobs . worker ##pool . sleep ( worker ##pool . java : 155 ) - locked < 0 ##x ##10 ##f ##7 ##7 ##9 ##40 > ( a org . eclipse . core . internal . jobs . worker ##pool ) at org . eclipse . core . internal . jobs . worker ##pool . start ##jo ##b ( worker ##pool . java : 181 ) at org . eclipse . core . internal . jobs . worker . run ( worker . java : 55 ) \" java index ##ing \" daemon pri ##o = 4 ti ##d = 0 ##x ##20 ##b ##52 ##4 ##f ##8 ni ##d = 0 ##x ##26 ##c in object . wait ( ) [ 21 ##d ##1 ##f ##00 ##0 . . 21 ##d ##1 ##f ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##11 ##65 ##b ##43 ##0 > ( a org . eclipse . jd ##t . internal . core . search . index ##ing . index ##mana ##ger ) at java . lang . object . wait ( object . java : 42 ##9 ) at org . eclipse . jd ##t . internal . core . search . processing . job ##mana ##ger . run ( job ##mana ##ger . java : 35 ##8 ) - locked < 0 ##x ##11 ##65 ##b ##43 ##0 > ( a org . eclipse . jd ##t . internal . core . search . index ##ing . index ##mana ##ger ) at java . lang . thread . run ( thread . java : 53 ##4 ) \" signal dispatch ##er \" daemon pri ##o = 10 ti ##d = 0 ##x ##00 ##9 ##21 ##0 ##80 ni ##d = 0 ##x ##7 ##e ##8 waiting on condition [ 0 . . 0 ] \" final ##izer \" daemon pri ##o = 9 ti ##d = 0 ##x ##00 ##9 ##1 ##e ##2 ##a ##8 ni ##d = 0 ##x ##6 ##fc in object . wait ( ) [ 209 ##ef ##00 ##0 . . 209 ##ef ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##10 ##f ##70 ##13 ##8 > ( a java . lang . ref . reference ##que ##ue $ lock ) at java . lang . ref . reference ##que ##ue . remove ( reference ##que ##ue . java : 111 ) - locked < 0 ##x ##10 ##f ##70 ##13 ##8 > ( a java . lang . ref . reference ##que ##ue $ lock ) at java . lang . ref . reference ##que ##ue . remove ( reference ##que ##ue . java : 127 ) at java . lang . ref . final ##izer $ final ##izer ##th ##rea ##d . run ( final ##izer . java : 159 ) \" reference handler \" daemon pri ##o = 10 ti ##d = 0 ##x ##00 ##9 ##1 ##ce ##38 ni ##d = 0 ##x ##7 ##3 ##c in object . wait ( ) [ 209 ##af ##00 ##0 . . 209 ##af ##d ##9 ##4 ] at java . lang . object . wait ( native method ) - waiting on < 0 ##x ##10 ##f ##70 ##1 ##a ##0 > ( a java . lang . ref . reference $ lock ) at java . lang . object . wait ( object . java : 42 ##9 ) at java . lang . ref . reference $ reference ##hand ##ler . run ( reference . java : 115 ) - locked < 0 ##x ##10 ##f ##70 ##1 ##a ##0 > ( a java . lang . ref . reference $ lock ) \" main \" pri ##o = 7 ti ##d = 0 ##x ##00 ##23 ##4 ##e ##58 ni ##d = 0 ##x ##7 ##d ##8 run ##nable [ 6 ##f ##00 ##0 . . 6 ##fc ##3 ##c ] at org . eclipse . sw ##t . internal . win ##32 . os . wait ##mes ##sa ##ge ( native method ) at org . eclipse . sw ##t . wi ##dgets . display . sleep ( display . java : 247 ##0 ) at org . eclipse . j ##face . window . window . rune ##vent ##lo ##op ( window . java : 58 ##4 ) at org . eclipse . j ##face . window . window . open ( window . java : 56 ##3 ) at org . eclipse . team . internal . ui . dial ##og ##s . prompting ##dial ##og $ 1 . run ( prompting ##dial ##og . java : 103 ) at org . eclipse . ui . internal . ui ##lock ##list ##ener . do ##pen ##ding ##work ( ui ##lock ##list ##ener . java : 101 ) at org . eclipse . ui . internal . ui ##sy ##nch ##ron ##izer $ 1 . run ( ui ##sy ##nch ##ron ##izer . java : 33 ) at org . eclipse . sw ##t . wi ##dgets . run ##nable ##lock . run ( run ##nable ##lock . java : 35 ) at org . eclipse . sw ##t . wi ##dgets . sync ##hr ##oni ##zer . run ##as ##yn ##cm ##ess ##ages ( sync ##hr ##oni ##zer . java : 102 ) - locked < 0 ##x ##10 ##23 ##38 ##70 > ( a org . eclipse . sw ##t . wi ##dgets . run ##nable ##lock ) at org . eclipse . sw ##t . wi ##dgets . display . run ##as ##yn ##cm ##ess ##ages ( display . java : 215 ##0 ) at org . eclipse . sw ##t . wi ##dgets . display . read ##and ##dis ##pa ##tch ( display . java : 1867 ) at org . eclipse . j ##face . operation . mod ##al ##con ##text $ mod ##al ##con ##text ##th ##rea ##d . block ( mod ##al ##con ##text . java : 136 ) at org . eclipse . j ##face . operation . mod ##al ##con ##text . run ( mod ##al ##con ##text . java : 261 ) at org . eclipse . j ##face . dial ##og ##s . progress ##mon ##itor ##dial ##og . run ( progress ##mon ##itor ##dial ##og . java : 38 ##6 ) at org . eclipse . team . internal . cc ##vs . ui . actions . cv ##sa ##ction . run ( cv ##sa ##ction . java : 272 ) at org . eclipse . team . internal . cc ##vs . ui . actions . replace ##with ##rem ##ote ##act ##ion . execute ( replace ##with ##rem ##ote ##act ##ion . java : 36 ) at org . eclipse . team . internal . cc ##vs . ui . actions . cv ##sa ##ction . run ( cv ##sa ##ction . java : 69 ) at org . eclipse . ui . actions . action ##del ##ega ##te . run ##with ##eve ##nt ( action ##del ##ega ##te . java : 71 ) at org . eclipse . ui . internal . plug ##ina ##ction . run ##with ##eve ##nt ( plug ##ina ##ction . java : 255 ) at org . eclipse . j ##face . action . action ##con ##tri ##bution ##ite ##m . handle ##wi ##dgets ##ele ##ction ( action ##con ##tri ##bution ##ite ##m . java : 54 ##7 ) at org . eclipse . j ##face . action . action ##con ##tri ##bution ##ite ##m . access $ 4 ( action ##con ##tri ##bution ##ite ##m . java : 49 ##4 ) at org . eclipse . j ##face . action . action ##con ##tri ##bution ##ite ##m $ 6 . handle ##eve ##nt ( action ##con ##tri ##bution ##ite ##m . java : 46 ##6 ) at org . eclipse . sw ##t . wi ##dgets . event ##table . send ##eve ##nt ( event ##table . java : 82 ) at org . eclipse . sw ##t . wi ##dgets . wi ##dget . send ##eve ##nt ( wi ##dget . java : 84 ##7 ) at org . eclipse . sw ##t . wi ##dgets . display . run ##de ##fer ##red ##eve ##nts ( display . java : 217 ##3 ) at org . eclipse . sw ##t . wi ##dgets . display . read ##and ##dis ##pa ##tch ( display . java : 1863 ) at org . eclipse . ui . internal . work ##ben ##ch . rune ##vent ##lo ##op ( work ##ben ##ch . java : 157 ##9 ) at org . eclipse . ui . internal . work ##ben ##ch . run ( work ##ben ##ch . java : 156 ##2 ) at org . eclipse . core . internal . boot . internal ##boot ##load ##er . run ( internal ##boot ##load ##er . java : 85 ##8 ) at org . eclipse . core . boot . boot ##load ##er . run ( boot ##load ##er . java : 46 ##1 ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ##0 ( native method ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 39 ) at sun . reflect . del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 25 ) at java . lang . reflect . method . in ##vo ##ke ( method . java : 324 ) at org . eclipse . core . launcher . main . basic ##run ( main . java : 299 ) at org . eclipse . core . launcher . main . run ( main . java : 76 ##5 ) at org . eclipse . core . launcher . main . main ( main . java : 59 ##9 ) \" v ##m thread \" pri ##o = 5 ti ##d = 0 ##x ##00 ##9 ##5 ##cd ##f ##0 ni ##d = 0 ##x ##4 ##ac run ##nable \" v ##m periodic task thread \" pri ##o = 10 ti ##d = 0 ##x ##00 ##23 ##e ##7 ##b ##8 ni ##d = 0 ##x ##7 ##6 ##c waiting on condition \" suspend check ##er thread \" pri ##o = 10 ti ##d = 0 ##x ##00 ##9 ##20 ##7 ##30 ni ##d = 0 ##x ##36 ##c run ##nable [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  3857,  1045, 28332, 21486,  2692, 17465,  1011,  6476,\n",
       "         2125,  1037, 25416, 21898,  1999,  1996,  2444, 26351,  3193,\n",
       "         1011,   102]),\n",
       " 'dup_id': '45306',\n",
       " 'issue_id': 45330,\n",
       " 'priority': '1\\n',\n",
       " 'product': '181\\n',\n",
       " 'resolution': 'DUPLICATE',\n",
       " 'textual_token': array([  101,  2757,  7878,  2725,  5672,  2076,  4281, 26351,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  3857,  1045, 28332, 21486,  2692, 17465,\n",
       "         1011,  6476,  2125,  1037, 25416, 21898,  1999,  1996,  2444,\n",
       "        26351,  3193,  1011,   102]),\n",
       " 'title': '[CLS] dead ##lock doing replace during background sync [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  2757,  7878,  2725,  5672,  2076,  4281, 26351,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '45\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    \n",
    "    for bug_id in batch_bugs:\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug)\n",
    "\n",
    "    batch_features['title'] = np.array(batch_features['title'])\n",
    "    batch_features['desc'] = np.array(batch_features['desc'])\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 498 ms, sys: 54 µs, total: 498 ms\n",
      "Wall time: 498 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title'], \n",
    "             valid_input_sample['description'],\n",
    "            valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 1682), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 16995)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 21175'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c196162c92340818a2c4837f80df3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3955bf2ee454745bd050fdadd771cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 36s, sys: 3.81 s, total: 1min 40s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, TimeDistributed\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 75\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Concatenate()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(50))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "#     return mask_positives\n",
    "\n",
    "   # pos \n",
    "    embed_pos = tf.matmul(mask_positives_centroids, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "    # negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "\n",
    "#     return mask_positives_centroids\n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    batch_centroid_matrix_neg = tf.Variable([])\n",
    "    batch_centroid_matrix_all = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all):\n",
    "        # centroid pos\n",
    "        mask_positives_batch = tf.reshape(tf.gather(mask_positives, i), (-1, 1))\n",
    "        centroid_pos = tf.gather(centroid_embed_pos, i)\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_pos], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_pos = mask_positives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_positives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_pos, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid neg\n",
    "        centroid_neg = tf.gather(centroid_embed_neg, i)\n",
    "        mask_negatives_batch = tf.reshape(tf.gather(mask_negatives, i), (-1, 1))\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_neg], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_neg = mask_negatives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_negatives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_neg, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix_neg = tf.concat([batch_centroid_matrix_neg, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid pos and neg\n",
    "        new_batch_centroids = tf.reduce_sum([new_batch_centroid_pos, new_batch_centroid_neg], axis=0, keepdims=True)[0]\n",
    "        vects_new_batch_centroids = tf.concat([labels_pos, new_batch_centroids], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch_centroids)\n",
    "        batch_centroid_matrix_all = tf.concat([batch_centroid_matrix_all, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all]\n",
    "    _, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all = tf.while_loop(lambda i, a, b, c: i<(batch_size // 3) // 2, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])])\n",
    "\n",
    "    TL_anchor_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    \n",
    "#     tl_weights = tf.truediv(num_of_pos, tf.reduce_max(num_of_pos))\n",
    "#     tl_w = tl_weights # tf.random_uniform_initializer(minval=0.0, maxval=tl_weights)(shape=[1])\n",
    "#     TL_pos_weighted = tf.reshape(batch_centroid_matrix, (-1, 1)) * tl_w\n",
    "#     TL_pos = tf.truediv(tf.reduce_sum(TL_pos_weighted), tf.reduce_sum(tl_w))\n",
    "\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL_neg = tf.reduce_mean(batch_centroid_matrix_neg) #triplet_loss(vects_neg)\n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix_all) # triplet_loss(vects_centroids)\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w]) #  \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, \n",
    "                    TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "        \n",
    "    output = Lambda(quintet_loss, name='quintet_loss')(output)\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    # optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                         TL, TL_pos, TL_neg, TL_centroid]) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'DMS_QL_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-34-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581804100   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581850792   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (9,)                 0           concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,164,159,792\n",
      "Trainable params: 958,392\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n",
      "Total of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "                \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "              \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], \n",
    "                                          h[4], h[5], h[6], h[7], h[8], recall))\n",
    "    else:\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = similarity_model.get_layer('concatenate_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=triplet_loss_adapted_from_tf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'DMS_QL_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "       \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "        \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "      \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exported_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some ideas to visualizate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
