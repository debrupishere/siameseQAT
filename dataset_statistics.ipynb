{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database statisctics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(df):\n",
    "    print(\"Creating the buckets...\")\n",
    "    buckets = {}\n",
    "    # Reading the buckets\n",
    "    df_buckets = df[df['dup_id'] == '[]']\n",
    "    loop = tqdm(total=df_buckets.shape[0])\n",
    "    for row in df_buckets.iterrows():\n",
    "        name = row[1]['bug_id']\n",
    "        buckets[name] = set()\n",
    "        buckets[name].add(name)\n",
    "        loop.update(1)\n",
    "    loop.close()\n",
    "    # Fill the buckets\n",
    "    df_duplicates = df[df['dup_id'] != '[]']\n",
    "    loop = tqdm(total=df_duplicates.shape[0])\n",
    "    for row_bug_id, row_dup_id in df_duplicates[['bug_id', 'dup_id']].values:\n",
    "        bucket_name = int(row_dup_id)\n",
    "        dup_id = row_bug_id\n",
    "        while bucket_name not in buckets:\n",
    "            query = df_duplicates[df_duplicates['bug_id'] == bucket_name]\n",
    "            if query.shape[0] <= 0: \n",
    "                break\n",
    "            bucket_name = int(query['dup_id'])\n",
    "        '''\n",
    "            Some bugs duplicates point to one master that\n",
    "            does not exist in the dataset like openoffice master=152778\n",
    "        '''\n",
    "        if bucket_name in buckets:\n",
    "            buckets[bucket_name].add(dup_id)\n",
    "        loop.update(1)\n",
    "    loop.close()\n",
    "    return buckets\n",
    "\n",
    "def read_pairs(file_path):\n",
    "    n = 0\n",
    "    with open(file_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            pairs = row.split(' ')\n",
    "            n += len(pairs) // 2\n",
    "    return n\n",
    "\n",
    "def getting_pairs(array):\n",
    "    res = []\n",
    "    for row in array:\n",
    "        _, dups = row\n",
    "        dups = list(dups)\n",
    "        while len(dups) > 1:\n",
    "            bucket = dups[0]\n",
    "            dups.remove(bucket)\n",
    "            for d in dups:\n",
    "                res.append([bucket, d])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['eclipse', 'netbeans', 'openoffice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>train_pairs</th>\n",
       "      <th>test_pairs</th>\n",
       "      <th>total_bugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eclipse</td>\n",
       "      <td>79073</td>\n",
       "      <td>7591</td>\n",
       "      <td>361006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>netbeans</td>\n",
       "      <td>87543</td>\n",
       "      <td>7930</td>\n",
       "      <td>216715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openoffice</td>\n",
       "      <td>53740</td>\n",
       "      <td>4549</td>\n",
       "      <td>98070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       domain train_pairs test_pairs total_bugs\n",
       "0     eclipse       79073       7591     361006\n",
       "1    netbeans       87543       7930     216715\n",
       "2  openoffice       53740       4549      98070"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['domain', 'train_pairs', 'test_pairs', 'total_bugs'])\n",
    "\n",
    "rows = []\n",
    "\n",
    "for DOMAIN in domains:\n",
    "    METHOD = 'baseline'\n",
    "    DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "    DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "    TRAIN = os.path.join(DIR, 'train.txt')\n",
    "    TEST = os.path.join(DIR, 'test.txt')\n",
    "    \n",
    "    dataset = pd.read_csv(DATASET)\n",
    "    \n",
    "    n_train = read_pairs(TRAIN)\n",
    "    n_test = read_pairs(TEST)\n",
    "    \n",
    "    rows.append({ 'domain' : DOMAIN, 'train_pairs' : n_train, 'test_pairs' : n_test, 'total_bugs' :  dataset.shape[0] })\n",
    "    \n",
    "df.append(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating split 90% train and 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0d7aaa85f040a1a806fb590d3af223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce0a5154b58432d816182e1072e136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train and test created\n",
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466c70dc52b54459ac29212ef7978948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38a7cab8ae64453b43b3fdf0eb7724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train and test created\n",
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a2cf135831430dadb5d1ef4c2308a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83503), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5dbd463f2e48088a2635b94be7764b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train and test created\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for DOMAIN in domains:\n",
    "    METHOD = 'baseline'\n",
    "    DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "    DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "    \n",
    "    dataset = pd.read_csv(DATASET)\n",
    "    buckets = create_bucket(dataset)\n",
    "    bucket_dups = []\n",
    "\n",
    "    for key in buckets:\n",
    "        if len(buckets[key]) > 1:\n",
    "            bucket_dups.append([key, list(buckets[key])])\n",
    "            \n",
    "    pairs = getting_pairs(bucket_dups)\n",
    "    \n",
    "    VALIDATION_SPLIT = 0.9\n",
    "    split_idx = int(len(pairs) * VALIDATION_SPLIT)\n",
    "\n",
    "    with open(os.path.join(DIR, 'train_chronological.txt'), 'w') as f:\n",
    "        for pair in pairs[:split_idx]:\n",
    "            f.write(\"{} {}\\n\".format(pair[0], pair[1]))\n",
    "\n",
    "    test_data = {}\n",
    "    for pair in pairs[split_idx:]:\n",
    "        bug1 = int(pair[0])\n",
    "        bug2 = int(pair[1])\n",
    "        if bug1 not in test_data:\n",
    "            test_data[bug1] = set()\n",
    "        test_data[bug1].add(bug2)\n",
    "    with open(os.path.join(DIR, 'test_chronological.txt'), 'w') as f:\n",
    "        for bug in test_data.keys():\n",
    "            f.write(\"{} {}\\n\".format(bug, ' '.join([str(x) for x in test_data[bug]])))\n",
    "    print('Train and test created')\n",
    "    \n",
    "    n_train = len(pairs[:split_idx])\n",
    "    n_test = len(pairs[split_idx:])\n",
    "    \n",
    "    rows.append({ 'domain' : DOMAIN, 'train_pairs' : n_train, 'test_pairs' : n_test, 'total_bugs' :  dataset.shape[0] })\n",
    "    \n",
    "df = pd.DataFrame(data=rows, columns=['domain', 'train_pairs', 'test_pairs', 'total_bugs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>train_pairs</th>\n",
       "      <th>test_pairs</th>\n",
       "      <th>total_bugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eclipse</td>\n",
       "      <td>78182</td>\n",
       "      <td>8687</td>\n",
       "      <td>361006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>netbeans</td>\n",
       "      <td>85355</td>\n",
       "      <td>9484</td>\n",
       "      <td>216715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openoffice</td>\n",
       "      <td>52020</td>\n",
       "      <td>5781</td>\n",
       "      <td>98070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       domain  train_pairs  test_pairs  total_bugs\n",
       "0     eclipse        78182        8687      361006\n",
       "1    netbeans        85355        9484      216715\n",
       "2  openoffice        52020        5781       98070"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a single split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = 'netbeans'\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "dataset = pd.read_csv(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85dbf60745247f1801ae557cb88666f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311d9cdb349a479797fc63af0e7dea34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buckets = create_bucket(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180483"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dups = []\n",
    "\n",
    "for key in buckets:\n",
    "    if len(buckets[key]) > 1:\n",
    "        bucket_dups.append([key, list(buckets[key])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18602"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bucket_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = getting_pairs(bucket_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[90024, 1289],\n",
       " [1408, 6256],\n",
       " [1787, 14975],\n",
       " [166804, 2020],\n",
       " [2337, 31362],\n",
       " [2337, 46020],\n",
       " [2337, 15205],\n",
       " [2337, 32942],\n",
       " [2337, 35023],\n",
       " [2337, 57495]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94839"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 85355\n",
      "Test: 9484\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: {}\".format(len(pairs[:split_idx])))\n",
    "print(\"Test: {}\".format(len(pairs[split_idx:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test created\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.9\n",
    "split_idx = int(len(pairs) * VALIDATION_SPLIT)\n",
    "\n",
    "with open(os.path.join(DIR, 'train_chronological.txt'), 'w') as f:\n",
    "    for pair in pairs[:split_idx]:\n",
    "        f.write(\"{} {}\\n\".format(pair[0], pair[1]))\n",
    "        \n",
    "test_data = {}\n",
    "for pair in pairs[split_idx:]:\n",
    "    bug1 = int(pair[0])\n",
    "    bug2 = int(pair[1])\n",
    "    if bug1 not in test_data:\n",
    "        test_data[bug1] = set()\n",
    "    test_data[bug1].add(bug2)\n",
    "with open(os.path.join(DIR, 'test_chronological.txt'), 'w') as f:\n",
    "    for bug in test_data.keys():\n",
    "        f.write(\"{} {}\\n\".format(bug, ' '.join([str(x) for x in test_data[bug]])))\n",
    "print('Train and test created')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
