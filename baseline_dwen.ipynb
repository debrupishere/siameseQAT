{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'netbeans'\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216715"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06b454f21744d3eb88e400ce3109c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea53c6ebd1e4d8f8a7a11f71ab047b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 37.7 s, sys: 11.4 s, total: 49.1 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f4ae5488bf4637af7a24ff396e272b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23370, 26780],\n",
       " [103239, 105067],\n",
       " [61954, 73016],\n",
       " [204317, 202674],\n",
       " [220178, 220147],\n",
       " [195089, 195269],\n",
       " [221186, 219028],\n",
       " [102315, 105397],\n",
       " [196611, 193682],\n",
       " [50448, 50450]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '0\\n',\n",
       " 'creation_ts': '2009-11-11 05:56:00 +0000',\n",
       " 'delta_ts': '2009-11-11 06:13:37 +0000',\n",
       " 'description': \"[CLS] net ##be ##ans id ##e 6 . 8 beta ( build 2009 ##10 ##21 ##200 ##1 ) when you search for a string in the project a window with results is shown , but when you double click on the found item , id ##e won ' t bring you to the editor on the found line of the item but just to the beginning of the document [SEP]\",\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  5658,  4783,  6962,  8909,  2063,  1020,  1012,  1022,\n",
       "         8247,  1006,  3857,  2268, 10790, 17465, 28332,  2487,  1007,\n",
       "         2043,   102]),\n",
       " 'dup_id': '175578',\n",
       " 'issue_id': 176517,\n",
       " 'priority': '1\\n',\n",
       " 'product': '13\\n',\n",
       " 'resolution': 'DUPLICATE',\n",
       " 'textual_token': array([  101,  3313, 11562,  2006,  9022,  8875,  2987,  1005,  1056,\n",
       "         3288,  2017,  2000,  1996,  3559,     0,     0,     0,     0,\n",
       "            0,   102,   101,  5658,  4783,  6962,  8909,  2063,  1020,\n",
       "         1012,  1022,  8247,  1006,  3857,  2268, 10790, 17465, 28332,\n",
       "         2487,  1007,  2043,   102]),\n",
       " 'title': \"[CLS] double click on searched item doesn ' t bring you to the editor [SEP]\",\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  3313, 11562,  2006,  9022,  8875,  2987,  1005,  1056,\n",
       "         3288,  2017,  2000,  1996,  3559,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '17\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 34596)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171396"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(list(issues_by_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110647 in experiment.baseline.bug_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 760 ms, sys: 2.99 ms, total: 763 ms\n",
      "Wall time: 763 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = experiment.batch_iterator(None, \n",
    "                                                                                          baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1,\n",
    "                                                                                          issues_by_buckets)\n",
    "\n",
    "pos = np.full((1, batch_size_test), 1)\n",
    "neg = np.full((1, batch_size_test), 0)\n",
    "valid_sim = np.concatenate([pos, neg], -1)[0]\n",
    "\n",
    "valid_title_sample_a = np.concatenate([valid_input_sample['title'], valid_input_sample['title']], 0)\n",
    "valid_title_sample_b = np.concatenate([valid_input_pos['title'], valid_input_neg['title']], 0)\n",
    "valid_desc_sample_a = np.concatenate([valid_input_sample['description'], valid_input_sample['description']], 0)\n",
    "valid_desc_sample_b = np.concatenate([valid_input_pos['description'], valid_input_neg['description']], 0)\n",
    "\n",
    "validation_sample = [valid_title_sample_a, valid_title_sample_b, valid_desc_sample_a, valid_desc_sample_b]\n",
    "\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_title_sample_a), len(valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (256,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 19061'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3935effbb545407a8e65ecddd2b4f820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa71a97d5141ab8e2c22eb71b4ae11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 34s, sys: 3.67 s, total: 1min 37s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### DWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, \\\n",
    "    Average, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import keras.backend as K\n",
    "\n",
    "def dwen_feature(title_feature_model, desc_feature_model, \\\n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    \n",
    "    # Embedding feature\n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    bug_d_feat = GlobalAveragePooling1D()(bug_d_feat)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = Average(name = 'merge_features_{}'.format(name))([bug_t_feat, bug_d_feat])\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model\n",
    "\n",
    "def dwen_model(bug_feature_output_a, bug_feature_output_b, name):\n",
    "    \n",
    "    inputs = np.concatenate([bug_feature_output_a.input, bug_feature_output_b.input], -1).tolist()\n",
    "    \n",
    "    bug_feature_output_a = bug_feature_output_a.output\n",
    "    bug_feature_output_b = bug_feature_output_b.output\n",
    "    \n",
    "    # 2D concatenate feature\n",
    "    bug_feature_output = concatenate([bug_feature_output_a, bug_feature_output_b])\n",
    "    \n",
    "    hidden_layers = 2\n",
    "    \n",
    "    # Deep Hidden MLPs\n",
    "    for _ in range(hidden_layers):\n",
    "        number_of_units = K.int_shape(bug_feature_output)[1]\n",
    "        bug_feature_output = Dense(number_of_units // 2)(bug_feature_output)\n",
    "#         bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "        bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "        #bug_feature_output = Dropout(.5)(bug_feature_output)\n",
    "    \n",
    "     # Sigmoid\n",
    "    output = Dense(1, activation='sigmoid')(bug_feature_output)\n",
    "\n",
    "    similarity_model = Model(inputs=inputs, outputs=[output], name = 'dwen_output')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'baseline_dwen_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      580966500   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      580966500   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,162,158,601\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,161,933,000\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.69, Loss_test: 0.69, acc: 0.52, acc_test: 0.49\n",
      "Epoch: 2 Loss: 0.69, Loss_test: 0.69, acc: 0.53, acc_test: 0.50\n",
      "Epoch: 3 Loss: 0.69, Loss_test: 0.69, acc: 0.52, acc_test: 0.50\n",
      "Epoch: 4 Loss: 0.68, Loss_test: 0.69, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 5 Loss: 0.68, Loss_test: 0.69, acc: 0.61, acc_test: 0.51\n",
      "Epoch: 6 Loss: 0.67, Loss_test: 0.69, acc: 0.66, acc_test: 0.51\n",
      "Epoch: 7 Loss: 0.69, Loss_test: 0.69, acc: 0.54, acc_test: 0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 0.65, Loss_test: 0.68, acc: 0.72, acc_test: 0.52\n",
      "Epoch: 9 Loss: 0.66, Loss_test: 0.68, acc: 0.70, acc_test: 0.52\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.64, Loss_test: 0.68, acc: 0.73, acc_test: 0.53\n",
      "Epoch: 11 Loss: 0.64, Loss_test: 0.68, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 12 Loss: 0.64, Loss_test: 0.68, acc: 0.72, acc_test: 0.52\n",
      "Epoch: 13 Loss: 0.65, Loss_test: 0.68, acc: 0.65, acc_test: 0.53\n",
      "Epoch: 14 Loss: 0.65, Loss_test: 0.68, acc: 0.66, acc_test: 0.53\n",
      "Epoch: 15 Loss: 0.64, Loss_test: 0.68, acc: 0.59, acc_test: 0.53\n",
      "Epoch: 16 Loss: 0.69, Loss_test: 0.68, acc: 0.55, acc_test: 0.54\n",
      "Epoch: 17 Loss: 0.62, Loss_test: 0.68, acc: 0.67, acc_test: 0.57\n",
      "Epoch: 18 Loss: 0.60, Loss_test: 0.68, acc: 0.74, acc_test: 0.54\n",
      "Epoch: 19 Loss: 0.59, Loss_test: 0.67, acc: 0.71, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.60, Loss_test: 0.68, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 21 Loss: 0.61, Loss_test: 0.68, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 22 Loss: 0.65, Loss_test: 0.68, acc: 0.60, acc_test: 0.57\n",
      "Epoch: 23 Loss: 0.60, Loss_test: 0.68, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 24 Loss: 0.56, Loss_test: 0.68, acc: 0.72, acc_test: 0.58\n",
      "Epoch: 25 Loss: 0.58, Loss_test: 0.68, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 26 Loss: 0.60, Loss_test: 0.68, acc: 0.68, acc_test: 0.58\n",
      "Epoch: 27 Loss: 0.51, Loss_test: 0.69, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 28 Loss: 0.61, Loss_test: 0.70, acc: 0.61, acc_test: 0.57\n",
      "Epoch: 29 Loss: 0.55, Loss_test: 0.70, acc: 0.70, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.59, Loss_test: 0.70, acc: 0.69, acc_test: 0.56\n",
      "Epoch: 31 Loss: 0.50, Loss_test: 0.71, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 32 Loss: 0.54, Loss_test: 0.70, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 33 Loss: 0.52, Loss_test: 0.70, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 34 Loss: 0.58, Loss_test: 0.71, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 35 Loss: 0.54, Loss_test: 0.73, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 36 Loss: 0.62, Loss_test: 0.73, acc: 0.65, acc_test: 0.56\n",
      "Epoch: 37 Loss: 0.55, Loss_test: 0.71, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 38 Loss: 0.58, Loss_test: 0.70, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 39 Loss: 0.54, Loss_test: 0.71, acc: 0.73, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.55, Loss_test: 0.73, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 41 Loss: 0.55, Loss_test: 0.76, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 42 Loss: 0.57, Loss_test: 0.78, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 43 Loss: 0.51, Loss_test: 0.79, acc: 0.70, acc_test: 0.56\n",
      "Epoch: 44 Loss: 0.55, Loss_test: 0.81, acc: 0.76, acc_test: 0.55\n",
      "Epoch: 45 Loss: 0.60, Loss_test: 0.81, acc: 0.66, acc_test: 0.55\n",
      "Epoch: 46 Loss: 0.53, Loss_test: 0.78, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 47 Loss: 0.52, Loss_test: 0.75, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 48 Loss: 0.61, Loss_test: 0.74, acc: 0.64, acc_test: 0.57\n",
      "Epoch: 49 Loss: 0.51, Loss_test: 0.75, acc: 0.71, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.50, Loss_test: 0.76, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 51 Loss: 0.52, Loss_test: 0.78, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 52 Loss: 0.47, Loss_test: 0.77, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 53 Loss: 0.48, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 54 Loss: 0.62, Loss_test: 0.71, acc: 0.61, acc_test: 0.56\n",
      "Epoch: 55 Loss: 0.49, Loss_test: 0.71, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 56 Loss: 0.44, Loss_test: 0.71, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 57 Loss: 0.56, Loss_test: 0.71, acc: 0.72, acc_test: 0.56\n",
      "Epoch: 58 Loss: 0.51, Loss_test: 0.72, acc: 0.76, acc_test: 0.56\n",
      "Epoch: 59 Loss: 0.54, Loss_test: 0.72, acc: 0.70, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.53, Loss_test: 0.73, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 61 Loss: 0.57, Loss_test: 0.73, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 62 Loss: 0.48, Loss_test: 0.72, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 63 Loss: 0.53, Loss_test: 0.72, acc: 0.69, acc_test: 0.57\n",
      "Epoch: 64 Loss: 0.55, Loss_test: 0.71, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 65 Loss: 0.52, Loss_test: 0.72, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 66 Loss: 0.57, Loss_test: 0.74, acc: 0.66, acc_test: 0.55\n",
      "Epoch: 67 Loss: 0.52, Loss_test: 0.75, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 68 Loss: 0.58, Loss_test: 0.75, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 69 Loss: 0.49, Loss_test: 0.72, acc: 0.73, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 71 Loss: 0.57, Loss_test: 0.72, acc: 0.71, acc_test: 0.53\n",
      "Epoch: 72 Loss: 0.50, Loss_test: 0.73, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 73 Loss: 0.40, Loss_test: 0.75, acc: 0.77, acc_test: 0.55\n",
      "Epoch: 74 Loss: 0.48, Loss_test: 0.77, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 75 Loss: 0.53, Loss_test: 0.77, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 76 Loss: 0.48, Loss_test: 0.75, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 77 Loss: 0.52, Loss_test: 0.74, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 78 Loss: 0.57, Loss_test: 0.73, acc: 0.68, acc_test: 0.54\n",
      "Epoch: 79 Loss: 0.50, Loss_test: 0.74, acc: 0.71, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.54, Loss_test: 0.75, acc: 0.67, acc_test: 0.56\n",
      "Epoch: 81 Loss: 0.51, Loss_test: 0.75, acc: 0.72, acc_test: 0.56\n",
      "Epoch: 82 Loss: 0.53, Loss_test: 0.74, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 83 Loss: 0.52, Loss_test: 0.73, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 84 Loss: 0.49, Loss_test: 0.72, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 85 Loss: 0.56, Loss_test: 0.73, acc: 0.68, acc_test: 0.54\n",
      "Epoch: 86 Loss: 0.49, Loss_test: 0.76, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 87 Loss: 0.50, Loss_test: 0.78, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 88 Loss: 0.53, Loss_test: 0.79, acc: 0.70, acc_test: 0.56\n",
      "Epoch: 89 Loss: 0.50, Loss_test: 0.77, acc: 0.68, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.57, Loss_test: 0.77, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 91 Loss: 0.52, Loss_test: 0.76, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 92 Loss: 0.59, Loss_test: 0.77, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 93 Loss: 0.51, Loss_test: 0.77, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 94 Loss: 0.53, Loss_test: 0.78, acc: 0.67, acc_test: 0.55\n",
      "Epoch: 95 Loss: 0.53, Loss_test: 0.77, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 96 Loss: 0.47, Loss_test: 0.75, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 97 Loss: 0.49, Loss_test: 0.73, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 98 Loss: 0.45, Loss_test: 0.72, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 99 Loss: 0.45, Loss_test: 0.71, acc: 0.79, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.48, Loss_test: 0.72, acc: 0.78, acc_tets: 0.58, recall@25: 0.23\n",
      "Best_epoch=73, Best_loss=0.40, Recall@25=0.23\n",
      "CPU times: user 1min 58s, sys: 4.37 s, total: 2min 2s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Similarity model\n",
    "bug_feature_output_a = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_a')\n",
    "bug_feature_output_b = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_b')\n",
    "similarity_model = dwen_model(bug_feature_output_a, bug_feature_output_b, 'dwen')\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, \n",
    "                                                        bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/netbeans/bert/exported_rank_baseline_dwen_100.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_100_feature_100epochs_64batch(netbeans).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      580966500   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      580966500   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,162,158,601\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,161,933,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('dense_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.54, Loss_test: 0.73, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 102 Loss: 0.47, Loss_test: 0.74, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 103 Loss: 0.55, Loss_test: 0.73, acc: 0.69, acc_test: 0.59\n",
      "Epoch: 104 Loss: 0.52, Loss_test: 0.73, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 105 Loss: 0.50, Loss_test: 0.73, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 106 Loss: 0.61, Loss_test: 0.72, acc: 0.65, acc_test: 0.59\n",
      "Epoch: 107 Loss: 0.55, Loss_test: 0.71, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 108 Loss: 0.51, Loss_test: 0.71, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 109 Loss: 0.48, Loss_test: 0.72, acc: 0.76, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.51, Loss_test: 0.73, acc: 0.68, acc_test: 0.59\n",
      "Epoch: 111 Loss: 0.48, Loss_test: 0.74, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 112 Loss: 0.48, Loss_test: 0.73, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 113 Loss: 0.51, Loss_test: 0.72, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 114 Loss: 0.48, Loss_test: 0.72, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 115 Loss: 0.50, Loss_test: 0.72, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 116 Loss: 0.50, Loss_test: 0.73, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 117 Loss: 0.49, Loss_test: 0.74, acc: 0.69, acc_test: 0.57\n",
      "Epoch: 118 Loss: 0.53, Loss_test: 0.74, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 119 Loss: 0.47, Loss_test: 0.75, acc: 0.75, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.53, Loss_test: 0.76, acc: 0.73, acc_test: 0.54\n",
      "Epoch: 121 Loss: 0.50, Loss_test: 0.74, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 122 Loss: 0.49, Loss_test: 0.72, acc: 0.74, acc_test: 0.56\n",
      "Epoch: 123 Loss: 0.47, Loss_test: 0.72, acc: 0.72, acc_test: 0.54\n",
      "Epoch: 124 Loss: 0.50, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 125 Loss: 0.56, Loss_test: 0.76, acc: 0.66, acc_test: 0.58\n",
      "Epoch: 126 Loss: 0.53, Loss_test: 0.77, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 127 Loss: 0.57, Loss_test: 0.74, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 128 Loss: 0.54, Loss_test: 0.72, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 129 Loss: 0.49, Loss_test: 0.71, acc: 0.76, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.49, Loss_test: 0.71, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 131 Loss: 0.51, Loss_test: 0.71, acc: 0.66, acc_test: 0.55\n",
      "Epoch: 132 Loss: 0.51, Loss_test: 0.72, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 133 Loss: 0.47, Loss_test: 0.73, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 134 Loss: 0.51, Loss_test: 0.74, acc: 0.76, acc_test: 0.55\n",
      "Epoch: 135 Loss: 0.46, Loss_test: 0.73, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 136 Loss: 0.49, Loss_test: 0.73, acc: 0.76, acc_test: 0.55\n",
      "Epoch: 137 Loss: 0.48, Loss_test: 0.72, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 138 Loss: 0.55, Loss_test: 0.72, acc: 0.70, acc_test: 0.56\n",
      "Epoch: 139 Loss: 0.52, Loss_test: 0.71, acc: 0.71, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.53, Loss_test: 0.72, acc: 0.68, acc_test: 0.57\n",
      "Epoch: 141 Loss: 0.43, Loss_test: 0.72, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 142 Loss: 0.55, Loss_test: 0.71, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 143 Loss: 0.45, Loss_test: 0.71, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 144 Loss: 0.47, Loss_test: 0.71, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 145 Loss: 0.45, Loss_test: 0.72, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 146 Loss: 0.52, Loss_test: 0.72, acc: 0.68, acc_test: 0.59\n",
      "Epoch: 147 Loss: 0.39, Loss_test: 0.74, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 148 Loss: 0.43, Loss_test: 0.74, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 149 Loss: 0.48, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.51, Loss_test: 0.73, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 151 Loss: 0.49, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 152 Loss: 0.46, Loss_test: 0.73, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 153 Loss: 0.51, Loss_test: 0.74, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 154 Loss: 0.52, Loss_test: 0.76, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 155 Loss: 0.46, Loss_test: 0.79, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 156 Loss: 0.46, Loss_test: 0.82, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 157 Loss: 0.52, Loss_test: 0.80, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 158 Loss: 0.48, Loss_test: 0.77, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 159 Loss: 0.46, Loss_test: 0.75, acc: 0.76, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.56, Loss_test: 0.74, acc: 0.67, acc_test: 0.61\n",
      "Epoch: 161 Loss: 0.51, Loss_test: 0.73, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 162 Loss: 0.49, Loss_test: 0.72, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 163 Loss: 0.46, Loss_test: 0.72, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 164 Loss: 0.44, Loss_test: 0.72, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 165 Loss: 0.51, Loss_test: 0.72, acc: 0.73, acc_test: 0.63\n",
      "Epoch: 166 Loss: 0.53, Loss_test: 0.71, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 167 Loss: 0.50, Loss_test: 0.71, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 168 Loss: 0.48, Loss_test: 0.72, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 169 Loss: 0.45, Loss_test: 0.73, acc: 0.79, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 171 Loss: 0.55, Loss_test: 0.71, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 172 Loss: 0.48, Loss_test: 0.72, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 173 Loss: 0.44, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 174 Loss: 0.47, Loss_test: 0.74, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 175 Loss: 0.48, Loss_test: 0.75, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 176 Loss: 0.46, Loss_test: 0.74, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 177 Loss: 0.49, Loss_test: 0.73, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 178 Loss: 0.45, Loss_test: 0.73, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 179 Loss: 0.46, Loss_test: 0.74, acc: 0.77, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.52, Loss_test: 0.75, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 181 Loss: 0.43, Loss_test: 0.76, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 182 Loss: 0.45, Loss_test: 0.77, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 183 Loss: 0.49, Loss_test: 0.79, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 184 Loss: 0.50, Loss_test: 0.78, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 185 Loss: 0.44, Loss_test: 0.77, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 186 Loss: 0.48, Loss_test: 0.76, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 187 Loss: 0.47, Loss_test: 0.75, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 188 Loss: 0.46, Loss_test: 0.75, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 189 Loss: 0.51, Loss_test: 0.75, acc: 0.71, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.46, Loss_test: 0.76, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 191 Loss: 0.52, Loss_test: 0.76, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 192 Loss: 0.47, Loss_test: 0.75, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 193 Loss: 0.49, Loss_test: 0.74, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 194 Loss: 0.39, Loss_test: 0.74, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 195 Loss: 0.47, Loss_test: 0.75, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 196 Loss: 0.42, Loss_test: 0.75, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 197 Loss: 0.48, Loss_test: 0.75, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 198 Loss: 0.50, Loss_test: 0.77, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 199 Loss: 0.39, Loss_test: 0.78, acc: 0.83, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.48, Loss_test: 0.76, acc: 0.72, acc_test: 0.58\n",
      "Epoch: 201 Loss: 0.40, Loss_test: 0.75, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 202 Loss: 0.54, Loss_test: 0.75, acc: 0.69, acc_test: 0.55\n",
      "Epoch: 203 Loss: 0.47, Loss_test: 0.76, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 204 Loss: 0.48, Loss_test: 0.79, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 205 Loss: 0.45, Loss_test: 0.80, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 206 Loss: 0.40, Loss_test: 0.80, acc: 0.84, acc_test: 0.56\n",
      "Epoch: 207 Loss: 0.45, Loss_test: 0.76, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 208 Loss: 0.46, Loss_test: 0.74, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 209 Loss: 0.43, Loss_test: 0.73, acc: 0.77, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.42, Loss_test: 0.73, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 211 Loss: 0.40, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 212 Loss: 0.43, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 213 Loss: 0.50, Loss_test: 0.73, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 214 Loss: 0.44, Loss_test: 0.73, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 215 Loss: 0.56, Loss_test: 0.73, acc: 0.66, acc_test: 0.60\n",
      "Epoch: 216 Loss: 0.48, Loss_test: 0.74, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 217 Loss: 0.50, Loss_test: 0.75, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 218 Loss: 0.53, Loss_test: 0.74, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 219 Loss: 0.49, Loss_test: 0.73, acc: 0.76, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.38, Loss_test: 0.74, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 221 Loss: 0.40, Loss_test: 0.76, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 222 Loss: 0.51, Loss_test: 0.76, acc: 0.73, acc_test: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 Loss: 0.39, Loss_test: 0.75, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 224 Loss: 0.39, Loss_test: 0.73, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 225 Loss: 0.44, Loss_test: 0.72, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 226 Loss: 0.50, Loss_test: 0.72, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 227 Loss: 0.47, Loss_test: 0.72, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 228 Loss: 0.50, Loss_test: 0.73, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 229 Loss: 0.47, Loss_test: 0.75, acc: 0.73, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.43, Loss_test: 0.75, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 231 Loss: 0.44, Loss_test: 0.75, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 232 Loss: 0.39, Loss_test: 0.75, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 233 Loss: 0.49, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 234 Loss: 0.49, Loss_test: 0.76, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 235 Loss: 0.49, Loss_test: 0.77, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 236 Loss: 0.44, Loss_test: 0.76, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 237 Loss: 0.50, Loss_test: 0.76, acc: 0.70, acc_test: 0.61\n",
      "Epoch: 238 Loss: 0.39, Loss_test: 0.77, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 239 Loss: 0.44, Loss_test: 0.80, acc: 0.76, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.41, Loss_test: 0.81, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 241 Loss: 0.42, Loss_test: 0.80, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 242 Loss: 0.51, Loss_test: 0.78, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 243 Loss: 0.46, Loss_test: 0.76, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 244 Loss: 0.50, Loss_test: 0.75, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 245 Loss: 0.45, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 246 Loss: 0.43, Loss_test: 0.80, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 247 Loss: 0.44, Loss_test: 0.83, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 248 Loss: 0.55, Loss_test: 0.82, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 249 Loss: 0.43, Loss_test: 0.77, acc: 0.78, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.44, Loss_test: 0.74, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 251 Loss: 0.41, Loss_test: 0.74, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 252 Loss: 0.50, Loss_test: 0.74, acc: 0.71, acc_test: 0.60\n",
      "Epoch: 253 Loss: 0.44, Loss_test: 0.75, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 254 Loss: 0.48, Loss_test: 0.76, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 255 Loss: 0.41, Loss_test: 0.77, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 256 Loss: 0.45, Loss_test: 0.76, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 257 Loss: 0.46, Loss_test: 0.73, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 258 Loss: 0.39, Loss_test: 0.73, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 259 Loss: 0.46, Loss_test: 0.72, acc: 0.79, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.45, Loss_test: 0.72, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 261 Loss: 0.49, Loss_test: 0.73, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 262 Loss: 0.33, Loss_test: 0.76, acc: 0.90, acc_test: 0.59\n",
      "Epoch: 263 Loss: 0.36, Loss_test: 0.78, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 264 Loss: 0.47, Loss_test: 0.77, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 265 Loss: 0.49, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 266 Loss: 0.48, Loss_test: 0.75, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 267 Loss: 0.42, Loss_test: 0.75, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 268 Loss: 0.43, Loss_test: 0.76, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 269 Loss: 0.49, Loss_test: 0.77, acc: 0.74, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.46, Loss_test: 0.80, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 271 Loss: 0.49, Loss_test: 0.84, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 272 Loss: 0.52, Loss_test: 0.82, acc: 0.69, acc_test: 0.60\n",
      "Epoch: 273 Loss: 0.37, Loss_test: 0.79, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 274 Loss: 0.47, Loss_test: 0.77, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 275 Loss: 0.45, Loss_test: 0.76, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 276 Loss: 0.46, Loss_test: 0.76, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 277 Loss: 0.39, Loss_test: 0.76, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 278 Loss: 0.43, Loss_test: 0.76, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 279 Loss: 0.38, Loss_test: 0.76, acc: 0.84, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.44, Loss_test: 0.76, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 281 Loss: 0.42, Loss_test: 0.76, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 282 Loss: 0.46, Loss_test: 0.76, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 283 Loss: 0.50, Loss_test: 0.75, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 284 Loss: 0.50, Loss_test: 0.74, acc: 0.72, acc_test: 0.58\n",
      "Epoch: 285 Loss: 0.51, Loss_test: 0.75, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 286 Loss: 0.40, Loss_test: 0.75, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 287 Loss: 0.44, Loss_test: 0.78, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 288 Loss: 0.45, Loss_test: 0.81, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 289 Loss: 0.42, Loss_test: 0.83, acc: 0.80, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.50, Loss_test: 0.81, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 291 Loss: 0.38, Loss_test: 0.77, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 292 Loss: 0.38, Loss_test: 0.76, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 293 Loss: 0.43, Loss_test: 0.76, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 294 Loss: 0.38, Loss_test: 0.76, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 295 Loss: 0.41, Loss_test: 0.75, acc: 0.81, acc_test: 0.57\n",
      "Epoch: 296 Loss: 0.44, Loss_test: 0.75, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 297 Loss: 0.37, Loss_test: 0.76, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 298 Loss: 0.49, Loss_test: 0.77, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 299 Loss: 0.49, Loss_test: 0.77, acc: 0.75, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.44, Loss_test: 0.76, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 301 Loss: 0.47, Loss_test: 0.76, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 302 Loss: 0.48, Loss_test: 0.76, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 303 Loss: 0.46, Loss_test: 0.77, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 304 Loss: 0.36, Loss_test: 0.78, acc: 0.86, acc_test: 0.58\n",
      "Epoch: 305 Loss: 0.45, Loss_test: 0.81, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 306 Loss: 0.48, Loss_test: 0.82, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 307 Loss: 0.43, Loss_test: 0.82, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 308 Loss: 0.47, Loss_test: 0.79, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 309 Loss: 0.48, Loss_test: 0.76, acc: 0.75, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.46, Loss_test: 0.76, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 311 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 312 Loss: 0.41, Loss_test: 0.78, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 313 Loss: 0.49, Loss_test: 0.81, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 314 Loss: 0.41, Loss_test: 0.83, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 315 Loss: 0.44, Loss_test: 0.81, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 316 Loss: 0.48, Loss_test: 0.78, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 317 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 318 Loss: 0.46, Loss_test: 0.76, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 319 Loss: 0.43, Loss_test: 0.77, acc: 0.84, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.39, Loss_test: 0.77, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 321 Loss: 0.34, Loss_test: 0.78, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 322 Loss: 0.42, Loss_test: 0.78, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 323 Loss: 0.53, Loss_test: 0.77, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 324 Loss: 0.40, Loss_test: 0.77, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 325 Loss: 0.43, Loss_test: 0.77, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 326 Loss: 0.42, Loss_test: 0.78, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 327 Loss: 0.43, Loss_test: 0.80, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 328 Loss: 0.46, Loss_test: 0.83, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 329 Loss: 0.47, Loss_test: 0.84, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.40, Loss_test: 0.84, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 331 Loss: 0.44, Loss_test: 0.83, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 332 Loss: 0.38, Loss_test: 0.82, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 333 Loss: 0.42, Loss_test: 0.81, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 334 Loss: 0.39, Loss_test: 0.81, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 335 Loss: 0.47, Loss_test: 0.81, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 336 Loss: 0.57, Loss_test: 0.81, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 337 Loss: 0.47, Loss_test: 0.81, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 338 Loss: 0.44, Loss_test: 0.80, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 339 Loss: 0.56, Loss_test: 0.79, acc: 0.69, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.44, Loss_test: 0.80, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 341 Loss: 0.37, Loss_test: 0.80, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 342 Loss: 0.38, Loss_test: 0.80, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 343 Loss: 0.47, Loss_test: 0.79, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 344 Loss: 0.48, Loss_test: 0.79, acc: 0.77, acc_test: 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 Loss: 0.50, Loss_test: 0.79, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 346 Loss: 0.43, Loss_test: 0.78, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 347 Loss: 0.41, Loss_test: 0.77, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 348 Loss: 0.45, Loss_test: 0.77, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 349 Loss: 0.46, Loss_test: 0.76, acc: 0.77, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.47, Loss_test: 0.75, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 351 Loss: 0.43, Loss_test: 0.75, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 352 Loss: 0.38, Loss_test: 0.75, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 353 Loss: 0.42, Loss_test: 0.74, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 354 Loss: 0.39, Loss_test: 0.75, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 355 Loss: 0.48, Loss_test: 0.76, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 356 Loss: 0.39, Loss_test: 0.76, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 357 Loss: 0.44, Loss_test: 0.77, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 358 Loss: 0.40, Loss_test: 0.78, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 359 Loss: 0.48, Loss_test: 0.78, acc: 0.73, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.44, Loss_test: 0.78, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 361 Loss: 0.41, Loss_test: 0.77, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 362 Loss: 0.38, Loss_test: 0.76, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 363 Loss: 0.49, Loss_test: 0.75, acc: 0.71, acc_test: 0.61\n",
      "Epoch: 364 Loss: 0.35, Loss_test: 0.74, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 365 Loss: 0.42, Loss_test: 0.74, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 366 Loss: 0.43, Loss_test: 0.74, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 367 Loss: 0.40, Loss_test: 0.73, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 368 Loss: 0.40, Loss_test: 0.73, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 369 Loss: 0.33, Loss_test: 0.74, acc: 0.88, acc_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.34, Loss_test: 0.76, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 371 Loss: 0.39, Loss_test: 0.79, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 372 Loss: 0.49, Loss_test: 0.79, acc: 0.76, acc_test: 0.62\n",
      "Epoch: 373 Loss: 0.39, Loss_test: 0.77, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 374 Loss: 0.33, Loss_test: 0.75, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 375 Loss: 0.42, Loss_test: 0.74, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 376 Loss: 0.44, Loss_test: 0.75, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 377 Loss: 0.45, Loss_test: 0.76, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 378 Loss: 0.40, Loss_test: 0.79, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 379 Loss: 0.43, Loss_test: 0.78, acc: 0.82, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.41, Loss_test: 0.75, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 381 Loss: 0.37, Loss_test: 0.75, acc: 0.86, acc_test: 0.58\n",
      "Epoch: 382 Loss: 0.45, Loss_test: 0.75, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 383 Loss: 0.40, Loss_test: 0.76, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 384 Loss: 0.45, Loss_test: 0.77, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 385 Loss: 0.49, Loss_test: 0.76, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 386 Loss: 0.44, Loss_test: 0.76, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 387 Loss: 0.41, Loss_test: 0.76, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 388 Loss: 0.41, Loss_test: 0.77, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 389 Loss: 0.40, Loss_test: 0.78, acc: 0.81, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.42, Loss_test: 0.78, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 391 Loss: 0.42, Loss_test: 0.79, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 392 Loss: 0.43, Loss_test: 0.80, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 393 Loss: 0.39, Loss_test: 0.80, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 394 Loss: 0.43, Loss_test: 0.81, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 395 Loss: 0.37, Loss_test: 0.81, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 396 Loss: 0.37, Loss_test: 0.81, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 397 Loss: 0.45, Loss_test: 0.81, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 398 Loss: 0.33, Loss_test: 0.81, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 399 Loss: 0.43, Loss_test: 0.81, acc: 0.76, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.38, Loss_test: 0.81, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 401 Loss: 0.41, Loss_test: 0.81, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 402 Loss: 0.45, Loss_test: 0.79, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 403 Loss: 0.40, Loss_test: 0.78, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 404 Loss: 0.37, Loss_test: 0.78, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 405 Loss: 0.42, Loss_test: 0.78, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 406 Loss: 0.39, Loss_test: 0.78, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 407 Loss: 0.43, Loss_test: 0.77, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 408 Loss: 0.43, Loss_test: 0.77, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 409 Loss: 0.47, Loss_test: 0.77, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.35, Loss_test: 0.78, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 411 Loss: 0.46, Loss_test: 0.77, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 412 Loss: 0.43, Loss_test: 0.77, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 413 Loss: 0.42, Loss_test: 0.78, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 414 Loss: 0.38, Loss_test: 0.78, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 415 Loss: 0.40, Loss_test: 0.78, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 416 Loss: 0.42, Loss_test: 0.77, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 417 Loss: 0.36, Loss_test: 0.78, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 418 Loss: 0.41, Loss_test: 0.79, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 419 Loss: 0.43, Loss_test: 0.79, acc: 0.78, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.42, Loss_test: 0.82, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 421 Loss: 0.35, Loss_test: 0.87, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 422 Loss: 0.34, Loss_test: 0.88, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 423 Loss: 0.40, Loss_test: 0.87, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 424 Loss: 0.46, Loss_test: 0.84, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 425 Loss: 0.37, Loss_test: 0.81, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 426 Loss: 0.44, Loss_test: 0.79, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 427 Loss: 0.35, Loss_test: 0.78, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 428 Loss: 0.35, Loss_test: 0.78, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 429 Loss: 0.39, Loss_test: 0.78, acc: 0.82, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.39, Loss_test: 0.78, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 431 Loss: 0.39, Loss_test: 0.79, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 432 Loss: 0.56, Loss_test: 0.78, acc: 0.69, acc_test: 0.60\n",
      "Epoch: 433 Loss: 0.42, Loss_test: 0.78, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 434 Loss: 0.44, Loss_test: 0.77, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 435 Loss: 0.49, Loss_test: 0.77, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 436 Loss: 0.42, Loss_test: 0.76, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 437 Loss: 0.37, Loss_test: 0.77, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 438 Loss: 0.43, Loss_test: 0.76, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 439 Loss: 0.46, Loss_test: 0.76, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.41, Loss_test: 0.76, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 441 Loss: 0.41, Loss_test: 0.76, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 442 Loss: 0.40, Loss_test: 0.77, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 443 Loss: 0.40, Loss_test: 0.77, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 444 Loss: 0.39, Loss_test: 0.77, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 445 Loss: 0.42, Loss_test: 0.77, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 446 Loss: 0.43, Loss_test: 0.77, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 447 Loss: 0.35, Loss_test: 0.77, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 448 Loss: 0.37, Loss_test: 0.77, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 449 Loss: 0.37, Loss_test: 0.77, acc: 0.83, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.45, Loss_test: 0.78, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 451 Loss: 0.38, Loss_test: 0.80, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 452 Loss: 0.36, Loss_test: 0.82, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 453 Loss: 0.31, Loss_test: 0.83, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 454 Loss: 0.39, Loss_test: 0.82, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 455 Loss: 0.37, Loss_test: 0.82, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 456 Loss: 0.42, Loss_test: 0.83, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 457 Loss: 0.43, Loss_test: 0.83, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 458 Loss: 0.41, Loss_test: 0.83, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 459 Loss: 0.50, Loss_test: 0.84, acc: 0.75, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.32, Loss_test: 0.85, acc: 0.87, acc_test: 0.59\n",
      "Epoch: 461 Loss: 0.47, Loss_test: 0.85, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 462 Loss: 0.41, Loss_test: 0.86, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 463 Loss: 0.51, Loss_test: 0.85, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 464 Loss: 0.44, Loss_test: 0.86, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 465 Loss: 0.43, Loss_test: 0.85, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 466 Loss: 0.50, Loss_test: 0.82, acc: 0.73, acc_test: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 467 Loss: 0.53, Loss_test: 0.81, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 468 Loss: 0.41, Loss_test: 0.78, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 469 Loss: 0.49, Loss_test: 0.77, acc: 0.73, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.36, Loss_test: 0.77, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 471 Loss: 0.42, Loss_test: 0.77, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 472 Loss: 0.47, Loss_test: 0.77, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 473 Loss: 0.47, Loss_test: 0.76, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 474 Loss: 0.49, Loss_test: 0.76, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 475 Loss: 0.37, Loss_test: 0.76, acc: 0.81, acc_test: 0.56\n",
      "Epoch: 476 Loss: 0.40, Loss_test: 0.76, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 477 Loss: 0.41, Loss_test: 0.75, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 478 Loss: 0.42, Loss_test: 0.74, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 479 Loss: 0.38, Loss_test: 0.73, acc: 0.83, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.34, Loss_test: 0.73, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 481 Loss: 0.47, Loss_test: 0.72, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 482 Loss: 0.31, Loss_test: 0.73, acc: 0.90, acc_test: 0.60\n",
      "Epoch: 483 Loss: 0.34, Loss_test: 0.74, acc: 0.89, acc_test: 0.62\n",
      "Epoch: 484 Loss: 0.39, Loss_test: 0.74, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 485 Loss: 0.43, Loss_test: 0.74, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 486 Loss: 0.40, Loss_test: 0.74, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 487 Loss: 0.36, Loss_test: 0.75, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 488 Loss: 0.37, Loss_test: 0.76, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 489 Loss: 0.39, Loss_test: 0.77, acc: 0.84, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.32, Loss_test: 0.79, acc: 0.90, acc_test: 0.60\n",
      "Epoch: 491 Loss: 0.48, Loss_test: 0.79, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 492 Loss: 0.38, Loss_test: 0.80, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 493 Loss: 0.45, Loss_test: 0.80, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 494 Loss: 0.49, Loss_test: 0.81, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 495 Loss: 0.34, Loss_test: 0.84, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 496 Loss: 0.42, Loss_test: 0.86, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 497 Loss: 0.56, Loss_test: 0.84, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 498 Loss: 0.34, Loss_test: 0.82, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 499 Loss: 0.46, Loss_test: 0.80, acc: 0.78, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.39, Loss_test: 0.79, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 501 Loss: 0.43, Loss_test: 0.79, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 502 Loss: 0.32, Loss_test: 0.78, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 503 Loss: 0.37, Loss_test: 0.78, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 504 Loss: 0.34, Loss_test: 0.78, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 505 Loss: 0.36, Loss_test: 0.78, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 506 Loss: 0.48, Loss_test: 0.78, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 507 Loss: 0.52, Loss_test: 0.78, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 508 Loss: 0.33, Loss_test: 0.78, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 509 Loss: 0.39, Loss_test: 0.79, acc: 0.80, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.41, Loss_test: 0.80, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 511 Loss: 0.42, Loss_test: 0.80, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 512 Loss: 0.39, Loss_test: 0.80, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 513 Loss: 0.40, Loss_test: 0.82, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 514 Loss: 0.36, Loss_test: 0.83, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 515 Loss: 0.37, Loss_test: 0.84, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 516 Loss: 0.37, Loss_test: 0.87, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 517 Loss: 0.40, Loss_test: 0.90, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 518 Loss: 0.36, Loss_test: 0.93, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 519 Loss: 0.59, Loss_test: 0.87, acc: 0.70, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.41, Loss_test: 0.81, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 521 Loss: 0.38, Loss_test: 0.79, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 522 Loss: 0.40, Loss_test: 0.78, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 523 Loss: 0.39, Loss_test: 0.78, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 524 Loss: 0.39, Loss_test: 0.77, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 525 Loss: 0.36, Loss_test: 0.77, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 526 Loss: 0.43, Loss_test: 0.77, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 527 Loss: 0.33, Loss_test: 0.76, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 528 Loss: 0.38, Loss_test: 0.76, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 529 Loss: 0.43, Loss_test: 0.77, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.34, Loss_test: 0.78, acc: 0.88, acc_test: 0.58\n",
      "Epoch: 531 Loss: 0.43, Loss_test: 0.77, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 532 Loss: 0.36, Loss_test: 0.77, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 533 Loss: 0.32, Loss_test: 0.80, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 534 Loss: 0.38, Loss_test: 0.85, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 535 Loss: 0.46, Loss_test: 0.86, acc: 0.75, acc_test: 0.64\n",
      "Epoch: 536 Loss: 0.34, Loss_test: 0.87, acc: 0.87, acc_test: 0.63\n",
      "Epoch: 537 Loss: 0.32, Loss_test: 0.86, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 538 Loss: 0.31, Loss_test: 0.84, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 539 Loss: 0.37, Loss_test: 0.83, acc: 0.85, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.43, Loss_test: 0.82, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 541 Loss: 0.46, Loss_test: 0.83, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 542 Loss: 0.43, Loss_test: 0.85, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 543 Loss: 0.37, Loss_test: 0.87, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 544 Loss: 0.42, Loss_test: 0.87, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 545 Loss: 0.42, Loss_test: 0.86, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 546 Loss: 0.45, Loss_test: 0.85, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 547 Loss: 0.41, Loss_test: 0.84, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 548 Loss: 0.36, Loss_test: 0.84, acc: 0.83, acc_test: 0.63\n",
      "Epoch: 549 Loss: 0.33, Loss_test: 0.84, acc: 0.85, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.44, Loss_test: 0.84, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 551 Loss: 0.36, Loss_test: 0.84, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 552 Loss: 0.41, Loss_test: 0.83, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 553 Loss: 0.36, Loss_test: 0.82, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 554 Loss: 0.49, Loss_test: 0.81, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 555 Loss: 0.39, Loss_test: 0.80, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 556 Loss: 0.41, Loss_test: 0.80, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 557 Loss: 0.45, Loss_test: 0.81, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 558 Loss: 0.31, Loss_test: 0.81, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 559 Loss: 0.41, Loss_test: 0.81, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.37, Loss_test: 0.82, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 561 Loss: 0.34, Loss_test: 0.82, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 562 Loss: 0.36, Loss_test: 0.83, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 563 Loss: 0.39, Loss_test: 0.84, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 564 Loss: 0.37, Loss_test: 0.84, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 565 Loss: 0.48, Loss_test: 0.82, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 566 Loss: 0.36, Loss_test: 0.82, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 567 Loss: 0.39, Loss_test: 0.83, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 568 Loss: 0.44, Loss_test: 0.85, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 569 Loss: 0.43, Loss_test: 0.87, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.40, Loss_test: 0.88, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 571 Loss: 0.37, Loss_test: 0.85, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 572 Loss: 0.33, Loss_test: 0.82, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 573 Loss: 0.39, Loss_test: 0.82, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 574 Loss: 0.32, Loss_test: 0.82, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 575 Loss: 0.36, Loss_test: 0.83, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 576 Loss: 0.41, Loss_test: 0.84, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 577 Loss: 0.37, Loss_test: 0.87, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 578 Loss: 0.40, Loss_test: 0.88, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 579 Loss: 0.37, Loss_test: 0.87, acc: 0.84, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.41, Loss_test: 0.85, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 581 Loss: 0.43, Loss_test: 0.82, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 582 Loss: 0.35, Loss_test: 0.82, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 583 Loss: 0.48, Loss_test: 0.82, acc: 0.72, acc_test: 0.56\n",
      "Epoch: 584 Loss: 0.43, Loss_test: 0.82, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 585 Loss: 0.36, Loss_test: 0.82, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 586 Loss: 0.31, Loss_test: 0.84, acc: 0.85, acc_test: 0.64\n",
      "Epoch: 587 Loss: 0.41, Loss_test: 0.87, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 588 Loss: 0.38, Loss_test: 0.86, acc: 0.77, acc_test: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 589 Loss: 0.38, Loss_test: 0.83, acc: 0.84, acc_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.36, Loss_test: 0.81, acc: 0.87, acc_test: 0.60\n",
      "Epoch: 591 Loss: 0.41, Loss_test: 0.81, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 592 Loss: 0.38, Loss_test: 0.81, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 593 Loss: 0.35, Loss_test: 0.83, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 594 Loss: 0.34, Loss_test: 0.85, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 595 Loss: 0.37, Loss_test: 0.84, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 596 Loss: 0.31, Loss_test: 0.83, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 597 Loss: 0.36, Loss_test: 0.82, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 598 Loss: 0.49, Loss_test: 0.82, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 599 Loss: 0.37, Loss_test: 0.82, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.33, Loss_test: 0.82, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 601 Loss: 0.34, Loss_test: 0.83, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 602 Loss: 0.39, Loss_test: 0.82, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 603 Loss: 0.44, Loss_test: 0.81, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 604 Loss: 0.43, Loss_test: 0.80, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 605 Loss: 0.39, Loss_test: 0.79, acc: 0.81, acc_test: 0.57\n",
      "Epoch: 606 Loss: 0.45, Loss_test: 0.78, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 607 Loss: 0.41, Loss_test: 0.77, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 608 Loss: 0.40, Loss_test: 0.79, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 609 Loss: 0.34, Loss_test: 0.83, acc: 0.84, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.40, Loss_test: 0.84, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 611 Loss: 0.36, Loss_test: 0.82, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 612 Loss: 0.34, Loss_test: 0.81, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 613 Loss: 0.45, Loss_test: 0.79, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 614 Loss: 0.38, Loss_test: 0.77, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 615 Loss: 0.35, Loss_test: 0.77, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 616 Loss: 0.32, Loss_test: 0.78, acc: 0.88, acc_test: 0.63\n",
      "Epoch: 617 Loss: 0.35, Loss_test: 0.78, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 618 Loss: 0.30, Loss_test: 0.79, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 619 Loss: 0.39, Loss_test: 0.79, acc: 0.83, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.43, Loss_test: 0.80, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 621 Loss: 0.33, Loss_test: 0.81, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 622 Loss: 0.51, Loss_test: 0.81, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 623 Loss: 0.38, Loss_test: 0.82, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 624 Loss: 0.36, Loss_test: 0.85, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 625 Loss: 0.41, Loss_test: 0.88, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 626 Loss: 0.42, Loss_test: 0.90, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 627 Loss: 0.42, Loss_test: 0.90, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 628 Loss: 0.45, Loss_test: 0.85, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 629 Loss: 0.45, Loss_test: 0.83, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.42, Loss_test: 0.83, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 631 Loss: 0.39, Loss_test: 0.84, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 632 Loss: 0.41, Loss_test: 0.86, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 633 Loss: 0.37, Loss_test: 0.89, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 634 Loss: 0.39, Loss_test: 0.90, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 635 Loss: 0.39, Loss_test: 0.85, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 636 Loss: 0.37, Loss_test: 0.84, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 637 Loss: 0.43, Loss_test: 0.86, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 638 Loss: 0.48, Loss_test: 0.86, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 639 Loss: 0.46, Loss_test: 0.84, acc: 0.75, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.40, Loss_test: 0.84, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 641 Loss: 0.40, Loss_test: 0.85, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 642 Loss: 0.39, Loss_test: 0.85, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 643 Loss: 0.40, Loss_test: 0.85, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 644 Loss: 0.44, Loss_test: 0.84, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 645 Loss: 0.45, Loss_test: 0.83, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 646 Loss: 0.34, Loss_test: 0.83, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 647 Loss: 0.44, Loss_test: 0.83, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 648 Loss: 0.40, Loss_test: 0.85, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 649 Loss: 0.29, Loss_test: 0.85, acc: 0.90, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.32, Loss_test: 0.86, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 651 Loss: 0.44, Loss_test: 0.84, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 652 Loss: 0.37, Loss_test: 0.83, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 653 Loss: 0.38, Loss_test: 0.82, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 654 Loss: 0.35, Loss_test: 0.82, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 655 Loss: 0.44, Loss_test: 0.82, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 656 Loss: 0.38, Loss_test: 0.82, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 657 Loss: 0.41, Loss_test: 0.84, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 658 Loss: 0.46, Loss_test: 0.88, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 659 Loss: 0.39, Loss_test: 0.89, acc: 0.81, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.37, Loss_test: 0.88, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 661 Loss: 0.34, Loss_test: 0.84, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 662 Loss: 0.38, Loss_test: 0.82, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 663 Loss: 0.32, Loss_test: 0.83, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 664 Loss: 0.33, Loss_test: 0.84, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 665 Loss: 0.30, Loss_test: 0.84, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 666 Loss: 0.40, Loss_test: 0.84, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 667 Loss: 0.37, Loss_test: 0.85, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 668 Loss: 0.45, Loss_test: 0.88, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 669 Loss: 0.40, Loss_test: 0.88, acc: 0.78, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.34, Loss_test: 0.87, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 671 Loss: 0.34, Loss_test: 0.85, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 672 Loss: 0.43, Loss_test: 0.85, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 673 Loss: 0.50, Loss_test: 0.86, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 674 Loss: 0.47, Loss_test: 0.87, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 675 Loss: 0.35, Loss_test: 0.87, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 676 Loss: 0.37, Loss_test: 0.87, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 677 Loss: 0.44, Loss_test: 0.88, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 678 Loss: 0.33, Loss_test: 0.89, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 679 Loss: 0.34, Loss_test: 0.89, acc: 0.86, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.39, Loss_test: 0.85, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 681 Loss: 0.35, Loss_test: 0.81, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 682 Loss: 0.36, Loss_test: 0.80, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 683 Loss: 0.35, Loss_test: 0.80, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 684 Loss: 0.42, Loss_test: 0.79, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 685 Loss: 0.31, Loss_test: 0.79, acc: 0.87, acc_test: 0.59\n",
      "Epoch: 686 Loss: 0.35, Loss_test: 0.81, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 687 Loss: 0.40, Loss_test: 0.82, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 688 Loss: 0.37, Loss_test: 0.82, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 689 Loss: 0.35, Loss_test: 0.80, acc: 0.86, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.34, Loss_test: 0.78, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 691 Loss: 0.35, Loss_test: 0.77, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 692 Loss: 0.32, Loss_test: 0.77, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 693 Loss: 0.40, Loss_test: 0.77, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 694 Loss: 0.36, Loss_test: 0.78, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 695 Loss: 0.44, Loss_test: 0.80, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 696 Loss: 0.39, Loss_test: 0.82, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 697 Loss: 0.38, Loss_test: 0.84, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 698 Loss: 0.39, Loss_test: 0.85, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 699 Loss: 0.38, Loss_test: 0.87, acc: 0.83, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.47, Loss_test: 0.87, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 701 Loss: 0.39, Loss_test: 0.84, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 702 Loss: 0.40, Loss_test: 0.82, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 703 Loss: 0.42, Loss_test: 0.81, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 704 Loss: 0.45, Loss_test: 0.81, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 705 Loss: 0.46, Loss_test: 0.81, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 706 Loss: 0.42, Loss_test: 0.83, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 707 Loss: 0.32, Loss_test: 0.85, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 708 Loss: 0.32, Loss_test: 0.85, acc: 0.87, acc_test: 0.63\n",
      "Epoch: 709 Loss: 0.38, Loss_test: 0.84, acc: 0.81, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.32, Loss_test: 0.82, acc: 0.83, acc_test: 0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 711 Loss: 0.32, Loss_test: 0.83, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 712 Loss: 0.38, Loss_test: 0.83, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 713 Loss: 0.42, Loss_test: 0.85, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 714 Loss: 0.39, Loss_test: 0.87, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 715 Loss: 0.39, Loss_test: 0.90, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 716 Loss: 0.42, Loss_test: 0.92, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 717 Loss: 0.37, Loss_test: 0.92, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 718 Loss: 0.35, Loss_test: 0.91, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 719 Loss: 0.38, Loss_test: 0.88, acc: 0.82, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.37, Loss_test: 0.88, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 721 Loss: 0.39, Loss_test: 0.87, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 722 Loss: 0.41, Loss_test: 0.88, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 723 Loss: 0.37, Loss_test: 0.90, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 724 Loss: 0.42, Loss_test: 0.89, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 725 Loss: 0.45, Loss_test: 0.88, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 726 Loss: 0.30, Loss_test: 0.86, acc: 0.88, acc_test: 0.60\n",
      "Epoch: 727 Loss: 0.43, Loss_test: 0.86, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 728 Loss: 0.40, Loss_test: 0.87, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 729 Loss: 0.33, Loss_test: 0.90, acc: 0.83, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.39, Loss_test: 0.90, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 731 Loss: 0.33, Loss_test: 0.88, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 732 Loss: 0.42, Loss_test: 0.87, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 733 Loss: 0.42, Loss_test: 0.85, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 734 Loss: 0.36, Loss_test: 0.84, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 735 Loss: 0.41, Loss_test: 0.82, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 736 Loss: 0.31, Loss_test: 0.81, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 737 Loss: 0.37, Loss_test: 0.81, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 738 Loss: 0.29, Loss_test: 0.81, acc: 0.90, acc_test: 0.62\n",
      "Epoch: 739 Loss: 0.27, Loss_test: 0.81, acc: 0.90, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.38, Loss_test: 0.80, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 741 Loss: 0.31, Loss_test: 0.78, acc: 0.87, acc_test: 0.61\n",
      "Epoch: 742 Loss: 0.37, Loss_test: 0.78, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 743 Loss: 0.31, Loss_test: 0.79, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 744 Loss: 0.34, Loss_test: 0.79, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 745 Loss: 0.43, Loss_test: 0.79, acc: 0.74, acc_test: 0.64\n",
      "Epoch: 746 Loss: 0.39, Loss_test: 0.81, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 747 Loss: 0.35, Loss_test: 0.83, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 748 Loss: 0.34, Loss_test: 0.83, acc: 0.84, acc_test: 0.65\n",
      "Epoch: 749 Loss: 0.40, Loss_test: 0.81, acc: 0.84, acc_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.39, Loss_test: 0.80, acc: 0.86, acc_test: 0.64\n",
      "Epoch: 751 Loss: 0.24, Loss_test: 0.81, acc: 0.89, acc_test: 0.64\n",
      "Epoch: 752 Loss: 0.34, Loss_test: 0.83, acc: 0.85, acc_test: 0.65\n",
      "Epoch: 753 Loss: 0.51, Loss_test: 0.83, acc: 0.73, acc_test: 0.65\n",
      "Epoch: 754 Loss: 0.44, Loss_test: 0.82, acc: 0.76, acc_test: 0.66\n",
      "Epoch: 755 Loss: 0.37, Loss_test: 0.81, acc: 0.82, acc_test: 0.66\n",
      "Epoch: 756 Loss: 0.35, Loss_test: 0.80, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 757 Loss: 0.40, Loss_test: 0.78, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 758 Loss: 0.32, Loss_test: 0.76, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 759 Loss: 0.35, Loss_test: 0.76, acc: 0.84, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.37, Loss_test: 0.76, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 761 Loss: 0.43, Loss_test: 0.76, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 762 Loss: 0.35, Loss_test: 0.77, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 763 Loss: 0.36, Loss_test: 0.77, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 764 Loss: 0.31, Loss_test: 0.77, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 765 Loss: 0.36, Loss_test: 0.78, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 766 Loss: 0.37, Loss_test: 0.78, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 767 Loss: 0.34, Loss_test: 0.79, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 768 Loss: 0.30, Loss_test: 0.79, acc: 0.88, acc_test: 0.60\n",
      "Epoch: 769 Loss: 0.33, Loss_test: 0.81, acc: 0.84, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.37, Loss_test: 0.83, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 771 Loss: 0.36, Loss_test: 0.84, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 772 Loss: 0.38, Loss_test: 0.84, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 773 Loss: 0.46, Loss_test: 0.82, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 774 Loss: 0.26, Loss_test: 0.82, acc: 0.85, acc_test: 0.58\n",
      "Epoch: 775 Loss: 0.40, Loss_test: 0.81, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 776 Loss: 0.33, Loss_test: 0.82, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 777 Loss: 0.32, Loss_test: 0.82, acc: 0.86, acc_test: 0.58\n",
      "Epoch: 778 Loss: 0.35, Loss_test: 0.82, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 779 Loss: 0.33, Loss_test: 0.82, acc: 0.84, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.33, Loss_test: 0.82, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 781 Loss: 0.32, Loss_test: 0.83, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 782 Loss: 0.31, Loss_test: 0.82, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 783 Loss: 0.44, Loss_test: 0.82, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 784 Loss: 0.28, Loss_test: 0.82, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 785 Loss: 0.37, Loss_test: 0.81, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 786 Loss: 0.37, Loss_test: 0.81, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 787 Loss: 0.33, Loss_test: 0.81, acc: 0.80, acc_test: 0.63\n",
      "Epoch: 788 Loss: 0.32, Loss_test: 0.80, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 789 Loss: 0.39, Loss_test: 0.80, acc: 0.80, acc_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.28, Loss_test: 0.80, acc: 0.89, acc_test: 0.64\n",
      "Epoch: 791 Loss: 0.36, Loss_test: 0.80, acc: 0.84, acc_test: 0.64\n",
      "Epoch: 792 Loss: 0.32, Loss_test: 0.81, acc: 0.86, acc_test: 0.64\n",
      "Epoch: 793 Loss: 0.41, Loss_test: 0.81, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 794 Loss: 0.30, Loss_test: 0.83, acc: 0.86, acc_test: 0.64\n",
      "Epoch: 795 Loss: 0.42, Loss_test: 0.85, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 796 Loss: 0.41, Loss_test: 0.88, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 797 Loss: 0.34, Loss_test: 0.90, acc: 0.87, acc_test: 0.64\n",
      "Epoch: 798 Loss: 0.35, Loss_test: 0.86, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 799 Loss: 0.35, Loss_test: 0.84, acc: 0.85, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.46, Loss_test: 0.83, acc: 0.76, acc_test: 0.64\n",
      "Epoch: 801 Loss: 0.42, Loss_test: 0.84, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 802 Loss: 0.33, Loss_test: 0.83, acc: 0.89, acc_test: 0.62\n",
      "Epoch: 803 Loss: 0.32, Loss_test: 0.84, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 804 Loss: 0.36, Loss_test: 0.84, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 805 Loss: 0.42, Loss_test: 0.84, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 806 Loss: 0.40, Loss_test: 0.83, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 807 Loss: 0.39, Loss_test: 0.81, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 808 Loss: 0.37, Loss_test: 0.80, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 809 Loss: 0.30, Loss_test: 0.80, acc: 0.88, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.41, Loss_test: 0.80, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 811 Loss: 0.33, Loss_test: 0.82, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 812 Loss: 0.34, Loss_test: 0.83, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 813 Loss: 0.41, Loss_test: 0.84, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 814 Loss: 0.40, Loss_test: 0.85, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 815 Loss: 0.38, Loss_test: 0.84, acc: 0.87, acc_test: 0.63\n",
      "Epoch: 816 Loss: 0.29, Loss_test: 0.85, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 817 Loss: 0.36, Loss_test: 0.84, acc: 0.79, acc_test: 0.63\n",
      "Epoch: 818 Loss: 0.34, Loss_test: 0.82, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 819 Loss: 0.35, Loss_test: 0.81, acc: 0.86, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.35, Loss_test: 0.82, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 821 Loss: 0.34, Loss_test: 0.83, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 822 Loss: 0.40, Loss_test: 0.84, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 823 Loss: 0.36, Loss_test: 0.85, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 824 Loss: 0.36, Loss_test: 0.86, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 825 Loss: 0.30, Loss_test: 0.84, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 826 Loss: 0.36, Loss_test: 0.81, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 827 Loss: 0.28, Loss_test: 0.80, acc: 0.87, acc_test: 0.57\n",
      "Epoch: 828 Loss: 0.27, Loss_test: 0.80, acc: 0.88, acc_test: 0.57\n",
      "Epoch: 829 Loss: 0.38, Loss_test: 0.80, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.31, Loss_test: 0.80, acc: 0.89, acc_test: 0.60\n",
      "Epoch: 831 Loss: 0.33, Loss_test: 0.81, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 832 Loss: 0.36, Loss_test: 0.83, acc: 0.85, acc_test: 0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 833 Loss: 0.36, Loss_test: 0.83, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 834 Loss: 0.35, Loss_test: 0.85, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 835 Loss: 0.32, Loss_test: 0.86, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 836 Loss: 0.30, Loss_test: 0.88, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 837 Loss: 0.37, Loss_test: 0.87, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 838 Loss: 0.27, Loss_test: 0.87, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 839 Loss: 0.39, Loss_test: 0.87, acc: 0.81, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.46, Loss_test: 0.87, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 841 Loss: 0.31, Loss_test: 0.87, acc: 0.87, acc_test: 0.59\n",
      "Epoch: 842 Loss: 0.41, Loss_test: 0.87, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 843 Loss: 0.29, Loss_test: 0.87, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 844 Loss: 0.37, Loss_test: 0.86, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 845 Loss: 0.42, Loss_test: 0.84, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 846 Loss: 0.37, Loss_test: 0.84, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 847 Loss: 0.35, Loss_test: 0.87, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 848 Loss: 0.36, Loss_test: 0.88, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 849 Loss: 0.38, Loss_test: 0.86, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.42, Loss_test: 0.85, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 851 Loss: 0.36, Loss_test: 0.86, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 852 Loss: 0.35, Loss_test: 0.88, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 853 Loss: 0.37, Loss_test: 0.91, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 854 Loss: 0.30, Loss_test: 0.94, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 855 Loss: 0.37, Loss_test: 0.94, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 856 Loss: 0.33, Loss_test: 0.93, acc: 0.87, acc_test: 0.60\n",
      "Epoch: 857 Loss: 0.29, Loss_test: 0.91, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 858 Loss: 0.35, Loss_test: 0.90, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 859 Loss: 0.29, Loss_test: 0.91, acc: 0.85, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.35, Loss_test: 0.92, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 861 Loss: 0.28, Loss_test: 0.94, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 862 Loss: 0.33, Loss_test: 0.93, acc: 0.87, acc_test: 0.61\n",
      "Epoch: 863 Loss: 0.35, Loss_test: 0.93, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 864 Loss: 0.32, Loss_test: 0.92, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 865 Loss: 0.36, Loss_test: 0.91, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 866 Loss: 0.37, Loss_test: 0.90, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 867 Loss: 0.42, Loss_test: 0.89, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 868 Loss: 0.30, Loss_test: 0.89, acc: 0.89, acc_test: 0.63\n",
      "Epoch: 869 Loss: 0.30, Loss_test: 0.90, acc: 0.86, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.30, Loss_test: 0.88, acc: 0.89, acc_test: 0.62\n",
      "Epoch: 871 Loss: 0.33, Loss_test: 0.89, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 872 Loss: 0.32, Loss_test: 0.90, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 873 Loss: 0.35, Loss_test: 0.90, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 874 Loss: 0.32, Loss_test: 0.93, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 875 Loss: 0.38, Loss_test: 0.94, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 876 Loss: 0.47, Loss_test: 0.92, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 877 Loss: 0.42, Loss_test: 0.93, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 878 Loss: 0.35, Loss_test: 0.96, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 879 Loss: 0.28, Loss_test: 0.98, acc: 0.87, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.33, Loss_test: 0.98, acc: 0.85, acc_test: 0.63\n",
      "Epoch: 881 Loss: 0.36, Loss_test: 0.97, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 882 Loss: 0.42, Loss_test: 0.94, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 883 Loss: 0.40, Loss_test: 0.92, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 884 Loss: 0.41, Loss_test: 0.92, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 885 Loss: 0.29, Loss_test: 0.93, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 886 Loss: 0.34, Loss_test: 0.95, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 887 Loss: 0.37, Loss_test: 0.97, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 888 Loss: 0.38, Loss_test: 0.98, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 889 Loss: 0.35, Loss_test: 0.97, acc: 0.84, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.32, Loss_test: 0.97, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 891 Loss: 0.39, Loss_test: 0.97, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 892 Loss: 0.36, Loss_test: 0.97, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 893 Loss: 0.40, Loss_test: 0.97, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 894 Loss: 0.34, Loss_test: 0.99, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 895 Loss: 0.31, Loss_test: 1.02, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 896 Loss: 0.35, Loss_test: 1.01, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 897 Loss: 0.36, Loss_test: 0.94, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 898 Loss: 0.33, Loss_test: 0.86, acc: 0.87, acc_test: 0.59\n",
      "Epoch: 899 Loss: 0.39, Loss_test: 0.83, acc: 0.84, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.35, Loss_test: 0.82, acc: 0.84, acc_test: 0.61\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_dwen_a')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-2]\n",
    "bug_feature_output_a = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'title_dwen_a:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'desc_dwen_a:0' shape=(?, 20) dtype=float32>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature1000epochs_64batch(netbeans)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(netbeans).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 Loss: 0.35, Loss_test: 0.82, acc: 0.84, acc_tets: 0.61, recall@25: 0.23\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['131079:124863|113328:0.3987081050872803,162864:0.35667872428894043,105649:0.3552509546279907,125999:0.3379114866256714,179131:0.3343706727027893,159111:0.33260560035705566,186208:0.33242470026016235,139742:0.33136165142059326,182083:0.3301457166671753,168140:0.32184046506881714,180299:0.320984423160553,140573:0.30820679664611816,157027:0.30562371015548706,144646:0.29684633016586304,90835:0.29566681385040283,145301:0.29225850105285645,197950:0.2912602424621582,174869:0.2882149815559387,199934:0.28646767139434814,184178:0.28540879487991333,204613:0.2832145690917969,215675:0.28245043754577637,175044:0.2819741368293762,172772:0.28015780448913574,188204:0.2798312306404114,70108:0.2797725200653076,168331:0.2793806791305542,181956:0.27889150381088257,122651:0.27874821424484253',\n",
       " '131082:131085|131085:1.0,226664:0.47495633363723755,136948:0.4439436197280884,160038:0.43521732091903687,125532:0.41653430461883545,42231:0.4092305898666382,24343:0.4082835912704468,125435:0.3930480480194092,70217:0.39183783531188965,173297:0.39059096574783325,158473:0.3874013423919678,30113:0.38014471530914307,20527:0.37993383407592773,46958:0.37798011302948,77039:0.37765079736709595,151903:0.3757282495498657,147015:0.3751292824745178,38353:0.37271207571029663,139589:0.37105464935302734,60092:0.3668643832206726,12240:0.3668552041053772,143381:0.3662622570991516,123764:0.364982008934021,55730:0.3645177483558655,124473:0.364452064037323,30337:0.36243927478790283,169648:0.3571052551269531,122505:0.35301095247268677,99212:0.34709346294403076',\n",
       " '131083:145537,124867,144649,131084,132247,129785,129786,145533,145534,143327|131084:1.0,132247:1.0,145533:1.0,145534:1.0,131086:0.8387083858251572,191647:0.7673719078302383,154440:0.7520807683467865,161248:0.7399588227272034,216123:0.7283378839492798,229339:0.7208846807479858,171109:0.7146112620830536,146754:0.7075122594833374,210772:0.6999233663082123,117596:0.6976188123226166,117656:0.6976188123226166,117657:0.6976188123226166,126775:0.6931186616420746,138199:0.6915641129016876,138584:0.6915641129016876,219485:0.6890838444232941,154750:0.6858461499214172,154751:0.6858461499214172,154721:0.6853324174880981,209913:0.6835202574729919,233115:0.6733329594135284,148322:0.6722567677497864,232596:0.6715771555900574,127115:0.6697611212730408,127983:0.6697611212730408',\n",
       " '131084:145537,124867,144649,131083,132247,129785,129786,145533,145534,143327|131083:1.0,132247:1.0,145533:1.0,145534:1.0,131086:0.8387083858251572,191647:0.7673719078302383,154440:0.7520807683467865,161248:0.7399588227272034,216123:0.7283378839492798,229339:0.7208846807479858,171109:0.7146112620830536,146754:0.7075122594833374,210772:0.6999233663082123,117596:0.6976188123226166,117656:0.6976188123226166,117657:0.6976188123226166,126775:0.6931186616420746,138199:0.6915641129016876,138584:0.6915641129016876,219485:0.6890838444232941,154750:0.6858461499214172,154751:0.6858461499214172,154721:0.6853324174880981,209913:0.6835202574729919,233115:0.6733329594135284,148322:0.6722567677497864,232596:0.6715771555900574,127115:0.6697611212730408,127983:0.6697611212730408',\n",
       " '131085:131082|131082:1.0,226664:0.47495633363723755,136948:0.4439436197280884,160038:0.43521732091903687,125532:0.41653430461883545,42231:0.4092305898666382,24343:0.4082835912704468,125435:0.3930480480194092,70217:0.39183783531188965,173297:0.39059096574783325,158473:0.3874013423919678,30113:0.38014471530914307,20527:0.37993383407592773,46958:0.37798011302948,77039:0.37765079736709595,151903:0.3757282495498657,147015:0.3751292824745178,38353:0.37271207571029663,139589:0.37105464935302734,60092:0.3668643832206726,12240:0.3668552041053772,143381:0.3662622570991516,123764:0.364982008934021,55730:0.3645177483558655,124473:0.364452064037323,30337:0.36243927478790283,169648:0.3571052551269531,122505:0.35301095247268677,99212:0.34709346294403076',\n",
       " '131086:118825,117035|131083:0.8387083858251572,131084:0.8387083858251572,132247:0.8387083858251572,145533:0.8387083858251572,145534:0.8387083858251572,191647:0.7309732139110565,161248:0.7100716233253479,210772:0.6704186201095581,219485:0.6583742797374725,193736:0.6518282890319824,195631:0.6518282890319824,232596:0.6474511921405792,158637:0.6451243162155151,143997:0.6416154503822327,193784:0.6398575305938721,158459:0.6397583484649658,158613:0.6397583484649658,166577:0.6397583484649658,168356:0.6317727863788605,159258:0.6274596452713013,160099:0.6274596452713013,153182:0.6193661987781525,170422:0.6190123856067657,156369:0.6167663633823395,167978:0.616449236869812,202714:0.616346687078476,232564:0.6124168634414673,155969:0.6121906936168671,160524:0.5956571996212006',\n",
       " '131103:129349,131439,130960,130033,132127|46913:0.2969787120819092,79965:0.25139498710632324,76712:0.24159455299377441,205415:0.229425847530365,51675:0.22652345895767212,146976:0.21572619676589966,207650:0.19973760843276978,64908:0.19810634851455688,82530:0.19771665334701538,135507:0.19377058744430542,23613:0.18561041355133057,131504:0.18455779552459717,156668:0.17243516445159912,162714:0.1577041745185852,237487:0.15356630086898804,130444:0.1524202823638916,22943:0.1523139476776123,52918:0.15091484785079956,122040:0.15063124895095825,72332:0.15015560388565063,105432:0.1482553482055664,33123:0.14822715520858765,25440:0.1458783745765686,153094:0.14569121599197388,122651:0.14251023530960083,219216:0.141606867313385,135505:0.14144790172576904,132691:0.14143484830856323,108360:0.13963687419891357',\n",
       " '131126:117856,118467,118468,118469,126180,116295,116680,118634,117003,103531,131983,118351,118735,109177,129907,117299,104665,117274,117981|55584:0.4333978295326233,109738:0.39991724491119385,86762:0.3908535838127136,88115:0.38050758838653564,153544:0.3702639937400818,10743:0.338689923286438,91262:0.32887428998947144,16928:0.3190423846244812,96995:0.24889647960662842,96665:0.2471451759338379,42881:0.23734039068222046,75467:0.23686212301254272,102290:0.23555946350097656,60982:0.23362129926681519,175463:0.23142659664154053,175464:0.23142659664154053,179965:0.22988373041152954,35809:0.22826921939849854,20670:0.22604233026504517,112824:0.22354042530059814,11615:0.22287112474441528,36577:0.22258055210113525,144739:0.22075068950653076,103986:0.21938425302505493,47998:0.21785378456115723,45693:0.215457022190094,24646:0.21333611011505127,50255:0.21186929941177368,51751:0.21174269914627075',\n",
       " '131145:131146|62307:0.431745707988739,185781:0.4093695878982544,81857:0.40847015380859375,84713:0.40717488527297974,168439:0.3907967805862427,184011:0.38687944412231445,65603:0.38471633195877075,168058:0.382748544216156,116443:0.3740769028663635,68190:0.36857056617736816,85788:0.3670872449874878,20799:0.3639064431190491,90067:0.3583354949951172,92551:0.3468988537788391,35233:0.34117281436920166,48904:0.33952194452285767,73657:0.33512234687805176,175781:0.33236855268478394,26133:0.33207613229751587,118573:0.3277909755706787,202209:0.3273967504501343,185275:0.32395583391189575,37379:0.32036757469177246,105006:0.31826919317245483,125871:0.3140239715576172,40112:0.3119145631790161,167968:0.3087899088859558,116241:0.3087158799171448,119658:0.30858129262924194',\n",
       " '131146:131145|207624:0.3540484309196472,112571:0.32953643798828125,211856:0.3177143931388855,105020:0.3139492869377136,48123:0.30977702140808105,155929:0.2981604337692261,13540:0.288213312625885,76598:0.2774187922477722,146153:0.2718103528022766,35233:0.26661068201065063,96711:0.2650073766708374,54289:0.26344019174575806,56787:0.26173269748687744,68190:0.2589510679244995,104148:0.2543832063674927,13976:0.2539888620376587,108515:0.24963611364364624,25209:0.24282819032669067,156802:0.23610901832580566,45562:0.2336166501045227,24759:0.2299816608428955,11010:0.22674506902694702,158630:0.2227475643157959,127147:0.22198039293289185,37325:0.21906793117523193,195636:0.2182164192199707,118487:0.2156614065170288,79776:0.21488040685653687,14307:0.21469467878341675',\n",
       " '131148:130987|161134:0.5532682538032532,219832:0.5467705726623535,220178:0.5446519255638123,220372:0.5417419075965881,140526:0.5064263939857483,182735:0.5061068832874298,140272:0.5031682848930359,143711:0.5001122355461121,139902:0.4997817873954773,140265:0.4975491166114807,140539:0.49628162384033203,143652:0.4937479496002197,140067:0.49357593059539795,103976:0.4879087209701538,140259:0.4873136281967163,140271:0.4873136281967163,140373:0.4827480912208557,128170:0.4826536774635315,170123:0.47934412956237793,174352:0.47739601135253906,132580:0.47727930545806885,140068:0.4736950397491455,140261:0.4702165126800537,202430:0.46913933753967285,144266:0.4666751027107239,140473:0.4650247097015381,169840:0.46455663442611694,95041:0.46071338653564453,69200:0.4585757255554199',\n",
       " '131149:131359,131335,131153,132377,130783|130783:0.6972573101520538,160548:0.6038557589054108,132377:0.5862948596477509,212909:0.5480256378650665,214657:0.5480256378650665,208369:0.5160119831562042,141495:0.5116883516311646,209650:0.49198633432388306,210396:0.47673743963241577,148458:0.44749075174331665,217982:0.42731034755706787,141497:0.4162359833717346,216240:0.40864402055740356,92200:0.3974769115447998,211293:0.3858380913734436,131153:0.34543371200561523,159261:0.13457053899765015,199233:0.13430726528167725,180039:0.12893950939178467,180040:0.12893950939178467,180041:0.12893950939178467,150800:0.12869328260421753,150960:0.12869328260421753,10133:0.11802011728286743,194567:0.11211514472961426,210070:0.11070805788040161,207551:0.10691529512405396,201928:0.09987270832061768,105360:0.09883689880371094',\n",
       " '131151:132544,131236|131439:0.4106358289718628,140573:0.3253892660140991,215675:0.32391107082366943,209788:0.31834501028060913,154793:0.31728535890579224,82828:0.31460899114608765,186308:0.304465651512146,184178:0.30027467012405396,82158:0.2963177561759949,182612:0.29380208253860474,229016:0.2908415198326111,204428:0.2891881465911865,197950:0.28657829761505127,162864:0.28479892015457153,181724:0.2807391285896301,158344:0.27964967489242554,201121:0.2796287536621094,40975:0.27455925941467285,205177:0.27442705631256104,107052:0.2675899863243103,46975:0.26752251386642456,144385:0.2665865421295166,176517:0.26547950506210327,203229:0.26361215114593506,156276:0.26179039478302,38022:0.26154381036758423,180299:0.2605443000793457,43650:0.2602649927139282,74377:0.25468236207962036',\n",
       " '131153:131359,131335,131149,132377,130783|180039:0.4357423186302185,180040:0.4357423186302185,180041:0.4357423186302185,130783:0.387279748916626,132377:0.35504573583602905,131149:0.34543371200561523,208369:0.3176594376564026,217982:0.30910152196884155,194567:0.3055983781814575,141495:0.30082404613494873,211293:0.2738376259803772,92200:0.26425331830978394,152696:0.257976233959198,143766:0.25616198778152466,209650:0.25546425580978394,155449:0.2526175379753113,216240:0.2485315203666687,212909:0.24780333042144775,214657:0.24780333042144775,189689:0.2447904348373413,118681:0.24355757236480713,126398:0.24233311414718628,158419:0.23360562324523926,158573:0.23296666145324707,150264:0.22215646505355835,143664:0.2220979928970337,146250:0.2201155424118042,162614:0.21331363916397095,152231:0.2128620147705078',\n",
       " '131166:131594|133480:0.35941243171691895,166362:0.28255367279052734,196380:0.2676241993904114,153197:0.20813238620758057,51960:0.12603479623794556,83630:0.056582510471343994,101864:0.05465269088745117,54727:0.05156916379928589,118719:0.039141297340393066,12640:0.034515321254730225,50012:0.026044130325317383,80783:0.024530470371246338,15464:0.023055851459503174,55149:0.02290511131286621,18438:0.020981550216674805,32584:0.02015310525894165,12732:0.015907764434814453,19135:0.012999653816223145,65310:0.01252126693725586,15736:0.011011362075805664,17269:0.006112933158874512,59608:0.005880832672119141,87598:0.001531362533569336,64751:0.0014696121215820312,153877:0.0014268159866333008,25026:0.0012949705123901367,55576:0.0008543729782104492,26137:0.0007946491241455078,44794:0.0',\n",
       " '131170:127369,125195,113747,143350|143350:0.22053271532058716,198298:0.21276473999023438,198438:0.21276473999023438,127369:0.20268255472183228,157357:0.1950581669807434,148459:0.17783212661743164,203908:0.17715531587600708,163853:0.1763756275177002,140172:0.17376196384429932,161786:0.1656518578529358,146772:0.15410113334655762,195073:0.1523992419242859,199943:0.15182143449783325,221245:0.14851617813110352,146947:0.14809513092041016,144005:0.14351314306259155,159093:0.14322715997695923,203505:0.13932567834854126,198569:0.1349407434463501,167269:0.13276022672653198,225306:0.13221019506454468,143766:0.13044977188110352,62299:0.1297457218170166,198669:0.12894850969314575,158533:0.12664318084716797,29558:0.12382668256759644,150264:0.12162727117538452,126398:0.12098878622055054,150032:0.12031298875808716',\n",
       " '131180:129163|79295:0.6092350780963898,72144:0.35530704259872437,124767:0.29452258348464966,26171:0.29290062189102173,14856:0.29120105504989624,22892:0.28103107213974,25688:0.27785658836364746,25573:0.27659177780151367,130234:0.2760658860206604,31230:0.2751598358154297,113279:0.2723146080970764,77760:0.2720770239830017,72957:0.2699909806251526,159535:0.2698478102684021,61056:0.2682546377182007,44723:0.26544755697250366,168833:0.26366573572158813,159226:0.2611766457557678,161826:0.25979554653167725,217817:0.2596439719200134,37379:0.2583380937576294,125828:0.2568921446800232,154279:0.25663888454437256,169633:0.2548410892486572,52181:0.25368762016296387,180982:0.2524212598800659,39223:0.25163567066192627,192002:0.25101685523986816,147146:0.24883264303207397',\n",
       " '131192:133635,131782,131306,131759,130869,130934,131293|28825:0.49727457761764526,61056:0.4958653450012207,81740:0.489643931388855,230069:0.48870015144348145,122359:0.48733633756637573,39714:0.4868341088294983,205739:0.48128455877304077,75197:0.4798274040222168,37876:0.47495007514953613,64486:0.47391581535339355,125582:0.4720211625099182,63056:0.46589183807373047,171453:0.46355390548706055,73511:0.461428701877594,135560:0.4589577317237854,79143:0.45846158266067505,99293:0.45733124017715454,23287:0.45729732513427734,64324:0.4562230110168457,171532:0.45417529344558716,97394:0.4521845579147339,114716:0.45205503702163696,128637:0.45054787397384644,108302:0.4503072500228882,224987:0.449005663394928,214924:0.4429667592048645,184797:0.4418909549713135,41828:0.44040918350219727,167432:0.43963897228240967',\n",
       " '131197:119617,131970,127169,123908,155137,126563,120745,167562,176395,126057,122219,120880,32755,136212,126260,187290,170110|170830:0.4436689615249634,89715:0.41694849729537964,173292:0.38490355014801025,96995:0.35456401109695435,76518:0.35223859548568726,97801:0.3514035940170288,189604:0.35119688510894775,203054:0.3500233292579651,203439:0.349490225315094,124767:0.3414500951766968,143381:0.34113967418670654,113742:0.3393111824989319,35824:0.3386465311050415,40234:0.33850163221359253,65087:0.3361870050430298,147455:0.33529824018478394,20512:0.33467406034469604,153522:0.33163684606552124,56600:0.32884514331817627,115742:0.32874423265457153,159817:0.3279867172241211,102290:0.3269662857055664,18400:0.32615137100219727,223665:0.3259788155555725,201650:0.3251579999923706,68765:0.3247576951980591,218456:0.3246990442276001,167881:0.3213561773300171,59728:0.3171783685684204',\n",
       " '131209:177304|42634:0.28067874908447266,74746:0.2687011957168579,33124:0.26164138317108154,213678:0.2563208341598511,221897:0.2539507746696472,215813:0.2472812533378601,45137:0.2390393614768982,180236:0.23601490259170532,209977:0.23444336652755737,25042:0.23370057344436646,225708:0.2295054793357849,123565:0.22914588451385498,122483:0.2257402539253235,232541:0.22210031747817993,110010:0.21975350379943848,91996:0.2152872085571289,77524:0.2148786187171936,155672:0.20919108390808105,133931:0.20542091131210327,111329:0.20358926057815552,209623:0.2025972604751587,75785:0.20191431045532227,177392:0.20136064291000366,137247:0.19854718446731567,234943:0.19749468564987183,126386:0.19645154476165771,117137:0.19367927312850952,230496:0.19325041770935059,201542:0.1924160122871399']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 17002\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(netbeans)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      580966500   title_dwen_a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      580966500   desc_dwen_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,161,933,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,161,933,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bug_feature_output_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/netbeans/bert/exported_rank_baseline_dwen_1000.txt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.19,\n",
       " '2 - recall_at_10': 0.21,\n",
       " '3 - recall_at_15': 0.22,\n",
       " '4 - recall_at_20': 0.23,\n",
       " '5 - recall_at_25': 0.23}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
