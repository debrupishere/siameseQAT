{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: epochs=1000\n",
      "env: base=openoffice\n"
     ]
    }
   ],
   "source": [
    "# %env epochs 1000\n",
    "# %env base openoffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 500\n",
    "'''\n",
    "    Sequence length\n",
    "    # Eclipse\n",
    "    # 100   recall@25 = \n",
    "    # 20    recall@25 = \n",
    "    # Netbeans\n",
    "    # 100   recall@25 = \n",
    "    # 20    recall@25 = \n",
    "    # Open office\n",
    "    # 100   recall@25 : 0.16\n",
    "    # 20    recall@25 : 0.20\n",
    "'''\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b0be3c309c46dea28aac7e0d17521c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfd80925f6d462b95caed88c859d8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 20s, sys: 4.65 s, total: 1min 24s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288f36269e2f4b1689bf5a40c36968d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '6\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '361\\n',\n",
       " 'creation_ts': '2012-05-22 04:50:00 -0400',\n",
       " 'delta_ts': '2012-07-09 06:49:56 -0400',\n",
       " 'description': '[CLS] when a new scout project is created , the cs ##s theme of the platform is used by default . when the sw ##t client is started the background - image . / gt ##k ##gre ##y . p ##ng is not found . see also bug 37 ##9 ##9 ##8 ##9 . java . io . file ##not ##fo ##und ##ex ##ception : . / gt ##k ##gre ##y . p ##ng ( date ##i oder ve ##rz ##ei ##ch ##nis nic ##ht ge ##fu ##nden ) at java . io . file ##in ##put ##stream . open ( native method ) at java . io . file ##in ##put ##stream . < in ##it > ( file ##in ##put ##stream . java : 137 ) at org . eclipse . e ##4 . ui . cs ##s . core . ut ##il . imp ##l . resources . file ##res ##our ##ces ##lo ##cat ##ori ##mp ##l . get ##in ##put ##stream ( file ##res ##our ##ces ##lo ##cat ##ori ##mp ##l . java : 41 ) at org . eclipse . e ##4 . ui . cs ##s . core . ut ##il . imp ##l . resources . resources ##lo ##cat ##or ##mana ##ger . get ##in ##put ##stream ( resources ##lo ##cat ##or ##mana ##ger . java : 105 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . help ##ers . cs ##ss ##wt ##ima ##ge ##hel ##per . load ##ima ##ge ##fr ##om ##ur ##l ( cs ##ss ##wt ##ima ##ge ##hel ##per . java : 44 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . help ##ers . cs ##ss ##wt ##ima ##ge ##hel ##per . get ##ima ##ge ( cs ##ss ##wt ##ima ##ge ##hel ##per . java : 33 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . properties . convert ##ers . cs ##s ##val ##ues ##wt ##ima ##ge ##con ##vert ##eri ##mp ##l . convert ( cs ##s ##val ##ues ##wt ##ima ##ge ##con ##vert ##eri ##mp ##l . java : 33 ) at org . eclipse . e ##4 . ui . cs ##s . core . imp ##l . engine . abstract ##cs ##sen ##gin ##e . convert ( abstract ##cs ##sen ##gin ##e . java : 112 ##5 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . properties . cs ##s ##2 . cs ##sp ##rop ##erty ##back ##grounds ##wt ##hand ##ler . apply ##cs ##sp ##rop ##erty ##back ##ground ##ima ##ge ( cs ##sp ##rop ##erty ##back ##grounds ##wt ##hand ##ler . java : 108 ) at org . eclipse . e ##4 . ui . cs ##s . core . dom . properties . cs ##s ##2 . abstract ##cs ##sp ##rop ##erty ##back ##ground ##hand ##ler . apply ##cs ##sp ##rop ##erty ( abstract ##cs ##sp ##rop ##erty ##back ##ground ##hand ##ler . java : 37 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . properties . cs ##s ##2 . cs ##sp ##rop ##erty ##back ##grounds ##wt ##hand ##ler . apply ##cs ##sp ##rop ##erty ( cs ##sp ##rop ##erty ##back ##grounds ##wt ##hand ##ler . java : 41 ) at org . eclipse . e ##4 . ui . cs ##s . core . imp ##l . engine . abstract ##cs ##sen ##gin ##e . apply ##cs ##sp ##rop ##erty ( abstract ##cs ##sen ##gin ##e . java : 77 ##9 ) at org . eclipse . e ##4 . ui . cs ##s . core . imp ##l . engine . abstract ##cs ##sen ##gin ##e . apply ##sty ##led ##ec ##lar ##ation ( abstract ##cs ##sen ##gin ##e . java : 55 ##1 ) at org . eclipse . e ##4 . ui . cs ##s . core . imp ##l . engine . abstract ##cs ##sen ##gin ##e . apply ##sty ##les ( abstract ##cs ##sen ##gin ##e . java : 46 ##5 ) at org . eclipse . e ##4 . ui . cs ##s . core . imp ##l . engine . abstract ##cs ##sen ##gin ##e . apply ##sty ##les ( abstract ##cs ##sen ##gin ##e . java : 405 ) at org . eclipse . e ##4 . ui . cs ##s . sw ##t . internal . theme . theme ##eng ##ine . apply ##sty ##les ( theme ##eng ##ine . java : 45 ##5 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e $ 10 . set ##class ##name ##and ##id ( part ##ren ##der ##ingen ##gin ##e . java : 115 ##9 ) at org . eclipse . e ##4 . ui . work ##ben ##ch . render ##ers . sw ##t . sw ##tp ##art ##ren ##der ##er . set ##cs ##sin ##fo ( sw ##tp ##art ##ren ##der ##er . java : 85 ) at org . eclipse . e ##4 . ui . work ##ben ##ch . render ##ers . sw ##t . sw ##tp ##art ##ren ##der ##er . bind ##wi ##dget ( sw ##tp ##art ##ren ##der ##er . java : 93 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . create ##wi ##dget ( part ##ren ##der ##ingen ##gin ##e . java : 89 ##0 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . safe ##cre ##ate ##gui ( part ##ren ##der ##ingen ##gin ##e . java : 62 ##2 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . safe ##cre ##ate ##gui ( part ##ren ##der ##ingen ##gin ##e . java : 72 ##4 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . access $ 2 ( part ##ren ##der ##ingen ##gin ##e . java : 69 ##5 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e $ 7 . run ( part ##ren ##der ##ingen ##gin ##e . java : 68 ##9 ) at org . eclipse . core . run ##time . safer ##un ##ner . run ( safer ##un ##ner . java : 42 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . create ##gui ( part ##ren ##der ##ingen ##gin ##e . java : 67 ##4 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e $ 1 . handle ##eve ##nt ( part ##ren ##der ##ingen ##gin ##e . java : 126 ) at org . eclipse . e ##4 . ui . services . internal . events . ui ##eve ##nt ##hand ##ler $ 1 . run ( ui ##eve ##nt ##hand ##ler . java : 41 ) at org . eclipse . sw ##t . wi ##dgets . sync ##hr ##oni ##zer . sync ##ex ##ec ( sync ##hr ##oni ##zer . java : 180 ) at org . eclipse . ui . internal . ui ##sy ##nch ##ron ##izer . sync ##ex ##ec ( ui ##sy ##nch ##ron ##izer . java : 150 ) at org . eclipse . sw ##t . wi ##dgets . display . sync ##ex ##ec ( display . java : 42 ##9 ##1 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . e ##4 ##app ##lica ##tion $ 1 . sync ##ex ##ec ( e ##4 ##app ##lica ##tion . java : 187 ) at org . eclipse . e ##4 . ui . services . internal . events . ui ##eve ##nt ##hand ##ler . handle ##eve ##nt ( ui ##eve ##nt ##hand ##ler . java : 38 ) at org . eclipse . e ##quin ##ox . internal . event . event ##hand ##ler ##wr ##app ##er . handle ##eve ##nt ( event ##hand ##ler ##wr ##app ##er . java : 197 ) at org . eclipse . e ##quin ##ox . internal . event . event ##hand ##ler ##tra ##cker . dispatch ##eve ##nt ( event ##hand ##ler ##tra ##cker . java : 197 ) at org . eclipse . e ##quin ##ox . internal . event . event ##hand ##ler ##tra ##cker . dispatch ##eve ##nt ( event ##hand ##ler ##tra ##cker . java : 1 ) at org . eclipse . os ##gi . framework . event ##mg ##r . event ##mana ##ger . dispatch ##eve ##nt ( event ##mana ##ger . java : 230 ) at org . eclipse . os ##gi . framework . event ##mg ##r . listener ##que ##ue . dispatch ##eve ##nts ##yn ##ch ##ron ##ous ( listener ##que ##ue . java : 148 ) at org . eclipse . e ##quin ##ox . internal . event . event ##ad ##mini ##mp ##l . dispatch ##eve ##nt ( event ##ad ##mini ##mp ##l . java : 135 ) at org . eclipse . e ##quin ##ox . internal . event . event ##ad ##mini ##mp ##l . send ##eve ##nt ( event ##ad ##mini ##mp ##l . java : 78 ) at org . eclipse . e ##quin ##ox . internal . event . event ##com ##pone ##nt . send ##eve ##nt ( event ##com ##pone ##nt . java : 39 ) at org . eclipse . e ##4 . ui . services . internal . events . event ##bro ##ker . send ( event ##bro ##ker . java : 81 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . ui ##eve ##nt ##pu ##bl ##ish ##er . not ##ify ##chang ##ed ( ui ##eve ##nt ##pu ##bl ##ish ##er . java : 57 ) at org . eclipse . em ##f . common . not ##ify . imp ##l . basic ##not ##ifier ##im ##pl . en ##ot ##ify ( basic ##not ##ifier ##im ##pl . java : 37 ##4 ) at org . eclipse . e ##4 . ui . model . application . ui . imp ##l . ui ##ele ##ment ##im ##pl . set ##to ##ber ##end ##ered ( ui ##ele ##ment ##im ##pl . java : 290 ) at org . eclipse . ui . internal . work ##ben ##ch ##wind ##ow . pop ##ulate ##top ##tri ##mc ##ont ##ri ##bution ##s ( work ##ben ##ch ##wind ##ow . java : 70 ##2 ) at org . eclipse . ui . internal . work ##ben ##ch ##wind ##ow . setup ( work ##ben ##ch ##wind ##ow . java : 54 ##4 ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ##0 ( native method ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 57 ) at sun . reflect . del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 43 ) at java . lang . reflect . method . in ##vo ##ke ( method . java : 61 ##6 ) at org . eclipse . e ##4 . core . internal . di . method ##re ##quest ##or . execute ( method ##re ##quest ##or . java : 56 ) at org . eclipse . e ##4 . core . internal . di . in ##ject ##ori ##mp ##l . process ##ann ##ota ##ted ( in ##ject ##ori ##mp ##l . java : 85 ##7 ) at org . eclipse . e ##4 . core . internal . di . in ##ject ##ori ##mp ##l . in ##ject ( in ##ject ##ori ##mp ##l . java : 111 ) at org . eclipse . e ##4 . core . internal . di . in ##ject ##ori ##mp ##l . in ##ject ( in ##ject ##ori ##mp ##l . java : 81 ) at org . eclipse . e ##4 . core . contexts . context ##in ##ject ##ion ##factory . in ##ject ( context ##in ##ject ##ion ##factory . java : 72 ) at org . eclipse . ui . internal . work ##ben ##ch . create ##work ##ben ##ch ##wind ##ow ( work ##ben ##ch . java : 124 ##9 ) at org . eclipse . ui . internal . work ##ben ##ch . get ##active ##work ##ben ##ch ##wind ##ow ( work ##ben ##ch . java : 122 ##1 ) at org . eclipse . ui . internal . services . work ##ben ##chs ##our ##ce ##pro ##vid ##er . update ##active ##sh ##ell ( work ##ben ##chs ##our ##ce ##pro ##vid ##er . java : 92 ##4 ) at org . eclipse . ui . internal . services . work ##ben ##chs ##our ##ce ##pro ##vid ##er . get ##cu ##rre ##nts ##tate ( work ##ben ##chs ##our ##ce ##pro ##vid ##er . java : 133 ) at org . eclipse . ui . internal . services . work ##ben ##chs ##our ##ce ##pro ##vid ##er $ 6 . handle ##eve ##nt ( work ##ben ##chs ##our ##ce ##pro ##vid ##er . java : 68 ##4 ) at org . eclipse . sw ##t . wi ##dgets . event ##table . send ##eve ##nt ( event ##table . java : 84 ) at org . eclipse . sw ##t . wi ##dgets . display . filter ##eve ##nt ( display . java : 148 ##3 ) at org . eclipse . sw ##t . wi ##dgets . wi ##dget . send ##eve ##nt ( wi ##dget . java : 127 ##5 ) at org . eclipse . sw ##t . wi ##dgets . wi ##dget . send ##eve ##nt ( wi ##dget . java : 1300 ) at org . eclipse . sw ##t . wi ##dgets . wi ##dget . send ##eve ##nt ( wi ##dget . java : 128 ##1 ) at org . eclipse . sw ##t . wi ##dgets . shell . filter ##pro ##c ( shell . java : 73 ##1 ) at org . eclipse . sw ##t . wi ##dgets . display . filter ##pro ##c ( display . java : 149 ##5 ) at org . eclipse . sw ##t . internal . gt ##k . os . _ g _ main _ context _ iteration ( native method ) at org . eclipse . sw ##t . internal . gt ##k . os . g _ main _ context _ iteration ( os . java : 233 ##2 ) at org . eclipse . sw ##t . wi ##dgets . display . read ##and ##dis ##pa ##tch ( display . java : 317 ##7 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e $ 9 . run ( part ##ren ##der ##ingen ##gin ##e . java : 102 ##1 ) at org . eclipse . core . data ##bin ##ding . ob ##ser ##vable . realm . run ##with ##de ##fa ##ult ( realm . java : 332 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . sw ##t . part ##ren ##der ##ingen ##gin ##e . run ( part ##ren ##der ##ingen ##gin ##e . java : 91 ##5 ) at org . eclipse . e ##4 . ui . internal . work ##ben ##ch . e ##4 ##work ##ben ##ch . create ##and ##run ##ui ( e ##4 ##work ##ben ##ch . java : 86 ) at org . eclipse . ui . internal . work ##ben ##ch $ 5 . run ( work ##ben ##ch . java : 58 ##5 ) at org . eclipse . core . data ##bin ##ding . ob ##ser ##vable . realm . run ##with ##de ##fa ##ult ( realm . java : 332 ) at org . eclipse . ui . internal . work ##ben ##ch . create ##and ##run ##work ##ben ##ch ( work ##ben ##ch . java : 540 ) at org . eclipse . ui . platform ##ui . create ##and ##run ##work ##ben ##ch ( platform ##ui . java : 149 ) at test . ui . sw ##t . core . application . application . starts ##ecure ( application . java : 43 ) at test . ui . sw ##t . core . application . application $ 1 . run ( application . java : 35 ) at java . security . access ##con ##tro ##ller . do ##pr ##iv ##ile ##ged ( native method ) at java ##x . security . au ##th . subject . do ##as ( subject . java : 41 ##6 ) at test . ui . sw ##t . core . application . application . start ( application . java : 32 ) at org . eclipse . e ##quin ##ox . internal . app . eclipse ##app ##hand ##le . run ( eclipse ##app ##hand ##le . java : 196 ) at org . eclipse . core . run ##time . internal . adapt ##or . eclipse ##app ##lau ##nche ##r . run ##app ##lica ##tion ( eclipse ##app ##lau ##nche ##r . java : 110 ) at org . eclipse . core . run ##time . internal . adapt ##or . eclipse ##app ##lau ##nche ##r . start ( eclipse ##app ##lau ##nche ##r . java : 79 ) at org . eclipse . core . run ##time . adapt ##or . eclipse ##star ##ter . run ( eclipse ##star ##ter . java : 35 ##3 ) at org . eclipse . core . run ##time . adapt ##or . eclipse ##star ##ter . run ( eclipse ##star ##ter . java : 180 ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ##0 ( native method ) at sun . reflect . native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( native ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 57 ) at sun . reflect . del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . in ##vo ##ke ( del ##ega ##ting ##met ##ho ##da ##cc ##ess ##ori ##mp ##l . java : 43 ) at java . lang . reflect . method . in ##vo ##ke ( method . java : 61 ##6 ) at org . eclipse . e ##quin ##ox . launcher . main . in ##vo ##ke ##frame ##work ( main . java : 62 ##9 ) at org . eclipse . e ##quin ##ox . launcher . main . basic ##run ( main . java : 58 ##4 ) at org . eclipse . e ##quin ##ox . launcher . main . run ( main . java : 143 ##8 ) at org . eclipse . e ##quin ##ox . launcher . main . main ( main . java : 141 ##4 ) [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  2043,  1037,  2047,  7464,  2622,  2003,  2580,  1010,\n",
       "         1996, 20116,  2015,  4323,  1997,  1996,  4132,  2003,  2109,\n",
       "         2011, 12398,  1012,  2043,  1996, 25430,  2102,  7396,  2003,\n",
       "         2318,  1996,  4281,  1011,  3746,  1012,  1013, 14181,  2243,\n",
       "        17603,  2100,  1012,  1052,  3070,  2003,  2025,  2179,  1012,\n",
       "         2156,  2036, 11829,  4261,  2683,  2683,  2620,  2683,  1012,\n",
       "         9262,  1012, 22834,  1012,  5371, 17048, 14876,  8630, 10288,\n",
       "        24422,  1024,  1012,  1013, 14181,  2243, 17603,  2100,  1012,\n",
       "         1052,  3070,  1006,  3058,  2072, 27215,  2310, 15378,  7416,\n",
       "         2818,  8977, 27969, 11039, 16216, 11263, 25915,  1007,  2012,\n",
       "         9262,  1012, 22834,  1012,  5371,  2378, 18780, 21422,  1012,\n",
       "          102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 380219,\n",
       " 'priority': '1\\n',\n",
       " 'product': '15\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_token': array([  101,  1031, 25430,  2102,  1033, 11603,  1011,  6453,  1999,\n",
       "         8833,  2043,  3225,  7396,  1024,  4281,  1011,  3746,  2003,\n",
       "         2025,  2179,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          102,   101,  2043,  1037,  2047,  7464,  2622,  2003,  2580,\n",
       "         1010,  1996, 20116,  2015,  4323,  1997,  1996,  4132,  2003,\n",
       "         2109,  2011, 12398,  1012,  2043,  1996, 25430,  2102,  7396,\n",
       "         2003,  2318,  1996,  4281,  1011,  3746,  1012,  1013, 14181,\n",
       "         2243, 17603,  2100,  1012,  1052,  3070,  2003,  2025,  2179,\n",
       "         1012,  2156,  2036, 11829,  4261,  2683,  2683,  2620,  2683,\n",
       "         1012,  9262,  1012, 22834,  1012,  5371, 17048, 14876,  8630,\n",
       "        10288, 24422,  1024,  1012,  1013, 14181,  2243, 17603,  2100,\n",
       "         1012,  1052,  3070,  1006,  3058,  2072, 27215,  2310, 15378,\n",
       "         7416,  2818,  8977, 27969, 11039, 16216, 11263, 25915,  1007,\n",
       "         2012,  9262,  1012, 22834,  1012,  5371,  2378, 18780, 21422,\n",
       "         1012,   102]),\n",
       " 'title': '[CLS] [ sw ##t ] linux - exception in log when starting client : background - image is not found [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  1031, 25430,  2102,  1033, 11603,  1011,  6453,  1999,\n",
       "         8833,  2043,  3225,  7396,  1024,  4281,  1011,  3746,  2003,\n",
       "         2025,  2179,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          102]),\n",
       " 'topic': 22,\n",
       " 'topic_index': 21,\n",
       " 'topics': array([0.  , 0.  , 0.  , 0.  , 0.01, 0.11, 0.  , 0.  , 0.  , 0.  , 0.01,\n",
       "        0.01, 0.05, 0.01, 0.  , 0.03, 0.05, 0.  , 0.02, 0.  , 0.  , 0.  ,\n",
       "        0.32, 0.01, 0.  , 0.  , 0.22, 0.11, 0.01, 0.  ]),\n",
       " 'version': '462\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58671"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(list(issues_by_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110647 in experiment.baseline.bug_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 484 ms, sys: 0 ns, total: 484 ms\n",
      "Wall time: 483 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = experiment.batch_iterator(None, \n",
    "                                                                                          baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1,\n",
    "                                                                                          issues_by_buckets)\n",
    "\n",
    "pos = np.full((1, batch_size_test), 1)\n",
    "neg = np.full((1, batch_size_test), 0)\n",
    "valid_sim = np.concatenate([pos, neg], -1)[0]\n",
    "\n",
    "valid_title_sample_a = np.concatenate([valid_input_sample['title'], valid_input_sample['title']], 0)\n",
    "valid_title_sample_b = np.concatenate([valid_input_pos['title'], valid_input_neg['title']], 0)\n",
    "valid_desc_sample_a = np.concatenate([valid_input_sample['description'], valid_input_sample['description']], 0)\n",
    "valid_desc_sample_b = np.concatenate([valid_input_pos['description'], valid_input_neg['description']], 0)\n",
    "\n",
    "validation_sample = [valid_title_sample_a, valid_title_sample_b, valid_desc_sample_a, valid_desc_sample_b]\n",
    "\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_title_sample_a), len(valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 100), (256,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 21175'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a839e39e04d5ba386c20d6f603ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9421d6d284034923b6471ba8e834aaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 35s, sys: 3.33 s, total: 1min 39s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### DWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, \\\n",
    "    Average, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import keras.backend as K\n",
    "\n",
    "def dwen_feature(title_feature_model, desc_feature_model, \\\n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    \n",
    "    # Embedding feature\n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    bug_d_feat = GlobalAveragePooling1D()(bug_d_feat)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = Average(name = 'merge_features_{}'.format(name))([bug_t_feat, bug_d_feat])\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model\n",
    "\n",
    "def dwen_model(bug_feature_output_a, bug_feature_output_b, name):\n",
    "    \n",
    "    inputs = np.concatenate([bug_feature_output_a.input, bug_feature_output_b.input], -1).tolist()\n",
    "    \n",
    "    bug_feature_output_a = bug_feature_output_a.output\n",
    "    bug_feature_output_b = bug_feature_output_b.output\n",
    "    \n",
    "    # 2D concatenate feature\n",
    "    bug_feature_output = concatenate([bug_feature_output_a, bug_feature_output_b])\n",
    "    \n",
    "    hidden_layers = 2\n",
    "    \n",
    "    # Deep Hidden MLPs\n",
    "    for _ in range(hidden_layers):\n",
    "        number_of_units = K.int_shape(bug_feature_output)[1]\n",
    "        bug_feature_output = Dense(number_of_units // 2)(bug_feature_output)\n",
    "#         bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "        bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "        #bug_feature_output = Dropout(.5)(bug_feature_output)\n",
    "    \n",
    "     # Sigmoid\n",
    "    output = Dense(1, activation='sigmoid')(bug_feature_output)\n",
    "\n",
    "    similarity_model = Model(inputs=inputs, outputs=[output], name = 'dwen_output')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'baseline_dwen_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 100, 300)     581600700   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 100, 300)     581600700   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,163,427,001\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.76, Loss_test: 0.69, acc: 0.50, acc_test: 0.50\n",
      "Epoch: 2 Loss: 0.70, Loss_test: 0.71, acc: 0.50, acc_test: 0.50\n",
      "Epoch: 3 Loss: 0.72, Loss_test: 0.70, acc: 0.50, acc_test: 0.50\n",
      "Epoch: 4 Loss: 0.70, Loss_test: 0.69, acc: 0.50, acc_test: 0.54\n",
      "Epoch: 5 Loss: 0.70, Loss_test: 0.70, acc: 0.50, acc_test: 0.50\n",
      "Epoch: 6 Loss: 0.69, Loss_test: 0.71, acc: 0.60, acc_test: 0.50\n",
      "Epoch: 7 Loss: 0.69, Loss_test: 0.72, acc: 0.49, acc_test: 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 0.69, Loss_test: 0.71, acc: 0.51, acc_test: 0.50\n",
      "Epoch: 9 Loss: 0.69, Loss_test: 0.70, acc: 0.53, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.68, Loss_test: 0.69, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 11 Loss: 0.68, Loss_test: 0.69, acc: 0.48, acc_test: 0.52\n",
      "Epoch: 12 Loss: 0.69, Loss_test: 0.69, acc: 0.47, acc_test: 0.52\n",
      "Epoch: 13 Loss: 0.69, Loss_test: 0.70, acc: 0.45, acc_test: 0.52\n",
      "Epoch: 14 Loss: 0.68, Loss_test: 0.71, acc: 0.55, acc_test: 0.51\n",
      "Epoch: 15 Loss: 0.67, Loss_test: 0.72, acc: 0.64, acc_test: 0.50\n",
      "Epoch: 16 Loss: 0.67, Loss_test: 0.72, acc: 0.62, acc_test: 0.50\n",
      "Epoch: 17 Loss: 0.68, Loss_test: 0.72, acc: 0.59, acc_test: 0.52\n",
      "Epoch: 18 Loss: 0.68, Loss_test: 0.72, acc: 0.56, acc_test: 0.51\n",
      "Epoch: 19 Loss: 0.67, Loss_test: 0.72, acc: 0.64, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.69, Loss_test: 0.72, acc: 0.55, acc_test: 0.51\n",
      "Epoch: 21 Loss: 0.67, Loss_test: 0.73, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 22 Loss: 0.66, Loss_test: 0.74, acc: 0.63, acc_test: 0.51\n",
      "Epoch: 23 Loss: 0.66, Loss_test: 0.75, acc: 0.64, acc_test: 0.52\n",
      "Epoch: 24 Loss: 0.67, Loss_test: 0.76, acc: 0.61, acc_test: 0.51\n",
      "Epoch: 25 Loss: 0.67, Loss_test: 0.77, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 26 Loss: 0.69, Loss_test: 0.77, acc: 0.54, acc_test: 0.52\n",
      "Epoch: 27 Loss: 0.67, Loss_test: 0.76, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 28 Loss: 0.66, Loss_test: 0.75, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 29 Loss: 0.66, Loss_test: 0.76, acc: 0.66, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.66, Loss_test: 0.78, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 31 Loss: 0.66, Loss_test: 0.81, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 32 Loss: 0.68, Loss_test: 0.82, acc: 0.60, acc_test: 0.50\n",
      "Epoch: 33 Loss: 0.65, Loss_test: 0.81, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 34 Loss: 0.69, Loss_test: 0.79, acc: 0.55, acc_test: 0.51\n",
      "Epoch: 35 Loss: 0.67, Loss_test: 0.78, acc: 0.57, acc_test: 0.51\n",
      "Epoch: 36 Loss: 0.67, Loss_test: 0.77, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 37 Loss: 0.68, Loss_test: 0.77, acc: 0.58, acc_test: 0.52\n",
      "Epoch: 38 Loss: 0.63, Loss_test: 0.78, acc: 0.67, acc_test: 0.51\n",
      "Epoch: 39 Loss: 0.66, Loss_test: 0.77, acc: 0.60, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.64, Loss_test: 0.76, acc: 0.63, acc_test: 0.51\n",
      "Epoch: 41 Loss: 0.65, Loss_test: 0.77, acc: 0.64, acc_test: 0.51\n",
      "Epoch: 42 Loss: 0.65, Loss_test: 0.78, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 43 Loss: 0.64, Loss_test: 0.79, acc: 0.67, acc_test: 0.52\n",
      "Epoch: 44 Loss: 0.66, Loss_test: 0.79, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 45 Loss: 0.62, Loss_test: 0.78, acc: 0.67, acc_test: 0.51\n",
      "Epoch: 46 Loss: 0.66, Loss_test: 0.80, acc: 0.60, acc_test: 0.51\n",
      "Epoch: 47 Loss: 0.64, Loss_test: 0.80, acc: 0.64, acc_test: 0.51\n",
      "Epoch: 48 Loss: 0.65, Loss_test: 0.80, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 49 Loss: 0.65, Loss_test: 0.78, acc: 0.62, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.63, Loss_test: 0.77, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 51 Loss: 0.67, Loss_test: 0.80, acc: 0.60, acc_test: 0.51\n",
      "Epoch: 52 Loss: 0.64, Loss_test: 0.80, acc: 0.61, acc_test: 0.51\n",
      "Epoch: 53 Loss: 0.64, Loss_test: 0.77, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 54 Loss: 0.62, Loss_test: 0.72, acc: 0.63, acc_test: 0.52\n",
      "Epoch: 55 Loss: 0.66, Loss_test: 0.71, acc: 0.59, acc_test: 0.52\n",
      "Epoch: 56 Loss: 0.67, Loss_test: 0.73, acc: 0.55, acc_test: 0.52\n",
      "Epoch: 57 Loss: 0.62, Loss_test: 0.77, acc: 0.62, acc_test: 0.51\n",
      "Epoch: 58 Loss: 0.63, Loss_test: 0.78, acc: 0.65, acc_test: 0.51\n",
      "Epoch: 59 Loss: 0.65, Loss_test: 0.75, acc: 0.62, acc_test: 0.52\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.70, Loss_test: 0.72, acc: 0.55, acc_test: 0.53\n",
      "Epoch: 61 Loss: 0.64, Loss_test: 0.72, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 62 Loss: 0.67, Loss_test: 0.73, acc: 0.55, acc_test: 0.52\n",
      "Epoch: 63 Loss: 0.67, Loss_test: 0.74, acc: 0.59, acc_test: 0.52\n",
      "Epoch: 64 Loss: 0.65, Loss_test: 0.72, acc: 0.65, acc_test: 0.52\n",
      "Epoch: 65 Loss: 0.59, Loss_test: 0.70, acc: 0.69, acc_test: 0.52\n",
      "Epoch: 66 Loss: 0.62, Loss_test: 0.69, acc: 0.65, acc_test: 0.52\n",
      "Epoch: 67 Loss: 0.64, Loss_test: 0.70, acc: 0.61, acc_test: 0.52\n",
      "Epoch: 68 Loss: 0.62, Loss_test: 0.72, acc: 0.56, acc_test: 0.52\n",
      "Epoch: 69 Loss: 0.65, Loss_test: 0.74, acc: 0.59, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.63, Loss_test: 0.72, acc: 0.62, acc_test: 0.52\n",
      "Epoch: 71 Loss: 0.67, Loss_test: 0.70, acc: 0.57, acc_test: 0.51\n",
      "Epoch: 72 Loss: 0.62, Loss_test: 0.69, acc: 0.66, acc_test: 0.50\n",
      "Epoch: 73 Loss: 0.65, Loss_test: 0.70, acc: 0.57, acc_test: 0.52\n",
      "Epoch: 74 Loss: 0.65, Loss_test: 0.70, acc: 0.62, acc_test: 0.52\n",
      "Epoch: 75 Loss: 0.66, Loss_test: 0.70, acc: 0.57, acc_test: 0.53\n",
      "Epoch: 76 Loss: 0.62, Loss_test: 0.69, acc: 0.64, acc_test: 0.55\n",
      "Epoch: 77 Loss: 0.64, Loss_test: 0.69, acc: 0.61, acc_test: 0.55\n",
      "Epoch: 78 Loss: 0.64, Loss_test: 0.69, acc: 0.65, acc_test: 0.55\n",
      "Epoch: 79 Loss: 0.68, Loss_test: 0.69, acc: 0.57, acc_test: 0.54\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.62, Loss_test: 0.69, acc: 0.69, acc_test: 0.53\n",
      "Epoch: 81 Loss: 0.63, Loss_test: 0.69, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 82 Loss: 0.67, Loss_test: 0.69, acc: 0.59, acc_test: 0.55\n",
      "Epoch: 83 Loss: 0.68, Loss_test: 0.69, acc: 0.56, acc_test: 0.55\n",
      "Epoch: 84 Loss: 0.62, Loss_test: 0.69, acc: 0.64, acc_test: 0.52\n",
      "Epoch: 85 Loss: 0.63, Loss_test: 0.70, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 86 Loss: 0.60, Loss_test: 0.71, acc: 0.67, acc_test: 0.50\n",
      "Epoch: 87 Loss: 0.67, Loss_test: 0.69, acc: 0.55, acc_test: 0.52\n",
      "Epoch: 88 Loss: 0.66, Loss_test: 0.69, acc: 0.65, acc_test: 0.52\n",
      "Epoch: 89 Loss: 0.62, Loss_test: 0.69, acc: 0.66, acc_test: 0.54\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.61, Loss_test: 0.69, acc: 0.67, acc_test: 0.55\n",
      "Epoch: 91 Loss: 0.65, Loss_test: 0.69, acc: 0.59, acc_test: 0.54\n",
      "Epoch: 92 Loss: 0.65, Loss_test: 0.69, acc: 0.60, acc_test: 0.53\n",
      "Epoch: 93 Loss: 0.64, Loss_test: 0.69, acc: 0.58, acc_test: 0.55\n",
      "Epoch: 94 Loss: 0.63, Loss_test: 0.69, acc: 0.66, acc_test: 0.54\n",
      "Epoch: 95 Loss: 0.60, Loss_test: 0.69, acc: 0.69, acc_test: 0.55\n",
      "Epoch: 96 Loss: 0.62, Loss_test: 0.69, acc: 0.64, acc_test: 0.54\n",
      "Epoch: 97 Loss: 0.61, Loss_test: 0.69, acc: 0.64, acc_test: 0.52\n",
      "Epoch: 98 Loss: 0.62, Loss_test: 0.69, acc: 0.66, acc_test: 0.52\n",
      "Epoch: 99 Loss: 0.67, Loss_test: 0.70, acc: 0.63, acc_test: 0.52\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.67, Loss_test: 0.69, acc: 0.59, acc_tets: 0.52, recall@25: 0.16\n",
      "Best_epoch=65, Best_loss=0.59, Recall@25=0.16\n",
      "CPU times: user 1min 43s, sys: 3.21 s, total: 1min 46s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Similarity model\n",
    "bug_feature_output_a = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_a')\n",
    "bug_feature_output_b = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_b')\n",
    "similarity_model = dwen_model(bug_feature_output_a, bug_feature_output_b, 'dwen')\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, \n",
    "                                                        bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/bert/exported_rank_baseline_dwen_100.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_100_feature_100epochs_64batch(openoffice).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      581257500   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      581257500   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,162,740,601\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('dense_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.53, Loss_test: 0.70, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 102 Loss: 0.48, Loss_test: 0.70, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 103 Loss: 0.60, Loss_test: 0.73, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 104 Loss: 0.64, Loss_test: 0.76, acc: 0.65, acc_test: 0.61\n",
      "Epoch: 105 Loss: 0.49, Loss_test: 0.70, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 106 Loss: 0.59, Loss_test: 0.70, acc: 0.69, acc_test: 0.59\n",
      "Epoch: 107 Loss: 0.58, Loss_test: 0.70, acc: 0.66, acc_test: 0.59\n",
      "Epoch: 108 Loss: 0.52, Loss_test: 0.69, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 109 Loss: 0.47, Loss_test: 0.68, acc: 0.80, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.55, Loss_test: 0.69, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 111 Loss: 0.47, Loss_test: 0.69, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 112 Loss: 0.49, Loss_test: 0.69, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 113 Loss: 0.48, Loss_test: 0.69, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 114 Loss: 0.49, Loss_test: 0.68, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 115 Loss: 0.59, Loss_test: 0.68, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 116 Loss: 0.57, Loss_test: 0.69, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 117 Loss: 0.47, Loss_test: 0.69, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 118 Loss: 0.54, Loss_test: 0.69, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 119 Loss: 0.62, Loss_test: 0.69, acc: 0.61, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.50, Loss_test: 0.74, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 121 Loss: 0.48, Loss_test: 0.73, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 122 Loss: 0.51, Loss_test: 0.70, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 123 Loss: 0.47, Loss_test: 0.70, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 124 Loss: 0.53, Loss_test: 0.73, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 125 Loss: 0.51, Loss_test: 0.76, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 126 Loss: 0.55, Loss_test: 0.74, acc: 0.68, acc_test: 0.58\n",
      "Epoch: 127 Loss: 0.50, Loss_test: 0.69, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 128 Loss: 0.60, Loss_test: 0.69, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 129 Loss: 0.53, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.50, Loss_test: 0.70, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 131 Loss: 0.59, Loss_test: 0.71, acc: 0.69, acc_test: 0.62\n",
      "Epoch: 132 Loss: 0.49, Loss_test: 0.69, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 133 Loss: 0.47, Loss_test: 0.68, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 134 Loss: 0.58, Loss_test: 0.71, acc: 0.69, acc_test: 0.59\n",
      "Epoch: 135 Loss: 0.56, Loss_test: 0.69, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 136 Loss: 0.51, Loss_test: 0.69, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 137 Loss: 0.55, Loss_test: 0.73, acc: 0.68, acc_test: 0.61\n",
      "Epoch: 138 Loss: 0.54, Loss_test: 0.71, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 139 Loss: 0.48, Loss_test: 0.68, acc: 0.73, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.48, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 141 Loss: 0.50, Loss_test: 0.68, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 142 Loss: 0.50, Loss_test: 0.69, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 143 Loss: 0.46, Loss_test: 0.70, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 144 Loss: 0.48, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 145 Loss: 0.51, Loss_test: 0.68, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 146 Loss: 0.58, Loss_test: 0.68, acc: 0.68, acc_test: 0.58\n",
      "Epoch: 147 Loss: 0.50, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 148 Loss: 0.46, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 149 Loss: 0.51, Loss_test: 0.68, acc: 0.72, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.48, Loss_test: 0.68, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 151 Loss: 0.61, Loss_test: 0.68, acc: 0.64, acc_test: 0.61\n",
      "Epoch: 152 Loss: 0.45, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 153 Loss: 0.47, Loss_test: 0.69, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 154 Loss: 0.60, Loss_test: 0.69, acc: 0.67, acc_test: 0.61\n",
      "Epoch: 155 Loss: 0.48, Loss_test: 0.68, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 156 Loss: 0.51, Loss_test: 0.68, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 157 Loss: 0.50, Loss_test: 0.69, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 158 Loss: 0.49, Loss_test: 0.72, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 159 Loss: 0.52, Loss_test: 0.69, acc: 0.78, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.47, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 161 Loss: 0.42, Loss_test: 0.70, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 162 Loss: 0.48, Loss_test: 0.69, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 163 Loss: 0.50, Loss_test: 0.70, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 164 Loss: 0.49, Loss_test: 0.70, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 165 Loss: 0.53, Loss_test: 0.70, acc: 0.71, acc_test: 0.60\n",
      "Epoch: 166 Loss: 0.44, Loss_test: 0.76, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 167 Loss: 0.48, Loss_test: 0.78, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 168 Loss: 0.48, Loss_test: 0.72, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 169 Loss: 0.47, Loss_test: 0.70, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.46, Loss_test: 0.70, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 171 Loss: 0.56, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 172 Loss: 0.51, Loss_test: 0.84, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 173 Loss: 0.62, Loss_test: 0.80, acc: 0.65, acc_test: 0.60\n",
      "Epoch: 174 Loss: 0.58, Loss_test: 0.71, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 175 Loss: 0.44, Loss_test: 0.70, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 176 Loss: 0.55, Loss_test: 0.69, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 177 Loss: 0.58, Loss_test: 0.69, acc: 0.69, acc_test: 0.60\n",
      "Epoch: 178 Loss: 0.46, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 179 Loss: 0.55, Loss_test: 0.68, acc: 0.69, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.50, Loss_test: 0.71, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 181 Loss: 0.48, Loss_test: 0.74, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 182 Loss: 0.49, Loss_test: 0.75, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 183 Loss: 0.49, Loss_test: 0.72, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 184 Loss: 0.54, Loss_test: 0.70, acc: 0.70, acc_test: 0.61\n",
      "Epoch: 185 Loss: 0.46, Loss_test: 0.69, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 186 Loss: 0.59, Loss_test: 0.70, acc: 0.68, acc_test: 0.60\n",
      "Epoch: 187 Loss: 0.50, Loss_test: 0.70, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 188 Loss: 0.49, Loss_test: 0.71, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 189 Loss: 0.46, Loss_test: 0.71, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.57, Loss_test: 0.77, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 191 Loss: 0.55, Loss_test: 0.78, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 192 Loss: 0.46, Loss_test: 0.74, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 193 Loss: 0.49, Loss_test: 0.73, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 194 Loss: 0.65, Loss_test: 0.72, acc: 0.67, acc_test: 0.61\n",
      "Epoch: 195 Loss: 0.51, Loss_test: 0.73, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 196 Loss: 0.56, Loss_test: 0.74, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 197 Loss: 0.53, Loss_test: 0.72, acc: 0.70, acc_test: 0.61\n",
      "Epoch: 198 Loss: 0.50, Loss_test: 0.70, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 199 Loss: 0.53, Loss_test: 0.70, acc: 0.72, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.49, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 201 Loss: 0.47, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 202 Loss: 0.50, Loss_test: 0.70, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 203 Loss: 0.49, Loss_test: 0.74, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 204 Loss: 0.58, Loss_test: 0.76, acc: 0.69, acc_test: 0.55\n",
      "Epoch: 205 Loss: 0.55, Loss_test: 0.74, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 206 Loss: 0.44, Loss_test: 0.69, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 207 Loss: 0.48, Loss_test: 0.69, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 208 Loss: 0.42, Loss_test: 0.73, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 209 Loss: 0.51, Loss_test: 0.72, acc: 0.73, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.54, Loss_test: 0.69, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 211 Loss: 0.49, Loss_test: 0.70, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 212 Loss: 0.54, Loss_test: 0.70, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 213 Loss: 0.56, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 214 Loss: 0.50, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 215 Loss: 0.47, Loss_test: 0.71, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 216 Loss: 0.60, Loss_test: 0.69, acc: 0.66, acc_test: 0.59\n",
      "Epoch: 217 Loss: 0.48, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 218 Loss: 0.47, Loss_test: 0.71, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 219 Loss: 0.50, Loss_test: 0.71, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.55, Loss_test: 0.69, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 221 Loss: 0.46, Loss_test: 0.70, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 222 Loss: 0.44, Loss_test: 0.72, acc: 0.77, acc_test: 0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 Loss: 0.51, Loss_test: 0.70, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 224 Loss: 0.50, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 225 Loss: 0.41, Loss_test: 0.70, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 226 Loss: 0.50, Loss_test: 0.70, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 227 Loss: 0.50, Loss_test: 0.70, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 228 Loss: 0.53, Loss_test: 0.70, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 229 Loss: 0.44, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.51, Loss_test: 0.70, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 231 Loss: 0.52, Loss_test: 0.73, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 232 Loss: 0.46, Loss_test: 0.74, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 233 Loss: 0.49, Loss_test: 0.72, acc: 0.71, acc_test: 0.61\n",
      "Epoch: 234 Loss: 0.51, Loss_test: 0.71, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 235 Loss: 0.52, Loss_test: 0.71, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 236 Loss: 0.50, Loss_test: 0.71, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 237 Loss: 0.53, Loss_test: 0.71, acc: 0.72, acc_test: 0.58\n",
      "Epoch: 238 Loss: 0.48, Loss_test: 0.71, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 239 Loss: 0.46, Loss_test: 0.71, acc: 0.75, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.47, Loss_test: 0.71, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 241 Loss: 0.53, Loss_test: 0.72, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 242 Loss: 0.48, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 243 Loss: 0.55, Loss_test: 0.72, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 244 Loss: 0.45, Loss_test: 0.71, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 245 Loss: 0.47, Loss_test: 0.72, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 246 Loss: 0.46, Loss_test: 0.72, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 247 Loss: 0.47, Loss_test: 0.71, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 248 Loss: 0.56, Loss_test: 0.70, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 249 Loss: 0.50, Loss_test: 0.70, acc: 0.72, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.50, Loss_test: 0.73, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 251 Loss: 0.50, Loss_test: 0.76, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 252 Loss: 0.56, Loss_test: 0.75, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 253 Loss: 0.47, Loss_test: 0.70, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 254 Loss: 0.45, Loss_test: 0.70, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 255 Loss: 0.56, Loss_test: 0.71, acc: 0.69, acc_test: 0.61\n",
      "Epoch: 256 Loss: 0.58, Loss_test: 0.69, acc: 0.69, acc_test: 0.61\n",
      "Epoch: 257 Loss: 0.49, Loss_test: 0.71, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 258 Loss: 0.40, Loss_test: 0.72, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 259 Loss: 0.48, Loss_test: 0.70, acc: 0.80, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.53, Loss_test: 0.69, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 261 Loss: 0.52, Loss_test: 0.71, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 262 Loss: 0.43, Loss_test: 0.72, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 263 Loss: 0.49, Loss_test: 0.70, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 264 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 265 Loss: 0.41, Loss_test: 0.68, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 266 Loss: 0.53, Loss_test: 0.69, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 267 Loss: 0.46, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 268 Loss: 0.49, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 269 Loss: 0.42, Loss_test: 0.68, acc: 0.82, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.46, Loss_test: 0.69, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 271 Loss: 0.52, Loss_test: 0.68, acc: 0.70, acc_test: 0.61\n",
      "Epoch: 272 Loss: 0.50, Loss_test: 0.69, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 273 Loss: 0.51, Loss_test: 0.70, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 274 Loss: 0.40, Loss_test: 0.70, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 275 Loss: 0.53, Loss_test: 0.68, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 276 Loss: 0.46, Loss_test: 0.70, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 277 Loss: 0.56, Loss_test: 0.70, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 278 Loss: 0.46, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 279 Loss: 0.41, Loss_test: 0.70, acc: 0.80, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.54, Loss_test: 0.71, acc: 0.71, acc_test: 0.62\n",
      "Epoch: 281 Loss: 0.54, Loss_test: 0.71, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 282 Loss: 0.56, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 283 Loss: 0.61, Loss_test: 0.68, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 284 Loss: 0.51, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 285 Loss: 0.49, Loss_test: 0.68, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 286 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 287 Loss: 0.44, Loss_test: 0.68, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 288 Loss: 0.43, Loss_test: 0.68, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 289 Loss: 0.48, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.42, Loss_test: 0.68, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 291 Loss: 0.47, Loss_test: 0.67, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 292 Loss: 0.44, Loss_test: 0.68, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 293 Loss: 0.47, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 294 Loss: 0.49, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 295 Loss: 0.41, Loss_test: 0.69, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 296 Loss: 0.53, Loss_test: 0.69, acc: 0.70, acc_test: 0.61\n",
      "Epoch: 297 Loss: 0.40, Loss_test: 0.71, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 298 Loss: 0.52, Loss_test: 0.71, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 299 Loss: 0.61, Loss_test: 0.69, acc: 0.68, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.48, Loss_test: 0.69, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 301 Loss: 0.51, Loss_test: 0.69, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 302 Loss: 0.48, Loss_test: 0.68, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 303 Loss: 0.46, Loss_test: 0.68, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 304 Loss: 0.48, Loss_test: 0.67, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 305 Loss: 0.47, Loss_test: 0.69, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 306 Loss: 0.47, Loss_test: 0.72, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 307 Loss: 0.43, Loss_test: 0.71, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 308 Loss: 0.48, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 309 Loss: 0.50, Loss_test: 0.67, acc: 0.78, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.58, Loss_test: 0.69, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 311 Loss: 0.44, Loss_test: 0.74, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 312 Loss: 0.51, Loss_test: 0.79, acc: 0.72, acc_test: 0.54\n",
      "Epoch: 313 Loss: 0.51, Loss_test: 0.77, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 314 Loss: 0.50, Loss_test: 0.70, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 315 Loss: 0.46, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 316 Loss: 0.47, Loss_test: 0.67, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 317 Loss: 0.55, Loss_test: 0.69, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 318 Loss: 0.50, Loss_test: 0.70, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 319 Loss: 0.43, Loss_test: 0.69, acc: 0.79, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.51, Loss_test: 0.68, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 321 Loss: 0.43, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 322 Loss: 0.42, Loss_test: 0.69, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 323 Loss: 0.48, Loss_test: 0.69, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 324 Loss: 0.39, Loss_test: 0.70, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 325 Loss: 0.51, Loss_test: 0.70, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 326 Loss: 0.47, Loss_test: 0.70, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 327 Loss: 0.54, Loss_test: 0.70, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 328 Loss: 0.51, Loss_test: 0.70, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 329 Loss: 0.49, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.44, Loss_test: 0.70, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 331 Loss: 0.52, Loss_test: 0.75, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 332 Loss: 0.51, Loss_test: 0.76, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 333 Loss: 0.45, Loss_test: 0.71, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 334 Loss: 0.56, Loss_test: 0.68, acc: 0.71, acc_test: 0.60\n",
      "Epoch: 335 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 336 Loss: 0.47, Loss_test: 0.68, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 337 Loss: 0.53, Loss_test: 0.70, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 338 Loss: 0.48, Loss_test: 0.72, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 339 Loss: 0.41, Loss_test: 0.72, acc: 0.81, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.42, Loss_test: 0.70, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 341 Loss: 0.44, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 342 Loss: 0.43, Loss_test: 0.69, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 343 Loss: 0.49, Loss_test: 0.70, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 344 Loss: 0.47, Loss_test: 0.70, acc: 0.77, acc_test: 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 Loss: 0.45, Loss_test: 0.70, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 346 Loss: 0.50, Loss_test: 0.70, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 347 Loss: 0.49, Loss_test: 0.70, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 348 Loss: 0.53, Loss_test: 0.70, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 349 Loss: 0.49, Loss_test: 0.69, acc: 0.71, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.51, Loss_test: 0.69, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 351 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 352 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 353 Loss: 0.40, Loss_test: 0.69, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 354 Loss: 0.44, Loss_test: 0.69, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 355 Loss: 0.43, Loss_test: 0.69, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 356 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 357 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 358 Loss: 0.47, Loss_test: 0.73, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 359 Loss: 0.52, Loss_test: 0.72, acc: 0.70, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.44, Loss_test: 0.71, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 361 Loss: 0.40, Loss_test: 0.70, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 362 Loss: 0.47, Loss_test: 0.69, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 363 Loss: 0.48, Loss_test: 0.68, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 364 Loss: 0.55, Loss_test: 0.68, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 365 Loss: 0.48, Loss_test: 0.68, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 366 Loss: 0.42, Loss_test: 0.68, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 367 Loss: 0.48, Loss_test: 0.68, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 368 Loss: 0.55, Loss_test: 0.68, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 369 Loss: 0.50, Loss_test: 0.69, acc: 0.73, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.39, Loss_test: 0.71, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 371 Loss: 0.52, Loss_test: 0.71, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 372 Loss: 0.37, Loss_test: 0.71, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 373 Loss: 0.43, Loss_test: 0.69, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 374 Loss: 0.42, Loss_test: 0.68, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 375 Loss: 0.40, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 376 Loss: 0.43, Loss_test: 0.68, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 377 Loss: 0.53, Loss_test: 0.68, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 378 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 379 Loss: 0.43, Loss_test: 0.68, acc: 0.80, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.54, Loss_test: 0.68, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 381 Loss: 0.43, Loss_test: 0.69, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 382 Loss: 0.38, Loss_test: 0.70, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 383 Loss: 0.42, Loss_test: 0.71, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 384 Loss: 0.46, Loss_test: 0.69, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 385 Loss: 0.43, Loss_test: 0.69, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 386 Loss: 0.47, Loss_test: 0.69, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 387 Loss: 0.42, Loss_test: 0.70, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 388 Loss: 0.40, Loss_test: 0.69, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 389 Loss: 0.46, Loss_test: 0.70, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.41, Loss_test: 0.70, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 391 Loss: 0.46, Loss_test: 0.69, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 392 Loss: 0.45, Loss_test: 0.69, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 393 Loss: 0.45, Loss_test: 0.69, acc: 0.75, acc_test: 0.62\n",
      "Epoch: 394 Loss: 0.40, Loss_test: 0.68, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 395 Loss: 0.51, Loss_test: 0.68, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 396 Loss: 0.57, Loss_test: 0.69, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 397 Loss: 0.44, Loss_test: 0.70, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 398 Loss: 0.45, Loss_test: 0.68, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 399 Loss: 0.43, Loss_test: 0.67, acc: 0.80, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.42, Loss_test: 0.66, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 401 Loss: 0.47, Loss_test: 0.67, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 402 Loss: 0.49, Loss_test: 0.67, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 403 Loss: 0.48, Loss_test: 0.67, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 404 Loss: 0.40, Loss_test: 0.67, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 405 Loss: 0.53, Loss_test: 0.67, acc: 0.75, acc_test: 0.62\n",
      "Epoch: 406 Loss: 0.51, Loss_test: 0.67, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 407 Loss: 0.49, Loss_test: 0.67, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 408 Loss: 0.45, Loss_test: 0.67, acc: 0.74, acc_test: 0.62\n",
      "Epoch: 409 Loss: 0.48, Loss_test: 0.66, acc: 0.74, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.41, Loss_test: 0.67, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 411 Loss: 0.54, Loss_test: 0.67, acc: 0.70, acc_test: 0.63\n",
      "Epoch: 412 Loss: 0.45, Loss_test: 0.68, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 413 Loss: 0.56, Loss_test: 0.68, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 414 Loss: 0.44, Loss_test: 0.68, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 415 Loss: 0.43, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 416 Loss: 0.44, Loss_test: 0.69, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 417 Loss: 0.43, Loss_test: 0.72, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 418 Loss: 0.41, Loss_test: 0.71, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 419 Loss: 0.39, Loss_test: 0.69, acc: 0.81, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.55, Loss_test: 0.68, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 421 Loss: 0.40, Loss_test: 0.68, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 422 Loss: 0.42, Loss_test: 0.68, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 423 Loss: 0.49, Loss_test: 0.68, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 424 Loss: 0.49, Loss_test: 0.68, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 425 Loss: 0.44, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 426 Loss: 0.47, Loss_test: 0.67, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 427 Loss: 0.47, Loss_test: 0.67, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 428 Loss: 0.49, Loss_test: 0.67, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 429 Loss: 0.48, Loss_test: 0.68, acc: 0.79, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.45, Loss_test: 0.69, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 431 Loss: 0.46, Loss_test: 0.68, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 432 Loss: 0.39, Loss_test: 0.68, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 433 Loss: 0.47, Loss_test: 0.69, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 434 Loss: 0.39, Loss_test: 0.71, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 435 Loss: 0.42, Loss_test: 0.72, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 436 Loss: 0.44, Loss_test: 0.72, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 437 Loss: 0.44, Loss_test: 0.71, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 438 Loss: 0.55, Loss_test: 0.70, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 439 Loss: 0.46, Loss_test: 0.71, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.44, Loss_test: 0.72, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 441 Loss: 0.42, Loss_test: 0.71, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 442 Loss: 0.50, Loss_test: 0.70, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 443 Loss: 0.44, Loss_test: 0.69, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 444 Loss: 0.38, Loss_test: 0.69, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 445 Loss: 0.48, Loss_test: 0.68, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 446 Loss: 0.42, Loss_test: 0.67, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 447 Loss: 0.45, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 448 Loss: 0.40, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 449 Loss: 0.52, Loss_test: 0.67, acc: 0.71, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.43, Loss_test: 0.70, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 451 Loss: 0.48, Loss_test: 0.72, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 452 Loss: 0.57, Loss_test: 0.70, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 453 Loss: 0.42, Loss_test: 0.68, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 454 Loss: 0.48, Loss_test: 0.67, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 455 Loss: 0.42, Loss_test: 0.67, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 456 Loss: 0.46, Loss_test: 0.67, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 457 Loss: 0.39, Loss_test: 0.69, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 458 Loss: 0.40, Loss_test: 0.70, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 459 Loss: 0.49, Loss_test: 0.69, acc: 0.75, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.44, Loss_test: 0.68, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 461 Loss: 0.38, Loss_test: 0.69, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 462 Loss: 0.52, Loss_test: 0.69, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 463 Loss: 0.41, Loss_test: 0.69, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 464 Loss: 0.46, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 465 Loss: 0.54, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 466 Loss: 0.47, Loss_test: 0.68, acc: 0.78, acc_test: 0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 467 Loss: 0.41, Loss_test: 0.68, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 468 Loss: 0.50, Loss_test: 0.69, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 469 Loss: 0.42, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 471 Loss: 0.40, Loss_test: 0.74, acc: 0.82, acc_test: 0.55\n",
      "Epoch: 472 Loss: 0.56, Loss_test: 0.76, acc: 0.72, acc_test: 0.54\n",
      "Epoch: 473 Loss: 0.41, Loss_test: 0.69, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 474 Loss: 0.49, Loss_test: 0.68, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 475 Loss: 0.52, Loss_test: 0.69, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 476 Loss: 0.51, Loss_test: 0.68, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 477 Loss: 0.43, Loss_test: 0.68, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 478 Loss: 0.45, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 479 Loss: 0.51, Loss_test: 0.67, acc: 0.74, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.43, Loss_test: 0.69, acc: 0.78, acc_test: 0.64\n",
      "Epoch: 481 Loss: 0.39, Loss_test: 0.70, acc: 0.84, acc_test: 0.64\n",
      "Epoch: 482 Loss: 0.43, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 483 Loss: 0.35, Loss_test: 0.69, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 484 Loss: 0.41, Loss_test: 0.70, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 485 Loss: 0.49, Loss_test: 0.69, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 486 Loss: 0.39, Loss_test: 0.70, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 487 Loss: 0.42, Loss_test: 0.72, acc: 0.79, acc_test: 0.64\n",
      "Epoch: 488 Loss: 0.46, Loss_test: 0.70, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 489 Loss: 0.44, Loss_test: 0.69, acc: 0.78, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.46, Loss_test: 0.70, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 491 Loss: 0.54, Loss_test: 0.68, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 492 Loss: 0.41, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 493 Loss: 0.45, Loss_test: 0.67, acc: 0.77, acc_test: 0.64\n",
      "Epoch: 494 Loss: 0.40, Loss_test: 0.66, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 495 Loss: 0.44, Loss_test: 0.68, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 496 Loss: 0.48, Loss_test: 0.72, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 497 Loss: 0.51, Loss_test: 0.71, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 498 Loss: 0.52, Loss_test: 0.67, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 499 Loss: 0.41, Loss_test: 0.65, acc: 0.82, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.38, Loss_test: 0.66, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 501 Loss: 0.42, Loss_test: 0.66, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 502 Loss: 0.49, Loss_test: 0.66, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 503 Loss: 0.43, Loss_test: 0.68, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 504 Loss: 0.44, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 505 Loss: 0.51, Loss_test: 0.67, acc: 0.72, acc_test: 0.58\n",
      "Epoch: 506 Loss: 0.40, Loss_test: 0.68, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 507 Loss: 0.52, Loss_test: 0.68, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 508 Loss: 0.48, Loss_test: 0.67, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 509 Loss: 0.45, Loss_test: 0.69, acc: 0.79, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.45, Loss_test: 0.72, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 511 Loss: 0.45, Loss_test: 0.71, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 512 Loss: 0.40, Loss_test: 0.69, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 513 Loss: 0.41, Loss_test: 0.68, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 514 Loss: 0.43, Loss_test: 0.68, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 515 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 516 Loss: 0.50, Loss_test: 0.68, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 517 Loss: 0.49, Loss_test: 0.69, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 518 Loss: 0.48, Loss_test: 0.69, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 519 Loss: 0.44, Loss_test: 0.67, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.39, Loss_test: 0.67, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 521 Loss: 0.40, Loss_test: 0.71, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 522 Loss: 0.50, Loss_test: 0.70, acc: 0.74, acc_test: 0.62\n",
      "Epoch: 523 Loss: 0.44, Loss_test: 0.67, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 524 Loss: 0.43, Loss_test: 0.67, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 525 Loss: 0.45, Loss_test: 0.69, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 526 Loss: 0.44, Loss_test: 0.69, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 527 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 528 Loss: 0.46, Loss_test: 0.67, acc: 0.72, acc_test: 0.63\n",
      "Epoch: 529 Loss: 0.40, Loss_test: 0.68, acc: 0.83, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.48, Loss_test: 0.67, acc: 0.76, acc_test: 0.63\n",
      "Epoch: 531 Loss: 0.48, Loss_test: 0.70, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 532 Loss: 0.45, Loss_test: 0.74, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 533 Loss: 0.42, Loss_test: 0.71, acc: 0.79, acc_test: 0.55\n",
      "Epoch: 534 Loss: 0.48, Loss_test: 0.67, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 535 Loss: 0.45, Loss_test: 0.66, acc: 0.80, acc_test: 0.63\n",
      "Epoch: 536 Loss: 0.53, Loss_test: 0.66, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 537 Loss: 0.52, Loss_test: 0.65, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 538 Loss: 0.49, Loss_test: 0.66, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 539 Loss: 0.39, Loss_test: 0.67, acc: 0.84, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.38, Loss_test: 0.67, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 541 Loss: 0.47, Loss_test: 0.66, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 542 Loss: 0.51, Loss_test: 0.65, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 543 Loss: 0.47, Loss_test: 0.65, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 544 Loss: 0.45, Loss_test: 0.65, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 545 Loss: 0.42, Loss_test: 0.66, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 546 Loss: 0.43, Loss_test: 0.68, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 547 Loss: 0.41, Loss_test: 0.68, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 548 Loss: 0.49, Loss_test: 0.68, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 549 Loss: 0.47, Loss_test: 0.67, acc: 0.76, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.45, Loss_test: 0.67, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 551 Loss: 0.42, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 552 Loss: 0.46, Loss_test: 0.68, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 553 Loss: 0.43, Loss_test: 0.68, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 554 Loss: 0.49, Loss_test: 0.68, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 555 Loss: 0.46, Loss_test: 0.68, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 556 Loss: 0.46, Loss_test: 0.68, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 557 Loss: 0.46, Loss_test: 0.68, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 558 Loss: 0.41, Loss_test: 0.67, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 559 Loss: 0.48, Loss_test: 0.67, acc: 0.78, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.39, Loss_test: 0.67, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 561 Loss: 0.47, Loss_test: 0.67, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 562 Loss: 0.39, Loss_test: 0.68, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 563 Loss: 0.45, Loss_test: 0.67, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 564 Loss: 0.42, Loss_test: 0.67, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 565 Loss: 0.47, Loss_test: 0.68, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 566 Loss: 0.41, Loss_test: 0.70, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 567 Loss: 0.46, Loss_test: 0.68, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 568 Loss: 0.35, Loss_test: 0.68, acc: 0.86, acc_test: 0.59\n",
      "Epoch: 569 Loss: 0.49, Loss_test: 0.70, acc: 0.74, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.45, Loss_test: 0.70, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 571 Loss: 0.45, Loss_test: 0.71, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 572 Loss: 0.47, Loss_test: 0.69, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 573 Loss: 0.40, Loss_test: 0.71, acc: 0.79, acc_test: 0.63\n",
      "Epoch: 574 Loss: 0.36, Loss_test: 0.78, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 575 Loss: 0.49, Loss_test: 0.77, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 576 Loss: 0.40, Loss_test: 0.71, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 577 Loss: 0.48, Loss_test: 0.69, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 578 Loss: 0.43, Loss_test: 0.69, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 579 Loss: 0.38, Loss_test: 0.69, acc: 0.82, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.47, Loss_test: 0.69, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 581 Loss: 0.34, Loss_test: 0.71, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 582 Loss: 0.44, Loss_test: 0.72, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 583 Loss: 0.49, Loss_test: 0.69, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 584 Loss: 0.46, Loss_test: 0.68, acc: 0.75, acc_test: 0.62\n",
      "Epoch: 585 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 586 Loss: 0.48, Loss_test: 0.68, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 587 Loss: 0.40, Loss_test: 0.67, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 588 Loss: 0.42, Loss_test: 0.67, acc: 0.79, acc_test: 0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 589 Loss: 0.41, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.43, Loss_test: 0.69, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 591 Loss: 0.34, Loss_test: 0.70, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 592 Loss: 0.37, Loss_test: 0.67, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 593 Loss: 0.40, Loss_test: 0.66, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 594 Loss: 0.41, Loss_test: 0.66, acc: 0.78, acc_test: 0.63\n",
      "Epoch: 595 Loss: 0.42, Loss_test: 0.67, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 596 Loss: 0.52, Loss_test: 0.67, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 597 Loss: 0.45, Loss_test: 0.69, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 598 Loss: 0.44, Loss_test: 0.69, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 599 Loss: 0.48, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.47, Loss_test: 0.71, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 601 Loss: 0.45, Loss_test: 0.75, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 602 Loss: 0.44, Loss_test: 0.71, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 603 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 604 Loss: 0.36, Loss_test: 0.66, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 605 Loss: 0.51, Loss_test: 0.66, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 606 Loss: 0.34, Loss_test: 0.66, acc: 0.88, acc_test: 0.59\n",
      "Epoch: 607 Loss: 0.39, Loss_test: 0.67, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 608 Loss: 0.47, Loss_test: 0.68, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 609 Loss: 0.36, Loss_test: 0.69, acc: 0.88, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.38, Loss_test: 0.70, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 611 Loss: 0.37, Loss_test: 0.75, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 612 Loss: 0.41, Loss_test: 0.80, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 613 Loss: 0.52, Loss_test: 0.78, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 614 Loss: 0.45, Loss_test: 0.74, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 615 Loss: 0.47, Loss_test: 0.69, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 616 Loss: 0.48, Loss_test: 0.67, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 617 Loss: 0.51, Loss_test: 0.67, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 618 Loss: 0.43, Loss_test: 0.69, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 619 Loss: 0.43, Loss_test: 0.69, acc: 0.80, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.46, Loss_test: 0.66, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 621 Loss: 0.42, Loss_test: 0.64, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 622 Loss: 0.46, Loss_test: 0.64, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 623 Loss: 0.42, Loss_test: 0.65, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 624 Loss: 0.43, Loss_test: 0.65, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 625 Loss: 0.51, Loss_test: 0.65, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 626 Loss: 0.42, Loss_test: 0.66, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 627 Loss: 0.40, Loss_test: 0.67, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 628 Loss: 0.37, Loss_test: 0.67, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 629 Loss: 0.54, Loss_test: 0.72, acc: 0.75, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.39, Loss_test: 0.77, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 631 Loss: 0.47, Loss_test: 0.74, acc: 0.73, acc_test: 0.63\n",
      "Epoch: 632 Loss: 0.35, Loss_test: 0.71, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 633 Loss: 0.42, Loss_test: 0.73, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 634 Loss: 0.43, Loss_test: 0.73, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 635 Loss: 0.37, Loss_test: 0.70, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 636 Loss: 0.43, Loss_test: 0.71, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 637 Loss: 0.47, Loss_test: 0.69, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 638 Loss: 0.53, Loss_test: 0.67, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 639 Loss: 0.46, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.40, Loss_test: 0.75, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 641 Loss: 0.42, Loss_test: 0.70, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 642 Loss: 0.51, Loss_test: 0.65, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 643 Loss: 0.40, Loss_test: 0.64, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 644 Loss: 0.35, Loss_test: 0.65, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 645 Loss: 0.45, Loss_test: 0.65, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 646 Loss: 0.38, Loss_test: 0.65, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 647 Loss: 0.35, Loss_test: 0.68, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 648 Loss: 0.39, Loss_test: 0.70, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 649 Loss: 0.47, Loss_test: 0.70, acc: 0.77, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.37, Loss_test: 0.69, acc: 0.81, acc_test: 0.57\n",
      "Epoch: 651 Loss: 0.35, Loss_test: 0.69, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 652 Loss: 0.39, Loss_test: 0.70, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 653 Loss: 0.49, Loss_test: 0.71, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 654 Loss: 0.49, Loss_test: 0.74, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 655 Loss: 0.41, Loss_test: 0.77, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 656 Loss: 0.45, Loss_test: 0.74, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 657 Loss: 0.38, Loss_test: 0.70, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 658 Loss: 0.36, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 659 Loss: 0.34, Loss_test: 0.68, acc: 0.84, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.41, Loss_test: 0.70, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 661 Loss: 0.47, Loss_test: 0.73, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 662 Loss: 0.45, Loss_test: 0.72, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 663 Loss: 0.49, Loss_test: 0.66, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 664 Loss: 0.42, Loss_test: 0.64, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 665 Loss: 0.43, Loss_test: 0.64, acc: 0.76, acc_test: 0.64\n",
      "Epoch: 666 Loss: 0.40, Loss_test: 0.64, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 667 Loss: 0.54, Loss_test: 0.63, acc: 0.70, acc_test: 0.62\n",
      "Epoch: 668 Loss: 0.40, Loss_test: 0.64, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 669 Loss: 0.45, Loss_test: 0.65, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.45, Loss_test: 0.65, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 671 Loss: 0.35, Loss_test: 0.64, acc: 0.85, acc_test: 0.60\n",
      "Epoch: 672 Loss: 0.42, Loss_test: 0.64, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 673 Loss: 0.44, Loss_test: 0.64, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 674 Loss: 0.42, Loss_test: 0.64, acc: 0.78, acc_test: 0.64\n",
      "Epoch: 675 Loss: 0.41, Loss_test: 0.65, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 676 Loss: 0.44, Loss_test: 0.66, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 677 Loss: 0.40, Loss_test: 0.67, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 678 Loss: 0.42, Loss_test: 0.68, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 679 Loss: 0.47, Loss_test: 0.69, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.40, Loss_test: 0.68, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 681 Loss: 0.41, Loss_test: 0.68, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 682 Loss: 0.52, Loss_test: 0.70, acc: 0.68, acc_test: 0.60\n",
      "Epoch: 683 Loss: 0.47, Loss_test: 0.71, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 684 Loss: 0.38, Loss_test: 0.71, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 685 Loss: 0.49, Loss_test: 0.69, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 686 Loss: 0.37, Loss_test: 0.68, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 687 Loss: 0.47, Loss_test: 0.66, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 688 Loss: 0.45, Loss_test: 0.66, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 689 Loss: 0.37, Loss_test: 0.66, acc: 0.84, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.38, Loss_test: 0.66, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 691 Loss: 0.40, Loss_test: 0.66, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 692 Loss: 0.37, Loss_test: 0.66, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 693 Loss: 0.41, Loss_test: 0.66, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 694 Loss: 0.43, Loss_test: 0.66, acc: 0.82, acc_test: 0.60\n",
      "Epoch: 695 Loss: 0.49, Loss_test: 0.67, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 696 Loss: 0.42, Loss_test: 0.66, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 697 Loss: 0.49, Loss_test: 0.65, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 698 Loss: 0.44, Loss_test: 0.65, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 699 Loss: 0.35, Loss_test: 0.65, acc: 0.85, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.37, Loss_test: 0.65, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 701 Loss: 0.42, Loss_test: 0.66, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 702 Loss: 0.52, Loss_test: 0.66, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 703 Loss: 0.45, Loss_test: 0.65, acc: 0.77, acc_test: 0.64\n",
      "Epoch: 704 Loss: 0.41, Loss_test: 0.66, acc: 0.79, acc_test: 0.64\n",
      "Epoch: 705 Loss: 0.42, Loss_test: 0.65, acc: 0.78, acc_test: 0.65\n",
      "Epoch: 706 Loss: 0.44, Loss_test: 0.64, acc: 0.75, acc_test: 0.65\n",
      "Epoch: 707 Loss: 0.40, Loss_test: 0.64, acc: 0.84, acc_test: 0.67\n",
      "Epoch: 708 Loss: 0.49, Loss_test: 0.66, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 709 Loss: 0.34, Loss_test: 0.69, acc: 0.86, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.45, Loss_test: 0.66, acc: 0.78, acc_test: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 711 Loss: 0.45, Loss_test: 0.63, acc: 0.77, acc_test: 0.65\n",
      "Epoch: 712 Loss: 0.37, Loss_test: 0.64, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 713 Loss: 0.48, Loss_test: 0.64, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 714 Loss: 0.40, Loss_test: 0.63, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 715 Loss: 0.51, Loss_test: 0.64, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 716 Loss: 0.39, Loss_test: 0.66, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 717 Loss: 0.43, Loss_test: 0.65, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 718 Loss: 0.46, Loss_test: 0.65, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 719 Loss: 0.34, Loss_test: 0.68, acc: 0.84, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.35, Loss_test: 0.70, acc: 0.86, acc_test: 0.62\n",
      "Epoch: 721 Loss: 0.44, Loss_test: 0.69, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 722 Loss: 0.43, Loss_test: 0.67, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 723 Loss: 0.40, Loss_test: 0.68, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 724 Loss: 0.42, Loss_test: 0.68, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 725 Loss: 0.44, Loss_test: 0.71, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 726 Loss: 0.41, Loss_test: 0.75, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 727 Loss: 0.38, Loss_test: 0.73, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 728 Loss: 0.42, Loss_test: 0.69, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 729 Loss: 0.42, Loss_test: 0.68, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.43, Loss_test: 0.67, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 731 Loss: 0.37, Loss_test: 0.67, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 732 Loss: 0.44, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 733 Loss: 0.36, Loss_test: 0.71, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 734 Loss: 0.40, Loss_test: 0.73, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 735 Loss: 0.40, Loss_test: 0.72, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 736 Loss: 0.44, Loss_test: 0.69, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 737 Loss: 0.37, Loss_test: 0.68, acc: 0.85, acc_test: 0.62\n",
      "Epoch: 738 Loss: 0.47, Loss_test: 0.67, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 739 Loss: 0.43, Loss_test: 0.68, acc: 0.79, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.40, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 741 Loss: 0.36, Loss_test: 0.68, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 742 Loss: 0.48, Loss_test: 0.68, acc: 0.71, acc_test: 0.62\n",
      "Epoch: 743 Loss: 0.45, Loss_test: 0.66, acc: 0.83, acc_test: 0.63\n",
      "Epoch: 744 Loss: 0.40, Loss_test: 0.68, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 745 Loss: 0.43, Loss_test: 0.72, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 746 Loss: 0.37, Loss_test: 0.74, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 747 Loss: 0.42, Loss_test: 0.70, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 748 Loss: 0.35, Loss_test: 0.66, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 749 Loss: 0.39, Loss_test: 0.65, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.42, Loss_test: 0.65, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 751 Loss: 0.32, Loss_test: 0.66, acc: 0.87, acc_test: 0.62\n",
      "Epoch: 752 Loss: 0.45, Loss_test: 0.67, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 753 Loss: 0.39, Loss_test: 0.66, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 754 Loss: 0.39, Loss_test: 0.65, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 755 Loss: 0.44, Loss_test: 0.65, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 756 Loss: 0.37, Loss_test: 0.66, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 757 Loss: 0.31, Loss_test: 0.66, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 758 Loss: 0.40, Loss_test: 0.67, acc: 0.82, acc_test: 0.62\n",
      "Epoch: 759 Loss: 0.48, Loss_test: 0.67, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.39, Loss_test: 0.68, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 761 Loss: 0.38, Loss_test: 0.67, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 762 Loss: 0.43, Loss_test: 0.67, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 763 Loss: 0.40, Loss_test: 0.67, acc: 0.80, acc_test: 0.63\n",
      "Epoch: 764 Loss: 0.42, Loss_test: 0.67, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 765 Loss: 0.39, Loss_test: 0.67, acc: 0.83, acc_test: 0.63\n",
      "Epoch: 766 Loss: 0.37, Loss_test: 0.66, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 767 Loss: 0.44, Loss_test: 0.66, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 768 Loss: 0.39, Loss_test: 0.67, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 769 Loss: 0.44, Loss_test: 0.67, acc: 0.80, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.45, Loss_test: 0.67, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 771 Loss: 0.44, Loss_test: 0.69, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 772 Loss: 0.42, Loss_test: 0.71, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 773 Loss: 0.38, Loss_test: 0.69, acc: 0.84, acc_test: 0.64\n",
      "Epoch: 774 Loss: 0.35, Loss_test: 0.68, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 775 Loss: 0.46, Loss_test: 0.68, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 776 Loss: 0.37, Loss_test: 0.69, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 777 Loss: 0.45, Loss_test: 0.68, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 778 Loss: 0.38, Loss_test: 0.69, acc: 0.83, acc_test: 0.63\n",
      "Epoch: 779 Loss: 0.33, Loss_test: 0.69, acc: 0.86, acc_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.45, Loss_test: 0.67, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 781 Loss: 0.38, Loss_test: 0.67, acc: 0.82, acc_test: 0.64\n",
      "Epoch: 782 Loss: 0.43, Loss_test: 0.74, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 783 Loss: 0.52, Loss_test: 0.77, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 784 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 785 Loss: 0.38, Loss_test: 0.66, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 786 Loss: 0.44, Loss_test: 0.65, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 787 Loss: 0.39, Loss_test: 0.65, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 788 Loss: 0.35, Loss_test: 0.67, acc: 0.86, acc_test: 0.57\n",
      "Epoch: 789 Loss: 0.38, Loss_test: 0.69, acc: 0.84, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.42, Loss_test: 0.68, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 791 Loss: 0.45, Loss_test: 0.66, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 792 Loss: 0.39, Loss_test: 0.67, acc: 0.85, acc_test: 0.61\n",
      "Epoch: 793 Loss: 0.36, Loss_test: 0.71, acc: 0.83, acc_test: 0.63\n",
      "Epoch: 794 Loss: 0.47, Loss_test: 0.70, acc: 0.80, acc_test: 0.63\n",
      "Epoch: 795 Loss: 0.42, Loss_test: 0.68, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 796 Loss: 0.40, Loss_test: 0.70, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 797 Loss: 0.44, Loss_test: 0.69, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 798 Loss: 0.54, Loss_test: 0.67, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 799 Loss: 0.36, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.39, Loss_test: 0.67, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 801 Loss: 0.36, Loss_test: 0.66, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 802 Loss: 0.42, Loss_test: 0.65, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 803 Loss: 0.34, Loss_test: 0.67, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 804 Loss: 0.40, Loss_test: 0.67, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 805 Loss: 0.38, Loss_test: 0.67, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 806 Loss: 0.38, Loss_test: 0.66, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 807 Loss: 0.44, Loss_test: 0.66, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 808 Loss: 0.48, Loss_test: 0.65, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 809 Loss: 0.36, Loss_test: 0.66, acc: 0.83, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.35, Loss_test: 0.67, acc: 0.83, acc_test: 0.57\n",
      "Epoch: 811 Loss: 0.31, Loss_test: 0.69, acc: 0.88, acc_test: 0.56\n",
      "Epoch: 812 Loss: 0.43, Loss_test: 0.68, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 813 Loss: 0.40, Loss_test: 0.66, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 814 Loss: 0.33, Loss_test: 0.66, acc: 0.83, acc_test: 0.61\n",
      "Epoch: 815 Loss: 0.46, Loss_test: 0.67, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 816 Loss: 0.42, Loss_test: 0.68, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 817 Loss: 0.38, Loss_test: 0.67, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 818 Loss: 0.36, Loss_test: 0.66, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 819 Loss: 0.37, Loss_test: 0.67, acc: 0.80, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.37, Loss_test: 0.69, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 821 Loss: 0.45, Loss_test: 0.67, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 822 Loss: 0.40, Loss_test: 0.66, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 823 Loss: 0.38, Loss_test: 0.66, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 824 Loss: 0.39, Loss_test: 0.66, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 825 Loss: 0.52, Loss_test: 0.67, acc: 0.74, acc_test: 0.62\n",
      "Epoch: 826 Loss: 0.41, Loss_test: 0.69, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 827 Loss: 0.38, Loss_test: 0.72, acc: 0.81, acc_test: 0.63\n",
      "Epoch: 828 Loss: 0.41, Loss_test: 0.69, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 829 Loss: 0.46, Loss_test: 0.67, acc: 0.79, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.39, Loss_test: 0.66, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 831 Loss: 0.37, Loss_test: 0.67, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 832 Loss: 0.37, Loss_test: 0.67, acc: 0.84, acc_test: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 833 Loss: 0.35, Loss_test: 0.67, acc: 0.83, acc_test: 0.55\n",
      "Epoch: 834 Loss: 0.38, Loss_test: 0.68, acc: 0.83, acc_test: 0.55\n",
      "Epoch: 835 Loss: 0.38, Loss_test: 0.69, acc: 0.84, acc_test: 0.55\n",
      "Epoch: 836 Loss: 0.38, Loss_test: 0.69, acc: 0.83, acc_test: 0.55\n",
      "Epoch: 837 Loss: 0.34, Loss_test: 0.69, acc: 0.83, acc_test: 0.58\n",
      "Epoch: 838 Loss: 0.35, Loss_test: 0.70, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 839 Loss: 0.33, Loss_test: 0.70, acc: 0.86, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.32, Loss_test: 0.71, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 841 Loss: 0.48, Loss_test: 0.71, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 842 Loss: 0.40, Loss_test: 0.70, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 843 Loss: 0.32, Loss_test: 0.69, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 844 Loss: 0.45, Loss_test: 0.68, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 845 Loss: 0.37, Loss_test: 0.67, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 846 Loss: 0.38, Loss_test: 0.66, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 847 Loss: 0.33, Loss_test: 0.66, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 848 Loss: 0.33, Loss_test: 0.66, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 849 Loss: 0.35, Loss_test: 0.67, acc: 0.83, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.34, Loss_test: 0.67, acc: 0.85, acc_test: 0.57\n",
      "Epoch: 851 Loss: 0.45, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 852 Loss: 0.38, Loss_test: 0.69, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 853 Loss: 0.42, Loss_test: 0.69, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 854 Loss: 0.36, Loss_test: 0.69, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 855 Loss: 0.39, Loss_test: 0.68, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 856 Loss: 0.35, Loss_test: 0.69, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 857 Loss: 0.36, Loss_test: 0.70, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 858 Loss: 0.37, Loss_test: 0.71, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 859 Loss: 0.41, Loss_test: 0.69, acc: 0.80, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.32, Loss_test: 0.68, acc: 0.88, acc_test: 0.61\n",
      "Epoch: 861 Loss: 0.32, Loss_test: 0.67, acc: 0.84, acc_test: 0.62\n",
      "Epoch: 862 Loss: 0.34, Loss_test: 0.69, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 863 Loss: 0.40, Loss_test: 0.73, acc: 0.78, acc_test: 0.63\n",
      "Epoch: 864 Loss: 0.33, Loss_test: 0.78, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 865 Loss: 0.38, Loss_test: 0.78, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 866 Loss: 0.36, Loss_test: 0.72, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 867 Loss: 0.39, Loss_test: 0.67, acc: 0.82, acc_test: 0.63\n",
      "Epoch: 868 Loss: 0.38, Loss_test: 0.66, acc: 0.78, acc_test: 0.63\n",
      "Epoch: 869 Loss: 0.41, Loss_test: 0.66, acc: 0.80, acc_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.39, Loss_test: 0.68, acc: 0.83, acc_test: 0.64\n",
      "Epoch: 871 Loss: 0.37, Loss_test: 0.67, acc: 0.81, acc_test: 0.65\n",
      "Epoch: 872 Loss: 0.40, Loss_test: 0.66, acc: 0.81, acc_test: 0.64\n",
      "Epoch: 873 Loss: 0.44, Loss_test: 0.65, acc: 0.77, acc_test: 0.63\n",
      "Epoch: 874 Loss: 0.39, Loss_test: 0.65, acc: 0.83, acc_test: 0.62\n",
      "Epoch: 875 Loss: 0.36, Loss_test: 0.67, acc: 0.84, acc_test: 0.63\n",
      "Epoch: 876 Loss: 0.37, Loss_test: 0.72, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 877 Loss: 0.40, Loss_test: 0.76, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 878 Loss: 0.46, Loss_test: 0.76, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 879 Loss: 0.33, Loss_test: 0.72, acc: 0.85, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.38, Loss_test: 0.68, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 881 Loss: 0.44, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 882 Loss: 0.39, Loss_test: 0.69, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 883 Loss: 0.43, Loss_test: 0.72, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 884 Loss: 0.38, Loss_test: 0.73, acc: 0.81, acc_test: 0.56\n",
      "Epoch: 885 Loss: 0.39, Loss_test: 0.72, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 886 Loss: 0.39, Loss_test: 0.69, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 887 Loss: 0.52, Loss_test: 0.69, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 888 Loss: 0.38, Loss_test: 0.70, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 889 Loss: 0.45, Loss_test: 0.73, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.35, Loss_test: 0.74, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 891 Loss: 0.42, Loss_test: 0.71, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 892 Loss: 0.29, Loss_test: 0.68, acc: 0.88, acc_test: 0.62\n",
      "Epoch: 893 Loss: 0.39, Loss_test: 0.68, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 894 Loss: 0.42, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 895 Loss: 0.37, Loss_test: 0.71, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 896 Loss: 0.39, Loss_test: 0.73, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 897 Loss: 0.34, Loss_test: 0.72, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 898 Loss: 0.44, Loss_test: 0.71, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 899 Loss: 0.44, Loss_test: 0.72, acc: 0.82, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.49, Loss_test: 0.72, acc: 0.74, acc_test: 0.56\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_dwen_a')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-2]\n",
    "bug_feature_output_a = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'title_dwen_a:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'desc_dwen_a:0' shape=(?, 20) dtype=float32>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(openoffice).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 Loss: 0.49, Loss_test: 0.72, acc: 0.74, acc_tets: 0.56, recall@25: 0.20\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98306:88871,50853,33630,90791|22485:0.8136442601680756,21975:0.8088147938251495,52149:0.8051595985889435,21976:0.802871823310852,12936:0.7992842644453049,55057:0.7969578802585602,20737:0.795499861240387,44398:0.7953686863183975,33974:0.7950585186481476,47054:0.7949663251638412,5788:0.7920135408639908,9850:0.7912836819887161,52219:0.7911051213741302,16049:0.7901453077793121,16050:0.7901453077793121,39098:0.7892025262117386,98670:0.7888880670070648,51118:0.7872904539108276,14063:0.7869907915592194,57446:0.7842895835638046,26997:0.7840646505355835,45068:0.783794566988945,7724:0.7837286591529846,11265:0.7834534645080566,86695:0.7829296588897705,61548:0.7827413380146027,1569:0.7823092937469482,45406:0.7822113484144211,10469:0.7822065055370331',\n",
       " '32771:32490,33548,32560,33879,32699|89982:0.8311087787151337,85708:0.8213895708322525,104067:0.8208613246679306,5745:0.8162567317485809,100550:0.8141317218542099,52506:0.8139511346817017,112299:0.8136376440525055,22038:0.8124389946460724,10306:0.811837911605835,112878:0.810198500752449,121345:0.8101980835199356,17062:0.8101786375045776,109513:0.8101116567850113,21034:0.8101028800010681,41500:0.8099445700645447,50926:0.809527650475502,29754:0.8090272098779678,10453:0.8087552636861801,38657:0.8079552054405212,50112:0.8075342178344727,40914:0.8075146973133087,2674:0.8067335784435272,43073:0.8056678473949432,19716:0.8045538812875748,56379:0.8044878989458084,31437:0.8044285029172897,102855:0.8043919950723648,11029:0.8041724860668182,101570:0.8040215522050858',\n",
       " '32772:22694,36970,39820,34488,33298,36760,34911|43546:0.824140876531601,91315:0.8203505426645279,94843:0.8151404112577438,115564:0.8120737820863724,30911:0.811122477054596,8473:0.8110876828432083,80174:0.8104362487792969,19255:0.8074731081724167,43983:0.8071835339069366,28573:0.806549072265625,44446:0.8061612695455551,44320:0.8059142231941223,9605:0.8054015189409256,50772:0.805252268910408,14729:0.8048411160707474,35567:0.8038712739944458,44296:0.8029413819313049,59833:0.8027487993240356,69907:0.8022646456956863,44008:0.8015927523374557,47688:0.8008084297180176,35702:0.8001626878976822,107634:0.7995624840259552,41925:0.7995359748601913,96463:0.7989259362220764,21062:0.7986843585968018,24182:0.798590749502182,118879:0.7985264658927917,44687:0.7973142266273499',\n",
       " '32776:30241,33762,31134,33183|6143:0.7988449931144714,5978:0.7980000823736191,51376:0.7885949611663818,39221:0.7870358824729919,3807:0.7820779234170914,75956:0.7819701284170151,61073:0.7815719097852707,39591:0.7812790870666504,38610:0.7810963243246078,27293:0.7800798565149307,4457:0.7797646224498749,13403:0.7795491367578506,87035:0.7781678587198257,103710:0.7776770144701004,119681:0.7776616215705872,35809:0.7775610387325287,4532:0.7774951159954071,39553:0.7773877382278442,8823:0.7773259431123734,18804:0.7771422266960144,23559:0.7767660319805145,99966:0.7766642570495605,48873:0.7764636278152466,60602:0.7758745849132538,39579:0.7754074782133102,5038:0.7753336280584335,93562:0.7752844393253326,18397:0.7752685844898224,17945:0.7751566022634506',\n",
       " '65545:55967|64932:0.7843313068151474,68004:0.7205723822116852,77422:0.7119770050048828,91393:0.6907357275485992,78767:0.6829055547714233,62920:0.6768519580364227,52326:0.6716700494289398,39040:0.6704457700252533,11867:0.6677160263061523,41821:0.6666230857372284,33413:0.6664075553417206,54452:0.6658894121646881,37305:0.6653777956962585,14994:0.6638930439949036,14995:0.6638930439949036,14996:0.6638930439949036,65088:0.6614607572555542,31688:0.6614163517951965,88567:0.6585872769355774,105369:0.6572599709033966,32872:0.6571334898471832,86812:0.6537657976150513,724:0.6534841358661652,52284:0.6531020104885101,23856:0.6522650420665741,43639:0.6503925025463104,30789:0.6451536118984222,24985:0.6449423730373383,37122:0.6434480249881744',\n",
       " '65549:26122,62365,3411,74204,94365|30872:0.7603028118610382,59970:0.7538935095071793,99834:0.7480306923389435,94124:0.7471941113471985,84384:0.7461215555667877,81295:0.7454647421836853,44346:0.7448050081729889,26203:0.7446632981300354,100941:0.7445952594280243,65743:0.7438620328903198,26204:0.7438445389270782,52355:0.7426259815692902,15203:0.7421348392963409,39586:0.7408459186553955,67388:0.7403931021690369,41537:0.7392998933792114,41992:0.7391474545001984,32542:0.738525927066803,82547:0.7384354770183563,39690:0.7383063733577728,76067:0.7380691766738892,46622:0.736402153968811,89789:0.7363181710243225,75796:0.7338267266750336,26542:0.7336090803146362,91865:0.7335159480571747,100449:0.7333219051361084,17155:0.7319729626178741,4063:0.7318181991577148',\n",
       " '98318:98570|40554:0.8020536005496979,21112:0.7896244823932648,23488:0.7753037363290787,95690:0.7751100212335587,103835:0.773713544011116,64092:0.7732143104076385,102040:0.7716838121414185,6693:0.7702350318431854,104378:0.7695745825767517,102465:0.7695396989583969,42080:0.7686502486467361,101632:0.7670704573392868,1076:0.7669212222099304,40550:0.7667262554168701,26757:0.7651064991950989,194:0.764525756239891,51090:0.764216884970665,88799:0.7639040499925613,5953:0.7627988308668137,84061:0.7622193992137909,45980:0.7620292007923126,116662:0.7617690563201904,36070:0.761130690574646,17511:0.7608717679977417,109757:0.7600660175085068,70319:0.760002002120018,38305:0.7592333555221558,39750:0.7591875493526459,37145:0.7590508311986923',\n",
       " '98319:107242,102941|120613:0.7629439532756805,3045:0.7412595450878143,36188:0.7410976886749268,121895:0.732559084892273,6795:0.7322119772434235,56409:0.7291905879974365,81898:0.7273516058921814,104999:0.7224017679691315,67866:0.7208775579929352,3893:0.7206902801990509,8302:0.7192089855670929,73167:0.7191425859928131,67283:0.7189837694168091,21882:0.7185835242271423,59775:0.7183449268341064,12428:0.7169950306415558,9285:0.7152358293533325,9286:0.7152358293533325,87263:0.7151626646518707,92817:0.7142674028873444,18620:0.7137594521045685,93939:0.712982565164566,8855:0.7129372954368591,63332:0.7122544050216675,95040:0.7117501199245453,39307:0.7097048461437225,15484:0.7088160812854767,10613:0.7085003852844238,50277:0.7068754434585571',\n",
       " '18:100,271|11817:0.7493346333503723,43495:0.7380549311637878,96344:0.7358102202415466,11096:0.7341616749763489,6389:0.7341008186340332,59272:0.734003484249115,72956:0.7305861115455627,42870:0.7299272418022156,73722:0.7279679775238037,109014:0.7271986305713654,17096:0.7262316346168518,8793:0.7252572476863861,58681:0.723627120256424,43004:0.7220912575721741,12495:0.7218178808689117,98300:0.7209718525409698,480:0.7209227979183197,57341:0.7203007638454437,24907:0.7199433445930481,95973:0.7193499803543091,62859:0.7192269563674927,10811:0.7191629409790039,20642:0.7188453674316406,38799:0.7188429534435272,54071:0.718741238117218,82443:0.7184743881225586,89427:0.7182708382606506,52223:0.718224048614502,40537:0.7177820205688477',\n",
       " '32788:31681,32789,31726|32789:1.0,101632:0.7803492695093155,114525:0.7739569246768951,1076:0.7729597240686417,36070:0.771948829293251,8316:0.7709847092628479,15632:0.7681499123573303,20206:0.7659783661365509,57681:0.764954149723053,16372:0.7645622491836548,49357:0.7637813836336136,46622:0.7635039985179901,64498:0.7631998807191849,115825:0.7615503370761871,28622:0.7613697201013565,102752:0.7600689083337784,46308:0.7598084658384323,117018:0.7588345557451248,65743:0.7579516917467117,37409:0.7576895207166672,46559:0.7574634253978729,32889:0.7561822086572647,19144:0.7560922801494598,41537:0.7554393410682678,84061:0.7550077438354492,36328:0.7541490495204926,3326:0.7535147368907928,37247:0.752770185470581,40637:0.7523816227912903',\n",
       " '32789:31681,32788,31726|32788:1.0,101632:0.7803492695093155,114525:0.7739569246768951,1076:0.7729597240686417,36070:0.771948829293251,8316:0.7709847092628479,15632:0.7681499123573303,20206:0.7659783661365509,57681:0.764954149723053,16372:0.7645622491836548,49357:0.7637813836336136,46622:0.7635039985179901,64498:0.7631998807191849,115825:0.7615503370761871,28622:0.7613697201013565,102752:0.7600689083337784,46308:0.7598084658384323,117018:0.7588345557451248,65743:0.7579516917467117,37409:0.7576895207166672,46559:0.7574634253978729,32889:0.7561822086572647,19144:0.7560922801494598,41537:0.7554393410682678,84061:0.7550077438354492,36328:0.7541490495204926,3326:0.7535147368907928,37247:0.752770185470581,40637:0.7523816227912903',\n",
       " '65561:27520,50753,21280,85187,51168,35298,83046,27630,81041,46738,46739,46740,103094,48951,55995,76895|9954:0.8146932721138,19395:0.8109065592288971,15398:0.8100098520517349,47998:0.8092304319143295,48518:0.8087142109870911,89648:0.8060631304979324,7074:0.8052557110786438,9420:0.8048545867204666,1752:0.8038551658391953,63997:0.8036344349384308,100827:0.803215891122818,12218:0.802255243062973,1139:0.8016609400510788,55268:0.7997853308916092,81098:0.7991722226142883,113107:0.7988995313644409,120068:0.7980256825685501,84328:0.7979871779680252,40911:0.797535240650177,13189:0.796915128827095,4945:0.7966285347938538,36309:0.7962129861116409,36743:0.794327899813652,30953:0.7942511290311813,110055:0.7940323650836945,13446:0.7928707003593445,67646:0.7928579151630402,90626:0.7918120175600052,10250:0.7917260527610779',\n",
       " '27:59,92|33041:0.8236292004585266,97302:0.8226357996463776,33557:0.8216318339109421,105337:0.8210712224245071,39537:0.8174716085195541,76553:0.8152261823415756,5788:0.8151184916496277,102243:0.8135524690151215,45323:0.8131663352251053,11244:0.8126222640275955,93508:0.812457799911499,56099:0.8120448738336563,13283:0.8119312673807144,75040:0.8118951171636581,25898:0.8117952048778534,78545:0.8114481717348099,32695:0.8108750134706497,86695:0.8108373880386353,20448:0.8101692795753479,45406:0.8100193589925766,55057:0.8094159066677094,37888:0.8093085139989853,21299:0.8089388757944107,26004:0.8084077090024948,4471:0.8077966570854187,9166:0.8077571988105774,20979:0.8075650930404663,11434:0.8073675185441971,32332:0.8073378205299377',\n",
       " '32:42944,6342,3207,18886,32905,54854,54855,67697,4756,23861,18648,58138,40286|29054:0.7625336647033691,1145:0.7610225826501846,26342:0.7493246495723724,16465:0.7425518035888672,16466:0.7425518035888672,16467:0.7425518035888672,74156:0.7414542734622955,20402:0.738047331571579,24472:0.7378925979137421,6629:0.7364601790904999,52287:0.735372394323349,39586:0.734679251909256,47911:0.7332428395748138,12334:0.7329899966716766,13580:0.7325015366077423,27432:0.7324111461639404,37247:0.7322261333465576,59543:0.7317594885826111,24146:0.7316893041133881,102717:0.7311700284481049,16708:0.7308422029018402,32889:0.7302919626235962,7852:0.7301524877548218,22281:0.7297700047492981,5013:0.7295885384082794,53085:0.729243665933609,70745:0.7291674315929413,94179:0.7289682328701019,5622:0.7285395264625549',\n",
       " '32813:33041|46176:0.8118835538625717,76706:0.8089118301868439,41925:0.8071236163377762,48149:0.8028547316789627,50602:0.8015744537115097,46978:0.800248995423317,47623:0.7996501177549362,44008:0.7969558984041214,65057:0.7965081632137299,118879:0.7949558347463608,43566:0.7948809117078781,6863:0.794133335351944,59686:0.7935127019882202,115564:0.7933845072984695,70675:0.7928002774715424,46341:0.7926279604434967,59052:0.7912738621234894,44177:0.7898745387792587,27212:0.7889425903558731,15688:0.7888132780790329,30911:0.7881828397512436,35734:0.7881214767694473,14251:0.7870688587427139,66222:0.7870570123195648,50730:0.7867315262556076,23945:0.7866920381784439,23251:0.7864283621311188,104075:0.7854489386081696,9714:0.7847225666046143',\n",
       " '32814:37274,38852,37357|52956:0.7918471693992615,32353:0.7890323996543884,47335:0.787921279668808,33307:0.757594034075737,21919:0.7488955557346344,96787:0.74195197224617,113231:0.7339820265769958,54741:0.7336306571960449,54742:0.7336306571960449,54743:0.7336306571960449,54744:0.7336306571960449,54745:0.7336306571960449,41678:0.7329420745372772,106009:0.7308930158615112,46852:0.7230546176433563,83046:0.7215261161327362,61786:0.7212225198745728,101499:0.7185494899749756,108088:0.7154067754745483,24540:0.7149552404880524,102766:0.7127314507961273,85965:0.7112915515899658,48324:0.7105767726898193,73722:0.7101864516735077,73293:0.71003058552742,76476:0.7093415856361389,96344:0.7087130546569824,25545:0.7086368799209595,80472:0.7084633409976959',\n",
       " '32815:32033,31362,31874,33062,31550|52382:0.7911461144685745,42942:0.7805020362138748,32850:0.7676914930343628,106298:0.7666812688112259,44215:0.7632874697446823,98173:0.7624493390321732,90104:0.7621362656354904,17161:0.7603593170642853,29026:0.7603326886892319,91799:0.7593794763088226,19698:0.7591570317745209,19699:0.7591570317745209,3151:0.7586524188518524,25548:0.7584535032510757,74537:0.7584295719861984,90217:0.757894441485405,25993:0.7578278630971909,115349:0.7573615312576294,115350:0.7573615312576294,44590:0.7570158988237381,31458:0.7569600492715836,8873:0.7567119747400284,49310:0.7564461678266525,75421:0.7560376673936844,63177:0.7557125985622406,57789:0.7555811703205109,10388:0.755132794380188,64525:0.75486259162426,5627:0.7528485804796219',\n",
       " '65586:71849,67371,67476,83252,88157,67642,71099,69789,81630|20508:0.8085567206144333,110392:0.8003834635019302,85752:0.799046665430069,44124:0.7987150102853775,13737:0.7965698838233948,79481:0.7957632690668106,12276:0.7939033657312393,98577:0.7927590906620026,44017:0.7905866652727127,41423:0.7902070581912994,15546:0.789347767829895,45321:0.789062574505806,67451:0.7882132083177567,22633:0.7878186255693436,68800:0.7877424955368042,63612:0.7874763607978821,2808:0.7870776504278183,18366:0.7870664298534393,4394:0.7869603335857391,90433:0.7868742644786835,90355:0.7864699065685272,60936:0.7859424203634262,86565:0.7853790372610092,17780:0.7848956733942032,30608:0.7847890853881836,7220:0.7842721492052078,80808:0.7839428186416626,60350:0.7839411944150925,114770:0.7837478667497635',\n",
       " '51:52|52:0.945373248308897,87691:0.7890071868896484,9675:0.7761966586112976,69195:0.7710107564926147,1537:0.7682196348905563,1437:0.7669225335121155,56045:0.7666887938976288,1423:0.7656196355819702,44182:0.7634394466876984,23656:0.7633980065584183,4183:0.7629381567239761,17105:0.7625744193792343,83320:0.7615745365619659,68806:0.7615106403827667,76623:0.7609338611364365,4129:0.7601946145296097,16049:0.7584732174873352,16050:0.7584732174873352,43944:0.7577010542154312,97114:0.7575037032365799,40842:0.7569783329963684,17364:0.7563595026731491,107037:0.7563167214393616,93778:0.7552291601896286,5184:0.7551186382770538,70863:0.7547658532857895,2728:0.7536017447710037,62716:0.7531271129846573,5171:0.7527487576007843',\n",
       " '52:51|51:0.945373248308897,87691:0.7898217886686325,39476:0.7662297338247299,1423:0.7614388167858124,6789:0.7610282599925995,56045:0.7603910714387894,9808:0.7601640671491623,87701:0.758913442492485,17105:0.7583288848400116,9675:0.7581416815519333,6141:0.7566439807415009,1437:0.7565514892339706,69195:0.7561039924621582,1537:0.7553892433643341,76623:0.7550641000270844,97114:0.7542659491300583,23656:0.7540378421545029,44182:0.7537912130355835,4129:0.7531732767820358,49056:0.7527292519807816,49057:0.7527292519807816,49058:0.7527292519807816,4183:0.7519367933273315,15468:0.7517729699611664,68806:0.7513550817966461,4486:0.7507510483264923,83320:0.7504851967096329,48180:0.7503188848495483,93778:0.7503074407577515']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      581257500   title_dwen_a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      581257500   desc_dwen_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,162,515,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bug_feature_output_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/bert/exported_rank_baseline_dwen_1000.txt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.15,\n",
       " '2 - recall_at_10': 0.17,\n",
       " '3 - recall_at_15': 0.18,\n",
       " '4 - recall_at_20': 0.19,\n",
       " '5 - recall_at_25': 0.2}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
