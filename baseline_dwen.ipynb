{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning - DWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8459143ba7c4313a4847fb5352b8781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57667), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d128e9bbc9dd446fbfdc04f1f64ecea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940580e9be9d4958893ef3128a2071fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9acd0aa83134e239e15cd4df83628e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5.45 s, sys: 620 ms, total: 6.06 s\n",
      "Wall time: 5.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dab9d1e223046ec90d5220caace9f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n"
     ]
    }
   ],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[59, 27],\n",
       " [59, 92],\n",
       " [27, 92],\n",
       " [64, 43],\n",
       " [44, 45],\n",
       " [53, 54],\n",
       " [84, 63],\n",
       " [75, 699],\n",
       " [105, 121],\n",
       " [186, 199]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '18\\n',\n",
       " 'creation_ts': '2002-08-31 16:50:00 +0000',\n",
       " 'delta_ts': '2013-02-24 20:34:09 +0000',\n",
       " 'description': '[CLS] it \\' s not same format in the di ##c files o ##o make self as in swedish di ##c file i can download . when i register and add the file in o ##o i can \\' t see any words . when i look in the map there is 2 files with al ##mo ##ast same name . se _ se ##1 . di ##c is made by o ##o self and contain no words . i try ##ed to convert th file to same format as so ##ffi ##ce . di ##c with lit ##te or no progress . the new file can i import but it stops in the middle of \" a \" i can \\' t do anymore to solve the problem so i leave it to yu ##o guys . regards ana ##kins [SEP]',\n",
       " 'description_bert': '[CLS] it \\' s not same format in the di ##c files o ##o make self as in swedish di ##c file i can download . when i register and add the file in o ##o i can \\' t see any words . when i look in the map there is 2 files with al ##mo ##ast same name . se _ se ##1 . di ##c is made by o ##o self and contain no words . i try ##ed to convert th file to same format as so ##ffi ##ce . di ##c with lit ##te or no progress . the new file can i import but it stops in the middle of \" a \" i can \\' t do anymore to solve the problem so i leave it to yu ##o guys . regards ana ##kins [SEP]',\n",
       " 'description_word': array([ 101, 2009, 1005, 1055, 2025, 2168, 4289, 1999, 1996, 4487, 2278,\n",
       "        6764, 1051, 2080, 2191, 2969, 2004, 1999, 4467, 4487]),\n",
       " 'description_word_bert': [101,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2168,\n",
       "  4289,\n",
       "  1999,\n",
       "  1996,\n",
       "  4487,\n",
       "  2278,\n",
       "  6764,\n",
       "  1051,\n",
       "  2080,\n",
       "  2191,\n",
       "  2969,\n",
       "  2004,\n",
       "  1999,\n",
       "  4467,\n",
       "  4487,\n",
       "  2278,\n",
       "  5371,\n",
       "  1045,\n",
       "  2064,\n",
       "  8816,\n",
       "  1012,\n",
       "  2043,\n",
       "  1045,\n",
       "  4236,\n",
       "  1998,\n",
       "  5587,\n",
       "  1996,\n",
       "  5371,\n",
       "  1999,\n",
       "  1051,\n",
       "  2080,\n",
       "  1045,\n",
       "  2064,\n",
       "  1005,\n",
       "  1056,\n",
       "  2156,\n",
       "  2151,\n",
       "  2616,\n",
       "  1012,\n",
       "  2043,\n",
       "  1045,\n",
       "  2298,\n",
       "  1999,\n",
       "  1996,\n",
       "  4949,\n",
       "  2045,\n",
       "  2003,\n",
       "  1016,\n",
       "  6764,\n",
       "  2007,\n",
       "  2632,\n",
       "  5302,\n",
       "  14083,\n",
       "  2168,\n",
       "  2171,\n",
       "  1012,\n",
       "  7367,\n",
       "  1035,\n",
       "  7367,\n",
       "  2487,\n",
       "  1012,\n",
       "  4487,\n",
       "  2278,\n",
       "  2003,\n",
       "  2081,\n",
       "  2011,\n",
       "  1051,\n",
       "  2080,\n",
       "  2969,\n",
       "  1998,\n",
       "  5383,\n",
       "  2053,\n",
       "  2616,\n",
       "  1012,\n",
       "  1045,\n",
       "  3046,\n",
       "  2098,\n",
       "  2000,\n",
       "  10463,\n",
       "  16215,\n",
       "  5371,\n",
       "  2000,\n",
       "  2168,\n",
       "  4289,\n",
       "  2004,\n",
       "  2061,\n",
       "  26989,\n",
       "  3401,\n",
       "  1012,\n",
       "  4487,\n",
       "  2278,\n",
       "  2007,\n",
       "  5507,\n",
       "  2618,\n",
       "  2030,\n",
       "  2053,\n",
       "  5082,\n",
       "  1012,\n",
       "  1996,\n",
       "  2047,\n",
       "  5371,\n",
       "  2064,\n",
       "  1045,\n",
       "  12324,\n",
       "  2021,\n",
       "  2009,\n",
       "  6762,\n",
       "  1999,\n",
       "  1996,\n",
       "  2690,\n",
       "  1997,\n",
       "  1000,\n",
       "  1037,\n",
       "  1000,\n",
       "  1045,\n",
       "  2064,\n",
       "  1005,\n",
       "  1056,\n",
       "  2079,\n",
       "  4902,\n",
       "  2000,\n",
       "  9611,\n",
       "  1996,\n",
       "  3291,\n",
       "  2061,\n",
       "  1045,\n",
       "  2681,\n",
       "  2009,\n",
       "  2000,\n",
       "  9805,\n",
       "  2080,\n",
       "  4364,\n",
       "  1012,\n",
       "  12362,\n",
       "  9617,\n",
       "  14322,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 7387,\n",
       " 'priority': '3\\n',\n",
       " 'product': '3\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_word': array([ 101, 3291, 2007, 9206, 1999, 4467, 2663, 2544,  102,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  101, 2009,\n",
       "        1005, 1055, 2025, 2168, 4289, 1999, 1996, 4487, 2278, 6764, 1051,\n",
       "        2080, 2191, 2969, 2004, 1999, 4467, 4487]),\n",
       " 'title': '[CLS] problem with dictionary in swedish win version [SEP]',\n",
       " 'title_bert': '[CLS] problem with dictionary in swedish win version [SEP]',\n",
       " 'title_word': array([ 101, 3291, 2007, 9206, 1999, 4467, 2663, 2544,  102,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'title_word_bert': [101,\n",
       "  3291,\n",
       "  2007,\n",
       "  9206,\n",
       "  1999,\n",
       "  4467,\n",
       "  2663,\n",
       "  2544,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'version': '498\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 11043)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(list(issues_by_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110647 in experiment.baseline.bug_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 ms, sys: 4.08 ms, total: 33.6 ms\n",
      "Wall time: 33.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = experiment.batch_iterator(None, \n",
    "                                                                                          baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1,\n",
    "                                                                                          issues_by_buckets)\n",
    "\n",
    "valid_title_sample_a = np.concatenate([valid_input_sample['title'], valid_input_sample['title']], 0)\n",
    "valid_title_sample_b = np.concatenate([valid_input_pos['title'], valid_input_neg['title']], 0)\n",
    "valid_desc_sample_a = np.concatenate([valid_input_sample['description'], valid_input_sample['description']], 0)\n",
    "valid_desc_sample_b = np.concatenate([valid_input_pos['description'], valid_input_neg['description']], 0)\n",
    "\n",
    "test_gen = ([valid_title_sample_a, valid_title_sample_b, valid_desc_sample_a, valid_desc_sample_b], valid_sim)\n",
    "\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (128,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 18562'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, f.readline().split())\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading FastText\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         embed = list(map(float, tokens[1:]))\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857cedd7f4442d2bcbd3632bd71343c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fc727379cd4dd58fd150b6151a6fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18562), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 22s, sys: 3.1 s, total: 1min 25s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### DWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, \\\n",
    "    Average, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import keras.backend as K\n",
    "\n",
    "def dwen_feature(title_feature_model, desc_feature_model, \\\n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    \n",
    "    # Embedding feature\n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    bug_d_feat = GlobalAveragePooling1D()(bug_d_feat)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = Average(name = 'merge_features_{}'.format(name))([bug_t_feat, bug_d_feat])\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model\n",
    "\n",
    "def dwen_model(bug_feature_output_a, bug_feature_output_b, name):\n",
    "    \n",
    "    inputs = np.concatenate([bug_feature_output_a.input, bug_feature_output_b.input], -1).tolist()\n",
    "    \n",
    "    bug_feature_output_a = bug_feature_output_a.output\n",
    "    bug_feature_output_b = bug_feature_output_b.output\n",
    "    \n",
    "    # 2D concatenate feature\n",
    "    bug_feature_output = concatenate([bug_feature_output_a, bug_feature_output_b])\n",
    "    \n",
    "    hidden_layers = 2\n",
    "    \n",
    "    # Deep Hidden MLPs\n",
    "    for _ in range(hidden_layers):\n",
    "        number_of_units = K.int_shape(bug_feature_output)[1]\n",
    "        bug_feature_output = Dense(number_of_units // 2)(bug_feature_output)\n",
    "#         bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "        bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "        #bug_feature_output = Dropout(.5)(bug_feature_output)\n",
    "    \n",
    "     # Sigmoid\n",
    "    output = Dense(1, activation='sigmoid')(bug_feature_output)\n",
    "\n",
    "    similarity_model = Model(inputs=inputs, outputs=[output], name = 'dwen_output')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      5568600     title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      5568600     desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 11,362,801\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 11,137,200\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss: 0.69, acc: 0.52\n",
      "Epoch: 2 Loss: 0.69, acc: 0.57\n",
      "Epoch: 3 Loss: 0.69, acc: 0.54\n",
      "Epoch: 4 Loss: 0.69, acc: 0.50\n",
      "Epoch: 5 Loss: 0.69, acc: 0.59\n",
      "Epoch: 6 Loss: 0.69, acc: 0.61\n",
      "Epoch: 7 Loss: 0.68, acc: 0.59\n",
      "Epoch: 8 Loss: 0.69, acc: 0.55\n",
      "Epoch: 9 Loss: 0.69, acc: 0.49\n",
      "Epoch: 10 Loss: 0.68, acc: 0.60\n",
      "Epoch: 11 Loss: 0.68, acc: 0.53\n",
      "Epoch: 12 Loss: 0.69, acc: 0.57\n",
      "Epoch: 13 Loss: 0.70, acc: 0.46\n",
      "Epoch: 14 Loss: 0.68, acc: 0.61\n",
      "Epoch: 15 Loss: 0.69, acc: 0.58\n",
      "Epoch: 16 Loss: 0.68, acc: 0.55\n",
      "Epoch: 17 Loss: 0.68, acc: 0.59\n",
      "Epoch: 18 Loss: 0.69, acc: 0.50\n",
      "Epoch: 19 Loss: 0.68, acc: 0.57\n",
      "Epoch: 20 Loss: 0.68, acc: 0.59\n",
      "Epoch: 21 Loss: 0.68, acc: 0.58\n",
      "Epoch: 22 Loss: 0.68, acc: 0.56\n",
      "Epoch: 23 Loss: 0.68, acc: 0.57\n",
      "Epoch: 24 Loss: 0.65, acc: 0.67\n",
      "Epoch: 25 Loss: 0.67, acc: 0.58\n",
      "Epoch: 26 Loss: 0.67, acc: 0.55\n",
      "Epoch: 27 Loss: 0.67, acc: 0.66\n",
      "Epoch: 28 Loss: 0.65, acc: 0.64\n",
      "Epoch: 29 Loss: 0.67, acc: 0.59\n",
      "Epoch: 30 Loss: 0.69, acc: 0.53\n",
      "Epoch: 31 Loss: 0.66, acc: 0.56\n",
      "Epoch: 32 Loss: 0.67, acc: 0.59\n",
      "Epoch: 33 Loss: 0.70, acc: 0.52\n",
      "Epoch: 34 Loss: 0.72, acc: 0.43\n",
      "Epoch: 35 Loss: 0.69, acc: 0.52\n",
      "Epoch: 36 Loss: 0.65, acc: 0.61\n",
      "Epoch: 37 Loss: 0.67, acc: 0.56\n",
      "Epoch: 38 Loss: 0.67, acc: 0.62\n",
      "Epoch: 39 Loss: 0.65, acc: 0.60\n",
      "Epoch: 40 Loss: 0.67, acc: 0.61\n",
      "Epoch: 41 Loss: 0.67, acc: 0.59\n",
      "Epoch: 42 Loss: 0.67, acc: 0.62\n",
      "Epoch: 43 Loss: 0.67, acc: 0.57\n",
      "Epoch: 44 Loss: 0.67, acc: 0.56\n",
      "Epoch: 45 Loss: 0.67, acc: 0.53\n",
      "Epoch: 46 Loss: 0.65, acc: 0.60\n",
      "Epoch: 47 Loss: 0.67, acc: 0.52\n",
      "Epoch: 48 Loss: 0.66, acc: 0.60\n",
      "Epoch: 49 Loss: 0.66, acc: 0.60\n",
      "Epoch: 50 Loss: 0.66, acc: 0.62\n",
      "Epoch: 51 Loss: 0.67, acc: 0.62\n",
      "Epoch: 52 Loss: 0.66, acc: 0.60\n",
      "Epoch: 53 Loss: 0.63, acc: 0.66\n",
      "Epoch: 54 Loss: 0.66, acc: 0.62\n",
      "Epoch: 55 Loss: 0.65, acc: 0.60\n",
      "Epoch: 56 Loss: 0.66, acc: 0.59\n",
      "Epoch: 57 Loss: 0.66, acc: 0.61\n",
      "Epoch: 58 Loss: 0.66, acc: 0.58\n",
      "Epoch: 59 Loss: 0.66, acc: 0.58\n",
      "Epoch: 60 Loss: 0.65, acc: 0.54\n",
      "Epoch: 61 Loss: 0.64, acc: 0.62\n",
      "Epoch: 62 Loss: 0.64, acc: 0.66\n",
      "Epoch: 63 Loss: 0.65, acc: 0.62\n",
      "Epoch: 64 Loss: 0.68, acc: 0.55\n",
      "Epoch: 65 Loss: 0.68, acc: 0.58\n",
      "Epoch: 66 Loss: 0.64, acc: 0.66\n",
      "Epoch: 67 Loss: 0.63, acc: 0.66\n",
      "Epoch: 68 Loss: 0.66, acc: 0.62\n",
      "Epoch: 69 Loss: 0.66, acc: 0.59\n",
      "Epoch: 70 Loss: 0.66, acc: 0.62\n",
      "Epoch: 71 Loss: 0.66, acc: 0.53\n",
      "Epoch: 72 Loss: 0.69, acc: 0.48\n",
      "Epoch: 73 Loss: 0.66, acc: 0.61\n",
      "Epoch: 74 Loss: 0.65, acc: 0.65\n",
      "Epoch: 75 Loss: 0.65, acc: 0.62\n",
      "Epoch: 76 Loss: 0.67, acc: 0.59\n",
      "Epoch: 77 Loss: 0.63, acc: 0.61\n",
      "Epoch: 78 Loss: 0.64, acc: 0.66\n",
      "Epoch: 79 Loss: 0.67, acc: 0.57\n",
      "Epoch: 80 Loss: 0.66, acc: 0.62\n",
      "Epoch: 81 Loss: 0.63, acc: 0.62\n",
      "Epoch: 82 Loss: 0.70, acc: 0.53\n",
      "Epoch: 83 Loss: 0.65, acc: 0.57\n",
      "Epoch: 84 Loss: 0.64, acc: 0.62\n",
      "Epoch: 85 Loss: 0.65, acc: 0.59\n",
      "Epoch: 86 Loss: 0.67, acc: 0.59\n",
      "Epoch: 87 Loss: 0.67, acc: 0.63\n",
      "Epoch: 88 Loss: 0.65, acc: 0.62\n",
      "Epoch: 89 Loss: 0.67, acc: 0.52\n",
      "Epoch: 90 Loss: 0.65, acc: 0.62\n",
      "Epoch: 91 Loss: 0.60, acc: 0.69\n",
      "Epoch: 92 Loss: 0.66, acc: 0.63\n",
      "Epoch: 93 Loss: 0.65, acc: 0.63\n",
      "Epoch: 94 Loss: 0.63, acc: 0.64\n",
      "Epoch: 95 Loss: 0.63, acc: 0.63\n",
      "Epoch: 96 Loss: 0.62, acc: 0.69\n",
      "Epoch: 97 Loss: 0.67, acc: 0.62\n",
      "Epoch: 98 Loss: 0.67, acc: 0.59\n",
      "Epoch: 99 Loss: 0.64, acc: 0.62\n",
      "Epoch: 100 Loss: 0.62, acc: 0.70\n",
      "Epoch: 101 Loss: 0.67, acc: 0.63\n",
      "Epoch: 102 Loss: 0.66, acc: 0.62\n",
      "Epoch: 103 Loss: 0.64, acc: 0.64\n",
      "Epoch: 104 Loss: 0.64, acc: 0.65\n",
      "Epoch: 105 Loss: 0.61, acc: 0.66\n",
      "Epoch: 106 Loss: 0.65, acc: 0.60\n",
      "Epoch: 107 Loss: 0.61, acc: 0.67\n",
      "Epoch: 108 Loss: 0.66, acc: 0.58\n",
      "Epoch: 109 Loss: 0.62, acc: 0.63\n",
      "Epoch: 110 Loss: 0.58, acc: 0.69\n",
      "Epoch: 111 Loss: 0.61, acc: 0.66\n",
      "Epoch: 112 Loss: 0.64, acc: 0.67\n",
      "Epoch: 113 Loss: 0.64, acc: 0.64\n",
      "Epoch: 114 Loss: 0.65, acc: 0.62\n",
      "Epoch: 115 Loss: 0.62, acc: 0.65\n",
      "Epoch: 116 Loss: 0.64, acc: 0.65\n",
      "Epoch: 117 Loss: 0.59, acc: 0.70\n",
      "Epoch: 118 Loss: 0.67, acc: 0.59\n",
      "Epoch: 119 Loss: 0.66, acc: 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120 Loss: 0.65, acc: 0.61\n",
      "Epoch: 121 Loss: 0.68, acc: 0.53\n",
      "Epoch: 122 Loss: 0.62, acc: 0.66\n",
      "Epoch: 123 Loss: 0.67, acc: 0.59\n",
      "Epoch: 124 Loss: 0.64, acc: 0.65\n",
      "Epoch: 125 Loss: 0.61, acc: 0.66\n",
      "Epoch: 126 Loss: 0.61, acc: 0.69\n",
      "Epoch: 127 Loss: 0.64, acc: 0.60\n",
      "Epoch: 128 Loss: 0.60, acc: 0.71\n",
      "Epoch: 129 Loss: 0.66, acc: 0.62\n",
      "Epoch: 130 Loss: 0.65, acc: 0.63\n",
      "Epoch: 131 Loss: 0.66, acc: 0.62\n",
      "Epoch: 132 Loss: 0.64, acc: 0.65\n",
      "Epoch: 133 Loss: 0.63, acc: 0.66\n",
      "Epoch: 134 Loss: 0.62, acc: 0.64\n",
      "Epoch: 135 Loss: 0.68, acc: 0.60\n",
      "Epoch: 136 Loss: 0.64, acc: 0.59\n",
      "Epoch: 137 Loss: 0.66, acc: 0.66\n",
      "Epoch: 138 Loss: 0.71, acc: 0.54\n",
      "Epoch: 139 Loss: 0.65, acc: 0.60\n",
      "Epoch: 140 Loss: 0.63, acc: 0.66\n",
      "Epoch: 141 Loss: 0.64, acc: 0.58\n",
      "Epoch: 142 Loss: 0.61, acc: 0.65\n",
      "Epoch: 143 Loss: 0.62, acc: 0.70\n",
      "Epoch: 144 Loss: 0.66, acc: 0.62\n",
      "Epoch: 145 Loss: 0.67, acc: 0.62\n",
      "Epoch: 146 Loss: 0.65, acc: 0.63\n",
      "Epoch: 147 Loss: 0.66, acc: 0.63\n",
      "Epoch: 148 Loss: 0.65, acc: 0.63\n",
      "Epoch: 149 Loss: 0.67, acc: 0.54\n",
      "Epoch: 150 Loss: 0.67, acc: 0.59\n",
      "Epoch: 151 Loss: 0.65, acc: 0.66\n",
      "Epoch: 152 Loss: 0.62, acc: 0.62\n",
      "Epoch: 153 Loss: 0.64, acc: 0.62\n",
      "Epoch: 154 Loss: 0.66, acc: 0.60\n",
      "Epoch: 155 Loss: 0.64, acc: 0.62\n",
      "Epoch: 156 Loss: 0.64, acc: 0.62\n",
      "Epoch: 157 Loss: 0.64, acc: 0.67\n",
      "Epoch: 158 Loss: 0.62, acc: 0.70\n",
      "Epoch: 159 Loss: 0.63, acc: 0.64\n",
      "Epoch: 160 Loss: 0.63, acc: 0.66\n",
      "Epoch: 161 Loss: 0.63, acc: 0.59\n",
      "Epoch: 162 Loss: 0.64, acc: 0.67\n",
      "Epoch: 163 Loss: 0.61, acc: 0.71\n",
      "Epoch: 164 Loss: 0.65, acc: 0.62\n",
      "Epoch: 165 Loss: 0.62, acc: 0.65\n",
      "Epoch: 166 Loss: 0.62, acc: 0.66\n",
      "Epoch: 167 Loss: 0.58, acc: 0.68\n",
      "Epoch: 168 Loss: 0.63, acc: 0.64\n",
      "Epoch: 169 Loss: 0.67, acc: 0.62\n",
      "Epoch: 170 Loss: 0.63, acc: 0.63\n",
      "Epoch: 171 Loss: 0.64, acc: 0.65\n",
      "Epoch: 172 Loss: 0.64, acc: 0.62\n",
      "Epoch: 173 Loss: 0.64, acc: 0.64\n",
      "Epoch: 174 Loss: 0.64, acc: 0.62\n",
      "Epoch: 175 Loss: 0.63, acc: 0.66\n",
      "Epoch: 176 Loss: 0.68, acc: 0.57\n",
      "Epoch: 177 Loss: 0.63, acc: 0.63\n",
      "Epoch: 178 Loss: 0.63, acc: 0.64\n",
      "Epoch: 179 Loss: 0.65, acc: 0.65\n",
      "Epoch: 180 Loss: 0.64, acc: 0.62\n",
      "Epoch: 181 Loss: 0.66, acc: 0.63\n",
      "Epoch: 182 Loss: 0.64, acc: 0.68\n",
      "Epoch: 183 Loss: 0.62, acc: 0.66\n",
      "Epoch: 184 Loss: 0.64, acc: 0.64\n",
      "Epoch: 185 Loss: 0.67, acc: 0.61\n",
      "Epoch: 186 Loss: 0.66, acc: 0.63\n",
      "Epoch: 187 Loss: 0.62, acc: 0.67\n",
      "Epoch: 188 Loss: 0.67, acc: 0.62\n",
      "Epoch: 189 Loss: 0.64, acc: 0.64\n",
      "Epoch: 190 Loss: 0.59, acc: 0.70\n",
      "Epoch: 191 Loss: 0.62, acc: 0.66\n",
      "Epoch: 192 Loss: 0.63, acc: 0.64\n",
      "Epoch: 193 Loss: 0.64, acc: 0.68\n",
      "Epoch: 194 Loss: 0.68, acc: 0.58\n",
      "Epoch: 195 Loss: 0.66, acc: 0.65\n",
      "Epoch: 196 Loss: 0.59, acc: 0.70\n",
      "Epoch: 197 Loss: 0.60, acc: 0.69\n",
      "Epoch: 198 Loss: 0.62, acc: 0.66\n",
      "Epoch: 199 Loss: 0.62, acc: 0.65\n",
      "Epoch: 200 Loss: 0.61, acc: 0.70\n",
      "Epoch: 201 Loss: 0.61, acc: 0.62\n",
      "Epoch: 202 Loss: 0.63, acc: 0.62\n",
      "Epoch: 203 Loss: 0.63, acc: 0.67\n",
      "Epoch: 204 Loss: 0.64, acc: 0.64\n",
      "Epoch: 205 Loss: 0.60, acc: 0.68\n",
      "Epoch: 206 Loss: 0.66, acc: 0.62\n",
      "Epoch: 207 Loss: 0.65, acc: 0.60\n",
      "Epoch: 208 Loss: 0.65, acc: 0.62\n",
      "Epoch: 209 Loss: 0.60, acc: 0.70\n",
      "Epoch: 210 Loss: 0.62, acc: 0.62\n",
      "Epoch: 211 Loss: 0.62, acc: 0.71\n",
      "Epoch: 212 Loss: 0.69, acc: 0.52\n",
      "Epoch: 213 Loss: 0.62, acc: 0.66\n",
      "Epoch: 214 Loss: 0.59, acc: 0.68\n",
      "Epoch: 215 Loss: 0.64, acc: 0.64\n",
      "Epoch: 216 Loss: 0.65, acc: 0.64\n",
      "Epoch: 217 Loss: 0.60, acc: 0.70\n",
      "Epoch: 218 Loss: 0.60, acc: 0.65\n",
      "Epoch: 219 Loss: 0.66, acc: 0.61\n",
      "Epoch: 220 Loss: 0.62, acc: 0.69\n",
      "Epoch: 221 Loss: 0.65, acc: 0.65\n",
      "Epoch: 222 Loss: 0.61, acc: 0.66\n",
      "Epoch: 223 Loss: 0.63, acc: 0.68\n",
      "Epoch: 224 Loss: 0.62, acc: 0.66\n",
      "Epoch: 225 Loss: 0.63, acc: 0.66\n",
      "Epoch: 226 Loss: 0.65, acc: 0.60\n",
      "Epoch: 227 Loss: 0.61, acc: 0.65\n",
      "Epoch: 228 Loss: 0.61, acc: 0.70\n",
      "Epoch: 229 Loss: 0.64, acc: 0.60\n",
      "Epoch: 230 Loss: 0.65, acc: 0.62\n",
      "Epoch: 231 Loss: 0.64, acc: 0.61\n",
      "Epoch: 232 Loss: 0.62, acc: 0.67\n",
      "Epoch: 233 Loss: 0.71, acc: 0.49\n",
      "Epoch: 234 Loss: 0.62, acc: 0.68\n",
      "Epoch: 235 Loss: 0.61, acc: 0.69\n",
      "Epoch: 236 Loss: 0.69, acc: 0.59\n",
      "Epoch: 237 Loss: 0.61, acc: 0.73\n",
      "Epoch: 238 Loss: 0.68, acc: 0.62\n",
      "Epoch: 239 Loss: 0.62, acc: 0.70\n",
      "Epoch: 240 Loss: 0.62, acc: 0.64\n",
      "Epoch: 241 Loss: 0.61, acc: 0.66\n",
      "Epoch: 242 Loss: 0.64, acc: 0.65\n",
      "Epoch: 243 Loss: 0.57, acc: 0.75\n",
      "Epoch: 244 Loss: 0.60, acc: 0.70\n",
      "Epoch: 245 Loss: 0.69, acc: 0.59\n",
      "Epoch: 246 Loss: 0.60, acc: 0.67\n",
      "Epoch: 247 Loss: 0.69, acc: 0.55\n",
      "Epoch: 248 Loss: 0.62, acc: 0.63\n",
      "Epoch: 249 Loss: 0.67, acc: 0.62\n",
      "Epoch: 250 Loss: 0.63, acc: 0.65\n",
      "Epoch: 251 Loss: 0.61, acc: 0.66\n",
      "Epoch: 252 Loss: 0.63, acc: 0.65\n",
      "Epoch: 253 Loss: 0.64, acc: 0.60\n",
      "Epoch: 254 Loss: 0.61, acc: 0.65\n",
      "Epoch: 255 Loss: 0.60, acc: 0.71\n",
      "Epoch: 256 Loss: 0.64, acc: 0.66\n",
      "Epoch: 257 Loss: 0.63, acc: 0.66\n",
      "Epoch: 258 Loss: 0.64, acc: 0.67\n",
      "Epoch: 259 Loss: 0.60, acc: 0.67\n",
      "Epoch: 260 Loss: 0.62, acc: 0.63\n",
      "Epoch: 261 Loss: 0.62, acc: 0.67\n",
      "Epoch: 262 Loss: 0.57, acc: 0.73\n",
      "Epoch: 263 Loss: 0.67, acc: 0.65\n",
      "Epoch: 264 Loss: 0.60, acc: 0.74\n",
      "Epoch: 265 Loss: 0.61, acc: 0.64\n",
      "Epoch: 266 Loss: 0.63, acc: 0.63\n",
      "Epoch: 267 Loss: 0.64, acc: 0.63\n",
      "Epoch: 268 Loss: 0.60, acc: 0.70\n",
      "Epoch: 269 Loss: 0.65, acc: 0.64\n",
      "Epoch: 270 Loss: 0.69, acc: 0.58\n",
      "Epoch: 271 Loss: 0.61, acc: 0.71\n",
      "Epoch: 272 Loss: 0.61, acc: 0.64\n",
      "Epoch: 273 Loss: 0.68, acc: 0.59\n",
      "Epoch: 274 Loss: 0.62, acc: 0.65\n",
      "Epoch: 275 Loss: 0.66, acc: 0.65\n",
      "Epoch: 276 Loss: 0.63, acc: 0.62\n",
      "Epoch: 277 Loss: 0.59, acc: 0.68\n",
      "Epoch: 278 Loss: 0.59, acc: 0.68\n",
      "Epoch: 279 Loss: 0.61, acc: 0.70\n",
      "Epoch: 280 Loss: 0.62, acc: 0.59\n",
      "Epoch: 281 Loss: 0.62, acc: 0.66\n",
      "Epoch: 282 Loss: 0.60, acc: 0.72\n",
      "Epoch: 283 Loss: 0.56, acc: 0.72\n",
      "Epoch: 284 Loss: 0.71, acc: 0.54\n",
      "Epoch: 285 Loss: 0.60, acc: 0.70\n",
      "Epoch: 286 Loss: 0.64, acc: 0.66\n",
      "Epoch: 287 Loss: 0.60, acc: 0.68\n",
      "Epoch: 288 Loss: 0.62, acc: 0.69\n",
      "Epoch: 289 Loss: 0.63, acc: 0.62\n",
      "Epoch: 290 Loss: 0.65, acc: 0.59\n",
      "Epoch: 291 Loss: 0.65, acc: 0.67\n",
      "Epoch: 292 Loss: 0.62, acc: 0.68\n",
      "Epoch: 293 Loss: 0.65, acc: 0.66\n",
      "Epoch: 294 Loss: 0.59, acc: 0.68\n",
      "Epoch: 295 Loss: 0.64, acc: 0.63\n",
      "Epoch: 296 Loss: 0.66, acc: 0.58\n",
      "Epoch: 297 Loss: 0.62, acc: 0.66\n",
      "Epoch: 298 Loss: 0.63, acc: 0.66\n",
      "Epoch: 299 Loss: 0.62, acc: 0.61\n",
      "Epoch: 300 Loss: 0.58, acc: 0.62\n",
      "Epoch: 301 Loss: 0.63, acc: 0.62\n",
      "Epoch: 302 Loss: 0.59, acc: 0.66\n",
      "Epoch: 303 Loss: 0.62, acc: 0.66\n",
      "Epoch: 304 Loss: 0.65, acc: 0.63\n",
      "Epoch: 305 Loss: 0.59, acc: 0.66\n",
      "Epoch: 306 Loss: 0.56, acc: 0.73\n",
      "Epoch: 307 Loss: 0.59, acc: 0.67\n",
      "Epoch: 308 Loss: 0.59, acc: 0.66\n",
      "Epoch: 309 Loss: 0.66, acc: 0.65\n",
      "Epoch: 310 Loss: 0.61, acc: 0.64\n",
      "Epoch: 311 Loss: 0.64, acc: 0.65\n",
      "Epoch: 312 Loss: 0.59, acc: 0.71\n",
      "Epoch: 313 Loss: 0.66, acc: 0.59\n",
      "Epoch: 314 Loss: 0.66, acc: 0.66\n",
      "Epoch: 315 Loss: 0.57, acc: 0.70\n",
      "Epoch: 316 Loss: 0.64, acc: 0.62\n",
      "Epoch: 317 Loss: 0.64, acc: 0.62\n",
      "Epoch: 318 Loss: 0.67, acc: 0.63\n",
      "Epoch: 319 Loss: 0.63, acc: 0.61\n",
      "Epoch: 320 Loss: 0.65, acc: 0.66\n",
      "Epoch: 321 Loss: 0.61, acc: 0.68\n",
      "Epoch: 322 Loss: 0.59, acc: 0.71\n",
      "Epoch: 323 Loss: 0.63, acc: 0.65\n",
      "Epoch: 324 Loss: 0.64, acc: 0.61\n",
      "Epoch: 325 Loss: 0.64, acc: 0.65\n",
      "Epoch: 326 Loss: 0.61, acc: 0.67\n",
      "Epoch: 327 Loss: 0.63, acc: 0.62\n",
      "Epoch: 328 Loss: 0.61, acc: 0.66\n",
      "Epoch: 329 Loss: 0.60, acc: 0.71\n",
      "Epoch: 330 Loss: 0.61, acc: 0.66\n",
      "Epoch: 331 Loss: 0.62, acc: 0.64\n",
      "Epoch: 332 Loss: 0.58, acc: 0.73\n",
      "Epoch: 333 Loss: 0.62, acc: 0.62\n",
      "Epoch: 334 Loss: 0.61, acc: 0.66\n",
      "Epoch: 335 Loss: 0.63, acc: 0.65\n",
      "Epoch: 336 Loss: 0.62, acc: 0.69\n",
      "Epoch: 337 Loss: 0.63, acc: 0.62\n",
      "Epoch: 338 Loss: 0.63, acc: 0.62\n",
      "Epoch: 339 Loss: 0.64, acc: 0.65\n",
      "Epoch: 340 Loss: 0.63, acc: 0.63\n",
      "Epoch: 341 Loss: 0.61, acc: 0.66\n",
      "Epoch: 342 Loss: 0.67, acc: 0.62\n",
      "Epoch: 343 Loss: 0.63, acc: 0.66\n",
      "Epoch: 344 Loss: 0.64, acc: 0.63\n",
      "Epoch: 345 Loss: 0.57, acc: 0.76\n",
      "Epoch: 346 Loss: 0.58, acc: 0.70\n",
      "Epoch: 347 Loss: 0.62, acc: 0.61\n",
      "Epoch: 348 Loss: 0.67, acc: 0.59\n",
      "Epoch: 349 Loss: 0.59, acc: 0.67\n",
      "Epoch: 350 Loss: 0.68, acc: 0.61\n",
      "Epoch: 351 Loss: 0.61, acc: 0.68\n",
      "Epoch: 352 Loss: 0.64, acc: 0.62\n",
      "Epoch: 353 Loss: 0.66, acc: 0.65\n",
      "Epoch: 354 Loss: 0.60, acc: 0.68\n",
      "Epoch: 355 Loss: 0.62, acc: 0.67\n",
      "Epoch: 356 Loss: 0.62, acc: 0.67\n",
      "Epoch: 357 Loss: 0.60, acc: 0.69\n",
      "Epoch: 358 Loss: 0.68, acc: 0.60\n",
      "Epoch: 359 Loss: 0.60, acc: 0.65\n",
      "Epoch: 360 Loss: 0.62, acc: 0.66\n",
      "Epoch: 361 Loss: 0.63, acc: 0.64\n",
      "Epoch: 362 Loss: 0.62, acc: 0.66\n",
      "Epoch: 363 Loss: 0.62, acc: 0.66\n",
      "Epoch: 364 Loss: 0.63, acc: 0.61\n",
      "Epoch: 365 Loss: 0.62, acc: 0.65\n",
      "Epoch: 366 Loss: 0.66, acc: 0.58\n",
      "Epoch: 367 Loss: 0.61, acc: 0.67\n",
      "Epoch: 368 Loss: 0.62, acc: 0.63\n",
      "Epoch: 369 Loss: 0.61, acc: 0.66\n",
      "Epoch: 370 Loss: 0.65, acc: 0.62\n",
      "Epoch: 371 Loss: 0.56, acc: 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 372 Loss: 0.60, acc: 0.69\n",
      "Epoch: 373 Loss: 0.66, acc: 0.60\n",
      "Epoch: 374 Loss: 0.62, acc: 0.63\n",
      "Epoch: 375 Loss: 0.62, acc: 0.66\n",
      "Epoch: 376 Loss: 0.62, acc: 0.65\n",
      "Epoch: 377 Loss: 0.62, acc: 0.66\n",
      "Epoch: 378 Loss: 0.64, acc: 0.62\n",
      "Epoch: 379 Loss: 0.61, acc: 0.70\n",
      "Epoch: 380 Loss: 0.63, acc: 0.62\n",
      "Epoch: 381 Loss: 0.59, acc: 0.66\n",
      "Epoch: 382 Loss: 0.63, acc: 0.63\n",
      "Epoch: 383 Loss: 0.60, acc: 0.70\n",
      "Epoch: 384 Loss: 0.58, acc: 0.70\n",
      "Epoch: 385 Loss: 0.61, acc: 0.62\n",
      "Epoch: 386 Loss: 0.67, acc: 0.59\n",
      "Epoch: 387 Loss: 0.70, acc: 0.53\n",
      "Epoch: 388 Loss: 0.61, acc: 0.65\n",
      "Epoch: 389 Loss: 0.62, acc: 0.69\n",
      "Epoch: 390 Loss: 0.59, acc: 0.68\n",
      "Epoch: 391 Loss: 0.63, acc: 0.61\n",
      "Epoch: 392 Loss: 0.56, acc: 0.73\n",
      "Epoch: 393 Loss: 0.62, acc: 0.62\n",
      "Epoch: 394 Loss: 0.60, acc: 0.65\n",
      "Epoch: 395 Loss: 0.61, acc: 0.70\n",
      "Epoch: 396 Loss: 0.65, acc: 0.64\n",
      "Epoch: 397 Loss: 0.60, acc: 0.67\n",
      "Epoch: 398 Loss: 0.61, acc: 0.70\n",
      "Epoch: 399 Loss: 0.64, acc: 0.66\n",
      "Epoch: 400 Loss: 0.61, acc: 0.63\n",
      "Epoch: 401 Loss: 0.60, acc: 0.69\n",
      "Epoch: 402 Loss: 0.63, acc: 0.58\n",
      "Epoch: 403 Loss: 0.63, acc: 0.62\n",
      "Epoch: 404 Loss: 0.60, acc: 0.66\n",
      "Epoch: 405 Loss: 0.64, acc: 0.64\n",
      "Epoch: 406 Loss: 0.57, acc: 0.70\n",
      "Epoch: 407 Loss: 0.63, acc: 0.60\n",
      "Epoch: 408 Loss: 0.61, acc: 0.67\n",
      "Epoch: 409 Loss: 0.64, acc: 0.63\n",
      "Epoch: 410 Loss: 0.62, acc: 0.66\n",
      "Epoch: 411 Loss: 0.65, acc: 0.64\n",
      "Epoch: 412 Loss: 0.59, acc: 0.63\n",
      "Epoch: 413 Loss: 0.64, acc: 0.65\n",
      "Epoch: 414 Loss: 0.60, acc: 0.69\n",
      "Epoch: 415 Loss: 0.62, acc: 0.62\n",
      "Epoch: 416 Loss: 0.66, acc: 0.63\n",
      "Epoch: 417 Loss: 0.63, acc: 0.64\n",
      "Epoch: 418 Loss: 0.61, acc: 0.66\n",
      "Epoch: 419 Loss: 0.64, acc: 0.67\n",
      "Epoch: 420 Loss: 0.60, acc: 0.66\n",
      "Epoch: 421 Loss: 0.60, acc: 0.69\n",
      "Epoch: 422 Loss: 0.57, acc: 0.73\n",
      "Epoch: 423 Loss: 0.60, acc: 0.69\n",
      "Epoch: 424 Loss: 0.61, acc: 0.66\n",
      "Epoch: 425 Loss: 0.66, acc: 0.61\n",
      "Epoch: 426 Loss: 0.59, acc: 0.70\n",
      "Epoch: 427 Loss: 0.60, acc: 0.68\n",
      "Epoch: 428 Loss: 0.63, acc: 0.63\n",
      "Epoch: 429 Loss: 0.63, acc: 0.66\n",
      "Epoch: 430 Loss: 0.67, acc: 0.60\n",
      "Epoch: 431 Loss: 0.60, acc: 0.65\n",
      "Epoch: 432 Loss: 0.62, acc: 0.64\n",
      "Epoch: 433 Loss: 0.67, acc: 0.60\n",
      "Epoch: 434 Loss: 0.59, acc: 0.67\n",
      "Epoch: 435 Loss: 0.56, acc: 0.72\n",
      "Epoch: 436 Loss: 0.67, acc: 0.61\n",
      "Epoch: 437 Loss: 0.63, acc: 0.68\n",
      "Epoch: 438 Loss: 0.59, acc: 0.73\n",
      "Epoch: 439 Loss: 0.66, acc: 0.56\n",
      "Epoch: 440 Loss: 0.64, acc: 0.62\n",
      "Epoch: 441 Loss: 0.57, acc: 0.70\n",
      "Epoch: 442 Loss: 0.59, acc: 0.66\n",
      "Epoch: 443 Loss: 0.62, acc: 0.67\n",
      "Epoch: 444 Loss: 0.62, acc: 0.70\n",
      "Epoch: 445 Loss: 0.58, acc: 0.77\n",
      "Epoch: 446 Loss: 0.59, acc: 0.70\n",
      "Epoch: 447 Loss: 0.58, acc: 0.70\n",
      "Epoch: 448 Loss: 0.61, acc: 0.66\n",
      "Epoch: 449 Loss: 0.59, acc: 0.70\n",
      "Epoch: 450 Loss: 0.62, acc: 0.70\n",
      "Epoch: 451 Loss: 0.62, acc: 0.66\n",
      "Epoch: 452 Loss: 0.58, acc: 0.73\n",
      "Epoch: 453 Loss: 0.60, acc: 0.70\n",
      "Epoch: 454 Loss: 0.56, acc: 0.70\n",
      "Epoch: 455 Loss: 0.58, acc: 0.66\n",
      "Epoch: 456 Loss: 0.61, acc: 0.66\n",
      "Epoch: 457 Loss: 0.62, acc: 0.62\n",
      "Epoch: 458 Loss: 0.57, acc: 0.68\n",
      "Epoch: 459 Loss: 0.58, acc: 0.71\n",
      "Epoch: 460 Loss: 0.63, acc: 0.70\n",
      "Epoch: 461 Loss: 0.51, acc: 0.79\n",
      "Epoch: 462 Loss: 0.63, acc: 0.63\n",
      "Epoch: 463 Loss: 0.63, acc: 0.64\n",
      "Epoch: 464 Loss: 0.66, acc: 0.67\n",
      "Epoch: 465 Loss: 0.64, acc: 0.66\n",
      "Epoch: 466 Loss: 0.58, acc: 0.70\n",
      "Epoch: 467 Loss: 0.69, acc: 0.63\n",
      "Epoch: 468 Loss: 0.61, acc: 0.70\n",
      "Epoch: 469 Loss: 0.64, acc: 0.59\n",
      "Epoch: 470 Loss: 0.72, acc: 0.56\n",
      "Epoch: 471 Loss: 0.64, acc: 0.63\n",
      "Epoch: 472 Loss: 0.66, acc: 0.61\n",
      "Epoch: 473 Loss: 0.64, acc: 0.68\n",
      "Epoch: 474 Loss: 0.65, acc: 0.62\n",
      "Epoch: 475 Loss: 0.58, acc: 0.71\n",
      "Epoch: 476 Loss: 0.63, acc: 0.66\n",
      "Epoch: 477 Loss: 0.62, acc: 0.70\n",
      "Epoch: 478 Loss: 0.61, acc: 0.68\n",
      "Epoch: 479 Loss: 0.61, acc: 0.67\n",
      "Epoch: 480 Loss: 0.65, acc: 0.62\n",
      "Epoch: 481 Loss: 0.65, acc: 0.62\n",
      "Epoch: 482 Loss: 0.66, acc: 0.62\n",
      "Epoch: 483 Loss: 0.61, acc: 0.69\n",
      "Epoch: 484 Loss: 0.60, acc: 0.69\n",
      "Epoch: 485 Loss: 0.63, acc: 0.67\n",
      "Epoch: 486 Loss: 0.64, acc: 0.66\n",
      "Epoch: 487 Loss: 0.65, acc: 0.59\n",
      "Epoch: 488 Loss: 0.58, acc: 0.74\n",
      "Epoch: 489 Loss: 0.63, acc: 0.62\n",
      "Epoch: 490 Loss: 0.63, acc: 0.66\n",
      "Epoch: 491 Loss: 0.55, acc: 0.73\n",
      "Epoch: 492 Loss: 0.59, acc: 0.69\n",
      "Epoch: 493 Loss: 0.60, acc: 0.68\n",
      "Epoch: 494 Loss: 0.62, acc: 0.63\n",
      "Epoch: 495 Loss: 0.60, acc: 0.64\n",
      "Epoch: 496 Loss: 0.58, acc: 0.72\n",
      "Epoch: 497 Loss: 0.64, acc: 0.59\n",
      "Epoch: 498 Loss: 0.61, acc: 0.70\n",
      "Epoch: 499 Loss: 0.57, acc: 0.70\n",
      "Epoch: 500 Loss: 0.60, acc: 0.65\n",
      "Epoch: 501 Loss: 0.54, acc: 0.71\n",
      "Epoch: 502 Loss: 0.66, acc: 0.62\n",
      "Epoch: 503 Loss: 0.65, acc: 0.64\n",
      "Epoch: 504 Loss: 0.60, acc: 0.65\n",
      "Epoch: 505 Loss: 0.58, acc: 0.68\n",
      "Epoch: 506 Loss: 0.62, acc: 0.67\n",
      "Epoch: 507 Loss: 0.58, acc: 0.70\n",
      "Epoch: 508 Loss: 0.63, acc: 0.62\n",
      "Epoch: 509 Loss: 0.63, acc: 0.68\n",
      "Epoch: 510 Loss: 0.59, acc: 0.66\n",
      "Epoch: 511 Loss: 0.59, acc: 0.72\n",
      "Epoch: 512 Loss: 0.60, acc: 0.67\n",
      "Epoch: 513 Loss: 0.61, acc: 0.66\n",
      "Epoch: 514 Loss: 0.55, acc: 0.70\n",
      "Epoch: 515 Loss: 0.64, acc: 0.66\n",
      "Epoch: 516 Loss: 0.59, acc: 0.70\n",
      "Epoch: 517 Loss: 0.58, acc: 0.65\n",
      "Epoch: 518 Loss: 0.63, acc: 0.65\n",
      "Epoch: 519 Loss: 0.56, acc: 0.71\n",
      "Epoch: 520 Loss: 0.63, acc: 0.65\n",
      "Epoch: 521 Loss: 0.60, acc: 0.68\n",
      "Epoch: 522 Loss: 0.67, acc: 0.56\n",
      "Epoch: 523 Loss: 0.65, acc: 0.64\n",
      "Epoch: 524 Loss: 0.60, acc: 0.67\n",
      "Epoch: 525 Loss: 0.66, acc: 0.64\n",
      "Epoch: 526 Loss: 0.63, acc: 0.59\n",
      "Epoch: 527 Loss: 0.60, acc: 0.69\n",
      "Epoch: 528 Loss: 0.61, acc: 0.66\n",
      "Epoch: 529 Loss: 0.57, acc: 0.73\n",
      "Epoch: 530 Loss: 0.55, acc: 0.72\n",
      "Epoch: 531 Loss: 0.60, acc: 0.63\n",
      "Epoch: 532 Loss: 0.62, acc: 0.62\n",
      "Epoch: 533 Loss: 0.58, acc: 0.67\n",
      "Epoch: 534 Loss: 0.60, acc: 0.67\n",
      "Epoch: 535 Loss: 0.66, acc: 0.64\n",
      "Epoch: 536 Loss: 0.67, acc: 0.58\n",
      "Epoch: 537 Loss: 0.68, acc: 0.61\n",
      "Epoch: 538 Loss: 0.57, acc: 0.70\n",
      "Epoch: 539 Loss: 0.63, acc: 0.62\n",
      "Epoch: 540 Loss: 0.65, acc: 0.63\n",
      "Epoch: 541 Loss: 0.60, acc: 0.68\n",
      "Epoch: 542 Loss: 0.61, acc: 0.67\n",
      "Epoch: 543 Loss: 0.63, acc: 0.62\n",
      "Epoch: 544 Loss: 0.61, acc: 0.62\n",
      "Epoch: 545 Loss: 0.60, acc: 0.70\n",
      "Epoch: 546 Loss: 0.60, acc: 0.68\n",
      "Epoch: 547 Loss: 0.64, acc: 0.62\n",
      "Epoch: 548 Loss: 0.65, acc: 0.64\n",
      "Epoch: 549 Loss: 0.58, acc: 0.70\n",
      "Epoch: 550 Loss: 0.68, acc: 0.57\n",
      "Epoch: 551 Loss: 0.65, acc: 0.61\n",
      "Epoch: 552 Loss: 0.63, acc: 0.70\n",
      "Epoch: 553 Loss: 0.57, acc: 0.72\n",
      "Epoch: 554 Loss: 0.62, acc: 0.63\n",
      "Epoch: 555 Loss: 0.60, acc: 0.67\n",
      "Epoch: 556 Loss: 0.58, acc: 0.70\n",
      "Epoch: 557 Loss: 0.61, acc: 0.67\n",
      "Epoch: 558 Loss: 0.63, acc: 0.61\n",
      "Epoch: 559 Loss: 0.54, acc: 0.74\n",
      "Epoch: 560 Loss: 0.61, acc: 0.70\n",
      "Epoch: 561 Loss: 0.62, acc: 0.61\n",
      "Epoch: 562 Loss: 0.70, acc: 0.59\n",
      "Epoch: 563 Loss: 0.58, acc: 0.67\n",
      "Epoch: 564 Loss: 0.60, acc: 0.68\n",
      "Epoch: 565 Loss: 0.60, acc: 0.64\n",
      "Epoch: 566 Loss: 0.65, acc: 0.63\n",
      "Epoch: 567 Loss: 0.58, acc: 0.69\n",
      "Epoch: 568 Loss: 0.67, acc: 0.59\n",
      "Epoch: 569 Loss: 0.53, acc: 0.71\n",
      "Epoch: 570 Loss: 0.66, acc: 0.60\n",
      "Epoch: 571 Loss: 0.63, acc: 0.62\n",
      "Epoch: 572 Loss: 0.62, acc: 0.64\n",
      "Epoch: 573 Loss: 0.61, acc: 0.73\n",
      "Epoch: 574 Loss: 0.58, acc: 0.68\n",
      "Epoch: 575 Loss: 0.59, acc: 0.70\n",
      "Epoch: 576 Loss: 0.59, acc: 0.67\n",
      "Epoch: 577 Loss: 0.61, acc: 0.70\n",
      "Epoch: 578 Loss: 0.59, acc: 0.63\n",
      "Epoch: 579 Loss: 0.58, acc: 0.73\n",
      "Epoch: 580 Loss: 0.63, acc: 0.66\n",
      "Epoch: 581 Loss: 0.57, acc: 0.68\n",
      "Epoch: 582 Loss: 0.66, acc: 0.62\n",
      "Epoch: 583 Loss: 0.65, acc: 0.67\n",
      "Epoch: 584 Loss: 0.61, acc: 0.70\n",
      "Epoch: 585 Loss: 0.63, acc: 0.67\n",
      "Epoch: 586 Loss: 0.66, acc: 0.61\n",
      "Epoch: 587 Loss: 0.61, acc: 0.68\n",
      "Epoch: 588 Loss: 0.61, acc: 0.66\n",
      "Epoch: 589 Loss: 0.58, acc: 0.66\n",
      "Epoch: 590 Loss: 0.63, acc: 0.62\n",
      "Epoch: 591 Loss: 0.61, acc: 0.67\n",
      "Epoch: 592 Loss: 0.58, acc: 0.66\n",
      "Epoch: 593 Loss: 0.62, acc: 0.70\n",
      "Epoch: 594 Loss: 0.63, acc: 0.71\n",
      "Epoch: 595 Loss: 0.56, acc: 0.71\n",
      "Epoch: 596 Loss: 0.56, acc: 0.73\n",
      "Epoch: 597 Loss: 0.61, acc: 0.62\n",
      "Epoch: 598 Loss: 0.59, acc: 0.70\n",
      "Epoch: 599 Loss: 0.63, acc: 0.68\n",
      "Epoch: 600 Loss: 0.62, acc: 0.70\n",
      "Epoch: 601 Loss: 0.54, acc: 0.74\n",
      "Epoch: 602 Loss: 0.58, acc: 0.67\n",
      "Epoch: 603 Loss: 0.61, acc: 0.69\n",
      "Epoch: 604 Loss: 0.64, acc: 0.60\n",
      "Epoch: 605 Loss: 0.60, acc: 0.70\n",
      "Epoch: 606 Loss: 0.59, acc: 0.67\n",
      "Epoch: 607 Loss: 0.56, acc: 0.71\n",
      "Epoch: 608 Loss: 0.58, acc: 0.70\n",
      "Epoch: 609 Loss: 0.59, acc: 0.69\n",
      "Epoch: 610 Loss: 0.64, acc: 0.63\n",
      "Epoch: 611 Loss: 0.57, acc: 0.73\n",
      "Epoch: 612 Loss: 0.60, acc: 0.69\n",
      "Epoch: 613 Loss: 0.65, acc: 0.63\n",
      "Epoch: 614 Loss: 0.64, acc: 0.63\n",
      "Epoch: 615 Loss: 0.56, acc: 0.73\n",
      "Epoch: 616 Loss: 0.59, acc: 0.66\n",
      "Epoch: 617 Loss: 0.59, acc: 0.69\n",
      "Epoch: 618 Loss: 0.61, acc: 0.67\n",
      "Epoch: 619 Loss: 0.57, acc: 0.69\n",
      "Epoch: 620 Loss: 0.55, acc: 0.72\n",
      "Epoch: 621 Loss: 0.66, acc: 0.64\n",
      "Epoch: 622 Loss: 0.62, acc: 0.64\n",
      "Epoch: 623 Loss: 0.61, acc: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 624 Loss: 0.54, acc: 0.74\n",
      "Epoch: 625 Loss: 0.60, acc: 0.73\n",
      "Epoch: 626 Loss: 0.59, acc: 0.70\n",
      "Epoch: 627 Loss: 0.59, acc: 0.66\n",
      "Epoch: 628 Loss: 0.65, acc: 0.66\n",
      "Epoch: 629 Loss: 0.67, acc: 0.59\n",
      "Epoch: 630 Loss: 0.62, acc: 0.64\n",
      "Epoch: 631 Loss: 0.61, acc: 0.68\n",
      "Epoch: 632 Loss: 0.60, acc: 0.66\n",
      "Epoch: 633 Loss: 0.62, acc: 0.68\n",
      "Epoch: 634 Loss: 0.60, acc: 0.73\n",
      "Epoch: 635 Loss: 0.60, acc: 0.71\n",
      "Epoch: 636 Loss: 0.56, acc: 0.72\n",
      "Epoch: 637 Loss: 0.60, acc: 0.66\n",
      "Epoch: 638 Loss: 0.61, acc: 0.62\n",
      "Epoch: 639 Loss: 0.58, acc: 0.67\n",
      "Epoch: 640 Loss: 0.62, acc: 0.68\n",
      "Epoch: 641 Loss: 0.55, acc: 0.68\n",
      "Epoch: 642 Loss: 0.60, acc: 0.68\n",
      "Epoch: 643 Loss: 0.61, acc: 0.66\n",
      "Epoch: 644 Loss: 0.61, acc: 0.66\n",
      "Epoch: 645 Loss: 0.65, acc: 0.62\n",
      "Epoch: 646 Loss: 0.54, acc: 0.74\n",
      "Epoch: 647 Loss: 0.57, acc: 0.69\n",
      "Epoch: 648 Loss: 0.55, acc: 0.74\n",
      "Epoch: 649 Loss: 0.66, acc: 0.65\n",
      "Epoch: 650 Loss: 0.56, acc: 0.73\n",
      "Epoch: 651 Loss: 0.68, acc: 0.62\n",
      "Epoch: 652 Loss: 0.60, acc: 0.65\n",
      "Epoch: 653 Loss: 0.60, acc: 0.67\n",
      "Epoch: 654 Loss: 0.60, acc: 0.68\n",
      "Epoch: 655 Loss: 0.61, acc: 0.65\n",
      "Epoch: 656 Loss: 0.59, acc: 0.65\n",
      "Epoch: 657 Loss: 0.61, acc: 0.66\n",
      "Epoch: 658 Loss: 0.61, acc: 0.66\n",
      "Epoch: 659 Loss: 0.67, acc: 0.60\n",
      "Epoch: 660 Loss: 0.67, acc: 0.60\n",
      "Epoch: 661 Loss: 0.66, acc: 0.61\n",
      "Epoch: 662 Loss: 0.59, acc: 0.68\n",
      "Epoch: 663 Loss: 0.60, acc: 0.65\n",
      "Epoch: 664 Loss: 0.60, acc: 0.69\n",
      "Epoch: 665 Loss: 0.65, acc: 0.66\n",
      "Epoch: 666 Loss: 0.61, acc: 0.70\n",
      "Epoch: 667 Loss: 0.60, acc: 0.64\n",
      "Epoch: 668 Loss: 0.64, acc: 0.62\n",
      "Epoch: 669 Loss: 0.62, acc: 0.66\n",
      "Epoch: 670 Loss: 0.59, acc: 0.67\n",
      "Epoch: 671 Loss: 0.60, acc: 0.71\n",
      "Epoch: 672 Loss: 0.58, acc: 0.68\n",
      "Epoch: 673 Loss: 0.61, acc: 0.65\n",
      "Epoch: 674 Loss: 0.59, acc: 0.69\n",
      "Epoch: 675 Loss: 0.64, acc: 0.64\n",
      "Epoch: 676 Loss: 0.56, acc: 0.70\n",
      "Epoch: 677 Loss: 0.57, acc: 0.70\n",
      "Epoch: 678 Loss: 0.62, acc: 0.62\n",
      "Epoch: 679 Loss: 0.59, acc: 0.68\n",
      "Epoch: 680 Loss: 0.57, acc: 0.73\n",
      "Epoch: 681 Loss: 0.62, acc: 0.64\n",
      "Epoch: 682 Loss: 0.65, acc: 0.64\n",
      "Epoch: 683 Loss: 0.67, acc: 0.62\n",
      "Epoch: 684 Loss: 0.69, acc: 0.58\n",
      "Epoch: 685 Loss: 0.62, acc: 0.64\n",
      "Epoch: 686 Loss: 0.64, acc: 0.61\n",
      "Epoch: 687 Loss: 0.63, acc: 0.64\n",
      "Epoch: 688 Loss: 0.65, acc: 0.63\n",
      "Epoch: 689 Loss: 0.60, acc: 0.70\n",
      "Epoch: 690 Loss: 0.62, acc: 0.62\n",
      "Epoch: 691 Loss: 0.60, acc: 0.64\n",
      "Epoch: 692 Loss: 0.61, acc: 0.66\n",
      "Epoch: 693 Loss: 0.61, acc: 0.66\n",
      "Epoch: 694 Loss: 0.63, acc: 0.60\n",
      "Epoch: 695 Loss: 0.61, acc: 0.66\n",
      "Epoch: 696 Loss: 0.62, acc: 0.67\n",
      "Epoch: 697 Loss: 0.64, acc: 0.67\n",
      "Epoch: 698 Loss: 0.62, acc: 0.69\n",
      "Epoch: 699 Loss: 0.62, acc: 0.68\n",
      "Epoch: 700 Loss: 0.58, acc: 0.70\n",
      "Epoch: 701 Loss: 0.65, acc: 0.61\n",
      "Epoch: 702 Loss: 0.54, acc: 0.73\n",
      "Epoch: 703 Loss: 0.59, acc: 0.66\n",
      "Epoch: 704 Loss: 0.58, acc: 0.70\n",
      "Epoch: 705 Loss: 0.58, acc: 0.70\n",
      "Epoch: 706 Loss: 0.60, acc: 0.70\n",
      "Epoch: 707 Loss: 0.60, acc: 0.66\n",
      "Epoch: 708 Loss: 0.61, acc: 0.66\n",
      "Epoch: 709 Loss: 0.64, acc: 0.66\n",
      "Epoch: 710 Loss: 0.62, acc: 0.64\n",
      "Epoch: 711 Loss: 0.63, acc: 0.62\n",
      "Epoch: 712 Loss: 0.62, acc: 0.65\n",
      "Epoch: 713 Loss: 0.56, acc: 0.70\n",
      "Epoch: 714 Loss: 0.63, acc: 0.62\n",
      "Epoch: 715 Loss: 0.66, acc: 0.61\n",
      "Epoch: 716 Loss: 0.63, acc: 0.62\n",
      "Epoch: 717 Loss: 0.61, acc: 0.68\n",
      "Epoch: 718 Loss: 0.62, acc: 0.59\n",
      "Epoch: 719 Loss: 0.62, acc: 0.69\n",
      "Epoch: 720 Loss: 0.64, acc: 0.62\n",
      "Epoch: 721 Loss: 0.56, acc: 0.68\n",
      "Epoch: 722 Loss: 0.64, acc: 0.60\n",
      "Epoch: 723 Loss: 0.65, acc: 0.62\n",
      "Epoch: 724 Loss: 0.58, acc: 0.67\n",
      "Epoch: 725 Loss: 0.57, acc: 0.69\n",
      "Epoch: 726 Loss: 0.58, acc: 0.72\n",
      "Epoch: 727 Loss: 0.58, acc: 0.73\n",
      "Epoch: 728 Loss: 0.61, acc: 0.66\n",
      "Epoch: 729 Loss: 0.61, acc: 0.67\n",
      "Epoch: 730 Loss: 0.58, acc: 0.73\n",
      "Epoch: 731 Loss: 0.61, acc: 0.66\n",
      "Epoch: 732 Loss: 0.61, acc: 0.66\n",
      "Epoch: 733 Loss: 0.63, acc: 0.63\n",
      "Epoch: 734 Loss: 0.62, acc: 0.65\n",
      "Epoch: 735 Loss: 0.59, acc: 0.71\n",
      "Epoch: 736 Loss: 0.59, acc: 0.67\n",
      "Epoch: 737 Loss: 0.61, acc: 0.61\n",
      "Epoch: 738 Loss: 0.57, acc: 0.69\n",
      "Epoch: 739 Loss: 0.59, acc: 0.66\n",
      "Epoch: 740 Loss: 0.59, acc: 0.70\n",
      "Epoch: 741 Loss: 0.57, acc: 0.72\n",
      "Epoch: 742 Loss: 0.60, acc: 0.68\n",
      "Epoch: 743 Loss: 0.59, acc: 0.70\n",
      "Epoch: 744 Loss: 0.61, acc: 0.63\n",
      "Epoch: 745 Loss: 0.60, acc: 0.70\n",
      "Epoch: 746 Loss: 0.59, acc: 0.69\n",
      "Epoch: 747 Loss: 0.60, acc: 0.68\n",
      "Epoch: 748 Loss: 0.60, acc: 0.65\n",
      "Epoch: 749 Loss: 0.59, acc: 0.66\n",
      "Epoch: 750 Loss: 0.57, acc: 0.69\n",
      "Epoch: 751 Loss: 0.61, acc: 0.67\n",
      "Epoch: 752 Loss: 0.58, acc: 0.72\n",
      "Epoch: 753 Loss: 0.59, acc: 0.70\n",
      "Epoch: 754 Loss: 0.63, acc: 0.66\n",
      "Epoch: 755 Loss: 0.60, acc: 0.68\n",
      "Epoch: 756 Loss: 0.63, acc: 0.65\n",
      "Epoch: 757 Loss: 0.58, acc: 0.72\n",
      "Epoch: 758 Loss: 0.60, acc: 0.62\n",
      "Epoch: 759 Loss: 0.58, acc: 0.63\n",
      "Epoch: 760 Loss: 0.63, acc: 0.68\n",
      "Epoch: 761 Loss: 0.55, acc: 0.70\n",
      "Epoch: 762 Loss: 0.59, acc: 0.67\n",
      "Epoch: 763 Loss: 0.59, acc: 0.70\n",
      "Epoch: 764 Loss: 0.65, acc: 0.62\n",
      "Epoch: 765 Loss: 0.61, acc: 0.67\n",
      "Epoch: 766 Loss: 0.59, acc: 0.68\n",
      "Epoch: 767 Loss: 0.59, acc: 0.70\n",
      "Epoch: 768 Loss: 0.61, acc: 0.65\n",
      "Epoch: 769 Loss: 0.63, acc: 0.62\n",
      "Epoch: 770 Loss: 0.60, acc: 0.70\n",
      "Epoch: 771 Loss: 0.65, acc: 0.68\n",
      "Epoch: 772 Loss: 0.63, acc: 0.62\n",
      "Epoch: 773 Loss: 0.59, acc: 0.73\n",
      "Epoch: 774 Loss: 0.58, acc: 0.71\n",
      "Epoch: 775 Loss: 0.57, acc: 0.70\n",
      "Epoch: 776 Loss: 0.54, acc: 0.68\n",
      "Epoch: 777 Loss: 0.57, acc: 0.67\n",
      "Epoch: 778 Loss: 0.62, acc: 0.69\n",
      "Epoch: 779 Loss: 0.60, acc: 0.65\n",
      "Epoch: 780 Loss: 0.68, acc: 0.61\n",
      "Epoch: 781 Loss: 0.70, acc: 0.61\n",
      "Epoch: 782 Loss: 0.61, acc: 0.67\n",
      "Epoch: 783 Loss: 0.60, acc: 0.66\n",
      "Epoch: 784 Loss: 0.55, acc: 0.75\n",
      "Epoch: 785 Loss: 0.57, acc: 0.70\n",
      "Epoch: 786 Loss: 0.63, acc: 0.61\n",
      "Epoch: 787 Loss: 0.61, acc: 0.67\n",
      "Epoch: 788 Loss: 0.59, acc: 0.62\n",
      "Epoch: 789 Loss: 0.57, acc: 0.71\n",
      "Epoch: 790 Loss: 0.60, acc: 0.72\n",
      "Epoch: 791 Loss: 0.63, acc: 0.63\n",
      "Epoch: 792 Loss: 0.56, acc: 0.66\n",
      "Epoch: 793 Loss: 0.53, acc: 0.76\n",
      "Epoch: 794 Loss: 0.57, acc: 0.73\n",
      "Epoch: 795 Loss: 0.65, acc: 0.64\n",
      "Epoch: 796 Loss: 0.64, acc: 0.60\n",
      "Epoch: 797 Loss: 0.60, acc: 0.66\n",
      "Epoch: 798 Loss: 0.62, acc: 0.62\n",
      "Epoch: 799 Loss: 0.59, acc: 0.70\n",
      "Epoch: 800 Loss: 0.63, acc: 0.59\n",
      "Epoch: 801 Loss: 0.61, acc: 0.64\n",
      "Epoch: 802 Loss: 0.57, acc: 0.70\n",
      "Epoch: 803 Loss: 0.60, acc: 0.66\n",
      "Epoch: 804 Loss: 0.64, acc: 0.64\n",
      "Epoch: 805 Loss: 0.64, acc: 0.69\n",
      "Epoch: 806 Loss: 0.63, acc: 0.68\n",
      "Epoch: 807 Loss: 0.55, acc: 0.73\n",
      "Epoch: 808 Loss: 0.65, acc: 0.58\n",
      "Epoch: 809 Loss: 0.57, acc: 0.76\n",
      "Epoch: 810 Loss: 0.59, acc: 0.70\n",
      "Epoch: 811 Loss: 0.60, acc: 0.67\n",
      "Epoch: 812 Loss: 0.60, acc: 0.69\n",
      "Epoch: 813 Loss: 0.57, acc: 0.68\n",
      "Epoch: 814 Loss: 0.67, acc: 0.57\n",
      "Epoch: 815 Loss: 0.59, acc: 0.66\n",
      "Epoch: 816 Loss: 0.62, acc: 0.65\n",
      "Epoch: 817 Loss: 0.57, acc: 0.71\n",
      "Epoch: 818 Loss: 0.53, acc: 0.77\n",
      "Epoch: 819 Loss: 0.66, acc: 0.61\n",
      "Epoch: 820 Loss: 0.62, acc: 0.63\n",
      "Epoch: 821 Loss: 0.60, acc: 0.68\n",
      "Epoch: 822 Loss: 0.56, acc: 0.73\n",
      "Epoch: 823 Loss: 0.58, acc: 0.73\n",
      "Epoch: 824 Loss: 0.65, acc: 0.62\n",
      "Epoch: 825 Loss: 0.58, acc: 0.70\n",
      "Epoch: 826 Loss: 0.58, acc: 0.65\n",
      "Epoch: 827 Loss: 0.59, acc: 0.65\n",
      "Epoch: 828 Loss: 0.66, acc: 0.62\n",
      "Epoch: 829 Loss: 0.56, acc: 0.71\n",
      "Epoch: 830 Loss: 0.58, acc: 0.71\n",
      "Epoch: 831 Loss: 0.54, acc: 0.76\n",
      "Epoch: 832 Loss: 0.60, acc: 0.62\n",
      "Epoch: 833 Loss: 0.59, acc: 0.70\n",
      "Epoch: 834 Loss: 0.57, acc: 0.63\n",
      "Epoch: 835 Loss: 0.63, acc: 0.69\n",
      "Epoch: 836 Loss: 0.57, acc: 0.69\n",
      "Epoch: 837 Loss: 0.56, acc: 0.70\n",
      "Epoch: 838 Loss: 0.65, acc: 0.62\n",
      "Epoch: 839 Loss: 0.62, acc: 0.66\n",
      "Epoch: 840 Loss: 0.55, acc: 0.71\n",
      "Epoch: 841 Loss: 0.65, acc: 0.63\n",
      "Epoch: 842 Loss: 0.56, acc: 0.70\n",
      "Epoch: 843 Loss: 0.66, acc: 0.58\n",
      "Epoch: 844 Loss: 0.62, acc: 0.62\n",
      "Epoch: 845 Loss: 0.59, acc: 0.70\n",
      "Epoch: 846 Loss: 0.57, acc: 0.71\n",
      "Epoch: 847 Loss: 0.63, acc: 0.63\n",
      "Epoch: 848 Loss: 0.57, acc: 0.66\n",
      "Epoch: 849 Loss: 0.57, acc: 0.70\n",
      "Epoch: 850 Loss: 0.53, acc: 0.68\n",
      "Epoch: 851 Loss: 0.56, acc: 0.73\n",
      "Epoch: 852 Loss: 0.59, acc: 0.66\n",
      "Epoch: 853 Loss: 0.64, acc: 0.61\n",
      "Epoch: 854 Loss: 0.58, acc: 0.72\n",
      "Epoch: 855 Loss: 0.61, acc: 0.71\n",
      "Epoch: 856 Loss: 0.60, acc: 0.62\n",
      "Epoch: 857 Loss: 0.62, acc: 0.64\n",
      "Epoch: 858 Loss: 0.56, acc: 0.73\n",
      "Epoch: 859 Loss: 0.58, acc: 0.69\n",
      "Epoch: 860 Loss: 0.59, acc: 0.67\n",
      "Epoch: 861 Loss: 0.62, acc: 0.66\n",
      "Epoch: 862 Loss: 0.56, acc: 0.73\n",
      "Epoch: 863 Loss: 0.60, acc: 0.68\n",
      "Epoch: 864 Loss: 0.55, acc: 0.71\n",
      "Epoch: 865 Loss: 0.61, acc: 0.66\n",
      "Epoch: 866 Loss: 0.59, acc: 0.70\n",
      "Epoch: 867 Loss: 0.58, acc: 0.68\n",
      "Epoch: 868 Loss: 0.58, acc: 0.72\n",
      "Epoch: 869 Loss: 0.60, acc: 0.66\n",
      "Epoch: 870 Loss: 0.57, acc: 0.75\n",
      "Epoch: 871 Loss: 0.63, acc: 0.61\n",
      "Epoch: 872 Loss: 0.55, acc: 0.71\n",
      "Epoch: 873 Loss: 0.61, acc: 0.68\n",
      "Epoch: 874 Loss: 0.63, acc: 0.64\n",
      "Epoch: 875 Loss: 0.62, acc: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 876 Loss: 0.63, acc: 0.68\n",
      "Epoch: 877 Loss: 0.57, acc: 0.71\n",
      "Epoch: 878 Loss: 0.57, acc: 0.72\n",
      "Epoch: 879 Loss: 0.62, acc: 0.62\n",
      "Epoch: 880 Loss: 0.60, acc: 0.68\n",
      "Epoch: 881 Loss: 0.62, acc: 0.62\n",
      "Epoch: 882 Loss: 0.61, acc: 0.72\n",
      "Epoch: 883 Loss: 0.61, acc: 0.66\n",
      "Epoch: 884 Loss: 0.66, acc: 0.61\n",
      "Epoch: 885 Loss: 0.53, acc: 0.69\n",
      "Epoch: 886 Loss: 0.58, acc: 0.71\n",
      "Epoch: 887 Loss: 0.58, acc: 0.70\n",
      "Epoch: 888 Loss: 0.60, acc: 0.68\n",
      "Epoch: 889 Loss: 0.62, acc: 0.68\n",
      "Epoch: 890 Loss: 0.62, acc: 0.64\n",
      "Epoch: 891 Loss: 0.62, acc: 0.67\n",
      "Epoch: 892 Loss: 0.64, acc: 0.66\n",
      "Epoch: 893 Loss: 0.61, acc: 0.65\n",
      "Epoch: 894 Loss: 0.59, acc: 0.72\n",
      "Epoch: 895 Loss: 0.62, acc: 0.68\n",
      "Epoch: 896 Loss: 0.55, acc: 0.68\n",
      "Epoch: 897 Loss: 0.66, acc: 0.60\n",
      "Epoch: 898 Loss: 0.60, acc: 0.67\n",
      "Epoch: 899 Loss: 0.64, acc: 0.65\n",
      "Epoch: 900 Loss: 0.64, acc: 0.64\n",
      "Epoch: 901 Loss: 0.57, acc: 0.73\n",
      "Epoch: 902 Loss: 0.60, acc: 0.67\n",
      "Epoch: 903 Loss: 0.61, acc: 0.67\n",
      "Epoch: 904 Loss: 0.62, acc: 0.65\n",
      "Epoch: 905 Loss: 0.60, acc: 0.66\n",
      "Epoch: 906 Loss: 0.59, acc: 0.69\n",
      "Epoch: 907 Loss: 0.61, acc: 0.66\n",
      "Epoch: 908 Loss: 0.62, acc: 0.68\n",
      "Epoch: 909 Loss: 0.65, acc: 0.62\n",
      "Epoch: 910 Loss: 0.59, acc: 0.72\n",
      "Epoch: 911 Loss: 0.66, acc: 0.55\n",
      "Epoch: 912 Loss: 0.60, acc: 0.64\n",
      "Epoch: 913 Loss: 0.63, acc: 0.66\n",
      "Epoch: 914 Loss: 0.66, acc: 0.63\n",
      "Epoch: 915 Loss: 0.63, acc: 0.67\n",
      "Epoch: 916 Loss: 0.62, acc: 0.62\n",
      "Epoch: 917 Loss: 0.62, acc: 0.62\n",
      "Epoch: 918 Loss: 0.62, acc: 0.63\n",
      "Epoch: 919 Loss: 0.61, acc: 0.68\n",
      "Epoch: 920 Loss: 0.59, acc: 0.70\n",
      "Epoch: 921 Loss: 0.60, acc: 0.67\n",
      "Epoch: 922 Loss: 0.56, acc: 0.73\n",
      "Epoch: 923 Loss: 0.61, acc: 0.65\n",
      "Epoch: 924 Loss: 0.61, acc: 0.63\n",
      "Epoch: 925 Loss: 0.57, acc: 0.65\n",
      "Epoch: 926 Loss: 0.63, acc: 0.64\n",
      "Epoch: 927 Loss: 0.63, acc: 0.68\n",
      "Epoch: 928 Loss: 0.60, acc: 0.69\n",
      "Epoch: 929 Loss: 0.60, acc: 0.68\n",
      "Epoch: 930 Loss: 0.65, acc: 0.64\n",
      "Epoch: 931 Loss: 0.59, acc: 0.67\n",
      "Epoch: 932 Loss: 0.61, acc: 0.66\n",
      "Epoch: 933 Loss: 0.59, acc: 0.67\n",
      "Epoch: 934 Loss: 0.59, acc: 0.72\n",
      "Epoch: 935 Loss: 0.61, acc: 0.66\n",
      "Epoch: 936 Loss: 0.60, acc: 0.67\n",
      "Epoch: 937 Loss: 0.59, acc: 0.69\n",
      "Epoch: 938 Loss: 0.58, acc: 0.73\n",
      "Epoch: 939 Loss: 0.63, acc: 0.63\n",
      "Epoch: 940 Loss: 0.57, acc: 0.70\n",
      "Epoch: 941 Loss: 0.58, acc: 0.72\n",
      "Epoch: 942 Loss: 0.61, acc: 0.70\n",
      "Epoch: 943 Loss: 0.63, acc: 0.64\n",
      "Epoch: 944 Loss: 0.57, acc: 0.74\n",
      "Epoch: 945 Loss: 0.62, acc: 0.59\n",
      "Epoch: 946 Loss: 0.55, acc: 0.69\n",
      "Epoch: 947 Loss: 0.62, acc: 0.68\n",
      "Epoch: 948 Loss: 0.59, acc: 0.71\n",
      "Epoch: 949 Loss: 0.62, acc: 0.63\n",
      "Epoch: 950 Loss: 0.63, acc: 0.65\n",
      "Epoch: 951 Loss: 0.53, acc: 0.73\n",
      "Epoch: 952 Loss: 0.52, acc: 0.75\n",
      "Epoch: 953 Loss: 0.62, acc: 0.66\n",
      "Epoch: 954 Loss: 0.57, acc: 0.73\n",
      "Epoch: 955 Loss: 0.67, acc: 0.59\n",
      "Epoch: 956 Loss: 0.59, acc: 0.72\n",
      "Epoch: 957 Loss: 0.56, acc: 0.72\n",
      "Epoch: 958 Loss: 0.53, acc: 0.77\n",
      "Epoch: 959 Loss: 0.60, acc: 0.67\n",
      "Epoch: 960 Loss: 0.63, acc: 0.62\n",
      "Epoch: 961 Loss: 0.61, acc: 0.60\n",
      "Epoch: 962 Loss: 0.60, acc: 0.73\n",
      "Epoch: 963 Loss: 0.64, acc: 0.66\n",
      "Epoch: 964 Loss: 0.64, acc: 0.64\n",
      "Epoch: 965 Loss: 0.59, acc: 0.67\n",
      "Epoch: 966 Loss: 0.59, acc: 0.66\n",
      "Epoch: 967 Loss: 0.61, acc: 0.69\n",
      "Epoch: 968 Loss: 0.60, acc: 0.66\n",
      "Epoch: 969 Loss: 0.63, acc: 0.62\n",
      "Epoch: 970 Loss: 0.58, acc: 0.66\n",
      "Epoch: 971 Loss: 0.59, acc: 0.69\n",
      "Epoch: 972 Loss: 0.60, acc: 0.69\n",
      "Epoch: 973 Loss: 0.63, acc: 0.60\n",
      "Epoch: 974 Loss: 0.61, acc: 0.69\n",
      "Epoch: 975 Loss: 0.63, acc: 0.62\n",
      "Epoch: 976 Loss: 0.55, acc: 0.72\n",
      "Epoch: 977 Loss: 0.63, acc: 0.62\n",
      "Epoch: 978 Loss: 0.52, acc: 0.73\n",
      "Epoch: 979 Loss: 0.57, acc: 0.69\n",
      "Epoch: 980 Loss: 0.64, acc: 0.65\n",
      "Epoch: 981 Loss: 0.63, acc: 0.64\n",
      "Epoch: 982 Loss: 0.57, acc: 0.73\n",
      "Epoch: 983 Loss: 0.54, acc: 0.73\n",
      "Epoch: 984 Loss: 0.60, acc: 0.66\n",
      "Epoch: 985 Loss: 0.62, acc: 0.68\n",
      "Epoch: 986 Loss: 0.62, acc: 0.66\n",
      "Epoch: 987 Loss: 0.61, acc: 0.63\n",
      "Epoch: 988 Loss: 0.55, acc: 0.73\n",
      "Epoch: 989 Loss: 0.59, acc: 0.73\n",
      "Epoch: 990 Loss: 0.67, acc: 0.62\n",
      "Epoch: 991 Loss: 0.60, acc: 0.69\n",
      "Epoch: 992 Loss: 0.62, acc: 0.61\n",
      "Epoch: 993 Loss: 0.60, acc: 0.68\n",
      "Epoch: 994 Loss: 0.58, acc: 0.71\n",
      "Epoch: 995 Loss: 0.60, acc: 0.66\n",
      "Epoch: 996 Loss: 0.54, acc: 0.71\n",
      "Epoch: 997 Loss: 0.61, acc: 0.68\n",
      "Epoch: 998 Loss: 0.62, acc: 0.59\n",
      "Epoch: 999 Loss: 0.61, acc: 0.67\n",
      "Epoch: 1000 Loss: 0.60, acc: 0.71, recall@25: 0.23\n",
      "Saved model 'modelos/model_baseline_dwen_1000_feature_1000epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=461, Best_loss=0.51, Recall@25=0.23\n",
      "CPU times: user 1min 28s, sys: 901 ms, total: 1min 29s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Similarity model\n",
    "bug_feature_output_a = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_a')\n",
    "bug_feature_output_b = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_b')\n",
    "similarity_model = dwen_model(bug_feature_output_a, bug_feature_output_b, 'dwen')\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, \n",
    "                                                        bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, acc: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0],  h[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, acc: {:.2f}\".format(epoch+1, h[0],  h[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|89620:0.638342946767807,96654:0.6307685673236847,91612:0.6286369562149048,96181:0.6194678246974945,73946:0.6190367937088013,98890:0.6184544265270233,116193:0.6168799698352814,117315:0.6158325374126434,99100:0.6154980063438416,94181:0.6152200698852539,95068:0.6129328310489655,94909:0.6125856041908264,112672:0.6117276847362518,101671:0.6112997829914093,107139:0.6094796657562256,99874:0.6094290912151337,122052:0.6088610589504242,97773:0.6086090505123138,92051:0.6063841581344604,103396:0.6059651672840118,91855:0.6050929725170135,28932:0.6042809188365936,98451:0.6040443181991577,118218:0.6036084592342377,102225:0.6026510894298553,109950:0.5995483100414276,85170:0.5985702872276306,55499:0.5979336798191071,96299:0.5974744558334351',\n",
       " '109674:108544,111059,108379,109366|99541:0.41072118282318115,93934:0.3501514196395874,100293:0.34763187170028687,100449:0.3395318388938904,105082:0.3392093777656555,119486:0.33525556325912476,123048:0.32127201557159424,99451:0.31886428594589233,106711:0.313917875289917,98659:0.31331074237823486,105461:0.3129173517227173,75660:0.3061392903327942,114431:0.3045000433921814,106234:0.30176544189453125,112155:0.30166321992874146,96045:0.2999020218849182,111452:0.29958969354629517,96162:0.2995781898498535,100327:0.2984996438026428,104068:0.2976943254470825,113829:0.29737913608551025,122635:0.29339534044265747,106607:0.29329317808151245,100328:0.29223519563674927,120130:0.2920616865158081,94544:0.29199469089508057,94443:0.29147279262542725,98310:0.2888362407684326,115888:0.28768885135650635',\n",
       " '111059:108544,109674,108379,109366|112299:0.7341528236865997,104817:0.727438747882843,108440:0.7142905592918396,117276:0.7120579481124878,93689:0.7114005088806152,96993:0.7108640372753143,99678:0.7104345262050629,103585:0.7092904150485992,99200:0.7079577445983887,89006:0.7078733444213867,99966:0.7076232731342316,108601:0.7062848508358002,100033:0.7060451805591583,101955:0.7052602767944336,115059:0.7032132148742676,92547:0.7018943727016449,112201:0.6997906565666199,100703:0.6993858516216278,116659:0.698106199502945,95255:0.6955004930496216,90086:0.6952182352542877,92082:0.6951193511486053,110637:0.6937635838985443,110397:0.6929806470870972,108943:0.6928615570068359,92024:0.692639172077179,118451:0.6922184228897095,116986:0.6917177438735962,105907:0.6906768977642059',\n",
       " '108379:108544,111059,109674,109366|98647:0.677797794342041,96455:0.6752451360225677,92568:0.6668739914894104,112393:0.6658096611499786,105646:0.6641721427440643,102142:0.6639333963394165,94924:0.6637235283851624,104657:0.660026341676712,91087:0.6583676040172577,97337:0.6566400527954102,95394:0.6566110551357269,96268:0.6564062535762787,110764:0.6526687741279602,104782:0.6503071188926697,89385:0.6500700414180756,101366:0.6499422490596771,116738:0.6498496830463409,112606:0.6495561599731445,91756:0.6493602693080902,102489:0.6490002274513245,102491:0.6490002274513245,108423:0.6489218175411224,91542:0.6479736864566803,95853:0.6472324132919312,103284:0.6452630460262299,106608:0.6450723707675934,100435:0.6444134712219238,115419:0.6433492004871368,98670:0.6425939798355103',\n",
       " '109366:108544,111059,109674,108379|93631:0.6467771530151367,108961:0.6438325941562653,104075:0.6211910843849182,117798:0.6134231388568878,103447:0.6081854701042175,112677:0.6061896979808807,92178:0.5988432466983795,99097:0.5949519872665405,97263:0.5930750072002411,107142:0.5926787257194519,52800:0.5925979018211365,119738:0.5846174955368042,87530:0.5845417678356171,116197:0.5836497843265533,115557:0.5779765248298645,105035:0.575219988822937,101471:0.5749984085559845,102370:0.5730410516262054,95688:0.5724921226501465,98570:0.5720610618591309,115720:0.5709827244281769,96597:0.5678938925266266,90804:0.56721630692482,116025:0.5671802163124084,110012:0.5666183233261108,96913:0.5653899610042572,97135:0.5653120875358582,108190:0.5650182068347931,115935:0.5619465410709381',\n",
       " '110594:107073,108355,108453,109162,111761,111800|115491:0.5175833404064178,103504:0.5115646123886108,97584:0.5101484060287476,113440:0.5073525011539459,46957:0.5004971921443939,116168:0.4776606559753418,99885:0.47730088233947754,109964:0.4715139865875244,94749:0.47099679708480835,96945:0.4709785580635071,105369:0.4690311551094055,90682:0.4657163619995117,119409:0.46523427963256836,103241:0.46450769901275635,109431:0.4607377052307129,98766:0.4554123878479004,96894:0.45328038930892944,96429:0.4527767300605774,115892:0.4522852897644043,106168:0.450359046459198,99110:0.44506627321243286,105250:0.44483691453933716,93732:0.4443638324737549,106817:0.4439106583595276,109628:0.4431336522102356,114671:0.44270026683807373,111767:0.44146883487701416,89920:0.4411032795906067,93318:0.43779486417770386',\n",
       " '111800:107073,110594,108355,108453,109162,111761|105859:0.7236564457416534,76001:0.7008225917816162,95223:0.6982167959213257,97548:0.6977289915084839,103188:0.6952025592327118,104964:0.6919240951538086,101791:0.6837283670902252,117607:0.6836152076721191,110829:0.679589033126831,95490:0.6733123362064362,116656:0.6726062595844269,95903:0.6705206632614136,110975:0.6704724431037903,96628:0.6691959798336029,93258:0.6685898303985596,102724:0.6680282950401306,119293:0.6678110361099243,100675:0.6676304340362549,103771:0.6667768657207489,91952:0.6661238670349121,95492:0.6658211350440979,100674:0.6654958128929138,98867:0.6648982167243958,113639:0.6640852987766266,95088:0.6632139086723328,106589:0.6630549132823944,94134:0.6624542772769928,100843:0.662444144487381,104270:0.6624072790145874',\n",
       " '111761:107073,110594,108355,108453,109162,111800|117761:0.6221582293510437,94539:0.6061315834522247,104306:0.6006225943565369,116293:0.5967066586017609,115825:0.5961919128894806,97216:0.5912529230117798,91589:0.5910899341106415,102436:0.5867388844490051,102582:0.5859884023666382,102483:0.5857338309288025,115715:0.578193187713623,98394:0.5770389139652252,111055:0.5763556957244873,120070:0.5749878287315369,104856:0.5746990740299225,113829:0.5733104646205902,94907:0.5683709681034088,112548:0.5675660967826843,101499:0.5631639957427979,96787:0.5621773898601532,75660:0.5597915649414062,114077:0.5597184598445892,117096:0.5595163404941559,115048:0.5593287348747253,91404:0.5581991672515869,89137:0.552760124206543,101690:0.551055520772934,99163:0.5486546754837036,91550:0.5482001006603241',\n",
       " '109162:107073,110594,108355,108453,111761,111800|30870:0.6240856945514679,92771:0.6127835214138031,108748:0.603746086359024,109249:0.5955279469490051,91432:0.5931214690208435,95624:0.5851395428180695,98583:0.5847025215625763,102484:0.5845517516136169,95907:0.5827757716178894,106207:0.5798392295837402,91404:0.5793794691562653,57538:0.5786334872245789,102984:0.5769596993923187,110392:0.575051873922348,97263:0.5748990476131439,98554:0.570701539516449,92532:0.5706966817378998,105991:0.570499986410141,100744:0.567245215177536,104013:0.562807023525238,90612:0.5617393255233765,91896:0.5615816116333008,98284:0.5612267255783081,105671:0.5607237219810486,106733:0.5602549612522125,85271:0.5602477788925171,101961:0.5601606965065002,100937:0.5595582723617554,103579:0.5590759217739105',\n",
       " '108355:107073,110594,108453,109162,111761,111800|115793:0.480735719203949,92115:0.46795380115509033,96471:0.4630941152572632,96912:0.45855414867401123,90864:0.4583613872528076,75190:0.4569011330604553,112216:0.45585453510284424,112218:0.45585453510284424,117812:0.4536163806915283,115979:0.449134886264801,115980:0.449134886264801,101569:0.44746512174606323,95365:0.44498878717422485,96179:0.4424338936805725,99081:0.4404190182685852,100321:0.4376164674758911,104540:0.434073805809021,98764:0.42981165647506714,112518:0.4297184348106384,95878:0.42953234910964966,101376:0.4274255633354187,93638:0.42495161294937134,116199:0.42412394285202026,102053:0.4187268614768982,91811:0.4159596562385559,79559:0.41470569372177124,113856:0.4141480326652527,118503:0.4126822352409363,106090:0.41003745794296265',\n",
       " '108453:107073,110594,108355,109162,111761,111800|96298:0.6966796815395355,113639:0.6946795582771301,102788:0.689286470413208,113312:0.6882219314575195,113313:0.6882219314575195,108665:0.6872455179691315,109393:0.6854575574398041,123087:0.6851372122764587,102386:0.6786406338214874,104118:0.678562730550766,121385:0.676898181438446,93406:0.6756483912467957,102251:0.6754187643527985,103502:0.6750093698501587,93989:0.6732507348060608,95132:0.6729266941547394,99482:0.6724485456943512,91527:0.671919584274292,48132:0.6712906956672668,90577:0.6712541878223419,117723:0.6710512340068817,123458:0.6706741154193878,104270:0.6705449223518372,94172:0.6703386008739471,110549:0.6696281731128693,99256:0.6691721677780151,94526:0.6678784489631653,103992:0.6672966480255127,110975:0.6663885712623596',\n",
       " '114705:114676|98302:0.5037876963615417,119738:0.4848676919937134,104075:0.48265016078948975,115557:0.4824942946434021,99737:0.4788551330566406,118517:0.47827619314193726,96913:0.4759283661842346,101563:0.4750105142593384,113225:0.4733639359474182,97253:0.47301244735717773,101105:0.4695156216621399,93631:0.46825718879699707,91495:0.4644818902015686,94775:0.4638898968696594,102454:0.4635317325592041,112677:0.46294403076171875,96205:0.4625694751739502,101160:0.46218806505203247,102458:0.46012169122695923,106936:0.458911657333374,90332:0.4586838483810425,105511:0.4580281972885132,103510:0.45802682638168335,120554:0.4567713141441345,107142:0.4527413845062256,90078:0.45221126079559326,98903:0.4503294825553894,95171:0.45032399892807007,101380:0.4500352740287781',\n",
       " '114676:114705|120142:0.5594629347324371,120089:0.5425452291965485,102082:0.5278824865818024,97760:0.5132081806659698,115892:0.5041336417198181,93249:0.5012192726135254,90395:0.49482446908950806,106067:0.494015097618103,103326:0.4929099678993225,119552:0.4922559857368469,95191:0.4905881881713867,109739:0.4880165457725525,101315:0.48703598976135254,122331:0.48578107357025146,106090:0.48543626070022583,113856:0.47591614723205566,115068:0.47506463527679443,119614:0.47366613149642944,91393:0.47251075506210327,93569:0.47155147790908813,104021:0.47138869762420654,96203:0.4710882902145386,101522:0.46509623527526855,93996:0.4646151661872864,94272:0.464602530002594,102428:0.4644157290458679,122822:0.46346139907836914,119778:0.46192675828933716,122935:0.4610448479652405',\n",
       " '110593:110618|103115:0.565537691116333,115858:0.5437175631523132,96179:0.5307140350341797,97232:0.5304602980613708,97231:0.5304602980613708,109284:0.5217902064323425,94554:0.5132940411567688,109021:0.5131620764732361,102053:0.5126430094242096,123293:0.510748565196991,93569:0.5040404498577118,109880:0.49628400802612305,97862:0.49452292919158936,95365:0.4939451217651367,106479:0.49319273233413696,102675:0.4925246238708496,88476:0.4922989010810852,86866:0.4921719431877136,94175:0.4821251630783081,110329:0.4802513122558594,101159:0.47913628816604614,115793:0.47865360975265503,97760:0.47773605585098267,91485:0.4770914316177368,101593:0.4750415086746216,95147:0.47109150886535645,95878:0.4693065285682678,101376:0.46742403507232666,95242:0.4668366312980652',\n",
       " '110618:110593|116649:0.6719076633453369,108012:0.663220226764679,102225:0.6553426086902618,94366:0.6523315012454987,88677:0.6509644687175751,106861:0.6503395438194275,121778:0.649947464466095,103396:0.6496833562850952,90892:0.6489172279834747,92568:0.6453478634357452,121276:0.645193874835968,121277:0.645193874835968,94181:0.6451555490493774,103810:0.6429319679737091,106708:0.6420753002166748,101671:0.6416849493980408,106950:0.6409412920475006,91542:0.6394591927528381,99133:0.639121949672699,90871:0.6372039020061493,96079:0.6370473206043243,99420:0.6364311277866364,113084:0.6355005204677582,117315:0.6331070065498352,95626:0.6328838169574738,95778:0.6317890882492065,95207:0.6306454837322235,98451:0.6305145919322968,102065:0.6303749978542328',\n",
       " '102409:102053|92973:0.5354459583759308,113107:0.5349828600883484,89520:0.4991617202758789,91543:0.4942033886909485,104589:0.4905017018318176,94226:0.48614585399627686,101124:0.48253333568573,74223:0.48228371143341064,110383:0.48074132204055786,106779:0.47797733545303345,100827:0.4772679805755615,102648:0.47662776708602905,92251:0.47647178173065186,105337:0.47052061557769775,104421:0.46990782022476196,111908:0.469856321811676,114133:0.4692877531051636,89255:0.46765297651290894,107749:0.46615952253341675,95380:0.4658604860305786,93406:0.4657924175262451,103914:0.46401309967041016,108665:0.46399688720703125,122484:0.46376878023147583,103363:0.46317416429519653,122476:0.4627445936203003,104118:0.46267539262771606,110055:0.4619811177253723,89496:0.4608311653137207',\n",
       " '102053:102409|101376:0.6767806112766266,96471:0.639040470123291,106479:0.638244092464447,115793:0.6302208602428436,101315:0.6022010445594788,106090:0.5917579233646393,93569:0.5909329950809479,96179:0.589882105588913,115068:0.5864123404026031,105661:0.5813507437705994,100282:0.5809087455272675,94554:0.5724080801010132,95365:0.5718992948532104,102675:0.5680951476097107,117812:0.567812591791153,100321:0.5623020231723785,109880:0.5622950494289398,94063:0.5617341995239258,101593:0.5564008951187134,109284:0.5553911924362183,108966:0.5540200173854828,95191:0.5512831807136536,106561:0.5512080788612366,110388:0.5505348443984985,101569:0.5504807531833649,103374:0.550129771232605,112518:0.549889326095581,105175:0.5492833852767944,108106:0.5479221343994141',\n",
       " '110604:110612|100846:0.6373114883899689,102465:0.624707043170929,91990:0.619709312915802,120070:0.6144915521144867,121089:0.6068334579467773,121090:0.6068334579467773,121091:0.6068334579467773,121020:0.6052690744400024,92645:0.6005563139915466,93990:0.598251074552536,116336:0.5973716676235199,105594:0.5963695049285889,102436:0.5958454608917236,108714:0.5957688391208649,105991:0.5947059392929077,122550:0.593899667263031,104075:0.5901561379432678,100465:0.5897973775863647,116229:0.5897067487239838,90612:0.5894991755485535,113585:0.5888093113899231,93635:0.5864237546920776,89899:0.5862387120723724,99097:0.5840639770030975,116025:0.5839895308017731,99186:0.5839055478572845,101471:0.582786500453949,95624:0.5823763906955719,99298:0.5791761875152588',\n",
       " '110612:110604|101734:0.5247971713542938,112556:0.4999921917915344,111648:0.4979557394981384,122593:0.4936392903327942,109833:0.49126362800598145,93889:0.48878180980682373,93165:0.48752856254577637,104620:0.4874916076660156,90395:0.48568403720855713,97907:0.47978776693344116,103277:0.47846031188964844,104631:0.4780946969985962,104021:0.47789567708969116,91407:0.4734475612640381,103980:0.4715750217437744,103736:0.46711254119873047,106268:0.46317142248153687,68942:0.459550142288208,116838:0.45742541551589966,96228:0.4563605785369873,103137:0.4558367133140564,102750:0.4556061029434204,121279:0.45505332946777344,95089:0.45322567224502563,104547:0.450528621673584,101645:0.44885700941085815,117591:0.4451788663864136,113252:0.4438055753707886,112930:0.4428081512451172',\n",
       " '116738:116938,117027|91756:0.7796950191259384,103491:0.7694689184427261,117315:0.7618849724531174,103284:0.7560221254825592,102142:0.7515544444322586,117150:0.744847297668457,100435:0.744141012430191,89691:0.7430381178855896,89692:0.7430381178855896,105646:0.7414193153381348,110764:0.7406329214572906,119327:0.7309680581092834,104155:0.7284854650497437,95309:0.7272200286388397,102065:0.7256223857402802,97809:0.7252542674541473,109884:0.7202556133270264,109889:0.7202556133270264,116649:0.7200290858745575,106395:0.7199820280075073,104657:0.7181722223758698,89620:0.717589944601059,105209:0.7169013023376465,102225:0.7163518667221069,94909:0.7160014808177948,91578:0.7150920629501343,105396:0.7138439118862152,100899:0.7130992710590363,102251:0.712965339422226']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recall@25 last epoch:', 0.23)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, \n",
    "                                                            bug_train_ids, method='dwen')\n",
    "\n",
    "\"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 2086\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_dwen_1000_feature_1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bug_feature_output_a\n",
    "#model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      5568600     title_dwen_a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      5568600     desc_dwen_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 11,137,200\n",
      "Trainable params: 0\n",
      "Non-trainable params: 11,137,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, \n",
    "                                                                   bug_train_ids, method='dwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/exported_rank_baseline_dwen_1000.txt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.14,\n",
       " '2 - recall_at_10': 0.16,\n",
       " '3 - recall_at_15': 0.19,\n",
       " '4 - recall_at_20': 0.22,\n",
       " '5 - recall_at_25': 0.23}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
