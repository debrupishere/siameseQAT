{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a54b8f5299406d86fe506c43153d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4321e86c41e4b82b7eaf4b52c6f24fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 52.1 s, sys: 13.3 s, total: 1min 5s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e732d3601b4e36a22001d3b4cbba66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[275492, 218812],\n",
       " [288296, 264093],\n",
       " [273286, 293887],\n",
       " [57162, 62059],\n",
       " [82146, 67997],\n",
       " [56777, 61857],\n",
       " [169445, 165179],\n",
       " [250521, 273893],\n",
       " [247266, 241461],\n",
       " [36781, 38338]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '5\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '213\\n',\n",
       " 'creation_ts': '2002-05-27 04:18:00 -0400',\n",
       " 'delta_ts': '2002-06-03 00:20:46 -0400',\n",
       " 'description': '[CLS] build f1 , windows ##x ##p professional japanese steps 1 ) at feature update view in install / update perspective , right click \" sites to visits \" then create \" new - update site book ##mark \" with db ##cs japanese name . 2 ) the string is displayed correctly at the feature updates view and preview . 3 ) close the eclipse and re ##lau ##ch 4 ) the japanese string is displayed as middle dot . [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  3857, 20069,  1010,  3645,  2595,  2361,  2658,  2887,\n",
       "         4084,  1015,  1007,  2012,  3444, 10651,  3193,  1999, 16500,\n",
       "         1013,   102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 17903,\n",
       " 'priority': '0\\n',\n",
       " 'product': '181\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_token': array([  101, 16962,  6169,  1024, 22132,  2271,  2171,  2005,  2609,\n",
       "         2338, 10665,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  3857, 20069,  1010,  3645,  2595,  2361,\n",
       "         2658,  2887,  4084,  1015,  1007,  2012,  3444, 10651,  3193,\n",
       "         1999, 16500,  1013,   102]),\n",
       " 'title': '[CLS] db ##cs : bog ##us name for site book ##mark [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101, 16962,  6169,  1024, 22132,  2271,  2171,  2005,  2609,\n",
       "         2338, 10665,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '381\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 39339)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370476"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(list(issues_by_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110647 in experiment.baseline.bug_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 454 ms, sys: 0 ns, total: 454 ms\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = experiment.batch_iterator(None, \n",
    "                                                                                          baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1,\n",
    "                                                                                          issues_by_buckets)\n",
    "\n",
    "pos = np.full((1, batch_size_test), 1)\n",
    "neg = np.full((1, batch_size_test), 0)\n",
    "valid_sim = np.concatenate([pos, neg], -1)[0]\n",
    "\n",
    "valid_title_sample_a = np.concatenate([valid_input_sample['title'], valid_input_sample['title']], 0)\n",
    "valid_title_sample_b = np.concatenate([valid_input_pos['title'], valid_input_neg['title']], 0)\n",
    "valid_desc_sample_a = np.concatenate([valid_input_sample['description'], valid_input_sample['description']], 0)\n",
    "valid_desc_sample_b = np.concatenate([valid_input_pos['description'], valid_input_neg['description']], 0)\n",
    "\n",
    "validation_sample = [valid_title_sample_a, valid_title_sample_b, valid_desc_sample_a, valid_desc_sample_b]\n",
    "\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_title_sample_a), len(valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (256,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 21175'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd5eac5f30a4393a43d5690b2f7abbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e581a229844bb6bf04b3e8dff9956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 36s, sys: 3.69 s, total: 1min 39s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### DWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum, Subtract, \\\n",
    "    Average, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import keras.backend as K\n",
    "\n",
    "def dwen_feature(title_feature_model, desc_feature_model, \\\n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    \n",
    "    # Embedding feature\n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    \n",
    "    bug_t_feat = GlobalAveragePooling1D()(bug_t_feat)\n",
    "    bug_d_feat = GlobalAveragePooling1D()(bug_d_feat)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = Average(name = 'merge_features_{}'.format(name))([bug_t_feat, bug_d_feat])\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model\n",
    "\n",
    "def dwen_model(bug_feature_output_a, bug_feature_output_b, name):\n",
    "    \n",
    "    inputs = np.concatenate([bug_feature_output_a.input, bug_feature_output_b.input], -1).tolist()\n",
    "    \n",
    "    bug_feature_output_a = bug_feature_output_a.output\n",
    "    bug_feature_output_b = bug_feature_output_b.output\n",
    "    \n",
    "    # 2D concatenate feature\n",
    "    bug_feature_output = concatenate([bug_feature_output_a, bug_feature_output_b])\n",
    "    \n",
    "    hidden_layers = 2\n",
    "    \n",
    "    # Deep Hidden MLPs\n",
    "    for _ in range(hidden_layers):\n",
    "        number_of_units = K.int_shape(bug_feature_output)[1]\n",
    "        bug_feature_output = Dense(number_of_units // 2)(bug_feature_output)\n",
    "#         bug_feature_output = BatchNormalization()(bug_feature_output)\n",
    "        bug_feature_output = Activation('relu')(bug_feature_output)\n",
    "        #bug_feature_output = Dropout(.5)(bug_feature_output)\n",
    "    \n",
    "     # Sigmoid\n",
    "    output = Dense(1, activation='sigmoid')(bug_feature_output)\n",
    "\n",
    "    similarity_model = Model(inputs=inputs, outputs=[output], name = 'dwen_output')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'baseline_dwen_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      581600700   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      581600700   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,163,427,001\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.69, Loss_test: 0.70, acc: 0.53, acc_test: 0.50\n",
      "Epoch: 2 Loss: 0.69, Loss_test: 0.70, acc: 0.56, acc_test: 0.50\n",
      "Epoch: 3 Loss: 0.69, Loss_test: 0.70, acc: 0.53, acc_test: 0.50\n",
      "Epoch: 4 Loss: 0.69, Loss_test: 0.70, acc: 0.53, acc_test: 0.50\n",
      "Epoch: 5 Loss: 0.70, Loss_test: 0.71, acc: 0.48, acc_test: 0.50\n",
      "Epoch: 6 Loss: 0.69, Loss_test: 0.71, acc: 0.50, acc_test: 0.50\n",
      "Epoch: 7 Loss: 0.69, Loss_test: 0.70, acc: 0.55, acc_test: 0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 0.68, Loss_test: 0.69, acc: 0.59, acc_test: 0.51\n",
      "Epoch: 9 Loss: 0.68, Loss_test: 0.69, acc: 0.59, acc_test: 0.50\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.67, Loss_test: 0.69, acc: 0.66, acc_test: 0.54\n",
      "Epoch: 11 Loss: 0.68, Loss_test: 0.69, acc: 0.60, acc_test: 0.54\n",
      "Epoch: 12 Loss: 0.67, Loss_test: 0.69, acc: 0.66, acc_test: 0.55\n",
      "Epoch: 13 Loss: 0.67, Loss_test: 0.69, acc: 0.62, acc_test: 0.53\n",
      "Epoch: 14 Loss: 0.65, Loss_test: 0.69, acc: 0.70, acc_test: 0.52\n",
      "Epoch: 15 Loss: 0.67, Loss_test: 0.69, acc: 0.59, acc_test: 0.55\n",
      "Epoch: 16 Loss: 0.66, Loss_test: 0.69, acc: 0.60, acc_test: 0.58\n",
      "Epoch: 17 Loss: 0.67, Loss_test: 0.69, acc: 0.60, acc_test: 0.57\n",
      "Epoch: 18 Loss: 0.67, Loss_test: 0.69, acc: 0.59, acc_test: 0.57\n",
      "Epoch: 19 Loss: 0.64, Loss_test: 0.69, acc: 0.63, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.65, Loss_test: 0.69, acc: 0.65, acc_test: 0.60\n",
      "Epoch: 21 Loss: 0.67, Loss_test: 0.69, acc: 0.56, acc_test: 0.59\n",
      "Epoch: 22 Loss: 0.63, Loss_test: 0.70, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 23 Loss: 0.63, Loss_test: 0.73, acc: 0.66, acc_test: 0.53\n",
      "Epoch: 24 Loss: 0.63, Loss_test: 0.73, acc: 0.62, acc_test: 0.53\n",
      "Epoch: 25 Loss: 0.62, Loss_test: 0.73, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 26 Loss: 0.64, Loss_test: 0.73, acc: 0.67, acc_test: 0.55\n",
      "Epoch: 27 Loss: 0.66, Loss_test: 0.72, acc: 0.62, acc_test: 0.54\n",
      "Epoch: 28 Loss: 0.66, Loss_test: 0.72, acc: 0.59, acc_test: 0.57\n",
      "Epoch: 29 Loss: 0.64, Loss_test: 0.70, acc: 0.62, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.64, Loss_test: 0.69, acc: 0.62, acc_test: 0.54\n",
      "Epoch: 31 Loss: 0.63, Loss_test: 0.69, acc: 0.61, acc_test: 0.52\n",
      "Epoch: 32 Loss: 0.58, Loss_test: 0.69, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 33 Loss: 0.64, Loss_test: 0.70, acc: 0.63, acc_test: 0.50\n",
      "Epoch: 34 Loss: 0.63, Loss_test: 0.72, acc: 0.62, acc_test: 0.52\n",
      "Epoch: 35 Loss: 0.61, Loss_test: 0.70, acc: 0.66, acc_test: 0.51\n",
      "Epoch: 36 Loss: 0.64, Loss_test: 0.72, acc: 0.66, acc_test: 0.56\n",
      "Epoch: 37 Loss: 0.57, Loss_test: 0.73, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 38 Loss: 0.59, Loss_test: 0.71, acc: 0.70, acc_test: 0.51\n",
      "Epoch: 39 Loss: 0.61, Loss_test: 0.71, acc: 0.66, acc_test: 0.51\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.57, Loss_test: 0.72, acc: 0.70, acc_test: 0.52\n",
      "Epoch: 41 Loss: 0.61, Loss_test: 0.73, acc: 0.65, acc_test: 0.54\n",
      "Epoch: 42 Loss: 0.60, Loss_test: 0.74, acc: 0.68, acc_test: 0.55\n",
      "Epoch: 43 Loss: 0.60, Loss_test: 0.75, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 44 Loss: 0.57, Loss_test: 0.75, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 45 Loss: 0.59, Loss_test: 0.73, acc: 0.65, acc_test: 0.54\n",
      "Epoch: 46 Loss: 0.57, Loss_test: 0.72, acc: 0.69, acc_test: 0.53\n",
      "Epoch: 47 Loss: 0.61, Loss_test: 0.75, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 48 Loss: 0.71, Loss_test: 0.74, acc: 0.55, acc_test: 0.57\n",
      "Epoch: 49 Loss: 0.57, Loss_test: 0.72, acc: 0.72, acc_test: 0.52\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.63, Loss_test: 0.73, acc: 0.62, acc_test: 0.50\n",
      "Epoch: 51 Loss: 0.62, Loss_test: 0.72, acc: 0.62, acc_test: 0.52\n",
      "Epoch: 52 Loss: 0.60, Loss_test: 0.73, acc: 0.65, acc_test: 0.54\n",
      "Epoch: 53 Loss: 0.58, Loss_test: 0.73, acc: 0.73, acc_test: 0.53\n",
      "Epoch: 54 Loss: 0.59, Loss_test: 0.72, acc: 0.70, acc_test: 0.50\n",
      "Epoch: 55 Loss: 0.60, Loss_test: 0.73, acc: 0.70, acc_test: 0.50\n",
      "Epoch: 56 Loss: 0.60, Loss_test: 0.73, acc: 0.67, acc_test: 0.51\n",
      "Epoch: 57 Loss: 0.62, Loss_test: 0.76, acc: 0.67, acc_test: 0.56\n",
      "Epoch: 58 Loss: 0.54, Loss_test: 0.80, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 59 Loss: 0.55, Loss_test: 0.77, acc: 0.74, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.56, Loss_test: 0.74, acc: 0.70, acc_test: 0.51\n",
      "Epoch: 61 Loss: 0.57, Loss_test: 0.74, acc: 0.68, acc_test: 0.51\n",
      "Epoch: 62 Loss: 0.55, Loss_test: 0.75, acc: 0.72, acc_test: 0.54\n",
      "Epoch: 63 Loss: 0.54, Loss_test: 0.78, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 64 Loss: 0.58, Loss_test: 0.78, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 65 Loss: 0.57, Loss_test: 0.76, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 66 Loss: 0.57, Loss_test: 0.75, acc: 0.72, acc_test: 0.56\n",
      "Epoch: 67 Loss: 0.60, Loss_test: 0.77, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 68 Loss: 0.59, Loss_test: 0.81, acc: 0.70, acc_test: 0.54\n",
      "Epoch: 69 Loss: 0.60, Loss_test: 0.84, acc: 0.71, acc_test: 0.54\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.54, Loss_test: 0.80, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 71 Loss: 0.56, Loss_test: 0.75, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 72 Loss: 0.54, Loss_test: 0.75, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 73 Loss: 0.57, Loss_test: 0.76, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 74 Loss: 0.62, Loss_test: 0.78, acc: 0.65, acc_test: 0.57\n",
      "Epoch: 75 Loss: 0.60, Loss_test: 0.78, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 76 Loss: 0.55, Loss_test: 0.76, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 77 Loss: 0.59, Loss_test: 0.74, acc: 0.70, acc_test: 0.53\n",
      "Epoch: 78 Loss: 0.56, Loss_test: 0.74, acc: 0.68, acc_test: 0.54\n",
      "Epoch: 79 Loss: 0.54, Loss_test: 0.76, acc: 0.74, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.62, Loss_test: 0.78, acc: 0.69, acc_test: 0.57\n",
      "Epoch: 81 Loss: 0.57, Loss_test: 0.78, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 82 Loss: 0.62, Loss_test: 0.76, acc: 0.66, acc_test: 0.57\n",
      "Epoch: 83 Loss: 0.57, Loss_test: 0.74, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 84 Loss: 0.49, Loss_test: 0.74, acc: 0.76, acc_test: 0.52\n",
      "Epoch: 85 Loss: 0.57, Loss_test: 0.75, acc: 0.72, acc_test: 0.53\n",
      "Epoch: 86 Loss: 0.54, Loss_test: 0.77, acc: 0.69, acc_test: 0.56\n",
      "Epoch: 87 Loss: 0.58, Loss_test: 0.77, acc: 0.67, acc_test: 0.55\n",
      "Epoch: 88 Loss: 0.57, Loss_test: 0.76, acc: 0.71, acc_test: 0.54\n",
      "Epoch: 89 Loss: 0.59, Loss_test: 0.78, acc: 0.65, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.51, Loss_test: 0.82, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 91 Loss: 0.55, Loss_test: 0.83, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 92 Loss: 0.54, Loss_test: 0.78, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 93 Loss: 0.57, Loss_test: 0.74, acc: 0.69, acc_test: 0.54\n",
      "Epoch: 94 Loss: 0.55, Loss_test: 0.73, acc: 0.69, acc_test: 0.52\n",
      "Epoch: 95 Loss: 0.53, Loss_test: 0.73, acc: 0.74, acc_test: 0.52\n",
      "Epoch: 96 Loss: 0.56, Loss_test: 0.73, acc: 0.69, acc_test: 0.54\n",
      "Epoch: 97 Loss: 0.52, Loss_test: 0.73, acc: 0.73, acc_test: 0.54\n",
      "Epoch: 98 Loss: 0.58, Loss_test: 0.73, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 99 Loss: 0.57, Loss_test: 0.74, acc: 0.69, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.56, Loss_test: 0.74, acc: 0.70, acc_tets: 0.55, recall@25: 0.13\n",
      "Best_epoch=84, Best_loss=0.49, Recall@25=0.13\n",
      "CPU times: user 1min 23s, sys: 8.38 s, total: 1min 31s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Similarity model\n",
    "bug_feature_output_a = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_a')\n",
    "bug_feature_output_b = dwen_feature(title_embedding_layer, desc_embedding_layer, \n",
    "                                    MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'dwen_b')\n",
    "similarity_model = dwen_model(bug_feature_output_a, bug_feature_output_b, 'dwen')\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, \n",
    "                                                        bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))\n",
    "    \n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_baseline_dwen_100.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_100_feature_100epochs_64batch(eclipse).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_dwen_b (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_b (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      581600700   title_dwen_a[0][0]               \n",
      "                                                                 title_dwen_b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      581600700   desc_dwen_a[0][0]                \n",
      "                                                                 desc_dwen_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 300)          0           embedding_layer_title[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 300)          0           embedding_layer_desc[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_b (Average) (None, 300)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           merge_features_dwen_a[0][0]      \n",
      "                                                                 merge_features_dwen_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 150)          45150       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            151         activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,163,427,001\n",
      "Trainable params: 225,601\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('dense_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'baseline_dwen_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.56, Loss_test: 0.75, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 102 Loss: 0.56, Loss_test: 0.79, acc: 0.72, acc_test: 0.55\n",
      "Epoch: 103 Loss: 0.65, Loss_test: 0.84, acc: 0.64, acc_test: 0.55\n",
      "Epoch: 104 Loss: 0.56, Loss_test: 0.79, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 105 Loss: 0.61, Loss_test: 0.76, acc: 0.68, acc_test: 0.55\n",
      "Epoch: 106 Loss: 0.56, Loss_test: 0.75, acc: 0.72, acc_test: 0.54\n",
      "Epoch: 107 Loss: 0.54, Loss_test: 0.77, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 108 Loss: 0.55, Loss_test: 0.82, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 109 Loss: 0.58, Loss_test: 0.75, acc: 0.64, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.55, Loss_test: 0.73, acc: 0.70, acc_test: 0.53\n",
      "Epoch: 111 Loss: 0.61, Loss_test: 0.74, acc: 0.66, acc_test: 0.54\n",
      "Epoch: 112 Loss: 0.62, Loss_test: 0.72, acc: 0.65, acc_test: 0.54\n",
      "Epoch: 113 Loss: 0.60, Loss_test: 0.78, acc: 0.64, acc_test: 0.56\n",
      "Epoch: 114 Loss: 0.57, Loss_test: 0.76, acc: 0.67, acc_test: 0.59\n",
      "Epoch: 115 Loss: 0.61, Loss_test: 0.72, acc: 0.61, acc_test: 0.54\n",
      "Epoch: 116 Loss: 0.57, Loss_test: 0.72, acc: 0.68, acc_test: 0.54\n",
      "Epoch: 117 Loss: 0.58, Loss_test: 0.72, acc: 0.63, acc_test: 0.55\n",
      "Epoch: 118 Loss: 0.57, Loss_test: 0.73, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 119 Loss: 0.56, Loss_test: 0.78, acc: 0.67, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.61, Loss_test: 0.76, acc: 0.66, acc_test: 0.59\n",
      "Epoch: 121 Loss: 0.47, Loss_test: 0.73, acc: 0.83, acc_test: 0.56\n",
      "Epoch: 122 Loss: 0.59, Loss_test: 0.73, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 123 Loss: 0.55, Loss_test: 0.73, acc: 0.75, acc_test: 0.54\n",
      "Epoch: 124 Loss: 0.54, Loss_test: 0.77, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 125 Loss: 0.55, Loss_test: 0.81, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 126 Loss: 0.61, Loss_test: 0.79, acc: 0.66, acc_test: 0.59\n",
      "Epoch: 127 Loss: 0.54, Loss_test: 0.74, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 128 Loss: 0.56, Loss_test: 0.72, acc: 0.70, acc_test: 0.57\n",
      "Epoch: 129 Loss: 0.58, Loss_test: 0.72, acc: 0.70, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 131 Loss: 0.48, Loss_test: 0.72, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 132 Loss: 0.53, Loss_test: 0.72, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 344 Loss: 0.54, Loss_test: 0.75, acc: 0.72, acc_test: 0.56\n",
      "Epoch: 345 Loss: 0.54, Loss_test: 0.73, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 346 Loss: 0.44, Loss_test: 0.77, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 347 Loss: 0.48, Loss_test: 0.80, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 348 Loss: 0.52, Loss_test: 0.76, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 349 Loss: 0.54, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.56, Loss_test: 0.75, acc: 0.70, acc_test: 0.54\n",
      "Epoch: 351 Loss: 0.51, Loss_test: 0.74, acc: 0.72, acc_test: 0.53\n",
      "Epoch: 352 Loss: 0.46, Loss_test: 0.73, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 353 Loss: 0.51, Loss_test: 0.75, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 354 Loss: 0.47, Loss_test: 0.78, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 355 Loss: 0.45, Loss_test: 0.78, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 356 Loss: 0.54, Loss_test: 0.74, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 357 Loss: 0.46, Loss_test: 0.73, acc: 0.78, acc_test: 0.54\n",
      "Epoch: 358 Loss: 0.48, Loss_test: 0.74, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 359 Loss: 0.52, Loss_test: 0.73, acc: 0.74, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.51, Loss_test: 0.76, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 361 Loss: 0.52, Loss_test: 0.79, acc: 0.69, acc_test: 0.57\n",
      "Epoch: 362 Loss: 0.49, Loss_test: 0.77, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 363 Loss: 0.57, Loss_test: 0.73, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 364 Loss: 0.55, Loss_test: 0.75, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 365 Loss: 0.59, Loss_test: 0.74, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 366 Loss: 0.45, Loss_test: 0.76, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 367 Loss: 0.56, Loss_test: 0.88, acc: 0.69, acc_test: 0.57\n",
      "Epoch: 368 Loss: 0.46, Loss_test: 0.94, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 369 Loss: 0.57, Loss_test: 0.85, acc: 0.70, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.46, Loss_test: 0.76, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 371 Loss: 0.48, Loss_test: 0.74, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 372 Loss: 0.50, Loss_test: 0.73, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 373 Loss: 0.48, Loss_test: 0.75, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 374 Loss: 0.48, Loss_test: 0.81, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 375 Loss: 0.52, Loss_test: 0.82, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 376 Loss: 0.58, Loss_test: 0.76, acc: 0.66, acc_test: 0.62\n",
      "Epoch: 377 Loss: 0.53, Loss_test: 0.73, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 378 Loss: 0.48, Loss_test: 0.74, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 379 Loss: 0.51, Loss_test: 0.74, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.56, Loss_test: 0.77, acc: 0.71, acc_test: 0.61\n",
      "Epoch: 381 Loss: 0.53, Loss_test: 0.81, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 382 Loss: 0.44, Loss_test: 0.81, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 383 Loss: 0.56, Loss_test: 0.78, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 384 Loss: 0.53, Loss_test: 0.74, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 385 Loss: 0.55, Loss_test: 0.72, acc: 0.67, acc_test: 0.57\n",
      "Epoch: 386 Loss: 0.48, Loss_test: 0.71, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 387 Loss: 0.53, Loss_test: 0.71, acc: 0.69, acc_test: 0.55\n",
      "Epoch: 388 Loss: 0.48, Loss_test: 0.71, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 389 Loss: 0.50, Loss_test: 0.72, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.50, Loss_test: 0.70, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 391 Loss: 0.48, Loss_test: 0.69, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 392 Loss: 0.43, Loss_test: 0.69, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 393 Loss: 0.51, Loss_test: 0.69, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 394 Loss: 0.44, Loss_test: 0.69, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 395 Loss: 0.48, Loss_test: 0.69, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 396 Loss: 0.43, Loss_test: 0.71, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 397 Loss: 0.51, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 398 Loss: 0.55, Loss_test: 0.70, acc: 0.68, acc_test: 0.57\n",
      "Epoch: 399 Loss: 0.44, Loss_test: 0.73, acc: 0.78, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.48, Loss_test: 0.77, acc: 0.74, acc_test: 0.52\n",
      "Epoch: 401 Loss: 0.52, Loss_test: 0.74, acc: 0.70, acc_test: 0.56\n",
      "Epoch: 402 Loss: 0.59, Loss_test: 0.71, acc: 0.64, acc_test: 0.58\n",
      "Epoch: 403 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 404 Loss: 0.50, Loss_test: 0.71, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 405 Loss: 0.50, Loss_test: 0.72, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 406 Loss: 0.47, Loss_test: 0.75, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 407 Loss: 0.50, Loss_test: 0.73, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 408 Loss: 0.49, Loss_test: 0.72, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 409 Loss: 0.50, Loss_test: 0.73, acc: 0.73, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.61, Loss_test: 0.72, acc: 0.67, acc_test: 0.62\n",
      "Epoch: 411 Loss: 0.50, Loss_test: 0.72, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 412 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.53\n",
      "Epoch: 413 Loss: 0.51, Loss_test: 0.72, acc: 0.80, acc_test: 0.53\n",
      "Epoch: 414 Loss: 0.44, Loss_test: 0.72, acc: 0.81, acc_test: 0.54\n",
      "Epoch: 415 Loss: 0.50, Loss_test: 0.71, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 416 Loss: 0.42, Loss_test: 0.71, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 417 Loss: 0.51, Loss_test: 0.71, acc: 0.71, acc_test: 0.60\n",
      "Epoch: 418 Loss: 0.49, Loss_test: 0.70, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 419 Loss: 0.52, Loss_test: 0.70, acc: 0.74, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.47, Loss_test: 0.70, acc: 0.76, acc_test: 0.54\n",
      "Epoch: 421 Loss: 0.54, Loss_test: 0.71, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 422 Loss: 0.51, Loss_test: 0.70, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 423 Loss: 0.49, Loss_test: 0.72, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 424 Loss: 0.53, Loss_test: 0.74, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 425 Loss: 0.50, Loss_test: 0.73, acc: 0.66, acc_test: 0.59\n",
      "Epoch: 426 Loss: 0.57, Loss_test: 0.70, acc: 0.68, acc_test: 0.58\n",
      "Epoch: 427 Loss: 0.51, Loss_test: 0.72, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 428 Loss: 0.51, Loss_test: 0.72, acc: 0.76, acc_test: 0.56\n",
      "Epoch: 429 Loss: 0.51, Loss_test: 0.70, acc: 0.73, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.51, Loss_test: 0.70, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 431 Loss: 0.53, Loss_test: 0.72, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 432 Loss: 0.56, Loss_test: 0.70, acc: 0.67, acc_test: 0.60\n",
      "Epoch: 433 Loss: 0.42, Loss_test: 0.69, acc: 0.80, acc_test: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 434 Loss: 0.55, Loss_test: 0.70, acc: 0.70, acc_test: 0.55\n",
      "Epoch: 435 Loss: 0.50, Loss_test: 0.70, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 436 Loss: 0.53, Loss_test: 0.69, acc: 0.67, acc_test: 0.56\n",
      "Epoch: 437 Loss: 0.50, Loss_test: 0.68, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 438 Loss: 0.42, Loss_test: 0.69, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 439 Loss: 0.48, Loss_test: 0.70, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.47, Loss_test: 0.70, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 441 Loss: 0.51, Loss_test: 0.69, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 442 Loss: 0.49, Loss_test: 0.69, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 443 Loss: 0.52, Loss_test: 0.69, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 444 Loss: 0.53, Loss_test: 0.69, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 445 Loss: 0.42, Loss_test: 0.70, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 446 Loss: 0.48, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 447 Loss: 0.49, Loss_test: 0.72, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 448 Loss: 0.55, Loss_test: 0.70, acc: 0.67, acc_test: 0.59\n",
      "Epoch: 449 Loss: 0.48, Loss_test: 0.69, acc: 0.74, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.50, Loss_test: 0.72, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 451 Loss: 0.46, Loss_test: 0.75, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 452 Loss: 0.46, Loss_test: 0.74, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 453 Loss: 0.47, Loss_test: 0.72, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 454 Loss: 0.44, Loss_test: 0.70, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 455 Loss: 0.49, Loss_test: 0.71, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 456 Loss: 0.49, Loss_test: 0.72, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 457 Loss: 0.49, Loss_test: 0.73, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 458 Loss: 0.49, Loss_test: 0.72, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 459 Loss: 0.56, Loss_test: 0.70, acc: 0.71, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.49, Loss_test: 0.71, acc: 0.78, acc_test: 0.60\n",
      "Epoch: 461 Loss: 0.54, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 462 Loss: 0.49, Loss_test: 0.75, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 463 Loss: 0.46, Loss_test: 0.76, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 464 Loss: 0.52, Loss_test: 0.75, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 465 Loss: 0.47, Loss_test: 0.76, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 466 Loss: 0.52, Loss_test: 0.75, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 467 Loss: 0.44, Loss_test: 0.72, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 468 Loss: 0.48, Loss_test: 0.71, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 469 Loss: 0.57, Loss_test: 0.71, acc: 0.70, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.50, Loss_test: 0.73, acc: 0.76, acc_test: 0.56\n",
      "Epoch: 471 Loss: 0.40, Loss_test: 0.75, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 472 Loss: 0.49, Loss_test: 0.74, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 473 Loss: 0.43, Loss_test: 0.73, acc: 0.79, acc_test: 0.55\n",
      "Epoch: 474 Loss: 0.48, Loss_test: 0.72, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 475 Loss: 0.50, Loss_test: 0.73, acc: 0.74, acc_test: 0.56\n",
      "Epoch: 476 Loss: 0.47, Loss_test: 0.74, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 477 Loss: 0.49, Loss_test: 0.74, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 478 Loss: 0.53, Loss_test: 0.72, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 479 Loss: 0.49, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.55, Loss_test: 0.74, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 481 Loss: 0.56, Loss_test: 0.73, acc: 0.70, acc_test: 0.60\n",
      "Epoch: 482 Loss: 0.47, Loss_test: 0.72, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 483 Loss: 0.40, Loss_test: 0.72, acc: 0.85, acc_test: 0.58\n",
      "Epoch: 484 Loss: 0.52, Loss_test: 0.72, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 485 Loss: 0.46, Loss_test: 0.72, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 486 Loss: 0.51, Loss_test: 0.72, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 487 Loss: 0.52, Loss_test: 0.73, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 488 Loss: 0.46, Loss_test: 0.75, acc: 0.79, acc_test: 0.55\n",
      "Epoch: 489 Loss: 0.48, Loss_test: 0.73, acc: 0.78, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.49, Loss_test: 0.72, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 491 Loss: 0.49, Loss_test: 0.74, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 492 Loss: 0.50, Loss_test: 0.73, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 493 Loss: 0.54, Loss_test: 0.71, acc: 0.65, acc_test: 0.57\n",
      "Epoch: 494 Loss: 0.43, Loss_test: 0.71, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 495 Loss: 0.46, Loss_test: 0.71, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 496 Loss: 0.54, Loss_test: 0.70, acc: 0.68, acc_test: 0.59\n",
      "Epoch: 497 Loss: 0.42, Loss_test: 0.70, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 498 Loss: 0.46, Loss_test: 0.71, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 499 Loss: 0.48, Loss_test: 0.72, acc: 0.79, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.51, Loss_test: 0.73, acc: 0.71, acc_test: 0.60\n",
      "Epoch: 501 Loss: 0.54, Loss_test: 0.73, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 502 Loss: 0.41, Loss_test: 0.75, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 503 Loss: 0.45, Loss_test: 0.76, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 504 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 505 Loss: 0.48, Loss_test: 0.75, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 506 Loss: 0.53, Loss_test: 0.75, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 507 Loss: 0.48, Loss_test: 0.75, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 508 Loss: 0.54, Loss_test: 0.75, acc: 0.71, acc_test: 0.56\n",
      "Epoch: 509 Loss: 0.50, Loss_test: 0.75, acc: 0.73, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.42, Loss_test: 0.73, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 511 Loss: 0.52, Loss_test: 0.74, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 512 Loss: 0.53, Loss_test: 0.77, acc: 0.74, acc_test: 0.56\n",
      "Epoch: 513 Loss: 0.52, Loss_test: 0.74, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 514 Loss: 0.53, Loss_test: 0.70, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 515 Loss: 0.51, Loss_test: 0.70, acc: 0.72, acc_test: 0.60\n",
      "Epoch: 516 Loss: 0.47, Loss_test: 0.69, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 517 Loss: 0.49, Loss_test: 0.68, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 518 Loss: 0.46, Loss_test: 0.68, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 519 Loss: 0.47, Loss_test: 0.68, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.47, Loss_test: 0.68, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 521 Loss: 0.45, Loss_test: 0.69, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 522 Loss: 0.48, Loss_test: 0.70, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 523 Loss: 0.49, Loss_test: 0.71, acc: 0.74, acc_test: 0.56\n",
      "Epoch: 524 Loss: 0.52, Loss_test: 0.71, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 525 Loss: 0.44, Loss_test: 0.71, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 526 Loss: 0.47, Loss_test: 0.72, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 527 Loss: 0.51, Loss_test: 0.73, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 528 Loss: 0.47, Loss_test: 0.73, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 529 Loss: 0.46, Loss_test: 0.73, acc: 0.80, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.46, Loss_test: 0.73, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 531 Loss: 0.47, Loss_test: 0.73, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 532 Loss: 0.41, Loss_test: 0.73, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 533 Loss: 0.49, Loss_test: 0.73, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 534 Loss: 0.47, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 535 Loss: 0.53, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 536 Loss: 0.48, Loss_test: 0.72, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 537 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 538 Loss: 0.41, Loss_test: 0.71, acc: 0.82, acc_test: 0.59\n",
      "Epoch: 539 Loss: 0.49, Loss_test: 0.71, acc: 0.73, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.53, Loss_test: 0.71, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 541 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 542 Loss: 0.45, Loss_test: 0.71, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 543 Loss: 0.46, Loss_test: 0.71, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 544 Loss: 0.47, Loss_test: 0.71, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 545 Loss: 0.40, Loss_test: 0.71, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 546 Loss: 0.48, Loss_test: 0.71, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 547 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 548 Loss: 0.49, Loss_test: 0.72, acc: 0.71, acc_test: 0.58\n",
      "Epoch: 549 Loss: 0.48, Loss_test: 0.74, acc: 0.74, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.53, Loss_test: 0.73, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 551 Loss: 0.46, Loss_test: 0.72, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 552 Loss: 0.49, Loss_test: 0.71, acc: 0.72, acc_test: 0.59\n",
      "Epoch: 553 Loss: 0.47, Loss_test: 0.71, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 554 Loss: 0.49, Loss_test: 0.73, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 555 Loss: 0.51, Loss_test: 0.72, acc: 0.77, acc_test: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 556 Loss: 0.51, Loss_test: 0.70, acc: 0.69, acc_test: 0.61\n",
      "Epoch: 557 Loss: 0.48, Loss_test: 0.70, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 558 Loss: 0.51, Loss_test: 0.70, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 559 Loss: 0.50, Loss_test: 0.70, acc: 0.72, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.48, Loss_test: 0.70, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 561 Loss: 0.54, Loss_test: 0.70, acc: 0.68, acc_test: 0.59\n",
      "Epoch: 562 Loss: 0.51, Loss_test: 0.70, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 563 Loss: 0.43, Loss_test: 0.70, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 564 Loss: 0.46, Loss_test: 0.70, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 565 Loss: 0.47, Loss_test: 0.70, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 566 Loss: 0.42, Loss_test: 0.70, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 567 Loss: 0.50, Loss_test: 0.70, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 568 Loss: 0.48, Loss_test: 0.71, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 569 Loss: 0.42, Loss_test: 0.71, acc: 0.84, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.55, Loss_test: 0.71, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 571 Loss: 0.41, Loss_test: 0.72, acc: 0.81, acc_test: 0.57\n",
      "Epoch: 572 Loss: 0.51, Loss_test: 0.72, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 573 Loss: 0.45, Loss_test: 0.72, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 574 Loss: 0.50, Loss_test: 0.75, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 575 Loss: 0.44, Loss_test: 0.77, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 576 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 577 Loss: 0.52, Loss_test: 0.75, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 578 Loss: 0.43, Loss_test: 0.74, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 579 Loss: 0.45, Loss_test: 0.74, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.55, Loss_test: 0.75, acc: 0.66, acc_test: 0.61\n",
      "Epoch: 581 Loss: 0.54, Loss_test: 0.75, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 582 Loss: 0.49, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 583 Loss: 0.41, Loss_test: 0.73, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 584 Loss: 0.49, Loss_test: 0.77, acc: 0.77, acc_test: 0.55\n",
      "Epoch: 585 Loss: 0.48, Loss_test: 0.81, acc: 0.77, acc_test: 0.54\n",
      "Epoch: 586 Loss: 0.52, Loss_test: 0.79, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 587 Loss: 0.52, Loss_test: 0.73, acc: 0.71, acc_test: 0.59\n",
      "Epoch: 588 Loss: 0.47, Loss_test: 0.72, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 589 Loss: 0.56, Loss_test: 0.71, acc: 0.67, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.46, Loss_test: 0.71, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 591 Loss: 0.44, Loss_test: 0.71, acc: 0.84, acc_test: 0.56\n",
      "Epoch: 592 Loss: 0.54, Loss_test: 0.70, acc: 0.73, acc_test: 0.55\n",
      "Epoch: 593 Loss: 0.44, Loss_test: 0.70, acc: 0.78, acc_test: 0.54\n",
      "Epoch: 594 Loss: 0.49, Loss_test: 0.72, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 595 Loss: 0.46, Loss_test: 0.75, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 596 Loss: 0.44, Loss_test: 0.77, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 597 Loss: 0.47, Loss_test: 0.76, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 598 Loss: 0.45, Loss_test: 0.75, acc: 0.73, acc_test: 0.56\n",
      "Epoch: 599 Loss: 0.46, Loss_test: 0.75, acc: 0.79, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.45, Loss_test: 0.74, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 601 Loss: 0.45, Loss_test: 0.73, acc: 0.76, acc_test: 0.55\n",
      "Epoch: 602 Loss: 0.47, Loss_test: 0.73, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 603 Loss: 0.42, Loss_test: 0.74, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 604 Loss: 0.52, Loss_test: 0.75, acc: 0.75, acc_test: 0.56\n",
      "Epoch: 605 Loss: 0.46, Loss_test: 0.76, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 606 Loss: 0.39, Loss_test: 0.77, acc: 0.84, acc_test: 0.54\n",
      "Epoch: 607 Loss: 0.42, Loss_test: 0.75, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 608 Loss: 0.41, Loss_test: 0.75, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 609 Loss: 0.54, Loss_test: 0.74, acc: 0.73, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.40, Loss_test: 0.75, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 611 Loss: 0.47, Loss_test: 0.74, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 612 Loss: 0.41, Loss_test: 0.73, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 613 Loss: 0.41, Loss_test: 0.73, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 614 Loss: 0.37, Loss_test: 0.77, acc: 0.84, acc_test: 0.56\n",
      "Epoch: 615 Loss: 0.46, Loss_test: 0.81, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 616 Loss: 0.42, Loss_test: 0.80, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 617 Loss: 0.52, Loss_test: 0.77, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 618 Loss: 0.45, Loss_test: 0.76, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 619 Loss: 0.51, Loss_test: 0.77, acc: 0.71, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.50, Loss_test: 0.75, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 621 Loss: 0.45, Loss_test: 0.73, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 622 Loss: 0.46, Loss_test: 0.73, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 623 Loss: 0.44, Loss_test: 0.73, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 624 Loss: 0.45, Loss_test: 0.74, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 625 Loss: 0.48, Loss_test: 0.74, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 626 Loss: 0.53, Loss_test: 0.74, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 627 Loss: 0.49, Loss_test: 0.74, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 628 Loss: 0.44, Loss_test: 0.75, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 629 Loss: 0.48, Loss_test: 0.76, acc: 0.78, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.48, Loss_test: 0.76, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 631 Loss: 0.35, Loss_test: 0.75, acc: 0.86, acc_test: 0.54\n",
      "Epoch: 632 Loss: 0.47, Loss_test: 0.75, acc: 0.76, acc_test: 0.56\n",
      "Epoch: 633 Loss: 0.49, Loss_test: 0.77, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 634 Loss: 0.45, Loss_test: 0.79, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 635 Loss: 0.48, Loss_test: 0.77, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 636 Loss: 0.50, Loss_test: 0.77, acc: 0.76, acc_test: 0.54\n",
      "Epoch: 637 Loss: 0.50, Loss_test: 0.77, acc: 0.74, acc_test: 0.55\n",
      "Epoch: 638 Loss: 0.50, Loss_test: 0.86, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 639 Loss: 0.47, Loss_test: 0.92, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.45, Loss_test: 0.86, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 641 Loss: 0.51, Loss_test: 0.76, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 642 Loss: 0.55, Loss_test: 0.74, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 643 Loss: 0.45, Loss_test: 0.73, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 644 Loss: 0.45, Loss_test: 0.72, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 645 Loss: 0.43, Loss_test: 0.72, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 646 Loss: 0.41, Loss_test: 0.72, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 647 Loss: 0.57, Loss_test: 0.71, acc: 0.64, acc_test: 0.62\n",
      "Epoch: 648 Loss: 0.40, Loss_test: 0.74, acc: 0.84, acc_test: 0.55\n",
      "Epoch: 649 Loss: 0.46, Loss_test: 0.79, acc: 0.79, acc_test: 0.54\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.54, Loss_test: 0.77, acc: 0.75, acc_test: 0.55\n",
      "Epoch: 651 Loss: 0.45, Loss_test: 0.73, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 652 Loss: 0.40, Loss_test: 0.72, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 653 Loss: 0.56, Loss_test: 0.72, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 654 Loss: 0.46, Loss_test: 0.73, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 655 Loss: 0.47, Loss_test: 0.74, acc: 0.75, acc_test: 0.60\n",
      "Epoch: 656 Loss: 0.46, Loss_test: 0.75, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 657 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 658 Loss: 0.44, Loss_test: 0.80, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 659 Loss: 0.44, Loss_test: 0.83, acc: 0.78, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.47, Loss_test: 0.79, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 661 Loss: 0.50, Loss_test: 0.78, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 662 Loss: 0.38, Loss_test: 0.79, acc: 0.82, acc_test: 0.56\n",
      "Epoch: 663 Loss: 0.46, Loss_test: 0.78, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 664 Loss: 0.47, Loss_test: 0.78, acc: 0.71, acc_test: 0.62\n",
      "Epoch: 665 Loss: 0.47, Loss_test: 0.78, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 666 Loss: 0.59, Loss_test: 0.76, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 667 Loss: 0.48, Loss_test: 0.75, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 668 Loss: 0.43, Loss_test: 0.75, acc: 0.80, acc_test: 0.55\n",
      "Epoch: 669 Loss: 0.44, Loss_test: 0.74, acc: 0.78, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.43, Loss_test: 0.72, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 671 Loss: 0.48, Loss_test: 0.71, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 672 Loss: 0.40, Loss_test: 0.71, acc: 0.84, acc_test: 0.60\n",
      "Epoch: 673 Loss: 0.51, Loss_test: 0.70, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 674 Loss: 0.42, Loss_test: 0.69, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 675 Loss: 0.46, Loss_test: 0.69, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 676 Loss: 0.47, Loss_test: 0.69, acc: 0.73, acc_test: 0.62\n",
      "Epoch: 677 Loss: 0.46, Loss_test: 0.72, acc: 0.74, acc_test: 0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 678 Loss: 0.54, Loss_test: 0.78, acc: 0.71, acc_test: 0.57\n",
      "Epoch: 679 Loss: 0.45, Loss_test: 0.79, acc: 0.75, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.46, Loss_test: 0.78, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 681 Loss: 0.44, Loss_test: 0.73, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 682 Loss: 0.39, Loss_test: 0.73, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 683 Loss: 0.44, Loss_test: 0.74, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 684 Loss: 0.48, Loss_test: 0.80, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 685 Loss: 0.47, Loss_test: 0.80, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 686 Loss: 0.44, Loss_test: 0.74, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 687 Loss: 0.41, Loss_test: 0.73, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 688 Loss: 0.49, Loss_test: 0.72, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 689 Loss: 0.52, Loss_test: 0.72, acc: 0.75, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.44, Loss_test: 0.72, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 691 Loss: 0.53, Loss_test: 0.72, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 692 Loss: 0.51, Loss_test: 0.73, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 693 Loss: 0.43, Loss_test: 0.73, acc: 0.84, acc_test: 0.57\n",
      "Epoch: 694 Loss: 0.50, Loss_test: 0.74, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 695 Loss: 0.40, Loss_test: 0.76, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 696 Loss: 0.48, Loss_test: 0.74, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 697 Loss: 0.45, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 698 Loss: 0.45, Loss_test: 0.72, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 699 Loss: 0.49, Loss_test: 0.72, acc: 0.75, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.39, Loss_test: 0.72, acc: 0.82, acc_test: 0.58\n",
      "Epoch: 701 Loss: 0.48, Loss_test: 0.72, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 702 Loss: 0.47, Loss_test: 0.72, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 703 Loss: 0.49, Loss_test: 0.71, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 704 Loss: 0.51, Loss_test: 0.72, acc: 0.73, acc_test: 0.61\n",
      "Epoch: 705 Loss: 0.32, Loss_test: 0.78, acc: 0.85, acc_test: 0.58\n",
      "Epoch: 706 Loss: 0.49, Loss_test: 0.81, acc: 0.72, acc_test: 0.57\n",
      "Epoch: 707 Loss: 0.44, Loss_test: 0.75, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 708 Loss: 0.49, Loss_test: 0.71, acc: 0.75, acc_test: 0.61\n",
      "Epoch: 709 Loss: 0.45, Loss_test: 0.72, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.46, Loss_test: 0.72, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 711 Loss: 0.44, Loss_test: 0.72, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 712 Loss: 0.46, Loss_test: 0.72, acc: 0.76, acc_test: 0.63\n",
      "Epoch: 713 Loss: 0.46, Loss_test: 0.72, acc: 0.76, acc_test: 0.63\n",
      "Epoch: 714 Loss: 0.55, Loss_test: 0.71, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 715 Loss: 0.48, Loss_test: 0.71, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 716 Loss: 0.60, Loss_test: 0.70, acc: 0.67, acc_test: 0.61\n",
      "Epoch: 717 Loss: 0.42, Loss_test: 0.70, acc: 0.86, acc_test: 0.58\n",
      "Epoch: 718 Loss: 0.45, Loss_test: 0.70, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 719 Loss: 0.47, Loss_test: 0.70, acc: 0.73, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.43, Loss_test: 0.70, acc: 0.84, acc_test: 0.54\n",
      "Epoch: 721 Loss: 0.48, Loss_test: 0.71, acc: 0.79, acc_test: 0.56\n",
      "Epoch: 722 Loss: 0.47, Loss_test: 0.72, acc: 0.77, acc_test: 0.55\n",
      "Epoch: 723 Loss: 0.38, Loss_test: 0.73, acc: 0.81, acc_test: 0.56\n",
      "Epoch: 724 Loss: 0.41, Loss_test: 0.72, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 725 Loss: 0.43, Loss_test: 0.71, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 726 Loss: 0.49, Loss_test: 0.71, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 727 Loss: 0.42, Loss_test: 0.71, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 728 Loss: 0.44, Loss_test: 0.73, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 729 Loss: 0.45, Loss_test: 0.74, acc: 0.78, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.48, Loss_test: 0.73, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 731 Loss: 0.47, Loss_test: 0.71, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 732 Loss: 0.42, Loss_test: 0.71, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 733 Loss: 0.45, Loss_test: 0.72, acc: 0.77, acc_test: 0.56\n",
      "Epoch: 734 Loss: 0.43, Loss_test: 0.72, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 735 Loss: 0.46, Loss_test: 0.74, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 736 Loss: 0.47, Loss_test: 0.73, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 737 Loss: 0.43, Loss_test: 0.73, acc: 0.79, acc_test: 0.57\n",
      "Epoch: 738 Loss: 0.45, Loss_test: 0.75, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 739 Loss: 0.40, Loss_test: 0.73, acc: 0.78, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.50, Loss_test: 0.70, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 741 Loss: 0.49, Loss_test: 0.69, acc: 0.73, acc_test: 0.58\n",
      "Epoch: 742 Loss: 0.37, Loss_test: 0.69, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 743 Loss: 0.50, Loss_test: 0.68, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 744 Loss: 0.48, Loss_test: 0.69, acc: 0.73, acc_test: 0.57\n",
      "Epoch: 745 Loss: 0.40, Loss_test: 0.68, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 746 Loss: 0.46, Loss_test: 0.68, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 747 Loss: 0.47, Loss_test: 0.68, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 748 Loss: 0.46, Loss_test: 0.68, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 749 Loss: 0.41, Loss_test: 0.70, acc: 0.77, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.56, Loss_test: 0.70, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 751 Loss: 0.47, Loss_test: 0.69, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 752 Loss: 0.50, Loss_test: 0.71, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 753 Loss: 0.44, Loss_test: 0.73, acc: 0.77, acc_test: 0.55\n",
      "Epoch: 754 Loss: 0.47, Loss_test: 0.71, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 755 Loss: 0.44, Loss_test: 0.74, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 756 Loss: 0.49, Loss_test: 0.77, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 757 Loss: 0.48, Loss_test: 0.74, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 758 Loss: 0.44, Loss_test: 0.71, acc: 0.80, acc_test: 0.58\n",
      "Epoch: 759 Loss: 0.49, Loss_test: 0.76, acc: 0.80, acc_test: 0.55\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.46, Loss_test: 0.79, acc: 0.74, acc_test: 0.52\n",
      "Epoch: 761 Loss: 0.54, Loss_test: 0.74, acc: 0.71, acc_test: 0.55\n",
      "Epoch: 762 Loss: 0.43, Loss_test: 0.70, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 763 Loss: 0.48, Loss_test: 0.71, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 764 Loss: 0.47, Loss_test: 0.71, acc: 0.74, acc_test: 0.60\n",
      "Epoch: 765 Loss: 0.42, Loss_test: 0.69, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 766 Loss: 0.45, Loss_test: 0.70, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 767 Loss: 0.46, Loss_test: 0.71, acc: 0.78, acc_test: 0.58\n",
      "Epoch: 768 Loss: 0.51, Loss_test: 0.73, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 769 Loss: 0.46, Loss_test: 0.72, acc: 0.79, acc_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.44, Loss_test: 0.70, acc: 0.78, acc_test: 0.56\n",
      "Epoch: 771 Loss: 0.43, Loss_test: 0.71, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 772 Loss: 0.50, Loss_test: 0.74, acc: 0.74, acc_test: 0.59\n",
      "Epoch: 773 Loss: 0.55, Loss_test: 0.79, acc: 0.69, acc_test: 0.58\n",
      "Epoch: 774 Loss: 0.52, Loss_test: 0.78, acc: 0.70, acc_test: 0.59\n",
      "Epoch: 775 Loss: 0.48, Loss_test: 0.75, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 776 Loss: 0.47, Loss_test: 0.72, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 777 Loss: 0.46, Loss_test: 0.70, acc: 0.75, acc_test: 0.57\n",
      "Epoch: 778 Loss: 0.39, Loss_test: 0.71, acc: 0.84, acc_test: 0.56\n",
      "Epoch: 779 Loss: 0.43, Loss_test: 0.71, acc: 0.80, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.49, Loss_test: 0.71, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 781 Loss: 0.39, Loss_test: 0.70, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 782 Loss: 0.36, Loss_test: 0.70, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 783 Loss: 0.44, Loss_test: 0.70, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 784 Loss: 0.43, Loss_test: 0.70, acc: 0.83, acc_test: 0.60\n",
      "Epoch: 785 Loss: 0.44, Loss_test: 0.71, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 786 Loss: 0.44, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 787 Loss: 0.37, Loss_test: 0.72, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 788 Loss: 0.49, Loss_test: 0.72, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 789 Loss: 0.47, Loss_test: 0.73, acc: 0.80, acc_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.39, Loss_test: 0.81, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 791 Loss: 0.46, Loss_test: 0.84, acc: 0.79, acc_test: 0.61\n",
      "Epoch: 792 Loss: 0.46, Loss_test: 0.75, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 793 Loss: 0.47, Loss_test: 0.71, acc: 0.73, acc_test: 0.59\n",
      "Epoch: 794 Loss: 0.50, Loss_test: 0.75, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 795 Loss: 0.43, Loss_test: 0.74, acc: 0.78, acc_test: 0.57\n",
      "Epoch: 796 Loss: 0.49, Loss_test: 0.69, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 797 Loss: 0.45, Loss_test: 0.72, acc: 0.76, acc_test: 0.63\n",
      "Epoch: 798 Loss: 0.56, Loss_test: 0.76, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 799 Loss: 0.59, Loss_test: 0.71, acc: 0.66, acc_test: 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.45, Loss_test: 0.68, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 801 Loss: 0.58, Loss_test: 0.69, acc: 0.70, acc_test: 0.58\n",
      "Epoch: 802 Loss: 0.46, Loss_test: 0.70, acc: 0.75, acc_test: 0.58\n",
      "Epoch: 803 Loss: 0.50, Loss_test: 0.70, acc: 0.74, acc_test: 0.57\n",
      "Epoch: 804 Loss: 0.50, Loss_test: 0.68, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 805 Loss: 0.45, Loss_test: 0.68, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 806 Loss: 0.42, Loss_test: 0.70, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 807 Loss: 0.48, Loss_test: 0.72, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 808 Loss: 0.43, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 809 Loss: 0.47, Loss_test: 0.70, acc: 0.75, acc_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.42, Loss_test: 0.68, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 811 Loss: 0.41, Loss_test: 0.68, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 812 Loss: 0.52, Loss_test: 0.69, acc: 0.74, acc_test: 0.58\n",
      "Epoch: 813 Loss: 0.43, Loss_test: 0.68, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 814 Loss: 0.49, Loss_test: 0.69, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 815 Loss: 0.39, Loss_test: 0.71, acc: 0.86, acc_test: 0.61\n",
      "Epoch: 816 Loss: 0.40, Loss_test: 0.73, acc: 0.82, acc_test: 0.61\n",
      "Epoch: 817 Loss: 0.42, Loss_test: 0.73, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 818 Loss: 0.49, Loss_test: 0.72, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 819 Loss: 0.45, Loss_test: 0.72, acc: 0.76, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.42, Loss_test: 0.73, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 821 Loss: 0.43, Loss_test: 0.73, acc: 0.81, acc_test: 0.59\n",
      "Epoch: 822 Loss: 0.46, Loss_test: 0.73, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 823 Loss: 0.42, Loss_test: 0.74, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 824 Loss: 0.37, Loss_test: 0.74, acc: 0.85, acc_test: 0.59\n",
      "Epoch: 825 Loss: 0.49, Loss_test: 0.74, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 826 Loss: 0.42, Loss_test: 0.74, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 827 Loss: 0.48, Loss_test: 0.73, acc: 0.76, acc_test: 0.60\n",
      "Epoch: 828 Loss: 0.42, Loss_test: 0.73, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 829 Loss: 0.47, Loss_test: 0.73, acc: 0.77, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.48, Loss_test: 0.72, acc: 0.75, acc_test: 0.59\n",
      "Epoch: 831 Loss: 0.44, Loss_test: 0.73, acc: 0.79, acc_test: 0.59\n",
      "Epoch: 832 Loss: 0.47, Loss_test: 0.72, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 833 Loss: 0.45, Loss_test: 0.72, acc: 0.76, acc_test: 0.59\n",
      "Epoch: 834 Loss: 0.46, Loss_test: 0.73, acc: 0.79, acc_test: 0.58\n",
      "Epoch: 835 Loss: 0.41, Loss_test: 0.72, acc: 0.81, acc_test: 0.57\n",
      "Epoch: 836 Loss: 0.45, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 837 Loss: 0.45, Loss_test: 0.70, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 838 Loss: 0.41, Loss_test: 0.72, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 839 Loss: 0.44, Loss_test: 0.74, acc: 0.80, acc_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.44, Loss_test: 0.74, acc: 0.80, acc_test: 0.57\n",
      "Epoch: 841 Loss: 0.33, Loss_test: 0.72, acc: 0.89, acc_test: 0.58\n",
      "Epoch: 842 Loss: 0.42, Loss_test: 0.73, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 843 Loss: 0.44, Loss_test: 0.73, acc: 0.76, acc_test: 0.57\n",
      "Epoch: 844 Loss: 0.36, Loss_test: 0.75, acc: 0.83, acc_test: 0.55\n",
      "Epoch: 845 Loss: 0.42, Loss_test: 0.76, acc: 0.80, acc_test: 0.56\n",
      "Epoch: 846 Loss: 0.44, Loss_test: 0.74, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 847 Loss: 0.43, Loss_test: 0.74, acc: 0.84, acc_test: 0.58\n",
      "Epoch: 848 Loss: 0.44, Loss_test: 0.73, acc: 0.77, acc_test: 0.57\n",
      "Epoch: 849 Loss: 0.37, Loss_test: 0.72, acc: 0.85, acc_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.38, Loss_test: 0.73, acc: 0.86, acc_test: 0.60\n",
      "Epoch: 851 Loss: 0.42, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 852 Loss: 0.43, Loss_test: 0.73, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 853 Loss: 0.43, Loss_test: 0.74, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 854 Loss: 0.38, Loss_test: 0.76, acc: 0.82, acc_test: 0.57\n",
      "Epoch: 855 Loss: 0.40, Loss_test: 0.76, acc: 0.83, acc_test: 0.59\n",
      "Epoch: 856 Loss: 0.47, Loss_test: 0.78, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 857 Loss: 0.46, Loss_test: 0.90, acc: 0.81, acc_test: 0.58\n",
      "Epoch: 858 Loss: 0.41, Loss_test: 0.89, acc: 0.77, acc_test: 0.58\n",
      "Epoch: 859 Loss: 0.45, Loss_test: 0.77, acc: 0.77, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.40, Loss_test: 0.75, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 861 Loss: 0.44, Loss_test: 0.76, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 862 Loss: 0.50, Loss_test: 0.75, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 863 Loss: 0.46, Loss_test: 0.72, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 864 Loss: 0.48, Loss_test: 0.72, acc: 0.74, acc_test: 0.61\n",
      "Epoch: 865 Loss: 0.41, Loss_test: 0.71, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 866 Loss: 0.43, Loss_test: 0.71, acc: 0.79, acc_test: 0.62\n",
      "Epoch: 867 Loss: 0.48, Loss_test: 0.72, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 868 Loss: 0.49, Loss_test: 0.71, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 869 Loss: 0.34, Loss_test: 0.70, acc: 0.86, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.42, Loss_test: 0.70, acc: 0.80, acc_test: 0.61\n",
      "Epoch: 871 Loss: 0.43, Loss_test: 0.71, acc: 0.81, acc_test: 0.62\n",
      "Epoch: 872 Loss: 0.35, Loss_test: 0.74, acc: 0.84, acc_test: 0.59\n",
      "Epoch: 873 Loss: 0.44, Loss_test: 0.75, acc: 0.76, acc_test: 0.58\n",
      "Epoch: 874 Loss: 0.46, Loss_test: 0.74, acc: 0.73, acc_test: 0.60\n",
      "Epoch: 875 Loss: 0.38, Loss_test: 0.73, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 876 Loss: 0.41, Loss_test: 0.73, acc: 0.77, acc_test: 0.61\n",
      "Epoch: 877 Loss: 0.40, Loss_test: 0.74, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 878 Loss: 0.38, Loss_test: 0.75, acc: 0.84, acc_test: 0.61\n",
      "Epoch: 879 Loss: 0.45, Loss_test: 0.75, acc: 0.79, acc_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.41, Loss_test: 0.75, acc: 0.80, acc_test: 0.62\n",
      "Epoch: 881 Loss: 0.48, Loss_test: 0.75, acc: 0.77, acc_test: 0.60\n",
      "Epoch: 882 Loss: 0.47, Loss_test: 0.74, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 883 Loss: 0.46, Loss_test: 0.75, acc: 0.77, acc_test: 0.59\n",
      "Epoch: 884 Loss: 0.42, Loss_test: 0.75, acc: 0.78, acc_test: 0.59\n",
      "Epoch: 885 Loss: 0.40, Loss_test: 0.73, acc: 0.80, acc_test: 0.60\n",
      "Epoch: 886 Loss: 0.48, Loss_test: 0.71, acc: 0.72, acc_test: 0.62\n",
      "Epoch: 887 Loss: 0.46, Loss_test: 0.72, acc: 0.78, acc_test: 0.61\n",
      "Epoch: 888 Loss: 0.44, Loss_test: 0.72, acc: 0.80, acc_test: 0.59\n",
      "Epoch: 889 Loss: 0.42, Loss_test: 0.69, acc: 0.80, acc_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.46, Loss_test: 0.69, acc: 0.77, acc_test: 0.62\n",
      "Epoch: 891 Loss: 0.41, Loss_test: 0.69, acc: 0.81, acc_test: 0.60\n",
      "Epoch: 892 Loss: 0.54, Loss_test: 0.69, acc: 0.72, acc_test: 0.61\n",
      "Epoch: 893 Loss: 0.44, Loss_test: 0.68, acc: 0.78, acc_test: 0.62\n",
      "Epoch: 894 Loss: 0.47, Loss_test: 0.71, acc: 0.76, acc_test: 0.61\n",
      "Epoch: 895 Loss: 0.45, Loss_test: 0.73, acc: 0.79, acc_test: 0.60\n",
      "Epoch: 896 Loss: 0.45, Loss_test: 0.70, acc: 0.81, acc_test: 0.61\n",
      "Epoch: 897 Loss: 0.46, Loss_test: 0.69, acc: 0.77, acc_test: 0.66\n",
      "Epoch: 898 Loss: 0.43, Loss_test: 0.70, acc: 0.80, acc_test: 0.64\n",
      "Epoch: 899 Loss: 0.37, Loss_test: 0.71, acc: 0.84, acc_test: 0.65\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.42, Loss_test: 0.72, acc: 0.79, acc_test: 0.66\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = experiment.batch_iterator(None, baseline.train_data, baseline.dup_sets_train, \n",
    "                                                  bug_train_ids, batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    \n",
    "    num_batch = train_input_sample['title'].shape[0]\n",
    "    pos = np.full((1, num_batch), 1)\n",
    "    neg = np.full((1, num_batch), 0)\n",
    "    train_sim = np.concatenate([pos, neg], -1)[0]\n",
    "    \n",
    "    title_sample_a = np.concatenate([train_input_sample['title'], train_input_sample['title']], 0)\n",
    "    title_sample_b = np.concatenate([train_input_pos['title'], train_input_neg['title']], 0)\n",
    "    desc_sample_a = np.concatenate([train_input_sample['description'], train_input_sample['description']], 0)\n",
    "    desc_sample_b = np.concatenate([train_input_pos['description'], train_input_neg['description']], 0)\n",
    "    train_batch = [title_sample_a, desc_sample_a, title_sample_b, desc_sample_b]\n",
    "    \n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_test: {:.2f}\".format(epoch+1, h[0], h_validation[0], h[1], h_validation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_dwen_a')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-2]\n",
    "bug_feature_output_a = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'title_dwen_a:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'desc_dwen_a:0' shape=(?, 20) dtype=float32>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature1000epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(eclipse).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(bug_feature_output_a, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 Loss: 0.42, Loss_test: 0.72, acc: 0.79, acc_tets: 0.66, recall@25: 0.13\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, bug_feature_output_a, issues_by_buckets, bug_train_ids, 'dwen')\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, acc: {:.2f}, acc_tets: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h_validation[0],  h[1], h_validation[1], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2:15392,9779,94|43732:0.719656765460968,78340:0.7187950015068054,333429:0.7107122242450714,33440:0.7089104354381561,26120:0.7078737318515778,301829:0.707709014415741,301830:0.707709014415741,301831:0.707709014415741,158250:0.7062311172485352,59311:0.7031238079071045,79004:0.7030397355556488,94701:0.7029220759868622,163079:0.7018101215362549,162411:0.6995002329349518,42078:0.6971099376678467,66587:0.6963174045085907,116875:0.6959206163883209,208295:0.695848286151886,352380:0.6939741671085358,58797:0.6937785148620605,375762:0.6936006546020508,376605:0.6934903562068939,255684:0.6931604444980621,2370:0.692665308713913,139097:0.6920564472675323,141028:0.6915636360645294,80388:0.6909751296043396,133693:0.6901670694351196,227010:0.6896843910217285',\n",
       " '393232:393282,390667,383388|194945:0.6596272885799408,105557:0.6550727784633636,105558:0.6550727784633636,320153:0.632684051990509,29302:0.6326440572738647,72698:0.6321587562561035,152050:0.6311775147914886,104673:0.6299512982368469,356782:0.6290341317653656,377222:0.6267510056495667,43630:0.6267032325267792,26500:0.6247715950012207,148894:0.6240522563457489,71190:0.6239696443080902,102490:0.6220751106739044,288347:0.6217557787895203,174146:0.621358335018158,33497:0.6191469132900238,71189:0.6188512742519379,176359:0.6180747151374817,183223:0.6180645227432251,105814:0.6173504889011383,4409:0.6172608137130737,115648:0.6169734299182892,12964:0.6168102025985718,114640:0.6163086593151093,47489:0.6163032650947571,105441:0.6159688234329224,31612:0.6158117055892944',\n",
       " '30:205979,38236,99332,71263|248604:0.7102565169334412,52871:0.6950463652610779,91780:0.6941578388214111,130251:0.6936195492744446,91474:0.6849206686019897,158038:0.6829924285411835,46694:0.6793359518051147,51463:0.6783008277416229,166801:0.6771990060806274,58418:0.6769916117191315,331719:0.6766237914562225,178151:0.6762720942497253,229494:0.6753947734832764,18817:0.6735760867595673,26015:0.6732787489891052,396488:0.672887533903122,58571:0.672756165266037,83083:0.6726689636707306,22138:0.6715194284915924,178723:0.6710538566112518,60022:0.6707355082035065,27121:0.6697514653205872,94590:0.668799877166748,6929:0.6687809228897095,248591:0.668595939874649,124207:0.667582094669342,260398:0.667304128408432,264689:0.6670444905757904,68167:0.6668301224708557',\n",
       " '131123:234849|38459:0.7814689725637436,21537:0.77547886967659,60496:0.7746271640062332,37483:0.7561910003423691,98284:0.7530525028705597,83280:0.752937451004982,260885:0.7516788393259048,274087:0.7512925416231155,212757:0.7506559044122696,44799:0.7506399750709534,101099:0.7497627437114716,144207:0.749330073595047,6487:0.7483325302600861,141649:0.748010903596878,111837:0.747882604598999,60803:0.7478646636009216,165088:0.7474713623523712,31178:0.7470066249370575,27288:0.7468627691268921,191738:0.7461123764514923,198591:0.7455562055110931,361679:0.7453697323799133,39162:0.7453481256961823,39163:0.7453481256961823,389659:0.7443784475326538,86020:0.7439969778060913,132450:0.7439775466918945,130938:0.7438436448574066,298228:0.7437459230422974',\n",
       " '393277:393864,400436|267365:0.7606444954872131,65472:0.7495747804641724,70666:0.747167319059372,101939:0.7425170838832855,164571:0.7422607839107513,73973:0.740941196680069,90417:0.7401205599308014,217197:0.7396030724048615,89568:0.739558756351471,240101:0.7394145429134369,256921:0.7388053238391876,40902:0.737738847732544,40903:0.737738847732544,42325:0.7373543381690979,266739:0.7368892729282379,255822:0.7365938723087311,77004:0.7360793352127075,159910:0.7354598045349121,77795:0.7343233227729797,290521:0.7338857650756836,149359:0.7315277457237244,221769:0.7305406630039215,60524:0.7304712533950806,262343:0.7298800349235535,21765:0.7298449575901031,96069:0.7293208539485931,41930:0.7283718585968018,168679:0.7281804382801056,100124:0.727794885635376',\n",
       " '131136:150817,149868,145937,65841,102812|45900:0.8136966526508331,173412:0.8023425191640854,36593:0.800232782959938,201851:0.7999252080917358,28786:0.7989975363016129,347031:0.7987267524003983,52043:0.7971288859844208,50404:0.7963358759880066,141649:0.7947684079408646,45742:0.7927540391683578,123910:0.7926840931177139,111506:0.7919300347566605,340695:0.7903480231761932,98192:0.7899501472711563,272300:0.7861245721578598,36669:0.785810649394989,24840:0.7844932675361633,32083:0.7839599549770355,77886:0.7835889607667923,188055:0.7832600474357605,7462:0.7831229120492935,283274:0.7830787748098373,125146:0.7828318774700165,164236:0.7826338708400726,53784:0.782195657491684,76133:0.7814384400844574,66402:0.7813651859760284,17934:0.781306728720665,98574:0.7811677306890488',\n",
       " '393282:393232,390667,383388|11402:0.7704531103372574,43734:0.768707811832428,116403:0.7650172859430313,104419:0.7591458708047867,262333:0.7586907744407654,197290:0.7585330754518509,8967:0.7579509019851685,111389:0.757721483707428,75596:0.7573572099208832,32447:0.75531005859375,71746:0.7541967183351517,37632:0.75203076004982,57585:0.7517083436250687,27821:0.7512660920619965,16121:0.7511707991361618,16123:0.7511707991361618,89565:0.7499129772186279,123610:0.7491189539432526,146630:0.7489269375801086,85108:0.7487238943576813,82831:0.7472966015338898,15138:0.7472405135631561,93617:0.7471058070659637,85511:0.7469014823436737,113381:0.7468374371528625,91395:0.7466239631175995,183606:0.7457132935523987,151943:0.7456172704696655,114949:0.745512455701828',\n",
       " '262212:292851,292845,291839|258248:0.7552826106548309,234366:0.7455237209796906,266669:0.7453100979328156,275317:0.7431114017963409,259113:0.7430600523948669,271713:0.7410333454608917,243396:0.7369889914989471,2002:0.7352742552757263,48014:0.7351221740245819,225365:0.7343763411045074,283119:0.7342610955238342,333937:0.7342368960380554,22131:0.7340639531612396,259013:0.7339185774326324,244001:0.7335450649261475,45203:0.7330516278743744,236196:0.7326149344444275,284953:0.7312040328979492,318705:0.7312019467353821,183608:0.7310835123062134,257988:0.7296366691589355,268863:0.7296202182769775,248533:0.7290559709072113,74309:0.7280891239643097,24554:0.727949321269989,247389:0.727848470211029,268177:0.7265623807907104,63416:0.7265156805515289,271117:0.7264091074466705',\n",
       " '131143:135599|122326:0.825410783290863,65110:0.8246601819992065,231829:0.8155916035175323,108063:0.8138218373060226,81510:0.8136016875505447,87189:0.8131417632102966,331606:0.811952993273735,42474:0.8103561699390411,24884:0.8100205063819885,147235:0.8096067011356354,119712:0.8091272413730621,108303:0.8089372366666794,23541:0.8084888905286789,260269:0.8073652237653732,288560:0.8072711676359177,107252:0.8072102665901184,22022:0.8063829690217972,194007:0.8056350350379944,366265:0.8056017905473709,144037:0.8050784170627594,215410:0.8047600537538528,64106:0.8042109459638596,45126:0.8035298138856888,167457:0.802914172410965,194141:0.8026200085878372,94035:0.8024166077375412,25875:0.8021587431430817,29791:0.8017486035823822,52080:0.8007052093744278',\n",
       " '71:256,228|47927:0.7456678748130798,212342:0.7430046200752258,296389:0.7307035028934479,205834:0.7297999858856201,45679:0.7294411361217499,199674:0.7279662191867828,230050:0.7252875864505768,326312:0.7234337627887726,60012:0.7222651541233063,233732:0.7212640047073364,97617:0.7202598750591278,174038:0.7197177708148956,117588:0.719519168138504,53682:0.7190732061862946,289204:0.7183232307434082,91463:0.7183093726634979,47884:0.7177364826202393,73706:0.7177204191684723,78469:0.7174515128135681,104492:0.7173052132129669,230310:0.7168882191181183,369417:0.7167759239673615,42092:0.7166755795478821,112274:0.7166540026664734,237077:0.7165125012397766,106907:0.7161636650562286,106908:0.7161636650562286,165711:0.7154996693134308,67263:0.7146902978420258',\n",
       " '85:247,10959|3462:0.7727373093366623,53336:0.7664436399936676,218395:0.7647269070148468,9529:0.7631924450397491,129853:0.7585374414920807,15160:0.7578560411930084,89981:0.7545197904109955,225154:0.7541470378637314,191396:0.7541211992502213,153100:0.7533279359340668,32551:0.7527972459793091,36068:0.7500466108322144,128949:0.7492799162864685,208331:0.7489378154277802,1606:0.7488831281661987,17462:0.7480053901672363,17376:0.7471833229064941,338177:0.7449351847171783,119259:0.744464635848999,1605:0.7417835891246796,175358:0.7410954833030701,1557:0.7402061522006989,68754:0.7392565608024597,391626:0.7391232252120972,383344:0.7390938401222229,50638:0.7388860881328583,45164:0.7388443052768707,183408:0.7388383150100708,159279:0.7386557757854462',\n",
       " '88:6099,14116|81705:0.7565707713365555,96795:0.7462535202503204,1544:0.7451280057430267,42862:0.74290531873703,80700:0.7426244616508484,131171:0.7418192327022552,3035:0.74074986577034,146120:0.7399480640888214,21852:0.7388216555118561,153573:0.73847696185112,375588:0.7381902933120728,123292:0.7344594597816467,399971:0.7332049310207367,16774:0.732018381357193,163129:0.7316778600215912,389676:0.7308142781257629,16905:0.7299734652042389,150559:0.7297608852386475,26711:0.7297305166721344,163464:0.7296798229217529,161632:0.7296750545501709,353815:0.7288202345371246,3274:0.728745847940445,57081:0.7284547686576843,40897:0.7280447781085968,108550:0.7270484566688538,31626:0.7266436517238617,4470:0.7263553440570831,87299:0.725384920835495',\n",
       " '131161:44438,103374,98057|73410:0.7279990613460541,1916:0.7211842834949493,204080:0.7163174450397491,21372:0.7160494923591614,38236:0.70921590924263,130707:0.7090064883232117,23089:0.708234578371048,48497:0.7075349986553192,7707:0.7067656815052032,45142:0.706486850976944,366247:0.7057200968265533,4677:0.7042600214481354,81858:0.703637957572937,160971:0.702500969171524,72423:0.700367271900177,130126:0.6984508335590363,66903:0.6980223953723907,73617:0.6971136331558228,22172:0.6966207921504974,128787:0.6960877478122711,241602:0.6959890425205231,170894:0.6953836977481842,85622:0.6942876279354095,122975:0.6942093372344971,311326:0.6940515637397766,70977:0.6940247118473053,128810:0.6939807236194611,136621:0.6929654479026794,219510:0.691454142332077',\n",
       " '262235:260098,263611|92226:0.7638705521821976,6609:0.7623168528079987,127259:0.7596789300441742,212148:0.7572862505912781,92738:0.7556486576795578,50396:0.7538921236991882,61841:0.7534415125846863,226294:0.75313800573349,371989:0.7520104795694351,28132:0.7503258883953094,266592:0.7498785257339478,167602:0.749713122844696,216028:0.7480412125587463,360266:0.7469376027584076,107475:0.7460643649101257,206523:0.7456423044204712,328398:0.7456388175487518,124264:0.7455923557281494,117637:0.7453696429729462,23060:0.7452864050865173,27844:0.7446888387203217,133569:0.7442502379417419,28432:0.744168221950531,127290:0.7439137101173401,219986:0.7434810698032379,83373:0.7431058883666992,362250:0.7412116825580597,375066:0.7411753535270691,44294:0.7396077215671539',\n",
       " '94:15392,2,9779|50910:0.7243644893169403,250567:0.7103540301322937,250568:0.7103540301322937,250570:0.7103540301322937,250571:0.7103540301322937,2281:0.7092889845371246,23064:0.7057303786277771,23071:0.7057303786277771,363746:0.7034156322479248,2345:0.7021747529506683,2276:0.7001720070838928,3970:0.6992534399032593,200364:0.6986228823661804,15429:0.6981646716594696,268889:0.6980058252811432,107436:0.6978442370891571,1585:0.6971059441566467,174836:0.6952808797359467,253960:0.6951278746128082,128818:0.6949925124645233,186546:0.6949799656867981,28385:0.6945877373218536,240977:0.6932511925697327,109799:0.6932202875614166,373975:0.6916944980621338,4922:0.6915838718414307,149074:0.6915293633937836,114377:0.6904144585132599,122290:0.6897666454315186',\n",
       " '131171:202560,96651,172374|131558:0.7794938534498215,80700:0.7735678404569626,81705:0.7634389698505402,130526:0.7590998262166977,319239:0.756892591714859,360871:0.7568125873804092,318855:0.7563035190105438,375588:0.7548705488443375,164571:0.7546514272689819,353815:0.7524925619363785,135208:0.7523825615644455,54235:0.7517663538455963,307096:0.7512461543083191,57935:0.7508595883846283,57940:0.7508595883846283,57941:0.7508595883846283,208417:0.750734269618988,14105:0.750656321644783,78078:0.7500907331705093,71987:0.7493421733379364,42325:0.7490124702453613,120731:0.7468693852424622,315036:0.7466248869895935,241112:0.746128261089325,293923:0.7459028363227844,359609:0.744777262210846,201750:0.7447592616081238,234126:0.7444283068180084,150148:0.7443165481090546',\n",
       " '393320:394988|268192:0.6875066459178925,33710:0.6866327822208405,283721:0.6864702999591827,31056:0.6849028170108795,220079:0.6839890778064728,26855:0.6804786920547485,26856:0.6804786920547485,14308:0.6771750450134277,155971:0.675805926322937,144833:0.6746814250946045,78120:0.6745485961437225,92372:0.6732150614261627,30708:0.6719569563865662,341937:0.6713806986808777,48497:0.6713002324104309,188958:0.6712060570716858,11001:0.6711706519126892,10949:0.6708594858646393,413011:0.6699036955833435,131237:0.6698479056358337,328541:0.6695527136325836,181176:0.6692869067192078,183960:0.6692869067192078,10947:0.6686558127403259,364584:0.6683284640312195,35203:0.6671494841575623,34632:0.6663698852062225,343304:0.6662309765815735,102728:0.6658885776996613',\n",
       " '131180:134058,132491,131646|30089:0.774847686290741,71829:0.7608557194471359,21502:0.7595227658748627,67652:0.7589950412511826,92092:0.7587522119283676,396390:0.7560131549835205,234109:0.7557980716228485,56293:0.7557242810726166,108335:0.7557083070278168,16313:0.755042091012001,76979:0.7546929121017456,222588:0.7524778991937637,5594:0.7524644136428833,34245:0.7521017789840698,206154:0.7516180574893951,86527:0.7500825375318527,70350:0.7498173117637634,121593:0.7471899092197418,140371:0.7466157376766205,64532:0.7459806501865387,17148:0.7457593083381653,110265:0.7455949485301971,118098:0.7450973093509674,359043:0.7450642883777618,99562:0.744983434677124,83918:0.7440961003303528,84770:0.744072437286377,90575:0.7439982295036316,86227:0.7430544793605804',\n",
       " '393332:405833,414629|22955:0.7592377662658691,46176:0.7581698000431061,23068:0.756916731595993,51403:0.7532846927642822,144099:0.7505530714988708,356199:0.7477914094924927,200241:0.7455190122127533,379996:0.7442532479763031,284260:0.7439047694206238,112330:0.7437884509563446,145141:0.7416052520275116,33836:0.7407853305339813,128648:0.7403623163700104,238017:0.7400546669960022,374926:0.7391435503959656,140477:0.739106148481369,380474:0.7390441000461578,148966:0.7385345101356506,133101:0.738288938999176,149649:0.7382431626319885,67219:0.7378373742103577,103383:0.7376644015312195,140728:0.737623393535614,49201:0.7375753223896027,90104:0.7373379766941071,69307:0.7367316782474518,39732:0.7363134324550629,157724:0.736183226108551,40899:0.7354398965835571',\n",
       " '262266:262871|178151:0.7093698382377625,45887:0.7009198069572449,45891:0.7009198069572449,76218:0.700438529253006,161205:0.6969691514968872,116260:0.6963155567646027,364680:0.6961511671543121,116339:0.6955099403858185,82483:0.6916010975837708,317313:0.6911126375198364,30397:0.6895922124385834,87063:0.689381867647171,50860:0.6889359056949615,105312:0.6878774762153625,12906:0.6875732243061066,65697:0.6875221729278564,339124:0.687018483877182,140137:0.6869037449359894,44781:0.6861298084259033,308779:0.6860981583595276,13221:0.6860130131244659,117152:0.6859428286552429,138486:0.6854729354381561,65754:0.6852291822433472,85501:0.6851598620414734,336830:0.6848724186420441,128818:0.6837049722671509,31450:0.6831141710281372,204657:0.6827242076396942']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 16995\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_dwen_1000_feature_1000epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_dwen_a (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_dwen_a (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_title (Embeddin (None, 20, 300)      581600700   title_dwen_a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_desc (Embedding (None, 20, 300)      581600700   desc_dwen_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           embedding_layer_title[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           embedding_layer_desc[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_dwen_a (Average) (None, 300)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,163,201,400\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,163,201,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bug_feature_output_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/bert/exported_rank_baseline_dwen_1000.txt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.11,\n",
       " '2 - recall_at_10': 0.12,\n",
       " '3 - recall_at_15': 0.12,\n",
       " '4 - recall_at_20': 0.13,\n",
       " '5 - recall_at_25': 0.13}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
