{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText pretrained in the dataset\n",
    "\n",
    "https://github.com/facebookresearch/fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.load_ids(DIR)\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(data, max_seq_length):\n",
    "    seq_lengths = [len(seq) for seq in data]\n",
    "    seq_lengths.append(6)\n",
    "    max_seq_length = min(max(seq_lengths), max_seq_length)\n",
    "    padded_data = np.zeros(shape=[len(data), max_seq_length])\n",
    "    for i, seq in enumerate(data):\n",
    "        seq = seq[:max_seq_length]\n",
    "        for j, token in enumerate(seq):\n",
    "            padded_data[i, j] = int(token)\n",
    "    return padded_data.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "def load_bugs(baseline):   \n",
    "    removed = []\n",
    "    baseline.corpus = []\n",
    "    baseline.sentence_dict = {}\n",
    "    baseline.bug_set = {}\n",
    "    title_padding, desc_padding = [], []\n",
    "    for bug_id in tqdm(baseline.bug_ids):\n",
    "        try:\n",
    "            bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "            title_padding.append(bug['title_word'])\n",
    "            desc_padding.append(bug['description_word'])\n",
    "            baseline.bug_set[bug_id] = bug\n",
    "            #break\n",
    "        except:\n",
    "            removed.append(bug_id)\n",
    "    \n",
    "    if len(removed) > 0:\n",
    "        for x in removed:\n",
    "            baseline.bug_ids.remove(x)\n",
    "        baseline.removed = removed\n",
    "        print(\"{} were removed. To see the list call self.removed\".format(len(removed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770c5d27f8d4dafb5cf7ba41c64d863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 13.7 s, sys: 3.88 s, total: 17.5 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_bugs(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus from bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_title = [baseline.bug_set[bug_id]['title'] for bug_id in baseline.bug_ids]\n",
    "lines_desc = [baseline.bug_set[bug_id]['description'] for bug_id in baseline.bug_ids]\n",
    "\n",
    "lines = []\n",
    "for title, desc in zip(lines_title, lines_desc):\n",
    "    lines.append(title)\n",
    "    lines.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722012"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/eclipse/corpus_fasttext.txt\n"
     ]
    }
   ],
   "source": [
    "EXPORT_PATH = os.path.join(DIR, 'corpus_fasttext.txt')\n",
    "print(EXPORT_PATH)\n",
    "with open(EXPORT_PATH, 'w') as f:\n",
    "    for row in lines:\n",
    "        f.write(\"{}\\n\".format(str(row).encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration to openoffice (recall@25=60)\n",
    "\n",
    "# model='cbow', \n",
    "# minCount=3,\n",
    "# neg=10,\n",
    "# wordNgrams=5,\n",
    "# epoch=25, \n",
    "# dim=300, \n",
    "# loss='hs', \n",
    "# thread=cpu, \n",
    "# verbose=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "cpu = os.cpu_count() - 1\n",
    "\n",
    "# Skipgram model :\n",
    "model = fasttext.train_unsupervised(EXPORT_PATH, \n",
    "                                    model='cbow', \n",
    "                                    minCount=3,\n",
    "                                    neg=10,\n",
    "                                    wordNgrams=5,\n",
    "                                    epoch=25, \n",
    "                                    dim=300, \n",
    "                                    loss='hs', \n",
    "                                    thread=cpu, \n",
    "                                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset vocabulary embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embed = {}\n",
    "for word in model.words:\n",
    "    vocab_embed[word] = model[word]\n",
    "    \n",
    "with open(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'), 'wb') as f:\n",
    "      pickle.dump(vocab_embed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words_selected = np.random.choice(model.words, 20)\n",
    "for word in words_selected:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vocab_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
