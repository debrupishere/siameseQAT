{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 200 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Log keras\n",
    "LOG_DIR='logs/training'\n",
    "# Checkpoint keras\n",
    "FILE_PATH = \"checkpoint_baseline_1000epoch_10steps_1024batch({})\".format(DOMAIN)\n",
    "# Save model\n",
    "SAVE_PATH = 'baseline_1000epoch_10steps_1024batch({})'.format(DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.load_ids(DIR)\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb7d4b3c96a4333bfcb315c9f0f0923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 37 s, sys: 2.07 s, total: 39 s\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline.load_preprocess()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0WZNngemNM8"
   },
   "source": [
    "## Geração de batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "### Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading the test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87b26e733714e08a5fef01d4cdb6236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading test data\n",
      "CPU times: user 475 ms, sys: 16.5 ms, total: 492 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17a13d21bb74d36b4a6da93e587bb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline.load_bugs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '3\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '633\\n',\n",
       " 'creation_ts': '2001-10-10 22:38:00 -0400',\n",
       " 'delta_ts': '2005-05-10 14:55:51 -0400',\n",
       " 'description': 'steps minimize all your windows go to any window and select the window menu pick any window notice that it only gets selected and not maximized this happens in country as well notes',\n",
       " 'description_word': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  287, 3292,  110,  664,  303,  482,    8,  251,   71,\n",
       "          17,  164,    6,   71,  216, 2089,  251,   71, 1020,   31,   24,\n",
       "         176,  892,  310,   17,   21, 3619,   25,  722,   12,   81,   53,\n",
       "         679, 1328]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 2521,\n",
       " 'priority': '3\\n',\n",
       " 'product': '55\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'title': 'selecting a window in the window menu does not maximize window gfitic',\n",
       " 'title_word': array([   0,    0,    0,    0,    0,    0,    0,    0, 1149,   11,   71,\n",
       "          12,    6,   71,  216,  138,   21, 3342,   71,    1]),\n",
       " 'version': '100\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.bug_set[2521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.6 ms, sys: 4.8 ms, total: 91.4 ms\n",
      "Wall time: 90.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 512\n",
    "batch_size_test = 1024\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "train_gen = baseline.siam_gen(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 200), (128, 1682), (128,))"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: subversive bug report id bxlft\n",
      "***Title***: subversive bug report id bxlft\n",
      "***Description***: version i svn client org eclipse team svn connector svnkit i svn svnkit http svnkit com r jvm properties java vendor apple inc osgi bundles default start level org osgi supports framework extension true sun management compiler hot spot organization cardinal eclipse p profile organization os name mac os x osgi ws carbon organization cardinal b osgi instance area file users home workspaces workspace mini rt user name home awt native double buffering true eclipse launcher applications eclipse cpp organization mac os eclipse org osgi framework language en user language en org osgi framework processor x osgi syspath applications eclipse cpp plugins sun boot library path system library frameworks java vm framework versions libraries osgi manifest cache applications eclipse cpp configuration org eclipse osgi manifests osgi compatibility bootdelegation true java version org osgi framework os name mac osx user timezone europe berlin sun arch data model java endorsed dirs system library frameworks java vm framework versions home lib endorsed organization sun jnu encoding mac roman person org osgi framework vendor person file separator java specification name java organization java class version user country us org eclipse equinox launcher splash location applications eclipse cpp plugins org eclipse platform v splash bmp java home system library frameworks java vm framework versions home osgi os macosx eclipse commands os macosx ws carbon arch x showsplash launcher applications eclipse cpp organization mac os eclipse name person launcher library applications eclipse cpp organization mac os plugins org eclipse equinox launcher carbon macosx rx v eclipse so startup applications eclipse cpp organization mac os plugins org eclipse equinox launcher rx v jar keyring users home eclipse keyring console log showlocation vm system library frameworks java vm framework java vm info mixed mode osgi splash location applications eclipse cpp plugins org eclipse platform v splash bmp os version osgi arch x path separator java vm version org osgi supports framework fragment true osgi framework shape jar osgi instance area default file users home documents workspace java awt printerjob apple awt cprinter job sun io unicode encoding unicode little org osgi framework version awt toolkit apple awt ctoolkit socks non proxy hosts local local ftp non proxy hosts local local osgi install area file applications eclipse cpp osgi framework file applications eclipse cpp plugins org eclipse osgi rx v jar user home users home org eclipse equinox simpleconfigurator config url file org eclipse equinox simpleconfigurator bundles info osgi bundlestore applications eclipse cpp configuration org eclipse osgi bundles osgi splash path platform base plugins org eclipse platform organization osgi nl en us organization organization java vendor url http www apple com org osgi framework os version eclipse p data area config dir p eclipse start time organization gopher proxy set false java runtime name java tm runtime environment standard edition java class path applications eclipse cpp organization mac os plugins org eclipse equinox launcher rx v jar osgi required java version org eclipse update reconcile false eclipse vm system library frameworks java vm framework java vm specification name java organization java vm specification version sun cpu endian little org eclipse swt internal carbon small fonts sun os patch level unknown java io tmpdir tmp java vendor url bug http bugreport apple com eclipse product org eclipse platform ide jna platform library path usr lib usr lib eclipse home location file applications eclipse cpp os arch i java awt graphicsenv apple awt cgraphics environment java ext dirs library java extensions system library java extensions system library frameworks java vm framework versions home lib ext mrj version user dir applications eclipse cpp organization mac os org osgi supports framework requirebundle true line separator java vm name java organization client vm org apache commons logging log org apache commons logging impl no op log eclipse ee install verify false file encoding mac roman osgi framework version rx v eclipse build id product eclipse vmargs dosgi required java version xdock icon resources person icns xstart on first thread xmsm xmxm xx max perm size m dorg eclipse swt internal carbon small fonts djava class path applications eclipse cpp organization mac os plugins org eclipse equinox launcher rx v jar java specification version org osgi framework executionenvironment osgi minimum osgi minimum jre jse jse jse jse osgi logfile users home workspaces workspace mini rt metadata log osgi configuration area file applications eclipse cpp configuration java lang no such method error org eclipse team svn core connector svnchange status init ljava lang string ljava lang string ijjjljava lang string iiiizzljava lang string ljava lang string ljava lang string ljava lang string jzljava lang string ljava lang string ljava lang string jlorg eclipse team svn core connector svnlock jjiljava lang string v java lang no such method error org eclipse team svn core connector svnchange status init ljava lang string ljava lang string ijjjljava lang string iiiizzljava lang string ljava lang string ljava lang string ljava lang string jzljava lang string ljava lang string ljava lang string jlorg eclipse team svn core connector svnlock jjiljava lang string v at org tigris subversion javahl conversion utility convert conversion utility java at org tigris subversion javahl conversion utility do status conversion utility java at org tmatesoft svn core javahl svnclient impl handle status svnclient impl java at org tmatesoft svn core internal wc svnstatus editor get dir status svnstatus editor java at org tmatesoft svn core internal wc svnstatus editor close edit svnstatus editor java at org tmatesoft svn core wc svnstatus client do status svnstatus client java at org tmatesoft svn core javahl svnclient impl status svnclient impl java at org tmatesoft svn core javahl svnclient impl status svnclient impl java at org polarion team svn connector svnkit svnkit connector status svnkit connector java at org eclipse team svn core extension factory thread name modifier status thread name modifier java at org eclipse team svn core utility svnutility status svnutility java at org eclipse team svn core utility svnutility get svninfo for not connected svnutility java at org eclipse team svn core svnteam provider upload repository resource svnteam provider java at org eclipse team svn core svnteam provider connect to project svnteam provider java at org eclipse team svn core svnteam provider get repository resource svnteam provider java at org eclipse team svn core svnstorage svnremote storage load local resources sub tree svnimpl svnremote storage java at org eclipse team svn core svnstorage svnremote storage load local resources sub tree svnremote storage java at org eclipse team svn core svnstorage svnremote storage get registered children svnremote storage java at org eclipse team svn core synchronize abstract svnsubscriber resources state changed impl abstract svnsubscriber java at org eclipse team svn core synchronize abstract svnsubscriber resources state changed abstract svnsubscriber java at org eclipse team svn core svnstorage svnremote storage run impl svnremote storage java at org eclipse team svn core operation abstract action operation run abstract action operation java at org eclipse team svn core operation logged operation run logged operation java at org eclipse team svn core utility progress monitor utility do task progress monitor utility java at org eclipse team svn core utility progress monitor utility do task external progress monitor utility java at org eclipse team svn core utility progress monitor utility run progress monitor utility java at org eclipse core internal resources workspace run workspace java at org eclipse team svn core utility progress monitor utility run progress monitor utility java at org eclipse core internal jobs worker run worker java\n",
      "***Description***: version i svn client org eclipse team svn connector svnkit i svn svnkit http svnkit com r java lang no such method error org eclipse team svn core connector svnchange status init ljava lang string ljava lang string ijjjljava lang string iiiizzljava lang string ljava lang string ljava lang string ljava lang string jzljava lang string ljava lang string ljava lang string jlorg eclipse team svn core connector svnlock jjiljava lang string v java lang no such method error org eclipse team svn core connector svnchange status init ljava lang string ljava lang string ijjjljava lang string iiiizzljava lang string ljava lang string ljava lang string ljava lang string jzljava lang string ljava lang string ljava lang string jlorg eclipse team svn core connector svnlock jjiljava lang string v at org tigris subversion javahl conversion utility convert conversion utility java at org tigris subversion javahl conversion utility do status conversion utility java at org tmatesoft svn core javahl svnclient impl handle status svnclient impl java at org tmatesoft svn core internal wc svnstatus editor get dir status svnstatus editor java at org tmatesoft svn core internal wc svnstatus editor close edit svnstatus editor java at org tmatesoft svn core wc svnstatus client do status svnstatus client java at org tmatesoft svn core javahl svnclient impl status svnclient impl java at org tmatesoft svn core javahl svnclient impl status svnclient impl java at org polarion team svn connector svnkit svnkit connector status svnkit connector java at org eclipse team svn core extension factory thread name modifier status thread name modifier java at org eclipse team svn core utility svnutility status svnutility java at org eclipse team svn core utility svnutility get svninfo for not connected svnutility java at org eclipse team svn core svnteam provider upload repository resource svnteam provider java at org eclipse team svn core svnteam provider connect to project svnteam provider java at org eclipse team svn core svnteam provider get repository resource svnteam provider java at org eclipse team svn core svnstorage svnremote storage load local resources sub tree svnimpl svnremote storage java at org eclipse team svn core svnstorage svnremote storage load local resources sub tree svnremote storage java at org eclipse team svn core svnstorage svnremote storage get registered children svnremote storage java at org eclipse team svn core operation local get all resources operation run impl get all resources operation java at org eclipse team svn core operation abstract action operation run abstract action operation java at org eclipse team svn core operation logged operation run logged operation java at org eclipse team svn core utility progress monitor utility do task progress monitor utility java at org eclipse team svn core utility progress monitor utility do task external progress monitor utility java at org eclipse team svn core utility progress monitor utility do task external progress monitor utility java at org eclipse team svn core synchronize persistent remote status cache all members persistent remote status cache java at org eclipse team svn core synchronize abstract svnsubscriber resources state changed impl abstract svnsubscriber java at org eclipse team svn core synchronize abstract svnsubscriber resources state changed abstract svnsubscriber java at org eclipse team svn core svnstorage svnremote storage run impl svnremote storage java at org eclipse team svn core operation abstract action operation run abstract action operation java at org eclipse team svn core operation logged operation run logged operation java at org eclipse team svn core utility progress monitor utility do task progress monitor utility java at org eclipse team svn core utility progress monitor utility do task external progress monitor utility java at org eclipse team svn core utility progress monitor utility run progress monitor utility java at org eclipse core internal resources workspace run workspace java at org eclipse team svn core utility progress monitor utility run progress monitor utility java at org eclipse core internal jobs worker run worker java\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: migrated transitions are not correctly recognized on country\n",
      "***Title***: can not organization bug from organization user interface as of v\n",
      "***Description***: steps open https bugs eclipse org bugstest show bug cgi id set status to in progress person an error is displayed\n",
      "***Description***: person in the task details where there are the options to change the status of the bug new assigned closed resolved reopened etc it seems that there is no longer a organization option to organization the bug only leave as verified fixed confirm resolve as and duplicate of i can still reopen the bug via the bugzilla web interface and using the same repository and credentials administrator credentials with an older version of person country in this case the option to reopen is still there it s not until i move to organization that the option disappears i d really like to have this organization option back without downgrading and i guess i just don t see any reason for it to no longer be there i am using person organization mylyn bugzilla connector v and organization stable from tarball reproducible always person to person mark a bug as resolved verified try to reopen the bug via mylyn plugin interface in eclipse fail\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: fatal crash after startup with autobuild enabled\n",
      "***Title***: http access to git fails on organization infrastructure\n",
      "***Description***: created attachment country report checking the log the error is reported on libjvm so pratically if i start person with autobuild enabled after completion of organization tools initialization the build process causes a crash sometimes refresh operation cause the same crash too at date file log the application claims that usr lib person does not exist anyway even if i created this directory the crash does not disappear\n",
      "***Description***: http access to git fails on organization infrastructure\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: jar references not exported while deploying to generic server in war ear\n",
      "***Title***: organization data type fails for organization\n",
      "***Description***: organization rc while references to utility projects are exported as jar files jar files references by means of variables in artwork are not expoted to ear war files it seems that variable references are not copied to metadata plugins org eclipse core resources projects project name com xxx serverdefinitions project name using the export wizzard works\n",
      "***Description***: organization data type fails for organization\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 40.2 ms, sys: 0 ns, total: 40.2 ms\n",
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841ced4741be43a8aab05eefc8546337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n",
      "Found 128207 unique tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a3de4ef1542d9b7054e4882dbb927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128207), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 13s, sys: 4.65 s, total: 2min 18s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline.generating_embed(GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM, MAX_NB_WORDS=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                              embedding_dim,\n",
    "                              weights=[embeddings],\n",
    "                              input_length=max_sequence_length,\n",
    "                              trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=index+1)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3, activation='relu')(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    \n",
    "    layer = Dense(100, activation=\"relu\")(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                          embedding_dim,\n",
    "                          weights=[embeddings],\n",
    "                          input_length=max_sequence_length,\n",
    "                          trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "    lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True, \n",
    "                                   dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm), \n",
    "                               merge_mode='ave')\n",
    "\n",
    "    layer = lstm_layer(embedded_sequences)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(100, activation=\"relu\")(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 100\n",
    "    \n",
    "    layer = Dense(input_size, activation='relu')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def l2_normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=True))\n",
    "    return K.maximum(x, K.epsilon()) / K.maximum(norm, K.epsilon())\n",
    "    #return K.sign(x) * K.maximum(K.abs(x), K.epsilon()) / K.maximum(norm, K.epsilon())\n",
    "\n",
    "def normalize(x):\n",
    "    return l2_normalize(x, axis=-1)\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    #x = l2_normalize(x, axis=-1)\n",
    "    #y = l2_normalize(y, axis=-1)\n",
    "    similarity = K.batch_dot(x, y, axes=1)\n",
    "    #similarity =  K.sum(x * y, axis=-1)\n",
    "    distance = K.constant(1) - similarity\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance)\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    loss = K.maximum(0.0, margin - y_pred[0] +  y_pred[1])\n",
    "    return K.mean(loss)\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return K.mean(y_pred[0])\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return K.mean(y_pred[1])\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects)\n",
    "    #return K.stack(K.squeeze(K.stack(vects), axis=1), axis=2) # stack adds a new dim. So squeeze it\n",
    "    # better method is to use concatenate\n",
    "    #return K.concatenate(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def siamese_model(lstm_feature_model, cnn_feature_model, mlp_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d):\n",
    "  \n",
    "    bug_t_in = Input(shape = (sequence_length_t, ), name = 'title_in')\n",
    "    bug_t_pos = Input(shape = (sequence_length_t, ), name = 'title_pos')\n",
    "    bug_t_neg = Input(shape = (sequence_length_t, ), name = 'title_neg')\n",
    "\n",
    "    bug_d_in = Input(shape = (sequence_length_d, ), name = 'desc_in')\n",
    "    bug_d_pos = Input(shape = (sequence_length_d, ), name = 'desc_pos')\n",
    "    bug_d_neg = Input(shape = (sequence_length_d, ), name = 'desc_neg')\n",
    "    \n",
    "    bug_i_in = Input(shape = (sequence_length_info, ), name = 'info_in')\n",
    "    bug_i_pos = Input(shape = (sequence_length_info, ), name = 'info_pos')\n",
    "    bug_i_neg = Input(shape = (sequence_length_info, ), name = 'info_neg')\n",
    "\n",
    "    bug_t_in_feat_lstm = lstm_feature_model(bug_t_in)\n",
    "    bug_t_pos_feat_lstm = lstm_feature_model(bug_t_pos)\n",
    "    bug_t_neg_feat_lstm = lstm_feature_model(bug_t_neg)\n",
    "\n",
    "    bug_d_in_feat_cnn = cnn_feature_model(bug_d_in)\n",
    "    bug_d_pos_feat_cnn = cnn_feature_model(bug_d_pos)\n",
    "    bug_d_neg_feat_cnn = cnn_feature_model(bug_d_neg)\n",
    "    \n",
    "    bug_i_in_feat_mlp = mlp_feature_model(bug_i_in)\n",
    "    bug_i_pos_feat_mlp = mlp_feature_model(bug_i_pos)\n",
    "    bug_i_neg_feat_mlp = mlp_feature_model(bug_i_neg)\n",
    "\n",
    "#     encoded_anchor = Add(name = 'merge_features_in')([bug_i_in_feat_mlp, bug_t_in_feat_lstm, bug_d_in_feat_cnn])\n",
    "#     encoded_positive = Add(name = 'merge_features_pos')([bug_i_pos_feat_mlp, bug_t_pos_feat_lstm, bug_d_pos_feat_cnn])\n",
    "#     encoded_negative = Add(name = 'merge_features_neg')([bug_i_neg_feat_mlp, bug_t_neg_feat_lstm, bug_d_neg_feat_cnn])\n",
    "    \n",
    "    encoded_anchor = concatenate([bug_i_in_feat_mlp, bug_t_in_feat_lstm, bug_d_in_feat_cnn], name = 'merge_features_in')\n",
    "    encoded_positive = concatenate([bug_i_pos_feat_mlp, bug_t_pos_feat_lstm, bug_d_pos_feat_cnn], name = 'merge_features_pos')\n",
    "    encoded_negative = concatenate([bug_i_neg_feat_mlp, bug_t_neg_feat_lstm, bug_d_neg_feat_cnn], name = 'merge_features_neg')\n",
    "    \n",
    "    # Reduction\n",
    "#     encoded_anchor = Dense(100, activation='relu')(encoded_anchor)\n",
    "#     encoded_positive = Dense(100, activation='relu')(encoded_positive)\n",
    "#     encoded_negative = Dense(100, activation='relu')(encoded_negative)\n",
    "    \n",
    "    # Normalization\n",
    "    encoded_anchor = Lambda(normalize, name='normalize_encoded_anchor')(encoded_anchor)\n",
    "    encoded_positive = Lambda(normalize, name='normalize_encoded_pos')(encoded_positive)\n",
    "    encoded_negative = Lambda(normalize, name='normalize_encoded_neg')(encoded_negative)\n",
    "\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "  \n",
    "    similarity_model = Model(inputs = [bug_t_in, bug_t_pos, bug_t_neg, \n",
    "                                       bug_d_in, bug_d_pos, bug_d_neg, \n",
    "                                       bug_i_in, bug_i_pos, bug_i_neg], \n",
    "                           outputs = output, name = 'Similarity_Model')\n",
    "    \n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=margin_loss, metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def checkpoint_model(name):\n",
    "    m_dir = os.path.join('checkpoint')\n",
    "    if not os.path.exists(m_dir):\n",
    "        os.mkdir(m_dir)\n",
    "    return ModelCheckpoint(os.path.join(m_dir, \"{}.hdf5\".format(name)), monitor='loss', \\\n",
    "                                        verbose=1, save_best_only=False, mode='min', period=1)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = checkpoint_model(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 100)          168300      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 100)          38607900    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 100)          38699492    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 300)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 300)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 300)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normalize_encoded_anchor (Lambd (None, 300)          0           merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "normalize_encoded_pos (Lambda)  (None, 300)          0           merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "normalize_encoded_neg (Lambda)  (None, 300)          0           merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           normalize_encoded_anchor[0][0]   \n",
      "                                                                 normalize_encoded_pos[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           normalize_encoded_anchor[0][0]   \n",
      "                                                                 normalize_encoded_neg[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 77,475,692\n",
      "Trainable params: 550,892\n",
      "Non-trainable params: 76,924,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.9281 - pos_distance: 0.5283 - neg_distance: 0.4564 - val_loss: 0.9220 - val_pos_distance: 0.5712 - val_neg_distance: 0.4932\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9165 - pos_distance: 0.5484 - neg_distance: 0.4649 - val_loss: 0.9217 - val_pos_distance: 0.5679 - val_neg_distance: 0.4896\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9131 - pos_distance: 0.5533 - neg_distance: 0.4665 - val_loss: 0.9209 - val_pos_distance: 0.4780 - val_neg_distance: 0.3989\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9127 - pos_distance: 0.5523 - neg_distance: 0.4650 - val_loss: 0.9217 - val_pos_distance: 0.6048 - val_neg_distance: 0.5265\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9105 - pos_distance: 0.5534 - neg_distance: 0.4639 - val_loss: 0.9175 - val_pos_distance: 0.5907 - val_neg_distance: 0.5082\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9106 - pos_distance: 0.5478 - neg_distance: 0.4584 - val_loss: 0.9175 - val_pos_distance: 0.4491 - val_neg_distance: 0.3666\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9100 - pos_distance: 0.5474 - neg_distance: 0.4574 - val_loss: 0.9132 - val_pos_distance: 0.5372 - val_neg_distance: 0.4504\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9090 - pos_distance: 0.5477 - neg_distance: 0.4567 - val_loss: 0.9144 - val_pos_distance: 0.5882 - val_neg_distance: 0.5026\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9076 - pos_distance: 0.5464 - neg_distance: 0.4540 - val_loss: 0.9140 - val_pos_distance: 0.4624 - val_neg_distance: 0.3764\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 93s 181ms/step - loss: 0.9087 - pos_distance: 0.5420 - neg_distance: 0.4508 - val_loss: 0.9136 - val_pos_distance: 0.4928 - val_neg_distance: 0.4064\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/512 [================>.............] - ETA: 37s - loss: 0.9086 - pos_distance: 0.5441 - neg_distance: 0.4527"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "\n",
    "cnn_feature_model = cnn_model(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "\n",
    "lstm_feature_model = lstm_model(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "mlp_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "similarity_model = siamese_model(lstm_feature_model, cnn_feature_model, mlp_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiments log\n",
    "    \n",
    "    \n",
    "'''\n",
    "h = similarity_model.fit_generator(train_gen, \n",
    "                               steps_per_epoch = 512,\n",
    "                                             epochs = 20,\n",
    "                                             verbose = 1,\n",
    "                                             validation_data=test_gen,\n",
    "                                               # callbacks=[tbCallBack, checkpoint]\n",
    "                                              )  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss=h.history['loss']\n",
    "val_loss=h.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='loss')\n",
    "plt.plot(val_loss, label='val_loss')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    m_dir = os.path.join('modelos')\n",
    "    if not os.path.exists(m_dir):\n",
    "        os.mkdir(m_dir)\n",
    "    model.save(os.path.join(m_dir, \"model_{}.h5\".format(name)))\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save_model(similarity_model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(baseline, bug):\n",
    "    info = np.concatenate((\n",
    "        baseline.to_one_hot(bug['bug_severity'], baseline.info_dict['bug_severity']),\n",
    "        baseline.to_one_hot(bug['bug_status'], baseline.info_dict['bug_status']),\n",
    "        baseline.to_one_hot(bug['component'], baseline.info_dict['component']),\n",
    "        baseline.to_one_hot(bug['priority'], baseline.info_dict['priority']),\n",
    "        baseline.to_one_hot(bug['product'], baseline.info_dict['product']),\n",
    "        baseline.to_one_hot(bug['version'], baseline.info_dict['version']))\n",
    "    )\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_normalized(a, b):\n",
    "    a = K.variable(a)\n",
    "    b = K.variable(b)\n",
    "    # normalization\n",
    "    a = normalize(a)\n",
    "    b = normalize(b)\n",
    "    # reshape\n",
    "    a = K.reshape(a, (a.shape[0], 1))\n",
    "    b = K.reshape(b, (1, b.shape[0]))\n",
    "    # dot\n",
    "    cos_sim = K.batch_dot(a, b, axes=1)\n",
    "    return K.eval(K.mean(K.constant(1) - cos_sim))\n",
    "\n",
    "def cosine(a, b):\n",
    "    a = K.variable(a)\n",
    "    b = K.variable(b)\n",
    "    # reshape\n",
    "    a = K.reshape(a, (a.shape[0], 1))\n",
    "    b = K.reshape(b, (1, b.shape[0]))\n",
    "    # dot\n",
    "    cos_sim = K.batch_dot(a, b, axes=1)\n",
    "    return K.eval(K.mean(K.constant(1) - cos_sim))\n",
    "#     return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bugs of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# bug_id = [96204, 2] # non-duplicate {15196, 2}\n",
    "bug_id = [96204, 85581] # duplicate {85581, 96204, 106979}\n",
    "bug_set = baseline.get_bug_set()\n",
    "dup_a, dup_b = bug_id\n",
    "bug_a = bug_set[dup_a]\n",
    "bug_b = bug_set[dup_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('preferences person text cut off using default fonts on organization',\n",
       " 'preferences the type filter text is cut off in the preference dialog')"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_a['title'], bug_b['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_vector_a_t = lstm_feature_model.predict(np.array([bug_a['title_word']]))[0]\n",
    "bug_vector_b_t = lstm_feature_model.predict(np.array([bug_b['title_word']]))[0]\n",
    "result = cosine_normalized(bug_vector_a_t, bug_vector_b_t)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_vector_a_t, bug_vector_b_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_a['description'], bug_b['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_vector_a_d = cnn_feature_model.predict(np.array([bug_a['description_word']]))[0]\n",
    "bug_vector_b_d = cnn_feature_model.predict(np.array([bug_b['description_word']]))[0]\n",
    "result = cosine_normalized(bug_vector_a_d, bug_vector_b_d)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bug_vector_a_d, bug_vector_b_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_vector_a_i = mlp_feature_model.predict(np.array([get_info(baseline, bug_a)]))[0]\n",
    "bug_vector_b_i = mlp_feature_model.predict(np.array([get_info(baseline, bug_b)]))[0]\n",
    "result = cosine_normalized(bug_vector_a_i, bug_vector_b_i)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bug_vector_a_i, bug_vector_b_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_vector_a = np.concatenate([ bug_vector_a_i, bug_vector_a_t, bug_vector_a_d ], -1)\n",
    "bug_vector_b = np.concatenate([ bug_vector_b_i, bug_vector_b_t, bug_vector_b_d ], -1)\n",
    "result = cosine_normalized(bug_vector_a, bug_vector_b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bug_vector_a, bug_vector_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_title =  similarity_model.get_layer('title_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_T, ), name = 'title')\n",
    "bug_desc =  similarity_model.get_layer('desc_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_D, ), name = 'desc')\n",
    "bug_info = similarity_model.get_layer('info_in').input # Input(shape = (MAX_SEQUENCE_LENGTH_I, ), name = 'info') # \n",
    "\n",
    "title_encoder = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "desc_encoder = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "info_encoder = similarity_model.get_layer('FeatureMlpGenerationModel')\n",
    "\n",
    "bug_t = title_encoder(bug_title)\n",
    "bug_d = desc_encoder(bug_desc)\n",
    "bug_i = info_encoder(bug_info)\n",
    "# Representation layer\n",
    "model = similarity_model.get_layer('merge_features_in')\n",
    "output = model([bug_i, bug_t, bug_d])\n",
    "# normalization = similarity_model.get_layer('normalize_encoded_anchor')\n",
    "# output = normalization(output)\n",
    "\n",
    "model = Model(inputs=[bug_title, bug_desc, bug_info], outputs=[output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9317614"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_vector_a = model.predict([ [bug_a['title_word']], [bug_a['description_word']], [get_info(baseline, bug_a)] ])[0]\n",
    "bug_vector_b = model.predict([ [bug_b['title_word']], [bug_b['description_word']], [get_info(baseline, bug_b)] ])[0]\n",
    "result = cosine_normalized(bug_vector_a, bug_vector_b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.retrieval import Retrieval\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading the test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d754d500164d5bbbb38652e5e1f725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82fc5fbcde748dcb6bb18a16a45038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c015ef0a7148d3b6e11e189c594aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcd805972df4bef8dc9220bd292eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516d1ad69c8f4b209b7177e745cc4380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval = Retrieval()\n",
    "\n",
    "path = 'data/processed/{}'.format(DOMAIN)\n",
    "path_buckets = 'data/normalized/{}/{}.csv'.format(DOMAIN, DOMAIN)\n",
    "path_train = 'data/processed/{}/train.txt'.format(DOMAIN)\n",
    "path_test = 'data/processed/{}/test.txt'.format(DOMAIN)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_T = 20 # Title\n",
    "MAX_SEQUENCE_LENGTH_D = 200 # Description\n",
    "MAX_SEQUENCE_LENGTH_I = 1682 # Status, Severity, Version, Component, Module\n",
    "\n",
    "# Create the instance from baseline\n",
    "retrieval.baseline = Baseline(path, path_buckets, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "df = pd.read_csv(path_buckets)\n",
    "\n",
    "# Load bug ids\n",
    "retrieval.load_bugs(path, path_train)\n",
    "# Create the buckets\n",
    "retrieval.create_bucket(df)\n",
    "# Read and create the test queries duplicate\n",
    "retrieval.create_queries(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ec6bcd1196430c86be5a563a65015b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = {}\n",
    "for bucket in tqdm(retrieval.buckets):\n",
    "    issues_by_buckets[bucket] = bucket\n",
    "    for issue in np.array(retrieval.buckets[bucket]).tolist():\n",
    "        issues_by_buckets[issue] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval.train_vectorized, retrieval.test_result = [], []\n",
    "# Infer vector to all train\n",
    "retrieval.read_train(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 12659\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting bugs from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dbc42ea94f4b2f850504e5ee0e3040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12659), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buckets_train = set()\n",
    "for row in tqdm(retrieval.test):\n",
    "    bug_id, ground_truth = row\n",
    "    vectorizer = [bug_id] \n",
    "    vectorizer += ground_truth\n",
    "    for test_bug_id in vectorizer:\n",
    "        buckets_train.add(issues_by_buckets[test_bug_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "bug_title =  similarity_model.get_layer('title_in').input \n",
    "bug_desc =  similarity_model.get_layer('desc_in').input \n",
    "bug_info = similarity_model.get_layer('info_in').input \n",
    "\n",
    "title_encoder = similarity_model.get_layer('FeatureLstmGenerationModel')\n",
    "desc_encoder = similarity_model.get_layer('FeatureCNNGenerationModel')\n",
    "info_encoder = similarity_model.get_layer('FeatureMlpGenerationModel')\n",
    "\n",
    "bug_t = title_encoder(bug_title)\n",
    "bug_d = desc_encoder(bug_desc)\n",
    "bug_i = info_encoder(bug_info)\n",
    "# Representation layer\n",
    "model = similarity_model.get_layer('merge_features_in')\n",
    "output = model([bug_i, bug_t, bug_d])\n",
    "# Normalization\n",
    "# model_normalized = similarity_model.get_layer('normalize_encoded_anchor')\n",
    "# output = model_normalized(output)\n",
    "\n",
    "model = Model(inputs=[bug_title, bug_desc, bug_info], outputs=[output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing bugs from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_buckets_train(buckets_train):\n",
    "    bug_set = retrieval.baseline.get_bug_set()\n",
    "    buckets_train_vectorized = []\n",
    "    for bug_id in tqdm(buckets_train): # retrieval.bugs_train\n",
    "        bug = bug_set[bug_id]\n",
    "        bug_vector = model.predict([ [bug['title_word']], [bug['description_word']], [retrieval.get_info(bug)] ])[0]\n",
    "        buckets_train_vectorized.append({ 'bug_id' : bug_id, 'vector' : bug_vector })\n",
    "    return buckets_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_train_vectorized = vectorizer_buckets_train(buckets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing bugs from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_set = retrieval.baseline.get_bug_set()\n",
    "queries_test_vectorized = []\n",
    "for row in tqdm(retrieval.test):\n",
    "    bug_id, ground_truth = row\n",
    "    vectorizer = [bug_id] \n",
    "    vectorizer += ground_truth\n",
    "    for test_bug_id in vectorizer:\n",
    "        if issues_by_buckets[test_bug_id] == test_bug_id: continue # if the bug is the master\n",
    "        bug = bug_set[test_bug_id]\n",
    "        bug_vector = model.predict([ [bug['title_word']], [bug['description_word']], [retrieval.get_info(bug)] ])[0]\n",
    "        queries_test_vectorized.append({ 'bug_id' : test_bug_id, 'vector' : bug_vector,\n",
    "                                        'ground_truth': issues_by_buckets[test_bug_id] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing all train\n",
    "def indexing_train(buckets_train_vectorized):\n",
    "    X = np.array(buckets_train_vectorized)\n",
    "    annoy = AnnoyIndex(X[0]['vector'].shape[0])  # Length of item vector that will be indexed\n",
    "\n",
    "    loop = tqdm(total=len(X))\n",
    "    for index, row in enumerate(X):\n",
    "        vector = row['vector']\n",
    "        annoy.add_item(index, vector)\n",
    "        loop.update(1)\n",
    "    loop.close()\n",
    "    annoy.build(10) # 10 trees\n",
    "    return annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy = indexing_train(buckets_train_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the list of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_test(queries_test_vectorized):\n",
    "    X_test = queries_test_vectorized\n",
    "    distance_test, indices_test = [], []\n",
    "    for index, row in tqdm(enumerate(X_test)):\n",
    "        vector = row['vector']\n",
    "        rank, dist = annoy.get_nns_by_vector(vector, 30, include_distances=True)\n",
    "        indices_test.append(rank)\n",
    "        distance_test.append(1 - np.array(dist)) # normalize the similarity between 0 and 1\n",
    "    return X_test, distance_test, indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, distance_test, indices_test = indexing_test(queries_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total buckets train vectorized: {}\".format(len(buckets_train_vectorized)))\n",
    "print(\"Total queries vectorized: {}\".format(len(queries_test_vectorized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_rank = []\n",
    "for row_index, row_sim in tqdm(zip(indices_test, distance_test)):\n",
    "    row_index, row_sim = row_index[:20], row_sim[:20]\n",
    "    formated_rank.append(\",\".join([\"{}:{}\".format(buckets_train_vectorized[index]['bug_id'], sim) \n",
    "                                   for index, sim in zip(row_index, row_sim)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the rank result\n",
    "rank_queries = []\n",
    "\n",
    "for index, row in tqdm(enumerate(X_test)):\n",
    "    dup_a, ground_truth = row['bug_id'], row['ground_truth']\n",
    "    rank_queries.append(\"{}:{}\".format(dup_a, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_rank = []\n",
    "loop = tqdm(total=len(rank_queries))\n",
    "\n",
    "for query, rank in zip(rank_queries, formated_rank):\n",
    "    exported_rank.append(\"{}|{}\".format(query, rank))\n",
    "    loop.update(1)\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'exported_rank.txt'), 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "        Rank recall_rate_@k\n",
    "        rank = \"query:master|master:id:sim,master:id:sim\"\n",
    "    \"\"\"\n",
    "    def top_k_recall(self, rank, k):\n",
    "        query, rank = rank.split('|')\n",
    "        query_dup_id, query_master = query.split(\":\")\n",
    "        query_master = int(query_master)\n",
    "        rank_masters = [int(item.split(':')[0]) for pos, item in enumerate(rank.split(\",\")[:20])]\n",
    "        corrects = len(set([query_master]) & set(rank_masters[:k]))\n",
    "        #total = len(retrieval.buckets[issues_by_buckets[query_master]])\n",
    "        total = 1 if corrects <= 0 else corrects\n",
    "        return float(corrects), total\n",
    "\n",
    "    def evaluate(self, path):\n",
    "        recall_at_5_corrects_sum, recall_at_10_corrects_sum, recall_at_15_corrects_sum, recall_at_20_corrects_sum = 0, 0, 0, 0\n",
    "        recall_at_5_total_sum, recall_at_10_total_sum, recall_at_15_total_sum, recall_at_20_total_sum = 0, 0, 0, 0\n",
    "        print(\"Evaluating...\")\n",
    "        with open(path, 'r') as file_input:\n",
    "            for row in file_input:\n",
    "                #if row == '': continue\n",
    "                recall_at_5_corrects, recall_at_5_total = self.top_k_recall(row, k=5)\n",
    "                recall_at_10_corrects, recall_at_10_total = self.top_k_recall(row, k=10)\n",
    "                recall_at_15_corrects, recall_at_15_total = self.top_k_recall(row, k=15)\n",
    "                recall_at_20_corrects, recall_at_20_total = self.top_k_recall(row, k=20)\n",
    "                \n",
    "                recall_at_5_corrects_sum += recall_at_5_corrects\n",
    "                recall_at_10_corrects_sum += recall_at_10_corrects\n",
    "                recall_at_15_corrects_sum += recall_at_15_corrects\n",
    "                recall_at_20_corrects_sum += recall_at_20_corrects\n",
    "                recall_at_5_total_sum += recall_at_5_total\n",
    "                recall_at_10_total_sum += recall_at_10_total\n",
    "                recall_at_15_total_sum += recall_at_15_total\n",
    "                recall_at_20_total_sum += recall_at_20_total\n",
    "        \n",
    "        report = {\n",
    "            '1 - recall_at_5' : round(recall_at_5_corrects_sum / recall_at_5_total_sum, 2),\n",
    "            '2 - recall_at_10' : round(recall_at_10_corrects_sum / recall_at_10_total_sum, 2),\n",
    "            '3 - recall_at_15' : round(recall_at_15_corrects_sum / recall_at_15_total_sum, 2),\n",
    "            '4 - recall_at_20' : round(recall_at_20_corrects_sum / recall_at_20_total_sum, 2)\n",
    "        }\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation()\n",
    "report = evaluation.evaluate(os.path.join(path, 'exported_rank.txt'))\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
