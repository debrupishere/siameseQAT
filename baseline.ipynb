{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19Oo5yKCrXYG"
   },
   "source": [
    "## Dataset bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "CxFKGhm9repF",
    "outputId": "d78a02ef-882f-4202-d1ea-7d709171b992"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'data/processed/eclipse'\n",
    "DIR_PAIRS = 'data/normalized/eclipse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pair = pd.read_csv(os.path.join(DIR_PAIRS, 'eclipse_pairs.csv'))\n",
    "baseline = Baseline(DIR, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.load_ids(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████▉  | 199213/212512 [22:08<01:58, 112.34it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8RdOXdw8q6rN",
    "outputId": "d98e260f-d633-44db-d1ff-50f0a2ae7cd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0WZNngemNM8"
   },
   "source": [
    "## Geração de batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "### Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bug_dir = os.path.join(DIR)\n",
    "baseline.prepare_dataset(bug_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline.load_bugs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 1\n",
    "batch_size_test = 512\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "train_gen = baseline.siam_gen(bug_dir, batch_size, 1)\n",
    "valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(bug_dir, batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description']], valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(bug_dir, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline.generating_embed(GLOVE_DIR='data/embed', EMBEDDING_DIM=EMBEDDING_DIM, MAX_NB_WORDS=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D \n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "def cnn_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "\n",
    "  embedding_layer = Embedding(num_words,\n",
    "                              embedding_dim,\n",
    "                              weights=[embeddings],\n",
    "                              input_length=max_sequence_length,\n",
    "                              trainable=trainable)\n",
    "\n",
    "  sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "  # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "  convs = []\n",
    "  filter_sizes = [3,4,5]\n",
    "\n",
    "  for filter_size in filter_sizes:\n",
    "      l_conv = Conv1D(filters=32, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "      l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "      convs.append(l_pool)\n",
    "\n",
    "  # l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "\n",
    "  l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "  # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "  conv = Conv1D(filters=64, kernel_size=3, activation='relu')(l_merge)\n",
    "  pool = GlobalMaxPooling1D()(conv) # pool_size=3\n",
    "  # Original Yoon Kim model\n",
    "  #x = Flatten()(pool)\n",
    "  #x = Dropout(0.5)(x)\n",
    "  layer = Activation('relu')(pool)\n",
    "\n",
    "  cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "  \n",
    "  return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "\n",
    "def lstm_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "  number_lstm_units = 100\n",
    "  rate_drop_lstm = 0\n",
    "  recurrent_dropout = 0\n",
    "\n",
    "  embedding_layer = Embedding(num_words,\n",
    "                          embedding_dim,\n",
    "                          weights=[embeddings],\n",
    "                          input_length=max_sequence_length,\n",
    "                          trainable=trainable)\n",
    "\n",
    "  sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "   # Creating LSTM Encoder\n",
    "  lstm_layer = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm), merge_mode='sum')\n",
    "\n",
    "  x = lstm_layer(embedded_sequences)\n",
    "\n",
    "  layer = Activation('relu')(x)\n",
    "\n",
    "  lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "  \n",
    "  return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGx3qyWhmLMF"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# https://stackoverflow.com/questions/50673196/keras-triplet-loss-crashes-when-training\n",
    "class MarginLoss(keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    bug_in, bug_pos, bug_neg  = inputs\n",
    "    loss, dis_pos, dis_neg = self.distance(bug_in, bug_pos, bug_neg)\n",
    "    self.add_loss(loss, inputs=inputs)\n",
    "    return  K.stack(inputs)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "  def distance(self, bug_in, bug_pos, bug_neg):\n",
    "    dis_pos, dis_neg = self.cos_distance(bug_in, bug_pos), self.cos_distance(bug_in, bug_neg)\n",
    "    ep = 1\n",
    "    d1 = K.maximum(0.0, ep - dis_pos + dis_neg)\n",
    "    return K.mean(d1), dis_pos, dis_neg\n",
    "    \n",
    "  def l2_normalize(self, x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=True))\n",
    "    return K.maximum(x, K.epsilon()) / K.maximum(norm, K.epsilon())\n",
    "\n",
    "  # https://github.com/keras-team/keras/issues/3031\n",
    "  def cos_distance(self, y_true, y_pred):\n",
    "    y_true = self.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = self.l2_normalize(y_pred, axis=-1)\n",
    "    return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
    "    \n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    return K.mean(K.maximum(0, margin - K.square(y_pred[:,0,0])+ K.square(y_pred[:,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir = './logs/', margin_loss=None):\n",
    "        super(myCallback, self).__init__()\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        self.summary_writer = tf.summary.FileWriter(training_log_dir)\n",
    "        self.margin_loss = margin_loss\n",
    "    #def on_batch_end(self, batch, logs = None):\n",
    "        #print(batch)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "#         print(self.model.get_layer('margin_loss_1'))\n",
    "#         print(dir(self.model.get_layer('margin_loss_1')))\n",
    "        model = self.model.get_layer('margin_loss_1')\n",
    "#         summary = tf.Summary()\n",
    "#         summary_value = summary.value.add()\n",
    "#         summary_value.simple_value = model.distance_pos\n",
    "#         summary_value.tag = 'distance_pos'\n",
    "#         self.summary_writer.add_summary(summary, epoch)\n",
    "#         self.val_writer.flush()\n",
    "        super(myCallback, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def siamese_model(lstm_feature_model, cnn_feature_model, max_sequence_length_t, max_sequence_length_d):\n",
    "  \n",
    "  bug_t_in = Input(shape = (max_sequence_length_t, ), name = 'title_in')\n",
    "  bug_t_pos = Input(shape = (max_sequence_length_t, ), name = 'title_pos')\n",
    "  bug_t_neg = Input(shape = (max_sequence_length_t, ), name = 'title_neg')\n",
    "  \n",
    "  bug_d_in = Input(shape = (max_sequence_length_d, ), name = 'desc_in')\n",
    "  bug_d_pos = Input(shape = (max_sequence_length_d, ), name = 'desc_pos')\n",
    "  bug_d_neg = Input(shape = (max_sequence_length_d, ), name = 'desc_neg')\n",
    "\n",
    "  bug_t_in_feat_lstm = lstm_feature_model(bug_t_in)\n",
    "  bug_t_pos_feat_lstm = lstm_feature_model(bug_t_pos)\n",
    "  bug_t_neg_feat_lstm = lstm_feature_model(bug_t_neg)\n",
    "  \n",
    "  bug_d_in_feat_cnn = cnn_feature_model(bug_d_in)\n",
    "  bug_d_pos_feat_cnn = cnn_feature_model(bug_d_pos)\n",
    "  bug_d_neg_feat_cnn = cnn_feature_model(bug_d_neg)\n",
    "\n",
    "  encoded_anchor = concatenate([bug_t_in_feat_lstm, bug_d_in_feat_cnn], name = 'merge_features_in')\n",
    "  encoded_positive = concatenate([bug_t_pos_feat_lstm, bug_d_pos_feat_cnn], name = 'merge_features_pos')\n",
    "  encoded_negative = concatenate([bug_t_neg_feat_lstm, bug_d_neg_feat_cnn], name = 'merge_features_neg')\n",
    "  \n",
    "  loss = MarginLoss()([encoded_anchor, encoded_positive, encoded_negative])\n",
    "  \n",
    "  #adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "  \n",
    "  similarity_model = Model(inputs = [bug_t_in, bug_t_pos, bug_t_neg, bug_d_in, bug_d_pos, bug_d_neg], \n",
    "                           outputs = loss, name = 'Similarity_Model')\n",
    "\n",
    "  # setup the optimization process \n",
    "  similarity_model.compile(optimizer='adam', loss=None) # metrics = ['accuracy']\n",
    "  \n",
    "  return similarity_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://pastebin.com/TaGFdcBA\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "cnn_feature_model = cnn_model(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "\n",
    "lstm_feature_model = lstm_model(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "similarity_model = siamese_model(lstm_feature_model, cnn_feature_model, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='logs/training', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "h = similarity_model.fit_generator(train_gen, \n",
    "                               steps_per_epoch = 16,\n",
    "                               #validation_data=test_gen, # \n",
    "                                             epochs = 100,\n",
    "                                             verbose = True\n",
    "                                              )  # callbacks=[tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=h.history['loss']\n",
    "\n",
    "plt.plot(loss, label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'baseline_100epoch_16steps_(eclipse)'\n",
    "Baseline.save_model('', similarity_model, name)\n",
    "Baseline.save_result('', h, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in similarity_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze weights\n",
    "for layer in similarity_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_in = similarity_model.get_layer('merge_features_in')\n",
    "model_pos = similarity_model.get_layer('merge_features_pos')\n",
    "x_in = model_in.output\n",
    "x_pos = model_pos.output\n",
    "x = Concatenate()([x_in, x_pos])\n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "x = Dense(32, activation = 'relu')(x)\n",
    "output = Dense(2, activation = 'softmax', name = 'output')(x)\n",
    "model_clf = Model(inputs=similarity_model.input, outputs=output)\n",
    "model_clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "valid_labels = encoder.fit_transform(test_gen[1])\n",
    "valid_labels = to_categorical(valid_labels)\n",
    "test_validation = (test_gen[0], valid_labels)\n",
    "\n",
    "h_clf = model_clf.fit_generator(baseline.siam_gen_classification(bug_dir, 512, 1), \n",
    "                               steps_per_epoch = 16,\n",
    "#                                 validation_split=0.2,\n",
    "                               validation_data=test_validation, # \n",
    "                                             epochs = 100,\n",
    "                                             verbose = True) # callbacks=[early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline.validation_accuracy_loss(h_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'baseline_10epoch_16steps_512batch(test)'\n",
    "save_model(similarity_model, name)\n",
    "save_result(h, name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
