{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 500 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = 'baseline_feature@number_of_epochs@epochs_64batch({})'.format(DOMAIN)\n",
    "SAVE_PATH_FEATURE = 'baseline_feature_@number_of_epochs@epochs_64batch({})'.format(DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "212512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.load_ids(DIR)\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(data, max_seq_length):\n",
    "    seq_lengths = [len(seq) for seq in data]\n",
    "    seq_lengths.append(6)\n",
    "    max_seq_length = min(max(seq_lengths), max_seq_length)\n",
    "    padded_data = np.zeros(shape=[len(data), max_seq_length])\n",
    "    for i, seq in enumerate(data):\n",
    "        seq = seq[:max_seq_length]\n",
    "        for j, token in enumerate(seq):\n",
    "            padded_data[i, j] = int(token)\n",
    "    return padded_data.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "def load_bugs(baseline):   \n",
    "    removed = []\n",
    "    baseline.corpus = []\n",
    "    baseline.sentence_dict = {}\n",
    "    baseline.bug_set = {}\n",
    "    title_padding, desc_padding = [], []\n",
    "    for bug_id in tqdm(baseline.bug_ids):\n",
    "        try:\n",
    "            bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "            title_padding.append(bug['title_word'])\n",
    "            desc_padding.append(bug['description_word'])\n",
    "            baseline.bug_set[bug_id] = bug\n",
    "            #break\n",
    "        except:\n",
    "            removed.append(bug_id)\n",
    "    \n",
    "    # Padding\n",
    "    title_padding = data_padding(title_padding, 100)\n",
    "    desc_padding = data_padding(desc_padding, 500)\n",
    "    \n",
    "    for bug_id, bug_title, bug_desc in tqdm(zip(baseline.bug_ids, title_padding, desc_padding)):\n",
    "        baseline.bug_set[bug_id]['title_word'] = bug_title\n",
    "        baseline.bug_set[bug_id]['description_word'] = bug_desc\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        baseline.sentence_dict[\",\".join(bug_title.astype(str))] = bug['title']\n",
    "        baseline.sentence_dict[\",\".join(bug_desc.astype(str))] = bug['description']\n",
    "    \n",
    "    if len(removed) > 0:\n",
    "        for x in removed:\n",
    "            baseline.bug_ids.remove(x)\n",
    "        baseline.removed = removed\n",
    "        print(\"{} were removed. To see the list call self.removed\".format(len(removed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313191c1edb24d77992e9d6e573dfe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9ec430ea8b40b99562c2786e0c1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 36min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_bugs(baseline)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f081ea7923f445e6aa6629385b52efa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=212512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = []\n",
    "export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "for bug_id in tqdm(baseline.bug_set):\n",
    "    bug = baseline.bug_set[bug_id]\n",
    "    title = bug['title']\n",
    "    desc = bug['description']\n",
    "    export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0WZNngemNM8"
   },
   "source": [
    "## Geração de batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Wall time: 606 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bug_status': '0\\n', 'issue_id': 2521, 'component': '416\\n', 'version': '373\\n', 'bug_severity': '0\\n', 'resolution': 'FIXED', 'description': 'steps minimize all your windows go to any window and select the nationality menu pick any window organization that it only gets selected and not maximized this happens in country as well organization', 'dup_id': '[]', 'priority': '0\\n', 'title': 'selecting window in the window menu does not maximize window gfitic', 'product': '70\\n', 'delta_ts': '2005-05-10 14:55:51 -0400', 'description_word': array([ 241, 3070,   86,  548,  297,  394,    9,  196,   95,   16,  131,\n",
      "          8,   17,  213, 1932,  196,   95,    2,   27,   22,  130,  784,\n",
      "        276,   16,   20, 3485,   23,  610,   11,   28,   44,  563,    2,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 'title_word': array([1036,   95,   11,    8,   95,  213,  101,   20, 3087,   95,    1,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0]), 'creation_ts': '2001-10-10 22:38:00 -0400'}\n"
     ]
    }
   ],
   "source": [
    "if 2521 in baseline.bug_set:\n",
    "    print(baseline.bug_set[2521])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "train_gen = baseline.siam_gen(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = baseline.batch_iterator(baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train, \n",
    "                                                                                          batch_size_test, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description'],\n",
    "            valid_input_sample['info'], valid_input_pos['info'], valid_input_neg['info']], valid_sim)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 43), (128, 500), (128, 1682), (128,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: nationality fonts no longer display correctly\n",
      "***Title***: organization texts can not be shown in the editor\n",
      "***Description***: in eclipse settings text fonts to organization or trebuchet ms is very desirable because it gives easy reading nationality fonts and nationality fonts but in selecting the above fonts displays only nationality characters and no more nationality characters\n",
      "***Description***: my comments are all in nationality but it shows squares when the encoding format is set at organization had no problems viewing these nationality characters previously in the builds under the organization encoding\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: propagation of build artifacts to sites causes missing updates\n",
      "***Title***: no repository found messages attempting to update from\n",
      "***Description***: was updating my maintenance build to the build date and received bunch of missing artifact errors pasted below assuming this is because of the way the builds are pushed to the sites that the metadata appears before the artifacts are actually there is there some way we could change the order in which this happens so that users don find updates until the content is actually there if we figure this out it would also be good to publish advice for others on how to do this otherwise it makes the update process look super flaky builds distrust of update sorry if this is duplicate did quick search on platform releng and didn find anything although sure this has been discussed in the past here the errors an error occurred while collecting items to be installed no repository found containing org eclipse core databinding osgi bundle no repository found containing org eclipse core databinding beans osgi bundle no repository found containing org eclipse core databinding beans source osgi bundle no repository found containing org eclipse core databinding source osgi bundle no repository found containing org eclipse core resources osgi bundle no repository found containing org eclipse core resources source osgi bundle no repository found containing org eclipse cvs org eclipse update feature jd aw no repository found containing org eclipse cvs source org eclipse update feature jd aw no repository found containing org eclipse equinox engine osgi bundle no repository found containing org eclipse equinox engine source osgi bundle no repository found containing org eclipse equinox extensionlocation osgi bundle no repository found containing org eclipse equinox extensionlocation source osgi bundle no repository found containing org eclipse equinox reconciler dropins osgi bundle no repository found containing org eclipse equinox reconciler dropins source osgi bundle no repository found containing org eclipse equinox user ui org eclipse update feature keno ph jlbhkaff hm no repository found containing org eclipse equinox user ui source org eclipse update feature keno ph jlbhkaff hm no repository found containing org eclipse help org eclipse update feature eix ei person eb ubp ckbht no repository found containing org eclipse help source org eclipse update feature eix ei person eb ubp ckbht no repository found containing org eclipse jdt apt core osgi bundle no repository found containing org eclipse jdt apt core source osgi bundle no repository found containing org eclipse jdt compiler tool osgi bundle no repository found containing org eclipse jdt compiler tool source osgi bundle no repository found containing org eclipse jdt core osgi bundle no repository found containing org eclipse jdt core source osgi bundle no repository found containing org eclipse jdt org eclipse update feature eaf ef qy ugrb hap no repository found containing org eclipse jdt source org eclipse update feature eaf ef qy ugrb hap no repository found containing org eclipse jdt ui osgi bundle no repository found containing org eclipse jdt ui source osgi bundle no repository found containing org eclipse jface osgi bundle no repository found containing org eclipse jface databinding osgi bundle no repository found containing org eclipse jface databinding source osgi bundle no repository found containing org eclipse jface source osgi bundle no repository found containing org eclipse jface text osgi bundle no repository found containing org eclipse jface text source osgi bundle no repository found containing org eclipse osgi osgi bundle no repository found containing org eclipse osgi source osgi bundle no repository found containing org eclipse platform osgi bundle no repository found containing org eclipse platform doc isv osgi bundle no repository found containing org eclipse platform org eclipse update feature ei del yevwvp zzx person agz iu no repository found containing org eclipse platform source osgi bundle no repository found containing org eclipse platform source org eclipse update feature ei del yevwvp zzx person agz iu no repository found containing org eclipse rcp org eclipse update feature jes person dxsw no repository found containing org eclipse rcp source org eclipse update feature jes person dxsw no repository found containing org eclipse sdk osgi bundle no repository found containing org eclipse sdk org eclipse update feature mc country ieu efi iswc organization no repository found containing org eclipse swt osgi bundle no repository found containing org eclipse swt win win osgi bundle no repository found containing org eclipse swt win win source osgi bundle no repository found containing org eclipse team core osgi bundle no repository found containing org eclipse team core source osgi bundle no repository found containing org eclipse team cvs core osgi bundle no repository found containing org eclipse team cvs core source osgi bundle no repository found containing org eclipse team ui osgi bundle no repository found containing org eclipse team ui source osgi bundle no repository found containing org eclipse ui osgi bundle no repository found containing org eclipse ui ide osgi bundle no repository found containing org eclipse ui ide source osgi bundle no repository found containing org eclipse ui navigator osgi bundle no repository found containing org eclipse ui navigator resources osgi bundle no repository found containing org eclipse ui navigator resources source osgi bundle no repository found containing org eclipse ui navigator source osgi bundle no repository found containing org eclipse ui source osgi bundle no repository found containing org eclipse ui workbench osgi bundle no repository found containing org eclipse ui workbench source osgi bundle no repository found containing org eclipse ui workbench texteditor osgi bundle no repository found containing org eclipse ui workbench texteditor source osgi bundle no repository found containing org eclipse sdk ide launcher win win binary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Description***: help check for updates select eclipse sdk finish observe got country dialog with the following details an error occurred while collecting items to be installed no repository found containing org eclipse ant ui osgi bundle no repository found containing org eclipse ant ui source osgi bundle no repository found containing org eclipse core contenttype osgi bundle no repository found containing org eclipse core contenttype source osgi bundle no repository found containing org eclipse core databinding osgi bundle no repository found containing org eclipse core databinding beans osgi bundle no repository found containing org eclipse core databinding beans source osgi bundle no repository found containing org eclipse core databinding source osgi bundle no repository found containing org eclipse core expressions osgi bundle no repository found containing org eclipse core expressions source osgi bundle no repository found containing org eclipse core filebuffers osgi bundle no repository found containing org eclipse core filebuffers source osgi bundle no repository found containing org eclipse core jobs osgi bundle no repository found containing org eclipse core jobs source osgi bundle no repository found containing org eclipse core net osgi bundle no repository found containing org eclipse core net source osgi bundle no repository found containing org eclipse core resources osgi bundle no repository found containing org eclipse core resources source osgi bundle no repository found containing org eclipse core resources win osgi bundle no repository found containing org eclipse debug core osgi bundle no repository found containing org eclipse debug core source osgi bundle no repository found containing org eclipse ecf osgi bundle no repository found containing org eclipse ecf filetransfer osgi bundle no repository found containing org eclipse ecf identity osgi bundle no repository found containing org eclipse ecf provider filetransfer osgi bundle no repository found containing org eclipse ecf provider filetransfer httpclient osgi bundle no repository found containing org eclipse ecf provider filetransfer httpclient ssl osgi bundle no repository found containing org eclipse ecf provider filetransfer ssl osgi bundle no repository found containing org eclipse ecf ssl osgi bundle no repository found containing org eclipse equinox frameworkadmin osgi bundle no repository found containing org eclipse equinox frameworkadmin source osgi bundle no repository found containing org eclipse equinox artifact repository osgi bundle no repository found containing org eclipse equinox artifact repository source osgi bundle no repository found containing org eclipse equinox core osgi bundle no repository found containing org eclipse equinox core source osgi bundle no repository found containing org eclipse equinox director osgi bundle no repository found containing org eclipse equinox director source osgi bundle no repository found containing org eclipse equinox engine osgi bundle no repository found containing org eclipse equinox engine source osgi bundle no repository found containing org eclipse equinox extensionlocation osgi bundle no repository found containing org eclipse equinox extensionlocation source osgi bundle no repository found containing org eclipse equinox garbagecollector osgi bundle no repository found containing org eclipse equinox garbagecollector source osgi bundle no repository found containing org eclipse equinox metadata osgi bundle no repository found containing org eclipse equinox metadata generator osgi bundle no repository found containing org eclipse equinox metadata generator source osgi bundle no repository found containing org eclipse equinox metadata repository osgi bundle no repository found containing org eclipse equinox metadata repository source osgi bundle no repository found containing org eclipse equinox metadata source osgi bundle no repository found containing org eclipse equinox publisher osgi bundle no repository found containing org eclipse equinox publisher source osgi bundle no repository found containing org eclipse equinox reconciler dropins osgi bundle no repository found containing org eclipse equinox reconciler dropins source osgi bundle no repository found containing org eclipse equinox touchpoint eclipse osgi bundle no repository found containing org eclipse equinox touchpoint eclipse source osgi bundle no repository found containing org eclipse equinox touchpoint natives osgi bundle no repository found containing org eclipse equinox touchpoint natives source osgi bundle no repository found containing org eclipse equinox ui osgi bundle no repository found containing org eclipse equinox ui source osgi bundle no repository found containing org eclipse equinox updatesite osgi bundle no repository found containing org eclipse equinox updatesite source osgi bundle no repository found containing org eclipse equinox user ui org eclipse update feature etfeya pch ez rxvfc no repository found containing org eclipse equinox user ui source org eclipse update feature etfeya pch ez rxvfc no repository found containing org eclipse help osgi bundle no repository found containing org eclipse help base osgi bundle no repository found containing org eclipse help base source osgi bundle no repository found containing org eclipse help org eclipse update feature ehkeio person dmc fn no repository found containing org eclipse help source osgi bundle no repository found containing org eclipse help source org eclipse update feature ehkeio person dmc fn no repository found containing org eclipse help ui osgi bundle no repository found containing org eclipse help ui source osgi bundle no repository found containing org eclipse help webapp osgi bundle no repository found containing org eclipse help webapp source osgi bundle no repository found containing org eclipse jdt osgi bundle no repository found containing org eclipse jdt compiler tool osgi bundle no repository found containing org eclipse jdt compiler tool source osgi bundle no repository found containing org eclipse jdt core osgi bundle no repository found containing org eclipse jdt core manipulation osgi bundle no repository found containing org eclipse jdt core manipulation source osgi bundle no repository found containing org eclipse jdt core source osgi bundle no repository found containing org eclipse jdt debug osgi bundle no repository found containing org eclipse jdt debug source osgi bundle no repository found containing org eclipse jdt debug ui osgi bundle no repository found containing org eclipse jdt debug ui source osgi bundle no repository found containing org eclipse jdt doc isv osgi bundle no repository found containing org eclipse jdt doc user osgi bundle no repository found containing org eclipse jdt org eclipse update feature ede egl tuh aqleo no repository found containing org eclipse jdt junit osgi bundle no repository found containing org eclipse jdt junit runtime osgi bundle no repository found containing org eclipse jdt junit runtime source osgi bundle no repository found containing org eclipse jdt junit source osgi bundle no repository found containing org eclipse jdt junit runtime osgi bundle no repository found containing org eclipse jdt junit runtime source osgi bundle no repository found containing org eclipse jdt source osgi bundle no repository found containing org eclipse jdt source org eclipse update feature ede egl tuh aqleo no repository found containing org eclipse jdt ui osgi bundle no repository found containing org eclipse jdt ui source osgi bundle no repository found containing org eclipse jface databinding osgi bundle no repository found containing org eclipse jface databinding source osgi bundle no repository found containing org eclipse jface text osgi bundle no repository found containing org eclipse jface text source osgi bundle no repository found containing org eclipse ltk core refactoring osgi bundle no repository found containing org eclipse ltk core refactoring source osgi bundle no repository found containing org eclipse ltk ui refactoring osgi bundle no repository found containing org eclipse ltk ui refactoring source osgi bundle no repository found containing org eclipse osgi osgi bundle no repository found containing org eclipse osgi source osgi bundle no repository found containing org eclipse pde osgi bundle no repository found containing org eclipse pde api tools osgi bundle no repository found containing org eclipse pde api tools source osgi bundle no repository found containing org eclipse pde api tools ui osgi bundle no repository found containing org eclipse pde api tools ui source osgi bundle no repository found containing org eclipse pde build osgi bundle no repository found containing org eclipse pde build source osgi bundle no repository found containing org eclipse pde core osgi bundle no repository found containing org eclipse pde core source osgi bundle no repository found containing org eclipse pde ds core osgi bundle no repository found containing org eclipse pde ds core source osgi bundle no repository found containing org eclipse pde ds ui osgi bundle no repository found containing org eclipse pde ds ui source osgi bundle no repository found containing org eclipse pde org eclipse update feature xe meb organization aqnv dll um no repository found containing org eclipse pde runtime osgi bundle no repository found containing org eclipse pde runtime source osgi bundle no repository found containing org eclipse pde source osgi bundle no repository found containing org eclipse pde source org eclipse update feature xe meb organization aqnv dll um no repository found containing org eclipse pde ui osgi bundle no repository found containing org eclipse pde ui source osgi bundle no repository found containing org eclipse platform osgi bundle no repository found containing org eclipse platform doc isv osgi bundle no repository found containing org eclipse platform doc user osgi bundle no repository found containing org eclipse platform org eclipse update feature organization kty cj kr qc pwm oc xxx go no repository found containing org eclipse platform source osgi bundle no repository found containing org eclipse platform source org eclipse update feature organization kty cj kr qc pwm oc xxx go no repository found containing org eclipse sdk osgi bundle no repository found containing org eclipse sdk org eclipse update feature bedm nr rex pm person ky py ef ng no repository found containing org eclipse search osgi bundle no repository found containing org eclipse search source osgi bundle no repository found containing org eclipse swt osgi bundle no repository found containing org eclipse swt win win osgi bundle no repository found containing org eclipse swt win win source osgi bundle no repository found containing org eclipse team core osgi bundle no repository found containing org eclipse team core source osgi bundle no repository found containing org eclipse team ui osgi bundle no repository found containing org eclipse team ui source osgi bundle no repository found containing org eclipse text osgi bundle no repository found containing org eclipse text source osgi bundle no repository found containing org eclipse ui cheatsheets osgi bundle no repository found containing org eclipse ui cheatsheets source osgi bundle no repository found containing org eclipse ui console osgi bundle no repository found containing org eclipse ui console source osgi bundle no repository found containing org eclipse ui editors osgi bundle no repository found containing org eclipse ui editors source osgi bundle no repository found containing org eclipse ui forms osgi bundle no repository found containing org eclipse ui forms source osgi bundle no repository found containing org eclipse ui ide osgi bundle no repository found containing org eclipse ui ide source osgi bundle no repository found containing org eclipse ui intro osgi bundle no repository found containing org eclipse ui intro source osgi bundle no repository found containing org eclipse ui intro universal osgi bundle no repository found containing org eclipse ui intro universal source osgi bundle no repository found containing org eclipse ui workbench texteditor osgi bundle no repository found containing org eclipse ui workbench texteditor source osgi bundle no repository found containing org eclipse rcp org eclipse update feature ev nel int em evz no repository found containing org eclipse rcp source org eclipse update feature ev nel int em evz no repository found containing org eclipse sdk ide launcher win win binary no repository found containing org eclipse ui osgi bundle no repository found containing org eclipse ui source osgi bundle no repository found containing org eclipse ui workbench osgi bundle no repository found containing org eclipse ui workbench source osgi bundle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***similar = 1\n",
      "########################\n",
      "***Title***: add decorator to the console view to indicate bad things happened\n",
      "***Title***: severe performance issues with visualiser on\n",
      "***Description***: organizationorganization view usually displays lot of information it would be nic to have scroll bar decorated with coloured marks to indicate sth special is in specific part of console display like exception keyword could be marked as red and warn as yellow see it just like the scroll bar of the person editor it has small yellow grey red pieces that indicates sth in specific place of the file and when you click on that it centralizes view in this place very good for lot of logs in long console view\n",
      "***Description***: severe performance issues with visualiser on\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: installed jre option bogus\n",
      "***Title***: organization should be provided with an extension of rptlibrary when exporting report item to new library\n",
      "***Description***: since organization does not use ignores the jre home directory which number sets in the appropriate dialog via organization jres we always get complaints from students developers when we update remove jre btw if you are windows developer dont read further but ask developer which has some unix knowledge and sense for enterprise issues to gurantee that every student developer is able to work with the latest software we use symbolic links to accomplish that and thus no number has to waste time to update its settings wrt to person we use the following layout dir usr apps lrwxrwxrwx root root jan sdk sdk lrwxrwxrwx root root jan sdk sdk lrwxrwxrwx root root jan sdk sdk drwxr xr root root mar sdk lrwxrwxrwx root root aug sdk sdk drwxr xr root root dec sdk drwxr xr root root aug sdk drwxrwxr root root aug sdk lrwxrwxrwx root root apr sdk sdk lrwxrwxrwx root root person sdk sdk beta lrwxrwxrwx root root apr sdk beta lrwxrwxrwx root root person sdk beta so usually all users have set their java home to usr apps sdk and if we add new release sdk we just change the link for sdk from sdk to sdk and all users are uptodate without the need of wasting time to adjust their settings furthermore sing this approach developers which insist to use certain person are still able to do that for normal java software this works quite well since date however with the strange behavior of eclipse this is not true anymore so if number sets the jre home directory to certain value eclipse should use this value and not dumb method which resolves links and stores them unconditionally in the prefs\n",
      "***Description***: organization should be provided with an extension of rptlibrary when exporting report item to new library\n",
      "***similar = 0\n",
      "########################\n",
      "Wall time: 202 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 113554'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edabb9353c464b2ea3603325316ab43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794ea0db4f8242a382229d8a37d02fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113554), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating for each epoch at same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods train experiment siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.retrieval import Retrieval\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_queries(retrieval, path_test):\n",
    "    print(\"Creating the queries...\")\n",
    "    test = []\n",
    "    with open(path_test, 'r') as file_test:\n",
    "        for row in tqdm(file_test):\n",
    "            tokens = row.strip().split()\n",
    "            test.append([int(tokens[0]), [int(bug) for bug in tokens[1:]]])\n",
    "    retrieval.test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff68dfd21054f459597705c254ce523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd44cd0cb26414c8bf48716bad0b2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval = Retrieval()\n",
    "\n",
    "path = 'data/processed/{}'.format(DOMAIN)\n",
    "path_buckets = 'data/normalized/{}/{}.csv'.format(DOMAIN, DOMAIN)\n",
    "path_train = 'data/processed/{}/train.txt'.format(DOMAIN)\n",
    "path_test = 'data/processed/{}/test.txt'.format(DOMAIN)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_I = number_of_columns_info # Status, Severity, Version, Component, Module\n",
    "\n",
    "# Create the instance from baseline\n",
    "#retrieval.baseline = Baseline(path, path_buckets, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "retrieval.baseline = baseline\n",
    "\n",
    "df = pd.read_csv(path_buckets)\n",
    "\n",
    "# Load bug ids\n",
    "#retrieval.load_bugs(path, path_train)\n",
    "# Create the buckets\n",
    "retrieval.create_bucket(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960a46b6417340dfb9743d095cb6d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read and create the test queries duplicate\n",
    "create_queries(retrieval, path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3859934da74743e49bb4eed081799b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_by_buckets = {}\n",
    "for bucket in tqdm(retrieval.buckets):\n",
    "    issues_by_buckets[bucket] = bucket\n",
    "    for issue in np.array(retrieval.buckets[bucket]).tolist():\n",
    "        issues_by_buckets[issue] = bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "def get_model_vectorizer(path=None, loaded_model=None):\n",
    "    if(path):\n",
    "        loaded_model = load_model(os.path.join(\"modelos\", \"model_{}.h5\".format(path)))\n",
    "        \n",
    "        '''\n",
    "            {'l2_normalize' : l2_normalize, \n",
    "                                     'margin_loss' : margin_loss,\n",
    "                                     'pos_distance' : pos_distance,\n",
    "                                     'neg_distance' : neg_distance,\n",
    "                                     'stack_tensors': stack_tensors}\n",
    "        '''\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the list of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_query(annoy, queries_test_vectorized, verbose=1):\n",
    "    X_test = queries_test_vectorized\n",
    "    distance_test, indices_test = [], []\n",
    "    loop = enumerate(X_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(enumerate(X_test))\n",
    "        loop.set_description('Getting the list of candidates from queries')\n",
    "    for index, row in loop:\n",
    "        vector = row['vector']\n",
    "        rank, dist = annoy.get_nns_by_vector(vector, 30, include_distances=True)\n",
    "        indices_test.append(rank)\n",
    "        distance_test.append(1 - np.array(dist)) # normalize the similarity between 0 and 1\n",
    "    if(verbose): loop.close()\n",
    "    return X_test, distance_test, indices_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing all train\n",
    "def indexing_test(buckets_train_vectorized, verbose=1):\n",
    "    X = np.array(buckets_train_vectorized)\n",
    "    annoy = AnnoyIndex(X[0]['vector'].shape[0])  # Length of item vector that will be indexed\n",
    "\n",
    "    loop = total=len(X)\n",
    "    if(verbose):\n",
    "        loop = tqdm(total=len(X))\n",
    "        loop.set_description(\"Indexing test in annoy\")\n",
    "    for index, row in enumerate(X):\n",
    "        vector = row['vector']\n",
    "        annoy.add_item(index, vector)\n",
    "        if(verbose): loop.update(1)\n",
    "    if(verbose): loop.close()\n",
    "    annoy.build(10) # 10 trees\n",
    "    return annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_result(test_vectorized, indices_test, distance_test, verbose=1):\n",
    "    formated_rank = []\n",
    "    loop = zip(indices_test, distance_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(zip(indices_test, distance_test))\n",
    "        loop.set_description('Generating the rank')\n",
    "    for row_index, row_sim in loop:\n",
    "        row_index, row_sim = row_index[:25], row_sim[:25]\n",
    "        formated_rank.append(\",\".join([\"{}:{}\".format(test_vectorized[index]['bug_id'], sim) \n",
    "                                       for index, sim in zip(row_index, row_sim)]))\n",
    "    if(verbose): loop.close()\n",
    "    return formated_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_test(bug_set, model, test, issues_by_buckets, verbose=1):\n",
    "    test_vectorized = []\n",
    "    title_data, desc_data, info_data = [], [], []\n",
    "    loop = test\n",
    "    if(verbose):\n",
    "        loop = tqdm(test)\n",
    "        loop.set_description('Vectorizing buckets')\n",
    "    buckets = set()\n",
    "    for row in loop: # retrieval.bugs_train\n",
    "        query, ground_truth = row\n",
    "        bugs = [query]\n",
    "        bugs += ground_truth\n",
    "        for bug_id in bugs:\n",
    "            buckets.add(issues_by_buckets[bug_id])\n",
    "    for bucket_id in buckets:\n",
    "        bug = bug_set[bucket_id]\n",
    "        title_data.append(bug['title_word'])\n",
    "        desc_data.append(bug['description_word'])\n",
    "        info_data.append(retrieval.get_info(bug))\n",
    "        test_vectorized.append({ 'bug_id' : bucket_id })\n",
    "    if(verbose):\n",
    "        loop.close()\n",
    "    # Get embedding of all buckets\n",
    "    embed_test = model.predict([ np.array(title_data), np.array(desc_data), np.array(info_data) ])\n",
    "    # Fill the buckets array\n",
    "    for index, vector in enumerate(embed_test):\n",
    "        test_vectorized[index]['vector'] = vector\n",
    "    \n",
    "    return test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_queries(bug_set, model, test, issues_by_buckets, verbose=1):\n",
    "    queries_test_vectorized = []\n",
    "    title_data, desc_data, info_data = [], [], []\n",
    "    loop = test\n",
    "    if(verbose):\n",
    "        loop = tqdm(test)\n",
    "    for row in loop:\n",
    "        test_bug_id, ground_truth = row\n",
    "        if issues_by_buckets[test_bug_id] == test_bug_id: # if the bug is the master\n",
    "            test_bug_id = np.random.choice(ground_truth, 1)[0]\n",
    "        queries = set()\n",
    "        queries.add(test_bug_id)\n",
    "        if test_bug_id in ground_truth:\n",
    "            ground_truth = list(set(ground_truth) - set([test_bug_id])) # Remove the same bug random choice to change the master\n",
    "        if len(ground_truth) > 0:\n",
    "            for bug in ground_truth:\n",
    "                if issues_by_buckets[bug] != bug: # if the bug is the master\n",
    "                    queries.add(bug)\n",
    "                \n",
    "        for bug_id in queries:\n",
    "            bug = bug_set[bug_id]\n",
    "            title_data.append(bug['title_word'])\n",
    "            desc_data.append(bug['description_word'])\n",
    "            info_data.append(retrieval.get_info(bug))\n",
    "            queries_test_vectorized.append({ 'bug_id' : bug_id, 'ground_truth': issues_by_buckets[bug_id] })\n",
    "\n",
    "    # Get embedding of all buckets\n",
    "    embed_queries = model.predict([ np.array(title_data), np.array(desc_data), np.array(info_data) ])\n",
    "    # Fill the queries array    \n",
    "    for index, vector in enumerate(embed_queries):\n",
    "        queries_test_vectorized[index]['vector'] = vector\n",
    "    \n",
    "    return queries_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the rank result\n",
    "def formating_rank(X_test, verbose=1):\n",
    "    rank_queries = []\n",
    "    loop = enumerate(X_test)\n",
    "    if(verbose):\n",
    "        loop = tqdm(enumerate(X_test))\n",
    "        loop.set_description('Generating the queries from rank')\n",
    "    for index, row in loop:\n",
    "        dup_a, ground_truth = row['bug_id'], row['ground_truth']\n",
    "        rank_queries.append(\"{}:{}\".format(dup_a, ground_truth))\n",
    "    if(verbose): loop.close()\n",
    "    return rank_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_rank(rank_queries, formated_rank, verbose=1):\n",
    "    exported_rank = []\n",
    "    loop = len(rank_queries)\n",
    "    if(verbose):\n",
    "        loop = tqdm(total=len(rank_queries))\n",
    "        loop.set_description('Exporting the rank')\n",
    "    for query, rank in zip(rank_queries, formated_rank):\n",
    "        exported_rank.append(\"{}|{}\".format(query, rank))\n",
    "        if(verbose): loop.update(1)\n",
    "    if(verbose): loop.close()\n",
    "    return exported_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_test(retrieval, verbose, loaded_model, issues_by_buckets):\n",
    "    \n",
    "    # Load test set\n",
    "    test = retrieval.test\n",
    "    bug_set = retrieval.baseline.get_bug_set()\n",
    "    \n",
    "    # Get model\n",
    "    model = get_model_vectorizer(loaded_model=loaded_model)\n",
    "    \n",
    "    # Test \n",
    "    test_vectorized = vectorizer_test(bug_set, model, test, issues_by_buckets, verbose)\n",
    "    queries_test_vectorized = vectorize_queries(bug_set, model, test, issues_by_buckets, verbose)\n",
    "    annoy = indexing_test(test_vectorized, verbose)\n",
    "    X_test, distance_test, indices_test = indexing_query(annoy, queries_test_vectorized, verbose)\n",
    "    formated_rank = rank_result(test_vectorized, indices_test, distance_test, verbose)\n",
    "    rank_queries = formating_rank(X_test, verbose)\n",
    "    exported_rank = export_rank(rank_queries, formated_rank, verbose)\n",
    "    evaluation = Evaluation(verbose)\n",
    "    recall = evaluation.evaluate(exported_rank)['5 - recall_at_25']\n",
    "    \n",
    "    # recall@25, loss, cosine_positive, cosine_negative\n",
    "    return recall, exported_rank\n",
    "    #return report['5 - recall_at_25'], evaluation_test_batch[0], evaluation_test_batch[1], evaluation_test_batch[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    def __init__(self, verbose=1):\n",
    "        self.verbose = verbose\n",
    "        self.MAX_RANK = 25\n",
    "    \n",
    "    \"\"\"\n",
    "        Rank recall_rate_@k\n",
    "        rank = \"query:master|master:id:sim,master:id:sim\"\n",
    "    \"\"\"\n",
    "    def top_k_recall(self, row, k):\n",
    "        query, rank = row.split('|')\n",
    "        query_dup_id, ground_truth = query.split(\":\")\n",
    "        candidates = [int(item.split(':')[0]) for pos, item in enumerate(rank.split(\",\")[:self.MAX_RANK])]\n",
    "        corrects = len(set([int(ground_truth)]) & set(candidates[:k]))\n",
    "        total = len([ground_truth]) # only one master from query\n",
    "        return float(corrects), total\n",
    "\n",
    "    def evaluate(self, path):\n",
    "        self.recall_at_5_corrects_sum, self.recall_at_10_corrects_sum, \\\n",
    "        self.recall_at_15_corrects_sum, self.recall_at_20_corrects_sum, self.recall_at_25_corrects_sum = 0, 0, 0, 0, 0\n",
    "        self.recall_at_5_total_sum, self.recall_at_10_total_sum, self.recall_at_15_total_sum, \\\n",
    "        self.recall_at_20_total_sum, self.recall_at_25_total_sum = 0, 0, 0, 0, 0 \n",
    "        if(self.verbose):\n",
    "            print(\"Evaluating...\")\n",
    "        if type(path) == str:\n",
    "            with open(path, 'r') as file_input:\n",
    "                for row in file_input:\n",
    "                    self.recall(row)\n",
    "        else:\n",
    "            for row in path:\n",
    "                self.recall(row)\n",
    "        \n",
    "        report = {\n",
    "            '1 - recall_at_5' : round(self.recall_at_5_corrects_sum / self.recall_at_5_total_sum, 2),\n",
    "            '2 - recall_at_10' : round(self.recall_at_10_corrects_sum / self.recall_at_10_total_sum, 2),\n",
    "            '3 - recall_at_15' : round(self.recall_at_15_corrects_sum / self.recall_at_15_total_sum, 2),\n",
    "            '4 - recall_at_20' : round(self.recall_at_20_corrects_sum / self.recall_at_20_total_sum, 2),\n",
    "            '5 - recall_at_25' : round(self.recall_at_25_corrects_sum / self.recall_at_25_total_sum, 2)\n",
    "        }\n",
    "\n",
    "        return report\n",
    "    def recall(self, row):\n",
    "        #if row == '': continue\n",
    "        self.recall_at_5_corrects, self.recall_at_5_total = self.top_k_recall(row, k=5)\n",
    "        self.recall_at_10_corrects, self.recall_at_10_total = self.top_k_recall(row, k=10)\n",
    "        self.recall_at_15_corrects, self.recall_at_15_total = self.top_k_recall(row, k=15)\n",
    "        self.recall_at_20_corrects, self.recall_at_20_total = self.top_k_recall(row, k=20)\n",
    "        self.recall_at_25_corrects, self.recall_at_25_total = self.top_k_recall(row, k=25)\n",
    "\n",
    "        self.recall_at_5_corrects_sum += self.recall_at_5_corrects\n",
    "        self.recall_at_10_corrects_sum += self.recall_at_10_corrects\n",
    "        self.recall_at_15_corrects_sum += self.recall_at_15_corrects\n",
    "        self.recall_at_20_corrects_sum += self.recall_at_20_corrects\n",
    "        self.recall_at_25_corrects_sum += self.recall_at_25_corrects\n",
    "\n",
    "        self.recall_at_5_total_sum += self.recall_at_5_total\n",
    "        self.recall_at_10_total_sum += self.recall_at_10_total\n",
    "        self.recall_at_15_total_sum += self.recall_at_15_total\n",
    "        self.recall_at_20_total_sum += self.recall_at_20_total\n",
    "        self.recall_at_25_total_sum += self.recall_at_25_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, verbose=0):\n",
    "    m_dir = os.path.join('modelos')\n",
    "    if not os.path.exists(m_dir):\n",
    "        os.mkdir(m_dir)\n",
    "    export = os.path.join(m_dir, \"model_{}.h5\".format(name))\n",
    "    model.save(export)\n",
    "    if(verbose):\n",
    "        print(\"Saved model '{}' to disk\".format(export))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def arcii_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    filters_1d=max_sequence_length\n",
    "    kernel_size_1d=3\n",
    "    num_conv2d_layers=2\n",
    "    filters_2d=[256,128]\n",
    "    kernel_size_2d=[[3,3], [3,3]]\n",
    "    mpool_size_2d=[[2,2], [2,2]]\n",
    "    dropout_rate=0\n",
    "    batch_size=128\n",
    "    \n",
    "    layer1_conv=Conv1D(filters=filters_1d, kernel_size=kernel_size_1d, padding='same')(embedded_sequences)\n",
    "    layer1_activation=Activation('relu')(layer1_conv)\n",
    "    layer1_reshaped=Reshape((max_sequence_length, max_sequence_length, -1))(layer1_activation)\n",
    "    z=MaxPooling2D(pool_size=(2,2))(layer1_reshaped)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z=Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same')(z)\n",
    "        z=Activation('relu')(z)\n",
    "        z=MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "\n",
    "    pool1_flat=Flatten()(z)\n",
    "    pool1_flat_drop=Dropout(rate=dropout_rate)(pool1_flat)\n",
    "    pool1_norm=BatchNormalization()(pool1_flat_drop)\n",
    "    mlp1=Dense(100)(pool1_norm)\n",
    "    output=Activation('relu')(mlp1)\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[output], name = 'FeatureARCIIGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n",
    "    def block(block_input):        \n",
    "        residual =    block_input\n",
    "        \n",
    "        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n",
    "                      dilation_rate=dilation, \n",
    "                      activation='linear', padding='causal', use_bias=False,\n",
    "                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n",
    "                      seed=42), kernel_regularizer=l2(l2_layer_reg))(block_input)                    \n",
    "        selu_out =    Activation('tanh')(layer_out)\n",
    "        \n",
    "        skip_out =    Conv1D(1,1, activation='linear', use_bias=False, \n",
    "                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n",
    "                      seed=42), kernel_regularizer=l2(l2_layer_reg))(selu_out)\n",
    "        \n",
    "        c1x1_out =    Conv1D(1,1, activation='linear', use_bias=False, \n",
    "                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n",
    "                      seed=42), kernel_regularizer=l2(l2_layer_reg))(selu_out)\n",
    "                      \n",
    "        block_out =   Add()([residual, c1x1_out])\n",
    "        \n",
    "        return block_out, skip_out\n",
    "    return block\n",
    "\n",
    "def cnn_dilated_model(embedding_layer, max_sequence_length):\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    units = 32\n",
    "\n",
    "    l1a, l1b = DC_CNN_Block(units,2,1,0.01)(embedded_sequences)    \n",
    "    l2a, l2b = DC_CNN_Block(units,2,2,0.01)(l1a) \n",
    "    l3a, l3b = DC_CNN_Block(units,2,4,0.01)(l2a)\n",
    "    l4a, l4b = DC_CNN_Block(units,2,8,0.01)(l3a)\n",
    "    l5a, l5b = DC_CNN_Block(units,2,16,0.01)(l4a)\n",
    "    l6a, l6b = DC_CNN_Block(units,2,32,0.01)(l5a)\n",
    "    l7a, l7b = DC_CNN_Block(units,2,64,0.01)(l6a)\n",
    "\n",
    "    l8 =   Add()([l1b, l2b, l3b, l4b, l5b, l6b, l7b])\n",
    "\n",
    "    l9 =   Activation('tanh')(l8)\n",
    "\n",
    "    x =  Conv1D(1,1, activation='linear', use_bias=False, \n",
    "           kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=42),\n",
    "           kernel_regularizer=l2(0.001))(l9)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(100)(x)\n",
    "    #x = Dropout(0.45)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    layer = Activation('tanh')(x)\n",
    "    #x = Dense(2)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #layer = Activation('relu')(x)\n",
    "    #yhat = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNDilatedGenerationModel') # inputs=visible\n",
    "    return feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(100, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Creating LSTM Encoder\n",
    "#     lstm_layer = Bidirectional(LSTM(number_lstm_units, return_sequences=True), # dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm \n",
    "#                                merge_mode='ave')\n",
    "\n",
    "    lstm_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    layer = LSTM(number_lstm_units)(lstm_layer)\n",
    "\n",
    "    #layer = lstm_layer(embedded_sequences)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(100, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 100\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def l2_normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "\n",
    "def normalize(x):\n",
    "    return l2_normalize(x, axis=-1)\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = l2_normalize(x, axis=-1)\n",
    "    y, y_norm = l2_normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    margin = 1.0\n",
    "    loss = K.maximum(0.0, margin - y_pred[0] + y_pred[1])\n",
    "    return loss\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def residual_bug():\n",
    "    def block(block_input):\n",
    "        shape_size = K.int_shape(block_input)[1]\n",
    "        \n",
    "        residual =  block_input\n",
    "        \n",
    "        layer_out = Dense(shape_size // 2, activation='tanh')(block_input)\n",
    "        \n",
    "        skip_out =  Dense(shape_size, activation='linear', use_bias=False)(layer_out)\n",
    "        # kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n",
    "         #             seed=42), kernel_regularizer=l2(0.01)\n",
    "        \n",
    "        dense_out =  Dense(shape_size, activation='linear', use_bias=False)(layer_out)\n",
    "        \n",
    "        block_out =   Add()([residual, dense_out])\n",
    "        return block_out, skip_out\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "#     encoded_1a, encoded_1b  = residual_bug()(bug_feature_output)\n",
    "#     bug_feature_output = encoded_1a\n",
    "    #     encoded_2a, encoded_2b  = residual_bug()(encoded_1a)\n",
    "    \n",
    "    #     bug_feature_output = Add()([encoded_1b, encoded_2b])\n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    \n",
    "    # Cosine\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=margin_loss, metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 100)          168300      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 100)          34161700    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 100)          34303292    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 300)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 300)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 300)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances (Lambda)        (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 68,633,292\n",
      "Trainable params: 500,892\n",
      "Non-trainable params: 68,132,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(vocab), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(vocab), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "'''\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 20\n",
    "best_recall = 0\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "recall = 0\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, \\\n",
    "            train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, batch_size, 1)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(train_batch, train_sim)\n",
    "    \n",
    "    if (epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _ = evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "        print(\"Epoch: {} - Loss: {:.2f}, positive_cosine: {:.2f}, negative_cosine: {:.2f}, recall@25: {:.2f}\".format(\n",
    "            epoch+1, h[0], h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} - Loss: {:.2f}, positive_cosine: {:.2f}, negative_cosine: {:.2f}\".format(\n",
    "            epoch+1, h[0], h[1], h[2]))\n",
    "    \n",
    "    if recall > best_recall:\n",
    "        save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "        save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "        best_recall = recall\n",
    "        best_epoch = epoch+1\n",
    "print('Best_epoch={}, Best_recall={:.2f}'.format(best_epoch, best_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['114688:44627|82208:0.9094618260860443,92315:0.9078811854124069,81971:0.9046715572476387,99738:0.9038981944322586,110030:0.9028568863868713,82349:0.9011246412992477,92264:0.9010708704590797,76313:0.8999578729271889,84496:0.8993142023682594,92451:0.8950971812009811,77894:0.8932379633188248,110336:0.8918304219841957,101380:0.8905267491936684,78018:0.8838822767138481,102422:0.8765283823013306,97220:0.8764417394995689,84944:0.876093901693821,79798:0.8741774708032608,92821:0.873424768447876,81618:0.8704424798488617,95394:0.8645401000976562,90879:0.8559570014476776,73784:0.8531851023435593,99507:0.8486966192722321,95056:0.8426463901996613',\n",
       " '165444:44627|107105:0.9257863909006119,114338:0.9204315915703773,148010:0.9187451303005219,118217:0.9167706966400146,106466:0.9116260930895805,117020:0.906024657189846,122610:0.8834674879908562,148380:0.8759315013885498,193045:0.8435272723436356,155013:0.8411222398281097,140156:0.836276963353157,140980:0.7740538418292999,149376:0.7548101097345352,124646:0.7435722649097443,136091:0.7327681183815002,120615:0.44072484970092773,185422:0.42482632398605347,140416:0.42226821184158325,112433:0.4065997004508972,162621:0.39888423681259155,147024:0.39350008964538574,261594:0.38741350173950195,317719:0.3854512572288513,313870:0.38537949323654175,299384:0.38304567337036133',\n",
       " '229443:229377|200743:0.8289164006710052,181864:0.8145924657583237,161533:0.7045969367027283,199350:0.6333947777748108,6983:0.46720558404922485,172888:0.4569218158721924,363309:0.4436156153678894,7101:0.4431830048561096,88958:0.44182640314102173,99266:0.44023334980010986,98087:0.4358658790588379,183444:0.43462222814559937,91666:0.43202918767929077,108813:0.4301155209541321,83308:0.4260365962982178,95479:0.42095130681991577,38937:0.4183266758918762,259013:0.416343092918396,34270:0.4156135320663452,63699:0.40861058235168457,29356:0.4075582027435303,291307:0.40739625692367554,276531:0.4066217541694641,44915:0.40644294023513794,33241:0.4062735438346863',\n",
       " '9779:2|9336:0.8987208604812622,21128:0.8975946828722954,22923:0.8937542960047722,18559:0.8927774876356125,14056:0.8912830278277397,15724:0.8890553787350655,22172:0.8863667845726013,8355:0.8826306238770485,7327:0.8793178722262383,16373:0.8698861002922058,85:0.8652480095624924,22337:0.8570732027292252,14533:0.8349960744380951,23777:0.8349418640136719,18647:0.7868320494890213,7564:0.508060872554779,30:0.5072343647480011,1916:0.5030035972595215,2027:0.5010892748832703,9295:0.501000165939331,11600:0.5009047985076904,12964:0.500743180513382,7872:0.5004059076309204,11239:0.5001267492771149,10509:0.498917818069458',\n",
       " '98309:128463|139517:0.42104244232177734,81997:0.3990466594696045,90100:0.37759125232696533,110070:0.17808961868286133,339910:0.17233890295028687,304182:0.14112377166748047,251955:0.1313517689704895,94534:0.11525237560272217,118245:0.11029022932052612,325824:0.10454446077346802,178602:0.09849077463150024,189651:0.09180891513824463,47716:0.0801236629486084,329383:0.06983524560928345,251565:0.0605129599571228,317562:0.05819898843765259,220218:0.042234838008880615,58705:0.04187506437301636,69524:0.04013657569885254,132699:0.03896230459213257,380274:0.03629493713378906,244124:0.0349116325378418,393957:0.030798912048339844,331301:0.03002631664276123,91857:0.018226683139801025',\n",
       " '376838:376841|376841:1.0,300750:0.20830142498016357,223911:0.20795553922653198,292390:0.20006465911865234,186478:0.19843262434005737,300819:0.1844133734703064,170633:0.18255358934402466,236797:0.17306840419769287,203191:0.16955620050430298,118775:0.1693858504295349,131248:0.1692376732826233,332276:0.1687871813774109,284835:0.1687503457069397,117666:0.16812598705291748,281310:0.1676989197731018,128358:0.16496491432189941,155632:0.16492116451263428,142596:0.16322070360183716,317562:0.1625300645828247,118623:0.1588817834854126,140756:0.1573849320411682,139344:0.15728342533111572,264813:0.1568584442138672,240753:0.15566372871398926,189585:0.15552383661270142',\n",
       " '278537:274618|10560:0.3874632716178894,291245:0.3562968969345093,97610:0.3527691960334778,239141:0.293861985206604,22761:0.25772953033447266,111653:0.23740005493164062,263477:0.21598905324935913,95945:0.20997822284698486,300056:0.19850999116897583,72679:0.19707226753234863,94984:0.17199170589447021,312893:0.16337621212005615,265740:0.15299689769744873,49562:0.14652377367019653,94470:0.14549487829208374,273211:0.13683193922042847,261430:0.13518548011779785,271612:0.1346883773803711,6855:0.12929117679595947,6611:0.1282925009727478,10688:0.12761849164962769,7761:0.1274225115776062,2543:0.12623757123947144,11116:0.1261981725692749,18095:0.12613433599472046',\n",
       " '121067:10|78491:0.18848055601119995,81850:0.1707831621170044,39242:0.15564429759979248,39708:0.1449485421180725,136102:0.14143872261047363,21794:0.12231749296188354,23420:0.11714029312133789,20928:0.1116681694984436,48962:0.08036941289901733,32688:0.06183743476867676,341232:0.057290732860565186,74035:0.05664783716201782,134227:0.043599605560302734,111558:0.03346371650695801,31921:0.031239032745361328,83656:0.029225528240203857,94950:0.028342902660369873,127169:0.02761662006378174,83706:0.02546393871307373,90659:0.023502051830291748,96685:0.02033054828643799,384204:0.0094987154006958,45465:0.005761444568634033,9909:0.004642784595489502,20115:0.0045691728591918945',\n",
       " '114702:182662|241087:0.27349090576171875,132409:0.24080026149749756,101336:0.1977720856666565,110436:0.19694453477859497,214788:0.14876574277877808,131423:0.14143532514572144,171623:0.06725484132766724,146072:0.0617748498916626,167962:0.04787510633468628,146618:0.04734092950820923,110733:0.04195237159729004,250445:0.03973507881164551,179367:0.03913480043411255,104503:0.019745171070098877,134999:0.0144273042678833,140783:0.013951659202575684,134445:0.013875901699066162,107785:0.011172235012054443,99378:0.004602909088134766,225125:0.0029883384704589844,135531:0.0019707679748535156,91952:0.0005239248275756836,169989:0.0001380443572998047,161565:-0.0003210306167602539,212040:-0.0009949207305908203',\n",
       " '393232:390667|402073:0.34610259532928467,18518:0.3379458785057068,9559:0.33707553148269653,12329:0.3366038203239441,24407:0.3328889012336731,70125:0.33158302307128906,418254:0.3252626657485962,178235:0.3183317184448242,33085:0.3114076256752014,28922:0.3084663152694702,25999:0.3040781617164612,207188:0.28392183780670166,323444:0.21935945749282837,381845:0.21645814180374146,367948:0.208329439163208,19229:0.18039768934249878,2345:0.17863130569458008,9262:0.1748911738395691,185841:0.17179298400878906,179255:0.17177456617355347,188958:0.17173975706100464,179461:0.17138755321502686,170368:0.1702028512954712,172261:0.17008936405181885,169346:0.16983073949813843',\n",
       " '98399:98321|98321:0.40349823236465454,105908:0.347062885761261,138474:0.24286365509033203,126745:0.18329459428787231,91857:0.16450083255767822,100070:0.15625590085983276,122107:0.15593552589416504,203159:0.1538335680961609,186627:0.15084350109100342,91294:0.13538777828216553,103705:0.09161794185638428,104308:0.08333301544189453,181334:0.07356548309326172,144023:0.06995153427124023,147306:0.0689268708229065,110414:0.06302124261856079,152468:0.06049168109893799,300714:0.054081499576568604,58705:0.04284632205963135,149160:0.04094576835632324,147469:0.04084372520446777,69976:0.040520310401916504,250727:0.036241352558135986,144825:0.03607290983200073,69524:0.03458625078201294',\n",
       " '49170:47296|42080:0.72214075922966,42511:0.6932004392147064,68026:0.6889564990997314,57758:0.6861564815044403,62586:0.668340414762497,60649:0.6481612026691437,47267:0.6414072513580322,39205:0.48166823387145996,68021:0.48141372203826904,78244:0.4791308045387268,46203:0.47817665338516235,52338:0.4750697612762451,69728:0.4733082056045532,40123:0.47154325246810913,55510:0.47001439332962036,45635:0.46108072996139526,57897:0.4610005021095276,62429:0.4538177251815796,56823:0.44779783487319946,61812:0.4472237229347229,41475:0.44711703062057495,40333:0.4468936324119568,49287:0.44625961780548096,39819:0.44579553604125977,45078:0.44533276557922363',\n",
       " '49499:47296|42080:0.7316904664039612,42511:0.7019749581813812,68026:0.6875649392604828,57758:0.6796680092811584,62586:0.6353538036346436,47267:0.6315025687217712,60649:0.6279380917549133,78244:0.4836890697479248,68021:0.48270726203918457,52338:0.48027753829956055,39205:0.4797942638397217,69728:0.47516655921936035,40123:0.4745268225669861,46203:0.474500834941864,55510:0.4710565209388733,62429:0.4694322347640991,45635:0.4657771587371826,57897:0.45100098848342896,45078:0.45006442070007324,56823:0.450042188167572,40333:0.44920796155929565,41475:0.44912832975387573,49287:0.4442196488380432,46889:0.4436084032058716,59373:0.4434020519256592',\n",
       " '311313:304182|331301:0.7771997600793839,329383:0.4679347276687622,240753:0.4576755166053772,343564:0.4459376931190491,393957:0.4419132471084595,347602:0.4203369617462158,324051:0.4193359613418579,300819:0.32490217685699463,236797:0.31607329845428467,142596:0.2592301368713379,170633:0.2464197874069214,265183:0.2211560606956482,332276:0.22007179260253906,418263:0.21952307224273682,134216:0.21818619966506958,336726:0.21772289276123047,305087:0.2175115942955017,139344:0.2168644666671753,264813:0.21680724620819092,128358:0.2162899374961853,189585:0.21614402532577515,284835:0.21609115600585938,140756:0.21578192710876465,131248:0.2157353162765503,203191:0.21544963121414185',\n",
       " '311317:304182|304182:0.26455074548721313,320013:0.26171475648880005,105639:0.2554536461830139,217088:0.1891128420829773,331301:0.1330556869506836,349788:0.10949563980102539,233062:0.09412509202957153,240753:0.08235478401184082,343564:0.08047676086425781,69976:0.07665467262268066,347602:0.07290971279144287,329383:0.05973625183105469,105090:0.05717974901199341,91857:0.05504012107849121,188524:0.05362093448638916,250560:0.05114835500717163,379605:0.04220205545425415,147012:0.039761126041412354,101189:0.035204291343688965,69524:0.034395694732666016,58705:0.03326821327209473,216011:0.03272247314453125,62345:0.031168699264526367,118623:0.029771089553833008,355445:0.02949744462966919',\n",
       " '81942:81997|81997:0.4148038625717163,110920:0.4145928621292114,125364:0.4008845090866089,138119:0.2789883017539978,161801:0.2751646041870117,185875:0.255622923374176,331301:0.23046720027923584,405513:0.21930932998657227,393957:0.2003549337387085,343564:0.19423866271972656,305996:0.19342541694641113,117387:0.18661874532699585,153525:0.18661391735076904,362809:0.1857580542564392,317562:0.18351686000823975,243303:0.1812766194343567,241754:0.17972290515899658,347602:0.17845648527145386,305087:0.17754006385803223,112884:0.17584842443466187,336726:0.17525959014892578,101189:0.16845738887786865,205829:0.16585451364517212,380274:0.16422951221466064,300819:0.16233974695205688',\n",
       " '248836:214092|224997:0.45897823572158813,157012:0.43014442920684814,239227:0.3988351821899414,240344:0.3964194655418396,200031:0.39562201499938965,237326:0.391856849193573,236775:0.3891233801841736,255981:0.3863469362258911,241155:0.36587172746658325,233939:0.3491132855415344,237598:0.31959033012390137,228717:0.31293433904647827,244145:0.31259411573410034,261147:0.31256920099258423,223756:0.301413893699646,233297:0.27868688106536865,271612:0.26696616411209106,273211:0.26615965366363525,326085:0.2652502655982971,125043:0.2593856453895569,147051:0.24760288000106812,142314:0.24586445093154907,146869:0.2456405758857727,335976:0.24411451816558838,257969:0.2420656681060791',\n",
       " '244360:214092|207827:0.2985144853591919,132020:0.23156201839447021,150994:0.2156946063041687,260273:0.21478193998336792,142815:0.20914024114608765,3979:0.2056596279144287,382812:0.20539110898971558,12670:0.2046796679496765,141751:0.19931930303573608,144876:0.19772911071777344,320487:0.19629931449890137,213194:0.19184154272079468,148675:0.18762367963790894,149121:0.18473613262176514,150782:0.1846410036087036,39102:0.17813342809677124,38713:0.16759032011032104,369177:0.1641717553138733,42465:0.15692126750946045,67652:0.15680789947509766,42202:0.1554763913154602,40693:0.15541869401931763,42225:0.15485435724258423,53544:0.15317106246948242,73543:0.15063655376434326',\n",
       " '274801:214092|260123:0.8984564915299416,277193:0.8908394053578377,276887:0.8832191005349159,262970:0.8826009780168533,262966:0.8758572116494179,260514:0.8701757490634918,270280:0.8617746531963348,287149:0.8610178381204605,284329:0.8591303378343582,251460:0.8466194421052933,277713:0.8425914645195007,270948:0.8322622179985046,294986:0.8241846263408661,259181:0.8105148822069168,251447:0.7859485894441605,287234:0.7688618153333664,277195:0.7572976499795914,290395:0.746502161026001,255730:0.7233648896217346,188031:0.5071143507957458,172792:0.5015484690666199,170459:0.48997044563293457,160870:0.4825240969657898,274911:0.47796475887298584,361313:0.4646192193031311',\n",
       " '242965:214092|41255:0.4366080164909363,399790:0.41840916872024536,248163:0.40529102087020874,106402:0.4049251079559326,150749:0.398140013217926,150123:0.3976970911026001,424403:0.39420318603515625,37565:0.3765460252761841,293861:0.3713032603263855,87170:0.35052812099456787,99578:0.3498944640159607,93061:0.3415380120277405,186547:0.32881438732147217,144497:0.23925411701202393,373094:0.23032289743423462,175305:0.22212821245193481,368161:0.21972787380218506,24957:0.21499186754226685,244145:0.2144584059715271,66078:0.21410226821899414,20644:0.21361017227172852,10215:0.2134963870048523,23676:0.21313238143920898,261147:0.2123371958732605,228717:0.21233433485031128']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84097606, 0.6328083 ], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_triplet_train, train_input_sample, train_input_pos, train_input_neg, train_sim = baseline.batch_iterator(baseline.train_data, baseline.dup_sets_train, 64, 1)\n",
    "train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'],\n",
    "           train_input_pos['title'], train_input_pos['description'], train_input_pos['info'], \n",
    "           train_input_neg['title'], train_input_neg['description'], train_input_neg['info']]\n",
    "    \n",
    "h = similarity_model.predict_on_batch(train_batch)\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recall@25 last epoch:', 0.23)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Between 0-10 epochs recall@25 = 0.31\n",
    "    Between 0-20 epochs recall@25 = 0.35\n",
    "    Between 0-70 epochs recall@25 = ?\n",
    "    Between 0-100 epochs recall@25 = ?\n",
    "'''\n",
    "recall, exported_rank = evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets)\n",
    "\n",
    "\"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loss=h.history['loss']\n",
    "# val_loss=h.history['val_loss']\n",
    "\n",
    "# plt.plot(loss, label='loss')\n",
    "# plt.plot(val_loss, label='val_loss')\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_normalized(a, b): # Cosine used in the siamese model\n",
    "    a = K.variable(a)\n",
    "    b = K.variable(b)\n",
    "    return K.eval(cosine_distance([a, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bugs of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'eclipse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96204 113046\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "if (DOMAIN == test):\n",
    "    bug_set = baseline.get_bug_set()\n",
    "    # Eclipse test\n",
    "    bug_id = [96204, np.random.choice(list(bug_set))] # non-duplicate {15196, 2}\n",
    "    # bug_id = [96204, 85581] # duplicate {85581, 96204, 106979}\n",
    "    dup_a, dup_b = bug_id\n",
    "    bug_a = bug_set[dup_a]\n",
    "    bug_b = bug_set[dup_b]\n",
    "\n",
    "    print(dup_a, dup_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preferences filter text cut off using default fonts on organization\n",
      "nationality strings on launch organization\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    print(bug_a['title'])\n",
    "    print(bug_b['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80127084\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_t = title_feature_model.predict(np.array([bug_a['title_word']]))[0]\n",
    "    bug_vector_b_t = title_feature_model.predict(np.array([bug_b['title_word']]))[0]\n",
    "    result = cosine_normalized(bug_vector_a_t, bug_vector_b_t)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_t, bug_vector_b_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the standard default font size for organization desktops is points at dpi at this size the message type filter text in the preferences dialog is being cut off\n",
      "product ibm web sphere integration developer toolkit file eclipse plugins org eclipse core runtime org eclipse core internal runtime messages properties message jobs internal person an internal error occurred during language string an internal error occurred during is displayed on nationality os for messages ko properties in the nl folder this seems translated\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    print(bug_a['description'])\n",
    "    print(bug_b['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65530145\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_d = desc_feature_model.predict(np.array([bug_a['description_word']]))[0]\n",
    "    bug_vector_b_d = desc_feature_model.predict(np.array([bug_b['description_word']]))[0]\n",
    "    result = cosine_normalized(bug_vector_a_d, bug_vector_b_d)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_d, bug_vector_b_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9743171\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_i = categorical_feature_model.predict(np.array([retrieval.get_info(bug_a)]))[0]\n",
    "    bug_vector_b_i = categorical_feature_model.predict(np.array([retrieval.get_info(bug_b)]))[0]\n",
    "    result = cosine_normalized(bug_vector_a_i, bug_vector_b_i)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a_i, bug_vector_b_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97426474\n"
     ]
    }
   ],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a = np.concatenate([ bug_vector_a_i, bug_vector_a_t, bug_vector_a_d ], -1)\n",
    "    bug_vector_b = np.concatenate([ bug_vector_b_i, bug_vector_b_t, bug_vector_b_d ], -1)\n",
    "    result = cosine_normalized(bug_vector_a, bug_vector_b)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (DOMAIN == test):\n",
    "    bug_vector_a, bug_vector_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 7253\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_feature_100epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 43)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          6105100     title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          6249492     desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,859,492\n",
      "Trainable params: 12,859,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank = evaluate_validation_test(0, model, retrieval.test, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'exported_rank.txt'), 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.33,\n",
       " '2 - recall_at_10': 0.39,\n",
       " '3 - recall_at_15': 0.43,\n",
       " '4 - recall_at_20': 0.47,\n",
       " '5 - recall_at_25': 0.5}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Eclipse\n",
    "    With CNN print all embeddings zero and 2 epochs\n",
    "    {'1 - recall_at_5': 0.13,\n",
    "     '2 - recall_at_10': 0.18,\n",
    "     '3 - recall_at_15': 0.22,\n",
    "     '4 - recall_at_20': 0.24}\n",
    "     Without relu activation for each feature siamese in 100 epochs\n",
    "     {'1 - recall_at_5': 0.16,\n",
    "     '2 - recall_at_10': 0.23,\n",
    "     '3 - recall_at_15': 0.27,\n",
    "     '4 - recall_at_20': 0.31}\n",
    "     Without dense in the last layer with 100 epochs with embed trainable\n",
    "     {'1 - recall_at_5': 0.16,\n",
    "     '2 - recall_at_10': 0.22,\n",
    "     '3 - recall_at_15': 0.26,\n",
    "     '4 - recall_at_20': 0.3}\n",
    "      \n",
    "      {'1 - recall_at_5': 0.16,\n",
    "         '2 - recall_at_10': 0.22,\n",
    "         '3 - recall_at_15': 0.26,\n",
    "         '4 - recall_at_20': 0.29,\n",
    "         '5 - recall_at_25': 0.29}\n",
    "    With title (100 padding) and desc (500 padding) and batch refactored\n",
    "        {'1 - recall_at_5': 0.2,\n",
    "         '2 - recall_at_10': 0.26,\n",
    "         '3 - recall_at_15': 0.3,\n",
    "         '4 - recall_at_20': 0.33,\n",
    "         '5 - recall_at_25': 0.33}\n",
    "         \n",
    "         {'1 - recall_at_5': 0.2,\n",
    "         '2 - recall_at_10': 0.27,\n",
    "         '3 - recall_at_15': 0.31,\n",
    "         '4 - recall_at_20': 0.34,\n",
    "         '5 - recall_at_25': 0.34}\n",
    "         With recall in validation step and split 90 train 10 to test\n",
    "         {'1 - recall_at_5': 0.25,\n",
    "         '2 - recall_at_10': 0.32,\n",
    "         '3 - recall_at_15': 0.37,\n",
    "         '4 - recall_at_20': 0.4,\n",
    "         '5 - recall_at_25': 0.4}\n",
    "         With 200 epochs validation_recall@25 = 58, optimizer=Nadam\n",
    "         {'1 - recall_at_5': 0.26,\n",
    "         '2 - recall_at_10': 0.34,\n",
    "         '3 - recall_at_15': 0.39,\n",
    "         '4 - recall_at_20': 0.42,\n",
    "         '5 - recall_at_25': 0.42}\n",
    "         With 100 epochs validation_recall@25 = 52, optimizer=Adam\n",
    "         {'1 - recall_at_5': 0.23,\n",
    "         '2 - recall_at_10': 0.3,\n",
    "         '3 - recall_at_15': 0.34,\n",
    "         '4 - recall_at_20': 0.37,\n",
    "         '5 - recall_at_25': 0.37}\n",
    "        With 1000 epochs validation_recall@25=60, optimizer=Nadam\n",
    "        {'1 - recall_at_5': 0.24,\n",
    "         '2 - recall_at_10': 0.32,\n",
    "         '3 - recall_at_15': 0.37,\n",
    "         '4 - recall_at_20': 0.41,\n",
    "         '5 - recall_at_25': 0.41}\n",
    "         With 1000 epochs validation_recall@25=64, optimizer=Nadam\n",
    "         {'1 - recall_at_5': 0.28,\n",
    "         '2 - recall_at_10': 0.36,\n",
    "         '3 - recall_at_15': 0.41,\n",
    "         '4 - recall_at_20': 0.45,\n",
    "         '5 - recall_at_25': 0.45}\n",
    "         Withou change the distance x when calculate the cosine\n",
    "         {'1 - recall_at_5': 0.18,\n",
    "         '2 - recall_at_10': 0.24,\n",
    "         '3 - recall_at_15': 0.28,\n",
    "         '4 - recall_at_20': 0.31,\n",
    "         '5 - recall_at_25': 0.31}\n",
    "         With concatenation\n",
    "         {'1 - recall_at_5': 0.23,\n",
    "         '2 - recall_at_10': 0.31,\n",
    "         '3 - recall_at_15': 0.36,\n",
    "         '4 - recall_at_20': 0.4,\n",
    "         '5 - recall_at_25': 0.43}\n",
    "             \n",
    "    # Open Office\n",
    "    {'1 - recall_at_5': 0.2,\n",
    "     '2 - recall_at_10': 0.27,\n",
    "     '3 - recall_at_15': 0.31,\n",
    "     '4 - recall_at_20': 0.34,\n",
    "     '5 - recall_at_25': 0.34}\n",
    "'''\n",
    "evaluation = Evaluation()\n",
    "report = evaluation.evaluate(os.path.join(path, 'exported_rank.txt'))\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
