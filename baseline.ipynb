{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19Oo5yKCrXYG"
   },
   "source": [
    "## Dataset bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "CxFKGhm9repF",
    "outputId": "d78a02ef-882f-4202-d1ea-7d709171b992"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pair = pd.read_csv('train_mozilla_firefox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_ids = []\n",
    "with open(os.path.join(DIR, 'bug_ids.txt'), 'r') as f:\n",
    "    for row in f:\n",
    "        bug_ids.append(int(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pickle as pickle\n",
    "\n",
    "def padding_embed(max_char, field, bug):\n",
    "    n = len(bug[field])\n",
    "    if (max_char - n) > 0: # desc or title\n",
    "        embed = np.empty(max_char - n)\n",
    "        embed.fill(0)\n",
    "        embed = np.concatenate([embed, bug[field]], axis=-1)\n",
    "        embed = embed.astype(int)\n",
    "    else:\n",
    "        embed = np.array(bug[field][:max_char])\n",
    "    return embed\n",
    "\n",
    "sentence_dict = {}\n",
    "corpus = []\n",
    "\n",
    "for bug_id in bug_ids:\n",
    "    bug = pickle.load(open(os.path.join('bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "#     print(str(bug['title_word']))\n",
    "    title = padding_embed(40, 'title_word', bug)\n",
    "    desc = padding_embed(200, 'description_word', bug)\n",
    "    #print(len(title), len(desc))\n",
    "    #print(\",\".join(title.astype(str)))\n",
    "    sentence_dict[\",\".join(title.astype(str))] = bug['title']\n",
    "    sentence_dict[\",\".join(desc.astype(str))] = bug['description']\n",
    "    corpus.append(bug['title'])\n",
    "    corpus.append(bug['description'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8RdOXdw8q6rN",
    "outputId": "d98e260f-d633-44db-d1ff-50f0a2ae7cd5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 40\n",
    "MAX_SEQUENCE_LENGTH_D = 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0WZNngemNM8"
   },
   "source": [
    "## Geração de batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "### Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "train_data, bug_ids, dup_sets = None, None, None\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids):\n",
    "  neg_bug = random.choice(bug_ids)\n",
    "  while neg_bug in invalid_bugs:\n",
    "    neg_bug = random.choice(bug_ids)\n",
    "  return neg_bug\n",
    "\n",
    "def read_train_data(data):\n",
    "  data_pairs = []\n",
    "  data_dup_sets = {}\n",
    "  print('Reading train data')\n",
    "  with open(os.path.join(data, 'train.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "      bug1, bug2 = line.strip().split()\n",
    "      data_pairs.append([int(bug1), int(bug2)])\n",
    "      if int(bug1) not in data_dup_sets.keys():\n",
    "        data_dup_sets[int(bug1)] = set()\n",
    "      data_dup_sets[int(bug1)].add(int(bug2))\n",
    "  return data_pairs, data_dup_sets\n",
    "\n",
    "def read_bug_ids(data):\n",
    "  bug_ids = []\n",
    "  print('Reading bug ids')\n",
    "  with open(os.path.join(data, 'bug_ids.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "      bug_ids.append(int(line.strip()))\n",
    "  return bug_ids\n",
    "\n",
    "# data - path\n",
    "def prepare_dataset(data):\n",
    "  global train_data\n",
    "  global dup_sets\n",
    "  global bug_ids\n",
    "  if not train_data:\n",
    "    train_data, dup_sets = read_train_data(data)\n",
    "    #print(len(train_data))\n",
    "  if not bug_ids:\n",
    "    bug_ids = read_bug_ids(data)\n",
    "\n",
    "def siam_gen(data, batch_size, n_neg):\n",
    "   input_sample, input_pos, input_neg, sim = batch_iterator(data, batch_size, n_neg)\n",
    "   yield { 'title_in' : input_sample['title'], 'title_pos': input_pos['title'], 'title_neg' : input_neg['title'],\n",
    "               'desc_in' : input_sample['description'], 'desc_pos' : input_pos['description'], 'desc_neg' : input_neg['description'] }, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92651"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_set = {}\n",
    "\n",
    "for bug_id in bug_ids:\n",
    "    bug_set[bug_id] = pickle.load(open(os.path.join(bug_dir, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "    \n",
    "len(bug_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KefeL9y2puC"
   },
   "outputs": [],
   "source": [
    "def read_batch_triplets(batch_triplets, data):\n",
    "  batch_input_bugs = []\n",
    "  batch_pos_bugs = []\n",
    "  batch_neg_bugs = []\n",
    "  for triplet in batch_triplets:\n",
    "    batch_input_bugs.append(triplet[0])\n",
    "    batch_pos_bugs.append(triplet[1])\n",
    "    batch_neg_bugs.append(triplet[2])\n",
    "  return read_batch_bugs(batch_input_bugs, data), \\\n",
    "         read_batch_bugs(batch_pos_bugs, data), \\\n",
    "         read_batch_bugs(batch_neg_bugs, data)\n",
    "\n",
    "def read_batch_bugs(batch_bugs, data, test=False):\n",
    "  global bug_set\n",
    "  desc_word = []\n",
    "  short_desc_word = []\n",
    "  for bug_id in batch_bugs:\n",
    "    #bug = pickle.load(open(os.path.join(data, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "    bug = bug_set[bug_id]\n",
    "    desc_word.append(bug['description_word'])\n",
    "    short_desc_word.append(bug['title_word'])\n",
    "    \n",
    "  desc_word = data_padding(desc_word, 200)\n",
    "  short_desc_word = data_padding(short_desc_word, 40)\n",
    "  batch_bugs = dict()\n",
    "  batch_bugs['desc'] = (desc_word)\n",
    "  batch_bugs['title'] = (short_desc_word)\n",
    "\n",
    "  return batch_bugs\n",
    "\n",
    "def data_padding(data, max_seq_length):\n",
    "  seq_lengths = [len(seq) for seq in data]\n",
    "  seq_lengths.append(6)\n",
    "  #max_seq_length = max_seq_length\n",
    "  #print(seq_lengths)\n",
    "  #max_seq_length = min(max(seq_lengths), max_seq_length)\n",
    "  padded_data = np.zeros(shape=[len(data), max_seq_length])\n",
    "  for i, seq in enumerate(data):\n",
    "    seq = seq[:max_seq_length]\n",
    "    padding_end = max_seq_length - len(seq)\n",
    "    #print(seq)\n",
    "    for j, token in enumerate(seq):\n",
    "      # padding_end = padding_end + j\n",
    "#       print(padding_end + j)\n",
    "      padded_data[i, padding_end + j] = int(token)\n",
    "  return padded_data.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QWkYiz77MDR"
   },
   "outputs": [],
   "source": [
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(data, batch_size, n_neg):\n",
    "  global train_data\n",
    "  global dup_sets\n",
    "  global bug_ids\n",
    "  random.shuffle(train_data)\n",
    "  num_batches = int(len(train_data) / batch_size)\n",
    "  if len(data) % batch_size > 0:\n",
    "    num_batches += 1\n",
    "  # print(num_batches)\n",
    "  # loop = tqdm(range(num_batches))\n",
    "  # loop.set_description('Training')\n",
    "  for i in range(num_batches):\n",
    "    batch_triplets = []\n",
    "    for j in range(batch_size):\n",
    "      offset = batch_size * i + j\n",
    "      if offset >= len(train_data):\n",
    "        break\n",
    "      for i in range(n_neg):\n",
    "        neg_bug = get_neg_bug(dup_sets[train_data[offset][0]], bug_ids)\n",
    "        batch_triplets.append([train_data[offset][0], train_data[offset][1], neg_bug])\n",
    "    #yield loop, read_batch_triplets(batch_triplets, data)\n",
    "    batch_input, batch_pos, batch_neg = read_batch_triplets(batch_triplets, data)\n",
    "    n_half = batch_size // 2\n",
    "    pos = np.full((1, n_half), 1)\n",
    "    neg = np.full((1, n_half), 0)\n",
    "    sim = np.concatenate([pos, neg], -1)[0]\n",
    "\n",
    "#     title_a, title_b, desc_a, desc_b = [], [], [], []\n",
    "    \n",
    "#     print(dir(batch_input['title'][:n_half]))\n",
    "    \n",
    "#     title_a += [row for row in batch_input['title'][:n_half]]\n",
    "#     title_b += [row for row in batch_pos['title'][:n_half]]\n",
    "#     title_a += [row for row in batch_input['title'][:n_half]]\n",
    "#     title_b += [row for row in batch_neg['title'][:n_half]]\n",
    "\n",
    "#     desc_a += [row for row in batch_input['desc'][:n_half]]\n",
    "#     desc_b += [row for row in batch_pos['desc'][:n_half]]\n",
    "#     desc_a += [row for row in batch_input['desc'][:n_half]]\n",
    "#     desc_b += [row for row in batch_neg['desc'][:n_half]]\n",
    "    \n",
    "#     print(\"Title a\", np.array(title_a))\n",
    "    \n",
    "    \n",
    "#     return np.stack(title_a, 0), np.stack(title_b, 0), np.stack(desc_a, 0), np.stack(desc_b, 0), sim.reshape(-1)\n",
    "\n",
    "    input_sample, input_pos, input_neg = {}, {}, {}\n",
    "    \n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'] }\n",
    "    \n",
    "    return input_sample, input_pos, input_neg, sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WIPZrQ7J8UNc",
    "outputId": "dfefbddf-9fa8-4135-844f-1632522a94df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bug_dir = os.path.join(DIR)\n",
    "\n",
    "prepare_dataset(bug_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "train_gen = siam_gen(bug_dir, batch_size, 1)\n",
    "valid_input_sample, valid_input_pos, valid_input_neg, valid_sim = batch_iterator(bug_dir, 64, 1)\n",
    "test_gen = ([valid_input_sample['title'], valid_input_pos['title'], valid_input_neg['title'], \n",
    "             valid_input_sample['description'], valid_input_pos['description'], valid_input_neg['description']], valid_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 40), (64, 200), (64,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pnMQT0-Soml"
   },
   "outputs": [],
   "source": [
    "def display_batch(groups, nb):\n",
    "  input_sample, input_pos, input_neg, v_sim = batch_iterator(groups, nb, 1)\n",
    "\n",
    "  t_a, t_b, d_a, d_b = [], [], [], []\n",
    "  \n",
    "  t_a = input_sample['title']\n",
    "  t_b = input_pos['title']\n",
    "  d_a = input_sample['description']\n",
    "  d_b = input_pos['description']\n",
    "#   v_sim = v_sim[0]\n",
    "  \n",
    "  for ta, tb, da, db, sim in zip(t_a, t_b, d_a, d_b, v_sim):\n",
    "    #print(ta.astype(str))\n",
    "    key_t_a = ','.join(ta.astype(str))\n",
    "    key_t_b = ','.join(tb.astype(str))\n",
    "    key_d_a = ','.join(da.astype(str))\n",
    "    key_d_b = ','.join(db.astype(str))\n",
    "    print(\"Title =\", sentence_dict[key_t_a])\n",
    "    print(\"Title =\", sentence_dict[key_t_b])\n",
    "    print(\"Description =\", sentence_dict[key_d_a])\n",
    "    print(\"Description =\", sentence_dict[key_d_b])\n",
    "    print(\"similar =\", str(sim))\n",
    "    print(\"########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title = installer ui badly needs replacement\n",
      "Title = unable to create new folder with custom install winnumber ntnumber\n",
      "Description = user agent mozilla number windows u windows nt number en us rv number gecko number firefox number build identifier mozilla number windows u windows nt number en us rv number gecko number firefox number it is very difficult to use firefoxs installer to install firefox to a custom location it is so bad that it uncovered the latent dataloss bug that had lain hidden for years in netscapes and mozillas installer specific problems number no editable location line an editable location line would for example allow a user to simply replace c program files mozilla firefox with d program files mozilla firefox number the creation of an installation directory is not automatic number a most installers allow you to navigate to a master directory and create a default installation subdirectory for example one should be able to navigate to d program files mozilla select that directory and have the installer create a mozilla firefox subdirectory inside it number b more to the point one ought to be able to navigate to d program files select it and have the installer create by default a mozilla firefox subdirectory for the installed browser number creating and naming a new installation directory is ferociously difficult requiring one to play games with the installers focus to create a new folder then rename it to something other than new folder from a users perspective the ui of the old mozilla installer was excellent it was good enough that no one ever knew there was a potential dataloss issue the excellence of the user interface meant that no one ever had a problem i have been told that the mozilla installer is a mares nest and must not be resurrected ok but suely its interface could be brought back heck even a standard interface from one of the installers by wise and microsoft would be a welcome relief from what exists now reproducible always work around someone who already knows what the issues are can create a mozilla firefox directory in the location where it is to be installed this allows the installer to be used without the mega hassle involved in trying to create a new folder\n",
      "Description = user agent mozilla number windows u winnumber en us rv numberb gecko number firebird number build identifier mozilla number windows u winnumber en us rv numberb gecko number firebird number just like the mozilla installer the firebird installer should allow the user to create a new directory on the hard drive in which the firebird files may be installed too currently if you type in something such as c program files mozilla firebirdthisisanewfolder into the browse box the ok button simply greys out reproducible always steps to reproduce number download a number branch nightly installer number run it and choose the custom installation number click on browse number type into the field c program files mozilla firebirdthisisanewfolder actual results the ok button greys out expected results it shouldve kept the ok button active so that the folder mozilla firebirdthisisanewfolder will be created under the program files directory for the firebird installation files to be installed into\n",
      "similar = 1\n",
      "########################\n",
      "Title = video ceases during attempt to adjust volume control slider\n",
      "Title = embed media player when i click play video disappear\n",
      "Description = user agent mozilla number windows u windows nt number en us rv number gecko number firefox number build identifier mozilla number windows u windows nt number en us rv number gecko number firefox number when playing video at the site attempts to drag the on screeen volume control slider using the mouse results in a blank screen audio continues as normal reproducible always steps to reproduce number play any video file at the http www mnumber org site number during play attempt to adjust sound volume using the slider number actual results blank screen with audio continuing normally expected results allowed adjustment of sound volume and maintained proper video display\n",
      "Description = user agent mozilla number windows u windows nt number it it rv number gecko number firefox number build identifier mozilla number windows u windows nt number it it rv number gecko number firefox number please visit this link and click play in one of the video in the page http www gunbus biz html version trailers main number html reproducible always steps to reproduce number go here http www gunbus biz html version trailers main number html number click play number see the bug video disappear actual results video disappear expected results i have to see the video\n",
      "similar = 0\n",
      "########################\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "display_batch(bug_dir, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojKT88YudQxO"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2S7PiEM7WxGN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def word_index_count(corpus):\n",
    "  \n",
    "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  word_index = tokenizer.word_index\n",
    "  print('Found %s unique tokens.' % len(word_index))\n",
    "  \n",
    "  return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1917494 word vectors in Glove 42B 300d.\n",
      "Found 138251 unique tokens.\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "GLOVE_DIR = \"\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.42B.300d.txt'), 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "word_index = word_index_count(corpus)\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Feature_BugInput (InputLayer)   (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     41475600    Feature_BugInput[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 32)      28832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 32)      48032       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 66, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 65, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 131, 32)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 129, 64)      6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 41,558,672\n",
      "Trainable params: 83,072\n",
      "Non-trainable params: 41,475,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D \n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "def cnn_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "\n",
    "  embedding_layer = Embedding(num_words,\n",
    "                              embedding_dim,\n",
    "                              weights=[embeddings],\n",
    "                              input_length=max_sequence_length,\n",
    "                              trainable=trainable)\n",
    "\n",
    "  sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "  # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "  convs = []\n",
    "  filter_sizes = [3,5]\n",
    "\n",
    "  for filter_size in filter_sizes:\n",
    "      l_conv = Conv1D(filters=32, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "      l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "      convs.append(l_pool)\n",
    "\n",
    "  # l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "\n",
    "  l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "  # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "  conv = Conv1D(filters=64, kernel_size=3, activation='relu')(l_merge)\n",
    "  pool = GlobalMaxPooling1D()(conv) # pool_size=3\n",
    "  # Original Yoon Kim model\n",
    "  #x = Flatten()(pool)\n",
    "  #x = Dropout(0.5)(x)\n",
    "  layer = Activation('relu')(pool)\n",
    "\n",
    "  cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "  cnn_feature_model.summary()\n",
    "  \n",
    "  return cnn_feature_model\n",
    "\n",
    "\n",
    "cnn_feature_model = cnn_model(embeddings=embedding_matrix, \n",
    "                              num_words=len(word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Feature_BugInput (InputLayer (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 40, 300)           41475600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                85248     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 41,560,848\n",
      "Trainable params: 85,248\n",
      "Non-trainable params: 41,475,600\n",
      "_________________________________________________________________\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "\n",
    "def lstm_model(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "  number_lstm_units = 32\n",
    "  rate_drop_lstm = 0.75\n",
    "  recurrent_dropout = 0.25\n",
    "\n",
    "  embedding_layer = Embedding(num_words,\n",
    "                          embedding_dim,\n",
    "                          weights=[embeddings],\n",
    "                          input_length=max_sequence_length,\n",
    "                          trainable=trainable)\n",
    "\n",
    "  sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "   # Creating LSTM Encoder\n",
    "  lstm_layer = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "  x = lstm_layer(embedded_sequences)\n",
    "\n",
    "  layer = Activation('relu')(x)\n",
    "\n",
    "  lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "  lstm_feature_model.summary()\n",
    "  \n",
    "  return lstm_feature_model\n",
    "\n",
    "lstm_feature_model = lstm_model(embeddings=embedding_matrix, \n",
    "                              num_words=len(word_index) + 1, \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGx3qyWhmLMF"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "class MarginLoss(keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    bug_in, bug_pos, bug_neg  = inputs\n",
    "    loss = self.distance(bug_in, bug_pos, bug_neg)\n",
    "    self.add_loss(loss, inputs=inputs)\n",
    "    return bug_in\n",
    "  def distance(self, bug_in, bug_pos, bug_neg):\n",
    "    dis_pos, dis_neg = self.cos_distance(bug_in, bug_pos), self.cos_distance(bug_in, bug_neg)\n",
    "    ep = 1\n",
    "    d1 = K.maximum(0.0, dis_pos - dis_neg + ep)\n",
    "    return K.mean(d1)\n",
    "  \n",
    "  def euclidean_sim(bug_in, bug_pos, bug_neg):\n",
    "    dis_pos = K.sum(K.square(bug_in - bug_pos), axis=1, keepdims=True)\n",
    "    dis_neg = K.sum(K.square(bug_in - bug_neg), axis=1, keepdims=True)\n",
    "    dis_pos = K.sqrt(dis_pos)\n",
    "    dis_neg = K.sqrt(dis_neg)\n",
    "    return dis_pos, dis_neg\n",
    "  \n",
    "  def l2_normalize(self, x, axis):\n",
    "        norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=True))\n",
    "        return K.maximum(x, K.epsilon()) / K.maximum(norm, K.epsilon())\n",
    "  # https://github.com/keras-team/keras/issues/3031\n",
    "  def cos_distance(self, y_true, y_pred):\n",
    "    y_true = self.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = self.l2_normalize(y_pred, axis=-1)\n",
    "    return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
    "  \n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "  \n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_pos (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_neg (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 64)           41560848    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 64)           41558672    desc_in[0][0]                    \n",
      "                                                                 desc_pos[0][0]                   \n",
      "                                                                 desc_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 128)          0           FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 128)          0           FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureCNNGenerationModel[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 128)          0           FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureCNNGenerationModel[3][0]  \n",
      "__________________________________________________________________________________________________\n",
      "margin_loss_1 (MarginLoss)      [(None, 128), (None, 0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         margin_loss_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 83,119,778\n",
      "Trainable params: 168,578\n",
      "Non-trainable params: 82,951,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate, Add, Lambda\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def siamese_model(max_sequence_length_t, max_sequence_length_d):\n",
    "  \n",
    "  bug_t_in = Input(shape = (max_sequence_length_t, ), name = 'title_in')\n",
    "  bug_t_pos = Input(shape = (max_sequence_length_t, ), name = 'title_pos')\n",
    "  bug_t_neg = Input(shape = (max_sequence_length_t, ), name = 'title_neg')\n",
    "  \n",
    "  bug_d_in = Input(shape = (max_sequence_length_d, ), name = 'desc_in')\n",
    "  bug_d_pos = Input(shape = (max_sequence_length_d, ), name = 'desc_pos')\n",
    "  bug_d_neg = Input(shape = (max_sequence_length_d, ), name = 'desc_neg')\n",
    "\n",
    "  bug_t_in_feat_lstm = lstm_feature_model(bug_t_in)\n",
    "  bug_t_pos_feat_lstm = lstm_feature_model(bug_t_pos)\n",
    "  bug_t_neg_feat_lstm = lstm_feature_model(bug_t_neg)\n",
    "  \n",
    "  bug_d_in_feat_cnn = cnn_feature_model(bug_d_in)\n",
    "  bug_d_pos_feat_cnn = cnn_feature_model(bug_d_pos)\n",
    "  bug_d_neg_feat_cnn = cnn_feature_model(bug_d_neg)\n",
    "\n",
    "  encoded_anchor = concatenate([bug_t_in_feat_lstm, bug_d_in_feat_cnn], name = 'merge_features_in')\n",
    "  encoded_positive = concatenate([bug_t_pos_feat_lstm, bug_d_pos_feat_cnn], name = 'merge_features_pos')\n",
    "  encoded_negative = concatenate([bug_t_neg_feat_lstm, bug_d_neg_feat_cnn], name = 'merge_features_neg')\n",
    "\n",
    "  #combined = concatenate([combined_features_a, combined_features_b])\n",
    "#   combined_in_pos = Add()([encoded_anchor, encoded_positive])\n",
    "#   combined_in_neg = Add()([encoded_anchor, encoded_negative])\n",
    "#   combined = Add()([combined_in_pos, combined_in_neg])\n",
    "\n",
    "  \n",
    "  loss = MarginLoss()([encoded_anchor, encoded_positive, encoded_negative])\n",
    "  \n",
    "  # combined_features = Dense(100, activation = 'linear')(combined)\n",
    "  # combined_features = BatchNormalization()(combined_features)\n",
    "  # combined_features = Activation('relu')(combined_features)\n",
    "  # combined_features = Dense(4, activation = 'linear')(combined_features)\n",
    "  # combined_features = BatchNormalization()(combined_features)\n",
    "#   combined_features = Activation('relu')(combined_features)\n",
    "  loss = Dense(2, activation = 'softmax')(loss)\n",
    "  \n",
    "  \n",
    "  # Implementation https://stackoverflow.com/questions/52306282/implementing-triplet-loss-inside-keras-layers\n",
    "  # https://github.com/maciejkula/triplet_recommendations_keras\n",
    "#   margin = 1\n",
    "  \n",
    "#   DAP = Lambda(lambda tensors:K.sum(K.square(tensors[0] - tensors[1]),axis=1,keepdims=True),name='DAP_loss') #Distance for Anchor-Positive pair\n",
    "#   DAN = Lambda(lambda tensors:K.sum(K.square(tensors[0] - tensors[1]),axis=1,keepdims=True),name='DAN_loss') #Distance for Anchor-Negative pair\n",
    "#   Triplet_loss = Lambda(lambda loss: K.maximum(loss[0] - loss[1] + margin, 0.0), name='Triplet_loss') #Distance for Anchor-Negative pair\n",
    "\n",
    "#   DAP_loss = DAP([encoded_anchor,encoded_positive])\n",
    "#   DAN_loss = DAN([encoded_anchor,encoded_negative])\n",
    "\n",
    "#   #call this layer on list of two input tensors.\n",
    "#   Final_loss = Triplet_loss([DAP_loss,DAN_loss])\n",
    "  \n",
    "  adam = Adam(lr=0.05, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  \n",
    "  similarity_model = Model(inputs = [bug_t_in, bug_t_pos, bug_t_neg, bug_d_in, bug_d_pos, bug_d_neg], outputs = loss, name = 'Similarity_Model')\n",
    "  # setup the optimization process \n",
    "  similarity_model.compile(optimizer=adam, loss = margin_loss, metrics = ['accuracy']) # 'binary_crossentropy'\n",
    "  similarity_model.summary()\n",
    "  \n",
    "  return similarity_model\n",
    "\n",
    "similarity_model = siamese_model(MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOK2zcnxd5PL"
   },
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "colab_type": "code",
    "id": "q8AOthGid3lS",
    "outputId": "95e97ceb-c879-433f-b479-6a8719940891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/10 [==>...........................] - ETA: 3:52 - loss: 1.2675 - acc: 0.9531"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# early = EarlyStopping(monitor='loss', patience = 5, min_delta=0, verbose=0)\n",
    "\n",
    "h = similarity_model.fit_generator(train_gen, \n",
    "                               steps_per_epoch = 10,\n",
    "                               validation_data=test_gen,\n",
    "                                             epochs = 10,\n",
    "                                             verbose = True) # validation_steps=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1j41YFByes7o"
   },
   "outputs": [],
   "source": [
    "# pred_t_a, pred_t_b, pred_t_sim = gen_random_batch(test_groups, 10)\n",
    "# validation_accuracy_loss(h)\n",
    "# curve_roc_auc(similarity_model, x=[pred_t_a, pred_t_b], y_valid=pred_t_sim)\n",
    "# _ = show_model_output(pred_t_a, pred_t_b, pred_t_sim, similarity_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLxWmHaU8y7O"
   },
   "source": [
    "### Plot ROC/AUC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srKKbyM2ffZm"
   },
   "source": [
    "### Plot validation accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULjGk-eCfT8B"
   },
   "outputs": [],
   "source": [
    "def validation_accuracy_loss(history):\n",
    "  acc=history.history['acc']\n",
    "  val_acc=history.history['val_acc']\n",
    "  loss=history.history['loss']\n",
    "  val_loss=history.history['val_loss']\n",
    "\n",
    "  plt.plot(acc, label='acc')\n",
    "  plt.plot(val_acc, label='val_acc')\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(loss, label='acc')\n",
    "  plt.plot(val_loss, label='val_acc')\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOOHpF2j6oY8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics\n",
    "\n",
    "def curve_roc_auc(model, x, y_valid):\n",
    "  y_hat = model.predict(x)\n",
    "  pct_auc = roc_auc_score(y_valid, y_hat) * 100\n",
    "  #print('ROC/AUC: {:0.2f}'.format(pct_auc))\n",
    "\n",
    "  fpr, tpr, _ = sklearn.metrics.roc_curve(y_valid, y_hat)\n",
    "  roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "  plt.figure()\n",
    "  lw = 2\n",
    "  plt.plot(fpr, tpr, color='darkorange',\n",
    "           lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('Taxa de Falsos Positivos')\n",
    "  plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "  plt.title('Receiver Operating Characteristic (ROC)')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zQaDKGoHB_s"
   },
   "outputs": [],
   "source": [
    "def show_model_output(valid_a, valid_b, valid_sim, model, nb_examples = 3):\n",
    "    #pv_a, pv_b, pv_sim = gen_random_batch(test_groups, nb_examples)\n",
    "    pred_sim = model.predict([valid_a, valid_b])\n",
    "#     pred_sim = [1,1,1,1,1,1]\n",
    "    for b_a, b_b, sim, pred in zip(valid_a, valid_b, valid_sim, pred_sim):\n",
    "        key_a = ','.join(b_a.astype(str))\n",
    "        key_b = ','.join(b_b.astype(str))\n",
    "        print(sentence_dict[key_a])\n",
    "        print(sentence_dict[key_b])\n",
    "        print(\"similar=\" + str(sim))\n",
    "        print(\"prediction=\" + str(pred[0]))\n",
    "        print(\"########################\")\n",
    "    return valid_a, valid_b, valid_sim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
