{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose BERT siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "# from tensorflow.python import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K_tf\n",
    "\n",
    "# sess = K_tf.get_session()\n",
    "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "# init_op = tf.variables_initializer(\n",
    "#     [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    "# )\n",
    "# sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 40\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 200\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'propose_bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0047c9dbe40d43e9a98ed54127b15456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=322339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ec6bb5634a45a5b49fbea1902a379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip -o uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import _pickle as pickle\n",
    "# from collections import defaultdict\n",
    "# from keras_bert import get_base_dict\n",
    "\n",
    "# def build_vocab(baseline):\n",
    "#     print('building frequency dictionaries')\n",
    "#     word_freq = get_base_dict()  # A dict that contains some special tokens\n",
    "#     for bug_id in tqdm(baseline.bug_ids):\n",
    "#         bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "#         title_txt = \"[CLS] \" + bug['title'] + \" [SEP]\"\n",
    "#         desc_txt = \"[CLS] \" + bug['description'] + \" [SEP]\"\n",
    "#         for word in title_txt.split():\n",
    "#             if word not in word_freq:\n",
    "#                 word_freq[word] = 0\n",
    "#             word_freq[word] += 1\n",
    "#         for word in desc_txt.split():\n",
    "#             if word not in word_freq:\n",
    "#                 word_freq[word] = 0\n",
    "#             word_freq[word] += 1\n",
    "#     return word_freq\n",
    "# token_dict = build_vocab(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = np.random.choice(range(len(token_dict.keys())), 10)\n",
    "# vocab_words = list(token_dict.keys())\n",
    "# for token in words:\n",
    "#     print(vocab_words[token], token_dict[vocab_words[token]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "from keras_bert import Tokenizer\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "\n",
    "def load_bugs(baseline):\n",
    "    global tokenizer\n",
    "    removed = []\n",
    "    baseline.corpus = []\n",
    "    baseline.sentence_dict = {}\n",
    "    baseline.bug_set = {}\n",
    "    title_padding, desc_padding = [], []\n",
    "    for bug_id in tqdm(baseline.bug_ids):\n",
    "        #try:\n",
    "            bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "            title_txt = \"[CLS] \" + bug['title'] + \" [SEP]\"\n",
    "            desc_txt = \"[CLS] \" + bug['description'] + \" [SEP]\"\n",
    "            #print(title_txt, desc_txt)\n",
    "            ids, segments = tokenizer.encode(title_txt, max_len=MAX_SEQUENCE_LENGTH_T)\n",
    "            title_padding.append(ids)\n",
    "            ids, segments = tokenizer.encode(desc_txt, max_len=MAX_SEQUENCE_LENGTH_D)\n",
    "            desc_padding.append(ids)\n",
    "            baseline.bug_set[bug_id] = bug\n",
    "            #break\n",
    "        #except:\n",
    "            removed.append(bug_id)    \n",
    "    # Padding\n",
    "    title_padding = baseline.data_padding(title_padding, baseline.MAX_SEQUENCE_LENGTH_T)\n",
    "    desc_padding = baseline.data_padding(desc_padding, baseline.MAX_SEQUENCE_LENGTH_D)\n",
    "    \n",
    "    for bug_id, bug_title, bug_desc in tqdm(zip(baseline.bug_ids, title_padding, desc_padding)):\n",
    "            baseline.bug_set[bug_id]['title_word'] = bug_title\n",
    "            baseline.bug_set[bug_id]['description_word'] = bug_desc\n",
    "            bug = baseline.bug_set[bug_id]\n",
    "            baseline.sentence_dict[\",\".join(bug_title.astype(str))] = bug['title']\n",
    "            baseline.sentence_dict[\",\".join(bug_desc.astype(str))] = bug['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde8d08dfd044959b1d001e8fe057012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_bugs(baseline)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n",
      "CPU times: user 2min 36s, sys: 4.97 ms, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '655\\n',\n",
       " 'creation_ts': '2011-04-18 14:16:00 -0400',\n",
       " 'delta_ts': '2011-05-02 12:44:45 -0400',\n",
       " 'description': 'build identifier mnemonic char is not underscore in preview mode and design view see attachment for details reproducible always steps to reproduce see attachment',\n",
       " 'description_word': array([  101,  1031, 18856,  2015,  1033,  3857,  8909,  4765, 18095,\n",
       "        24098, 26941,  2594, 25869,  2003,  2025,  2104,  9363,  2890,\n",
       "         1999, 19236,  5549,  1998,  2640,  3193,  2156, 14449,  2005,\n",
       "         4751, 16360, 14127, 21104,  2467,  4084,  2000, 21376,  2156,\n",
       "        14449,  1031, 19802,  1033,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 343188,\n",
       " 'priority': '0\\n',\n",
       " 'product': '149\\n',\n",
       " 'resolution': 'INVALID',\n",
       " 'title': 'mnemonic char is not underscore in preview mode and design view',\n",
       " 'title_word': array([  101,  1031, 18856,  2015,  1033, 24098, 26941,  2594, 25869,\n",
       "         2003,  2025,  2104,  9363,  2890,  1999, 19236,  5549,  1998,\n",
       "         2640,   102]),\n",
       " 'version': '179\\n'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 34882)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "\n",
    "import random\n",
    "\n",
    "def read_batch_bugs(self, batch, bug):\n",
    "        info = np.concatenate((\n",
    "            self.to_one_hot(bug['bug_severity'], self.info_dict['bug_severity']),\n",
    "            self.to_one_hot(bug['bug_status'], self.info_dict['bug_status']),\n",
    "            self.to_one_hot(bug['component'], self.info_dict['component']),\n",
    "            self.to_one_hot(bug['priority'], self.info_dict['priority']),\n",
    "            self.to_one_hot(bug['product'], self.info_dict['product']),\n",
    "            self.to_one_hot(bug['version'], self.info_dict['version']))\n",
    "        )\n",
    "        #info.append(info_)\n",
    "        batch['info'].append(info)\n",
    "        batch['title'].append(bug['title_word'])\n",
    "        batch['desc'].append(bug['description_word'])\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids, issues_by_buckets):\n",
    "    neg_bug = random.choice(list(issues_by_buckets.keys()))\n",
    "    try:\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    except:\n",
    "        invalid_bugs = [invalid_bugs]\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    return neg_bug\n",
    "\n",
    "def batch_iterator(baseline, data, dup_sets, bug_train_ids, batch_size, n_neg, issues_by_buckets):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg, master_batch_input, master_batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []},\\\n",
    "                                                    {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                        {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets = []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        neg_bug = baseline.get_neg_bug(dup_sets[data[offset][0]], bug_train_ids)\n",
    "        anchor, pos, neg = data[offset][0], data[offset][1], neg_bug\n",
    "        bug_anchor = baseline.bug_set[anchor]\n",
    "        bug_pos = baseline.bug_set[pos]\n",
    "        bug_neg = baseline.bug_set[neg]\n",
    "        # master anchor and neg\n",
    "        master_anchor = baseline.bug_set[issues_by_buckets[anchor]]\n",
    "        master_neg = baseline.bug_set[issues_by_buckets[neg]]\n",
    "        \n",
    "        baseline.read_batch_bugs(batch_input, bug_anchor)\n",
    "        baseline.read_batch_bugs(batch_pos, bug_pos)\n",
    "        baseline.read_batch_bugs(batch_neg, bug_neg)\n",
    "        # master anchor and neg\n",
    "        baseline.read_batch_bugs(master_batch_input, master_anchor)\n",
    "        baseline.read_batch_bugs(master_batch_neg, master_neg)\n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([data[offset][0], data[offset][1], neg_bug, master_anchor, master_neg])\n",
    "        \n",
    "        \n",
    "    ids = np.full((len(batch_triplets), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "\n",
    "    batch_input['title'] = np.array(batch_input['title'])\n",
    "    batch_input['desc'] = { 'token' : np.array(batch_input['desc']), 'segment' : ids }\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = np.array(batch_pos['title'])\n",
    "    batch_pos['desc'] = { 'token' : np.array(batch_pos['desc']), 'segment' : ids }\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = np.array(batch_neg['title'])\n",
    "    batch_neg['desc'] = { 'token' : np.array(batch_neg['desc']), 'segment' : ids }\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "    \n",
    "    # master\n",
    "    master_batch_input['title'] = np.array(master_batch_input['title'])\n",
    "    master_batch_input['desc'] ={ 'token' : np.array(master_batch_input['desc']), 'segment' : ids }\n",
    "    master_batch_input['info'] = np.array(master_batch_input['info'])\n",
    "    \n",
    "    master_batch_neg['title'] = np.array(master_batch_neg['title'])\n",
    "    master_batch_neg['desc'] = { 'token' : np.array(master_batch_neg['desc']), 'segment' : ids }\n",
    "    master_batch_neg['info'] = np.array(master_batch_neg['info'])\n",
    "\n",
    "    n_half = len(batch_triplets) // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg, master_input_sample, master_neg = {}, {}, {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "    # master \n",
    "    master_input_sample = { 'title' : master_batch_input['title'], 'description' : master_batch_input['desc'], \n",
    "                           'info' : master_batch_input['info'] }\n",
    "    master_neg = { 'title' : master_batch_neg['title'], 'description' : master_batch_neg['desc'], \n",
    "                           'info' : master_batch_neg['info'] }\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, master_input_sample, master_neg, sim #sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.2 ms, sys: 0 ns, total: 49.2 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = batch_iterator(baseline, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, issues_by_buckets)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 100), (128, 1682), (128,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description']['token'].shape, \\\n",
    "    valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Title***: jface pde color manager leaks colors\n",
      "***Title***: jface jface colors should reuse colors\n",
      "***Description***: every time you open and close the pde plugin manifest editor you get number new colors allocated from organization manager they are always the same number number number number number number number number number number number number number number number heres the stack trace java lang error at organization at org eclipse swt graphics device new object unknown source at org eclipse swt graphics color init unknown source at org eclipse jface resource jface colors get color setting unknown source at org eclipse jface resource jface colors get hyperlink text unknown source at org eclipse update ui forms internal form widget factory update hyperlink colors unknown source at org eclipse update ui forms internal form widget factory initialize unknown source at org eclipse update ui forms internal form widget factory init unknown source at org eclipse update ui forms internal form widget factory init unknown source at org eclipse update ui forms internal abstract form init unknown source at org eclipse update ui forms internal abstract section form init unknown source at org eclipse update ui forms internal section form init unknown source at org eclipse update ui forms internal scrollable section form init unknown source at org eclipse pde internal ui editor manifest manifest form init unknown source at org eclipse pde internal ui editor manifest manifest form page create form unknown source at org eclipse pde internal ui editor pdeform page init unknown source at org eclipse pde internal ui editor pdeform page init unknown source at date unknown source at org eclipse pde internal ui editor manifest manifest editor create pages unknown source at org eclipse pde internal ui editor pdemulti page editor init unknown source at org eclipse pde internal ui editor manifest manifest editor init unknown source at java lang class new instance impl native method at java lang class new instance unknown source at org eclipse core internal plugins plugin dkeyboardriptor create executable extension unknown source at org eclipse core internal plugins plugin dkeyboardriptor create executable extension unknown source at org eclipse core internal plugins configuration element create executable extension unknown source at org eclipse ui internal workbench plugin create extension unknown source at org eclipse ui internal editor manager run unknown source at product at organization at org eclipse ui internal editor manager create part unknown source at org eclipse ui internal editor manager open internal editor unknown source at org eclipse ui internal editor manager open editor from dkeyboardriptor unknown source at org eclipse ui internal editor manager open editor unknown source at org eclipse ui internal workbench page open editor unknown source at org eclipse ui internal workbench page open editor unknown source at organization at org eclipse ui actions open with menu access unknown source at org eclipse ui actions open with menu handle event unknown source at org eclipse swt widgets event table send event unknown source at org eclipse swt widgets widget send event unknown source at organization at org eclipse swt widgets display read and dispatch unknown source at org eclipse ui internal workbench run event loop unknown source at organization at organization at org eclipse core boot boot loader run unknown source at java lang reflect accessible object invoke l unknown source at java lang reflect method invoke unknown source at org eclipse core launcher main basic run unknown source at org eclipse core launcher main run unknown source at org eclipse core launcher main main unknown source\n",
      "***Description***: number jface colors allocates a new product everytime i ask for the same thing test case for the sleak tool open a build properties editor close snap open again close organization new product are allocated both in jface colors at org eclipse jface resource jface colors get color setting unknown source at org eclipse jface resource jface colors get active hyperlink text unknown source and at org eclipse jface resource jface colors get color setting unknown source at organization so i seems that everytime someone asks jface colors for any color they get a new instance clients do not create them so they do not dispose them it should be possible to reuse them why are not they reused allocating them anew everytime is suboptimal\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: organization is thrown out in when getting table or column completion proposals\n",
      "***Title***: no available table prompt context when input schema in query\n",
      "***Description***: dkeyboardription in dataset editor i define query like select from root after input the dot table or column proposals did not show but organization was thrown out in error log page build number v actual result no table or column proposals but organization occurred error log caused by java lang null pointer exception at org eclipse birt report data oda jdbc ui editors jdbc sqlcontent assist processor get table or column completion proposals jdbc sqlcontent assist processor java at org eclipse birt report data oda jdbc ui editors jdbc sqlcontent assist processor compute completion proposals jdbc sqlcontent assist processor java at org eclipse jface text contentassist content assistant compute completion proposals content assistant java at org eclipse jface text contentassist completion proposal popup compute proposals completion proposal popup java at org eclipse jface text contentassist completion proposal popup access completion proposal popup java at org eclipse jface text contentassist completion proposal popup run completion proposal popup java at org eclipse swt custom busy indicator show while busy indicator java at org eclipse jface text contentassist completion proposal popup show proposals completion proposal popup java at org eclipse jface text contentassist content assistant run content assistant java at org eclipse swt widgets runnable lock run runnable lock java at org eclipse swt widgets synchronizer run async messages synchronizer java x timeore\n",
      "***Description***: in query editor when i input select from root i hope after the dot an available table prompt context will show to give the prompt like before build id v build steps to reproduce number new a sample or jdbc dataset number in query page input select from organization expected result after input dot an available tables context will give user a prompt\n",
      "***similar = 1\n",
      "########################\n",
      "***Title***: hotbug j ee ear libraries does not include organization files in directories\n",
      "***Title***: formatters should have option to use tabs only for leading indents\n",
      "***Description***: jar files included in any directory under organization are not included in the ear libraries classpath container i have imported an ear file that contains an product jar file as well as a jar file log j that resides the lib directory which the product jar depends on the product for the organization jar contains the following manifest version number class path lib log j jar the log j jar file is copied correctly to the organization directory upon importing but the organization project that is constructed does not contain product on the classpath nor in the ear libraries container upon further investigation i found that it was because the organization file is not in the earcontent directory but rather it is in a sub directory of the earcontent directory if i move the log j jar from the organization directory to the earcontent directory fix up the organization file manually due to bug date then the log j jar is added to the ear libraries container and therefore is on the classpath of my organization project\n",
      "***Description***: formatters should have option to use tabs only for leading indents\n",
      "***similar = 0\n",
      "########################\n",
      "***Title***: organization changing any filter breaks working sets filter\n",
      "***Title***: organization number error for preiew in different format\n",
      "***Description***: build id steps to reproduce step index create number projects project and project step index assign the projects to different working sets set and set step index open project explorer step index using select working set view menu select set step index the view shows project only step index using customize view view menu enable disable any filter like filters resources as is step index the view shows now both projects to be step index the view still shows organization only as no working set changes was made more information it look like product removes filter added by product see method execute line number common viewer reset filters organization visible filters filter service get visible filters true for int i i visible filters length organization common viewer add filter visible filters i the point is that product did not appear as a visible product for filter service as it works completely different so after reset filters call working set filter is gone forever the only way to get it back is to close open organization based view i found this bug working on a custom organization based view but later i have realized that this is an issue of the framework i think working set action provider should participate in the usual way in organization filtering\n",
      "***Description***: organization number error for preiew in different format\n",
      "***similar = 0\n",
      "########################\n",
      "CPU times: user 41.2 ms, sys: 8 µs, total: 41.2 ms\n",
      "Wall time: 40.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed_fasttext.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "#     embeddings_index = {}\n",
    "#     embed_path = os.path.join(EMBED_DIR, 'crawl-300d-2M.vec')\n",
    "#     f = open(embed_path, 'rb')\n",
    "#     f = io.open(embed_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, f.readline().split())\n",
    "\n",
    "#     vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed_fasttext.pkl'))\n",
    "#     vocab_size = len(vocab) \n",
    "\n",
    "#     # Initialize uniform the vector considering the Tanh activation\n",
    "#     embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "#     embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#     loop = tqdm(f)\n",
    "#     loop.set_description(\"Loading FastText\")\n",
    "#     for line in loop:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         embed = list(map(float, tokens[1:]))\n",
    "#         word = tokens[0]\n",
    "#         embeddings_index[word] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#     f.close()\n",
    "#     loop.close()\n",
    "\n",
    "#     print('Total %s word vectors in FastText 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "#     loop = tqdm(total=vocab_size)\n",
    "#     loop.set_description('Loading embedding from dataset pretrained')\n",
    "#     i = 0\n",
    "#     for word, embed in vocab.items():\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_matrix[i] = embeddings_index[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "#         loop.update(1)\n",
    "#         i+=1\n",
    "#     loop.close()\n",
    "#     baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, EMBED_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(EMBED_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d34ebde28747d18745c2668fe460aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe9a158259f4175920c84b52420dce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=149145), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 26s, sys: 3.66 s, total: 1min 30s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, EMBED_DIR=EMBED_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable, name):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer_{}'.format(name),\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def bert_model(MAX_SEQUENCE_LENGTH_D):\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH_D,\n",
    "#         )\n",
    "    \n",
    "    layer_num = 12\n",
    "    #with tf.get_default_graph().as_default():\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#         config_path,\n",
    "#         model_path,\n",
    "#         training=False,\n",
    "#         use_adapter=True,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH_D,\n",
    "#         trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(layer_num)] +\n",
    "#         ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(layer_num)] +\n",
    "#         ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(layer_num)] +\n",
    "#         ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "#     )\n",
    "    model = get_model(\n",
    "        token_num=len(token_dict),\n",
    "        head_num=5,\n",
    "        transformer_num=layer_num,\n",
    "        embed_dim=100,\n",
    "        feed_forward_dim=100,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH_D,\n",
    "        pos_num=MAX_SEQUENCE_LENGTH_D,\n",
    "        dropout_rate=0.05,\n",
    "    )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    outputs = model.get_layer('Encoder-{}-FeedForward-Norm'.format(layer_num)).output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "    #outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, \\\n",
    "GlobalAveragePooling1D, Permute, Dot, TimeDistributed\n",
    "\n",
    "def bilstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 50\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Add()([left_layer, right_layer])\n",
    "    \n",
    "    layer = Flatten()(lstm_layer)\n",
    "    #layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, Average, Maximum, Subtract, Average, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, master_positive, \n",
    "                         NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input, \n",
    "                                 master_anchor.input, master_positive.input, master_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = master_positive.output\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters anchor\n",
    "    master_anchor_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance', output_shape=[1])([encoded_anchor, master_positive])\n",
    "    master_anchor_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance', output_shape=[1])([encoded_anchor, master_negative])\n",
    "    \n",
    "    # Distance master positive\n",
    "    master_pos_positive_d = Lambda(cosine_distance, name='pos_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_positive])\n",
    "    master_pos_negative_d = Lambda(cosine_distance, name='neg_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_negative])\n",
    "    \n",
    "    # Distance master negative\n",
    "    master_neg_positive_d = Lambda(cosine_distance, name='pos_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_negative])\n",
    "    master_neg_negative_d = Lambda(cosine_distance, name='neg_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_positive])\n",
    "    \n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output_bug = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-bug',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    output_master = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-anchor',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_anchor_positive_d, master_anchor_negative_d])\n",
    "    \n",
    "    output_master_pos = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-pos',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_pos_positive_d, master_pos_negative_d])\n",
    "    \n",
    "    output_master_neg = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-neg',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_neg_positive_d, master_neg_negative_d])\n",
    "    \n",
    "    #output = Average()([output_bug, output_master, output_master_pos, output_master_neg])\n",
    "    \n",
    "    output_avg_master = Average()([output_master, output_master_pos, output_master_neg])\n",
    "    output = Average()([output_bug, output_avg_master])\n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    #optimizer = RAdam(lr=1e-10)\n",
    "    LR = 1e-4\n",
    "    decay_steps, warmup_steps = calc_train_steps(\n",
    "        NUMBER_OF_INSTANCES,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    optimizer = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer=optimizer, loss=custom_margin_loss, \n",
    "                                 metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pos (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_pos (InputLayer)     (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_pos (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_neg (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_neg (InputLayer)     (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_neg (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_pos (InputLayer)    (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_pos (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_master_pos (InputLay (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_master_pos (InputL (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_neg (InputLayer)    (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_master_neg (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_master_neg (InputLay (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_master_neg (InputL (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "                                                                 info_master_pos[0][0]            \n",
      "                                                                 info_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          45134200    title_in[0][0]                   \n",
      "                                                                 title_pos[0][0]                  \n",
      "                                                                 title_neg[0][0]                  \n",
      "                                                                 title_master_pos[0][0]           \n",
      "                                                                 title_master_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModel (Mod (None, 300)          3824900     desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "                                                                 desc_token_pos[0][0]             \n",
      "                                                                 desc_segment_pos[0][0]           \n",
      "                                                                 desc_token_neg[0][0]             \n",
      "                                                                 desc_segment_neg[0][0]           \n",
      "                                                                 desc_token_master_pos[0][0]      \n",
      "                                                                 desc_segment_master_pos[0][0]    \n",
      "                                                                 desc_token_master_neg[0][0]      \n",
      "                                                                 desc_segment_master_neg[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureBERTGenerationModel[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 900)          0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureLstmGenerationModel[2][0] \n",
      "                                                                 FeatureBERTGenerationModel[2][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 900)          0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureLstmGenerationModel[3][0] \n",
      "                                                                 FeatureBERTGenerationModel[3][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_pos (Conc (None, 900)          0           FeatureMlpGenerationModel[5][0]  \n",
      "                                                                 FeatureLstmGenerationModel[5][0] \n",
      "                                                                 FeatureBERTGenerationModel[5][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_neg (Conc (None, 900)          0           FeatureMlpGenerationModel[6][0]  \n",
      "                                                                 FeatureLstmGenerationModel[6][0] \n",
      "                                                                 FeatureBERTGenerationModel[6][0] \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-anchor ( (None, 2, 1)         0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-pos (Lam (None, 2, 1)         0           pos_master_pos_cosine_distance[0]\n",
      "                                                                 neg_master_pos_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-neg (Lam (None, 2, 1)         0           pos_master_neg_cosine_distance[0]\n",
      "                                                                 neg_master_neg_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-bug (Lambda)    (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2, 1)         0           stack-distances-master-anchor[0][\n",
      "                                                                 stack-distances-master-pos[0][0] \n",
      "                                                                 stack-distances-master-neg[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 2, 1)         0           stack-distances-bug[0][0]        \n",
      "                                                                 average_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,464,000\n",
      "Trainable params: 4,720,500\n",
      "Non-trainable params: 44,743,500\n",
      "__________________________________________________________________________________________________\n",
      "Batch size  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 2 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 3 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 4 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 5 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 6 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 7 Loss: 1.00, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 8 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 9 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 10 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 11 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 12 Loss: 1.00, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 13 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 14 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 15 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 16 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 17 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 18 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 19 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 20 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 21 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 22 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 23 Loss: 1.00, pos_cosine: 0.97, neg_cosine: 0.97\n",
      "Epoch: 24 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 25 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 26 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 27 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 28 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 29 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 30 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 31 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 32 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 33 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 34 Loss: 0.99, pos_cosine: 0.97, neg_cosine: 0.97\n",
      "Epoch: 35 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 36 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 37 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 38 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 39 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 40 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 41 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 42 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 43 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 44 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 45 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 46 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 47 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 48 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 49 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 50 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 51 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 52 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 53 Loss: 0.99, pos_cosine: 0.97, neg_cosine: 0.97\n",
      "Epoch: 54 Loss: 1.00, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 55 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 56 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 57 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 58 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 59 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 60 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 61 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 62 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 63 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 64 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 65 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 66 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 67 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 68 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 69 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 70 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 71 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 72 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 73 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 74 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 75 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 76 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 77 Loss: 1.00, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 78 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 79 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 80 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 81 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 82 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 83 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 84 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 85 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 86 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 87 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 88 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 89 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 90 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 91 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 92 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 93 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 94 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 95 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 96 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 97 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 98 Loss: 1.00, pos_cosine: 0.98, neg_cosine: 0.97\n",
      "Epoch: 99 Loss: 0.99, pos_cosine: 0.98, neg_cosine: 0.97\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 100\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "print(\"Batch size \", batch_size)\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# TF_KERAS must be added to environment variables in order to use TPU\n",
    "#os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "# Embeddings\n",
    "# desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "#                               num_words=len(baseline.embedding_matrix), \n",
    "#                               embedding_dim=EMBEDDING_DIM, \n",
    "#                               max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "#                               trainable=False, name='desc')\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False, name='title')\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D)\n",
    "#desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "master_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_in')\n",
    "master_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_pos')\n",
    "master_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_neg')\n",
    "\n",
    "NUMBER_OF_INSTANCES = len(baseline.dup_sets_train)\n",
    "BATCH_SIZE = batch_size\n",
    "EPOCHS = epochs\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, master_positive,\n",
    "                                            NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = batch_iterator(baseline, baseline.train_data, baseline.dup_sets_train, bug_train_ids, \n",
    "                                       batch_size, 1, issues_by_buckets)\n",
    "    \n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description']['token'], train_input_sample['description']['segment'], train_input_sample['info'],\n",
    "                   train_input_pos['title'], train_input_pos['description']['token'], train_input_pos['description']['segment'], train_input_pos['info'], \n",
    "                   train_input_neg['title'], train_input_neg['description']['token'], train_input_neg['description']['segment'], train_input_neg['info'],\n",
    "                  train_master_input['title'], train_master_input['description']['token'], train_master_input['description']['segment'], train_master_input['info'],\n",
    "                  train_master_input['title'], train_master_input['description']['token'], train_master_input['description']['segment'], train_master_input['info'],\n",
    "                   train_master_neg['title'], train_master_neg['description']['token'], train_master_neg['description']['segment'], train_master_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}s, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoded_anchor\n",
    "# model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, \n",
    "                                                                   bug_train_ids, method='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
