{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose BERT siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "# from tensorflow.python import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K_tf\n",
    "\n",
    "# sess = K_tf.get_session()\n",
    "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "# init_op = tf.variables_initializer(\n",
    "#     [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    "# )\n",
    "# sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 20\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 80\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'propose_bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D)\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8871db72210d4cb9b394b6d86a2a599f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57667), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb9ad08e46c485c98e69254633e2a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14567), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72234"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip -o uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "from keras_bert import Tokenizer\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "\n",
    "def load_bugs(baseline):\n",
    "    global tokenizer\n",
    "    removed = []\n",
    "    baseline.corpus = []\n",
    "    baseline.sentence_dict = {}\n",
    "    baseline.bug_set = {}\n",
    "    title_padding, desc_padding = [], []\n",
    "    for bug_id in tqdm(baseline.bug_ids):\n",
    "        #try:\n",
    "            bug = pickle.load(open(os.path.join(baseline.DIR, 'bugs', '{}.pkl'.format(bug_id)), 'rb'))\n",
    "            title_padding.append(bug['title_word_bert'][:MAX_SEQUENCE_LENGTH_T])\n",
    "            desc_padding.append(bug['description_word_bert'][:MAX_SEQUENCE_LENGTH_D])\n",
    "            baseline.bug_set[bug_id] = bug\n",
    "            #break\n",
    "        #except:\n",
    "            removed.append(bug_id)    \n",
    "    \n",
    "    for bug_id, bug_title, bug_desc in tqdm(zip(baseline.bug_ids, title_padding, desc_padding)):\n",
    "            bug = baseline.bug_set[bug_id]\n",
    "            baseline.sentence_dict[\",\".join(np.array(bug_title, str))] = bug['title']\n",
    "            baseline.sentence_dict[\",\".join(np.array(bug_desc, str))] = bug['description']\n",
    "            bug['title_word_bert'] = bug_title\n",
    "            bug['description_word_bert'] = bug_desc\n",
    "            bug['textual_word_bert'] = np.concatenate([bug_title, bug_desc], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16cc2cbcc3b4a08ac2907f9c09300f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da1593bdc6413cb184de4747d74250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 4.17 s, sys: 816 ms, total: 4.98 s\n",
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_bugs(baseline)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6be21d5b579419ba5fd08b54b421fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 19.9 s, sys: 323 µs, total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '1\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '37\\n',\n",
       " 'creation_ts': '2012-07-16 02:41:00 +0000',\n",
       " 'delta_ts': '2012-07-25 09:27:32 +0000',\n",
       " 'description': 'r1359641 build\\n\\nWindows:\\non install panels the product version is still 3.4 but not 3.4.1.\\n\\nThe shortcuts on desktop and start menu remain 3.4\\n\\nDEB:\\nversion in the desktop-integration package name is 3.4',\n",
       " 'description_bert': '[CLS] r ##13 ##59 ##64 ##1 build windows : on install panels the product version is still 3 . 4 but not 3 . 4 . 1 . the short ##cut ##s on desktop and start menu remain 3 . 4 de ##b : version in the desktop - integration package name is 3 . 4 [SEP]',\n",
       " 'description_word_bert': [101,\n",
       "  1054,\n",
       "  17134,\n",
       "  28154,\n",
       "  21084,\n",
       "  2487,\n",
       "  3857,\n",
       "  3645,\n",
       "  1024,\n",
       "  2006,\n",
       "  16500,\n",
       "  9320,\n",
       "  1996,\n",
       "  4031,\n",
       "  2544,\n",
       "  2003,\n",
       "  2145,\n",
       "  1017,\n",
       "  1012,\n",
       "  1018],\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 120286,\n",
       " 'priority': '3\\n',\n",
       " 'product': '8\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_word_bert': array([  101,  4031,  2544,  2323,  2022,  2904,  2013,  1017,  1012,\n",
       "         1018,  2000,  1017,  1012,  1018,  1012,  1015,   102,     0,\n",
       "            0,     0,   101,  1054, 17134, 28154, 21084,  2487,  3857,\n",
       "         3645,  1024,  2006, 16500,  9320,  1996,  4031,  2544,  2003,\n",
       "         2145,  1017,  1012,  1018]),\n",
       " 'title': 'product version should be changed from 3.4 to 3.4.1',\n",
       " 'title_bert': '[CLS] product version should be changed from 3 . 4 to 3 . 4 . 1 [SEP]',\n",
       " 'title_word_bert': [101,\n",
       "  4031,\n",
       "  2544,\n",
       "  2323,\n",
       "  2022,\n",
       "  2904,\n",
       "  2013,\n",
       "  1017,\n",
       "  1012,\n",
       "  1018,\n",
       "  2000,\n",
       "  1017,\n",
       "  1012,\n",
       "  1018,\n",
       "  1012,\n",
       "  1015,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'version': '524\\n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 11043)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indexed all train'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_idx = bug_train_ids[0]\n",
    "vector = baseline.bug_set[bug_idx]['textual_word_bert']\n",
    "annoy_train = AnnoyIndex(vector.shape[0])\n",
    "for bug_id in bug_train_ids:\n",
    "    annoy_train.add_item(bug_id, baseline.bug_set[bug_id]['textual_word_bert'])\n",
    "annoy_train.build(10) # 10 trees\n",
    "\"Indexed all train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "\n",
    "import random\n",
    "\n",
    "def read_batch_bugs(baseline, batch, bug):\n",
    "        info = np.concatenate((\n",
    "            baseline.to_one_hot(bug['bug_severity'], baseline.info_dict['bug_severity']),\n",
    "            baseline.to_one_hot(bug['bug_status'], baseline.info_dict['bug_status']),\n",
    "            baseline.to_one_hot(bug['component'], baseline.info_dict['component']),\n",
    "            baseline.to_one_hot(bug['priority'], baseline.info_dict['priority']),\n",
    "            baseline.to_one_hot(bug['product'], baseline.info_dict['product']),\n",
    "            baseline.to_one_hot(bug['version'], baseline.info_dict['version']))\n",
    "        )\n",
    "        #info.append(info_)\n",
    "        batch['info'].append(info)\n",
    "        batch['title'].append(bug['title_word_bert'])\n",
    "        batch['desc'].append(bug['description_word_bert'])\n",
    "\n",
    "def get_neg_bug(invalid_bugs, bug_ids, issues_by_buckets):\n",
    "    neg_bug = random.choice(list(issues_by_buckets.keys()))\n",
    "    try:\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    except:\n",
    "        invalid_bugs = [invalid_bugs]\n",
    "        while neg_bug in invalid_bugs or neg_bug not in issues_by_buckets:\n",
    "            neg_bug = random.choice(bug_ids)\n",
    "    return neg_bug\n",
    "\n",
    "def get_neg_bug_semihard(baseline, model, batch_bugs, anchor, invalid_bugs):\n",
    "    vector = model.predict([ np.array([baseline.bug_set[anchor]['title_word_bert']]), \n",
    "                            np.zeros_like([baseline.bug_set[anchor]['title_word_bert']]), \n",
    "                            np.array([baseline.bug_set[anchor]['description_word_bert']]), \n",
    "                            np.zeros_like([baseline.bug_set[anchor]['description_word_bert']]), \n",
    "                            np.array([retrieval.get_info(baseline.bug_set[anchor])]) ])\n",
    "    annoy = AnnoyIndex(vector.shape[1])\n",
    "    embeds = []\n",
    "    title_data, desc_data, info_data = [], [], []\n",
    "    batch_bugs_wo_positives = list(set(batch_bugs) - set(invalid_bugs)) \n",
    "    for bug_id in batch_bugs_wo_positives:\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title_data.append(bug['title_word_bert'])\n",
    "        desc_data.append(bug['description_word_bert'])\n",
    "        info_data.append(retrieval.get_info(bug))\n",
    "    embeds = model.predict([ np.array(title_data), np.zeros_like(title_data), np.array(desc_data), np.zeros_like(desc_data), np.array(info_data) ])\n",
    "    for bug_id, embed in zip(batch_bugs_wo_positives, embeds):\n",
    "        annoy.add_item(bug_id, embed)\n",
    "    annoy.build(10) # 10 trees\n",
    "    rank = annoy.get_nns_by_vector(vector[0], 20, include_distances=False)\n",
    "    neg_bug = rank[0]\n",
    "    return neg_bug\n",
    "\n",
    "def batch_iterator(baseline, model, data, dup_sets, bug_train_ids, batch_size, n_neg, issues_by_buckets):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_input, batch_pos, batch_neg, master_batch_input, master_batch_neg = {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                            {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                {'title' : [], 'desc' : [], 'info' : []},\\\n",
    "                                                    {'title' : [], 'desc' : [], 'info' : []}, \\\n",
    "                                                        {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs += dup_sets[anchor]\n",
    "    \n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            if model == None:\n",
    "                neg = get_neg_bug(anchor, dup_sets[anchor], issues_by_buckets)\n",
    "            else:\n",
    "                neg = get_neg_bug_semihard(baseline, model, batch_bugs, anchor, dup_sets[anchor])\n",
    "            bug_anchor = baseline.bug_set[anchor]\n",
    "            bug_pos = baseline.bug_set[pos]\n",
    "            if neg not in baseline.bug_set:\n",
    "                continue\n",
    "            bug_neg = baseline.bug_set[neg]\n",
    "            break\n",
    "        # master anchor and neg\n",
    "        master_anchor = baseline.bug_set[issues_by_buckets[anchor]]\n",
    "        while True:\n",
    "            if model == None:\n",
    "                master_neg_id = get_neg_bug(issues_by_buckets[anchor], dup_sets[anchor], issues_by_buckets)\n",
    "            else:\n",
    "                master_neg_id = get_neg_bug_semihard(baseline, model, batch_bugs, issues_by_buckets[anchor], dup_sets[anchor])\n",
    "            \n",
    "            if master_neg_id not in baseline.bug_set:\n",
    "                continue\n",
    "            \n",
    "            master_neg = baseline.bug_set[master_neg_id]\n",
    "            break\n",
    "        \n",
    "        read_batch_bugs(baseline, batch_input, bug_anchor)\n",
    "        read_batch_bugs(baseline, batch_pos, bug_pos)\n",
    "        read_batch_bugs(baseline, batch_neg, bug_neg)\n",
    "        # master anchor and neg\n",
    "        read_batch_bugs(baseline, master_batch_input, master_anchor)\n",
    "        read_batch_bugs(baseline, master_batch_neg, master_neg)\n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg, master_anchor, master_neg])\n",
    "        \n",
    "        \n",
    "    title_ids = np.full((len(batch_triplets), MAX_SEQUENCE_LENGTH_T), 0)\n",
    "    description_ids = np.full((len(batch_triplets), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "\n",
    "    batch_input['title'] = { 'token' : np.array(batch_input['title']), 'segment' : title_ids }\n",
    "    batch_input['desc'] = { 'token' : np.array(batch_input['desc']), 'segment' : description_ids }\n",
    "    batch_input['info'] = np.array(batch_input['info'])\n",
    "    batch_pos['title'] = { 'token' : np.array(batch_pos['title']), 'segment' : title_ids }\n",
    "    batch_pos['desc'] = { 'token' : np.array(batch_pos['desc']), 'segment' : description_ids }\n",
    "    batch_pos['info'] = np.array(batch_pos['info'])\n",
    "    batch_neg['title'] = { 'token' : np.array(batch_neg['title']), 'segment' : title_ids }\n",
    "    batch_neg['desc'] = { 'token' : np.array(batch_neg['desc']), 'segment' : description_ids }\n",
    "    batch_neg['info'] = np.array(batch_neg['info'])\n",
    "    \n",
    "    # master\n",
    "    master_batch_input['title'] = { 'token' : np.array(master_batch_input['title']), 'segment' : title_ids }\n",
    "    master_batch_input['desc'] ={ 'token' : np.array(master_batch_input['desc']), 'segment' : description_ids }\n",
    "    master_batch_input['info'] = np.array(master_batch_input['info'])\n",
    "    \n",
    "    master_batch_neg['title'] = { 'token' : np.array(master_batch_neg['title']), 'segment' : title_ids }\n",
    "    master_batch_neg['desc'] = { 'token' : np.array(master_batch_neg['desc']), 'segment' : description_ids }\n",
    "    master_batch_neg['info'] = np.array(master_batch_neg['info'])\n",
    "\n",
    "    n_half = len(batch_triplets) // 2\n",
    "    if n_half > 0:\n",
    "        pos = np.full((1, n_half), 1)\n",
    "        neg = np.full((1, n_half), 0)\n",
    "        sim = np.concatenate([pos, neg], -1)[0]\n",
    "    else:\n",
    "        sim = np.array([np.random.choice([1, 0])])\n",
    "\n",
    "    input_sample, input_pos, input_neg, master_input_sample, master_neg = {}, {}, {}, {}, {}\n",
    "\n",
    "    input_sample = { 'title' : batch_input['title'], 'description' : batch_input['desc'], 'info' : batch_input['info'] }\n",
    "    input_pos = { 'title' : batch_pos['title'], 'description' : batch_pos['desc'], 'info': batch_pos['info'] }\n",
    "    input_neg = { 'title' : batch_neg['title'], 'description' : batch_neg['desc'], 'info': batch_neg['info'] }\n",
    "    # master \n",
    "    master_input_sample = { 'title' : master_batch_input['title'], 'description' : master_batch_input['desc'], \n",
    "                           'info' : master_batch_input['info'] }\n",
    "    master_neg = { 'title' : master_batch_neg['title'], 'description' : master_batch_neg['desc'], \n",
    "                           'info' : master_batch_neg['info'] }\n",
    "    return batch_triplets, input_sample, input_pos, input_neg, master_input_sample, master_neg, sim #sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 208 ms, sys: 0 ns, total: 208 ms\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = batch_iterator(baseline, None, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, \n",
    "                                                                                              issues_by_buckets)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (128, 729), (128,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, valid_input_sample['description']['token'].shape, \\\n",
    "    valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5, batch_iterator, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#     )\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=False,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "#     model = get_model(\n",
    "#         token_num=len(token_dict),\n",
    "#         head_num=10,\n",
    "#         transformer_num=layer_num,\n",
    "#         embed_dim=100,\n",
    "#         feed_forward_dim=100,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#         pos_num=MAX_SEQUENCE_LENGTH,\n",
    "#         dropout_rate=0.05,\n",
    "#     )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    outputs = model.get_layer('Encoder-{}-FeedForward-Norm'.format(layer_num)).output\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "#     outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    # Normalize https://stats.stackexchange.com/questions/53068/euclidean-distance-score-and-similarity\n",
    "    distance = K.constant(1) / (K.constant(1) + distance)\n",
    "    return K.mean(distance, keepdims=False)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "# https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def custom_margin_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, Average, Maximum, Subtract, Average, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, master_positive, \n",
    "                         NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input, \n",
    "                                 master_anchor.input, master_positive.input, master_negative.input], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = master_positive.output\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance', output_shape=[1])([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance', output_shape=[1])([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters anchor\n",
    "    master_anchor_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance', output_shape=[1])([encoded_anchor, master_positive])\n",
    "    master_anchor_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance', output_shape=[1])([encoded_anchor, master_negative])\n",
    "    \n",
    "    # Distance master positive\n",
    "    master_pos_positive_d = Lambda(cosine_distance, name='pos_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_positive])\n",
    "    master_pos_negative_d = Lambda(cosine_distance, name='neg_master_pos_cosine_distance', output_shape=[1])([encoded_positive, master_negative])\n",
    "    \n",
    "    # Distance master negative\n",
    "    master_neg_positive_d = Lambda(cosine_distance, name='pos_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_negative])\n",
    "    master_neg_negative_d = Lambda(cosine_distance, name='neg_master_neg_cosine_distance', output_shape=[1])([encoded_negative, master_positive])\n",
    "    \n",
    "\n",
    "    # Loss function only works with a single output\n",
    "    output_bug = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-bug',\n",
    "        output_shape=(2, 1)\n",
    "    )([positive_d, negative_d])\n",
    "    \n",
    "    output_master = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-anchor',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_anchor_positive_d, master_anchor_negative_d])\n",
    "    \n",
    "    output_master_pos = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-pos',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_pos_positive_d, master_pos_negative_d])\n",
    "    \n",
    "    output_master_neg = Lambda(\n",
    "        lambda vects: stack_tensors(vects),\n",
    "        name='stack-distances-master-neg',\n",
    "        output_shape=(2, 1)\n",
    "    )([master_neg_positive_d, master_neg_negative_d])\n",
    "    \n",
    "    output = Average()([output_bug, output_master, output_master_pos, output_master_neg])\n",
    "    \n",
    "    #output_avg_master = Average()([output_master, output_master_pos, output_master_neg])\n",
    "    #output = Average()([output_bug, output_avg_master])\n",
    "    #loss = MarginLoss()(output)\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_margin_loss, \n",
    "                                 metrics=[pos_distance, neg_distance])\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size  64\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_pos (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_pos (InputLayer)  (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_pos (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_pos (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_neg (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_neg (InputLayer)  (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_neg (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_neg (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_pos (InputLayer)    (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_master_pos (InputLa (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_master_pos (Input (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_master_pos (InputLay (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_master_pos (InputL (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_master_neg (InputLayer)    (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_master_neg (InputLa (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_master_neg (Input (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_master_neg (InputLay (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_master_neg (InputL (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "                                                                 info_master_pos[0][0]            \n",
      "                                                                 info_master_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 768)          80346736    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "                                                                 title_token_pos[0][0]            \n",
      "                                                                 title_segment_pos[0][0]          \n",
      "                                                                 title_token_neg[0][0]            \n",
      "                                                                 title_segment_neg[0][0]          \n",
      "                                                                 title_token_master_pos[0][0]     \n",
      "                                                                 title_segment_master_pos[0][0]   \n",
      "                                                                 title_token_master_neg[0][0]     \n",
      "                                                                 title_segment_master_neg[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 768)          80346736    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "                                                                 desc_token_pos[0][0]             \n",
      "                                                                 desc_segment_pos[0][0]           \n",
      "                                                                 desc_token_neg[0][0]             \n",
      "                                                                 desc_segment_neg[0][0]           \n",
      "                                                                 desc_token_master_pos[0][0]      \n",
      "                                                                 desc_segment_master_pos[0][0]    \n",
      "                                                                 desc_token_master_neg[0][0]      \n",
      "                                                                 desc_segment_master_neg[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1836)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 1836)         0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[2\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 1836)         0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[3\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_pos (Conc (None, 1836)         0           FeatureMlpGenerationModel[5][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[5\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_master_neg (Conc (None, 1836)         0           FeatureMlpGenerationModel[6][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[6\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (None, 1)            0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_pos_cosine_distance  (None, 1)            0           merge_features_pos[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_neg[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "neg_master_neg_cosine_distance  (None, 1)            0           merge_features_neg[0][0]         \n",
      "                                                                 merge_features_master_pos[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-bug (Lambda)    (None, 2, 1)         0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-anchor ( (None, 2, 1)         0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-pos (Lam (None, 2, 1)         0           pos_master_pos_cosine_distance[0]\n",
      "                                                                 neg_master_pos_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "stack-distances-master-neg (Lam (None, 2, 1)         0           pos_master_neg_cosine_distance[0]\n",
      "                                                                 neg_master_neg_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2, 1)         0           stack-distances-bug[0][0]        \n",
      "                                                                 stack-distances-master-anchor[0][\n",
      "                                                                 stack-distances-master-pos[0][0] \n",
      "                                                                 stack-distances-master-neg[0][0] \n",
      "==================================================================================================\n",
      "Total params: 160,912,472\n",
      "Trainable params: 440,296\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 2 Loss: 1.00, pos_cosine: 0.89, neg_cosine: 0.90\n",
      "Epoch: 3 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 4 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 5 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.91\n",
      "Epoch: 7 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 8 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 9 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 10 Loss: 0.99, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 11 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 12 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 13 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 14 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 15 Loss: 1.00, pos_cosine: 0.91, neg_cosine: 0.91\n",
      "Epoch: 16 Loss: 1.01, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 17 Loss: 0.99, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 18 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 19 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 20 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 21 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 22 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.90\n",
      "Epoch: 23 Loss: 1.00, pos_cosine: 0.89, neg_cosine: 0.90\n",
      "Epoch: 24 Loss: 1.00, pos_cosine: 0.89, neg_cosine: 0.89\n",
      "Epoch: 25 Loss: 1.00, pos_cosine: 0.90, neg_cosine: 0.89\n",
      "Epoch: 26 Loss: 1.00, pos_cosine: 0.89, neg_cosine: 0.89\n",
      "Epoch: 27 Loss: 1.00, pos_cosine: 0.89, neg_cosine: 0.89\n",
      "Epoch: 28 Loss: 0.99, pos_cosine: 0.89, neg_cosine: 0.88\n",
      "Epoch: 29 Loss: 0.99, pos_cosine: 0.88, neg_cosine: 0.88\n",
      "Epoch: 30 Loss: 1.00, pos_cosine: 0.87, neg_cosine: 0.87\n",
      "Epoch: 31 Loss: 1.00, pos_cosine: 0.87, neg_cosine: 0.87\n",
      "Epoch: 32 Loss: 0.99, pos_cosine: 0.86, neg_cosine: 0.86\n",
      "Epoch: 33 Loss: 0.99, pos_cosine: 0.86, neg_cosine: 0.85\n",
      "Epoch: 34 Loss: 0.99, pos_cosine: 0.85, neg_cosine: 0.84\n",
      "Epoch: 35 Loss: 1.00, pos_cosine: 0.83, neg_cosine: 0.83\n",
      "Epoch: 36 Loss: 1.00, pos_cosine: 0.82, neg_cosine: 0.82\n",
      "Epoch: 37 Loss: 0.99, pos_cosine: 0.81, neg_cosine: 0.80\n",
      "Epoch: 38 Loss: 0.99, pos_cosine: 0.80, neg_cosine: 0.80\n",
      "Epoch: 39 Loss: 0.99, pos_cosine: 0.79, neg_cosine: 0.78\n",
      "Epoch: 40 Loss: 0.99, pos_cosine: 0.78, neg_cosine: 0.78\n",
      "Epoch: 41 Loss: 0.99, pos_cosine: 0.77, neg_cosine: 0.77\n",
      "Epoch: 42 Loss: 0.99, pos_cosine: 0.78, neg_cosine: 0.76\n",
      "Epoch: 43 Loss: 0.99, pos_cosine: 0.77, neg_cosine: 0.76\n",
      "Epoch: 44 Loss: 1.00, pos_cosine: 0.75, neg_cosine: 0.75\n",
      "Epoch: 45 Loss: 0.99, pos_cosine: 0.75, neg_cosine: 0.74\n",
      "Epoch: 46 Loss: 0.99, pos_cosine: 0.75, neg_cosine: 0.74\n",
      "Epoch: 47 Loss: 0.98, pos_cosine: 0.75, neg_cosine: 0.72\n",
      "Epoch: 48 Loss: 0.98, pos_cosine: 0.74, neg_cosine: 0.72\n",
      "Epoch: 49 Loss: 0.99, pos_cosine: 0.73, neg_cosine: 0.72\n",
      "Epoch: 50 Loss: 0.99, pos_cosine: 0.73, neg_cosine: 0.71\n",
      "Epoch: 51 Loss: 0.99, pos_cosine: 0.71, neg_cosine: 0.70\n",
      "Epoch: 52 Loss: 0.98, pos_cosine: 0.72, neg_cosine: 0.70\n",
      "Epoch: 53 Loss: 0.98, pos_cosine: 0.70, neg_cosine: 0.69\n",
      "Epoch: 54 Loss: 0.99, pos_cosine: 0.70, neg_cosine: 0.69\n",
      "Epoch: 55 Loss: 0.98, pos_cosine: 0.70, neg_cosine: 0.68\n",
      "Epoch: 56 Loss: 0.98, pos_cosine: 0.70, neg_cosine: 0.67\n",
      "Epoch: 57 Loss: 0.99, pos_cosine: 0.67, neg_cosine: 0.67\n",
      "Epoch: 58 Loss: 0.98, pos_cosine: 0.69, neg_cosine: 0.68\n",
      "Epoch: 59 Loss: 0.97, pos_cosine: 0.69, neg_cosine: 0.66\n",
      "Epoch: 60 Loss: 0.99, pos_cosine: 0.67, neg_cosine: 0.66\n",
      "Epoch: 61 Loss: 0.99, pos_cosine: 0.68, neg_cosine: 0.66\n",
      "Epoch: 62 Loss: 0.99, pos_cosine: 0.68, neg_cosine: 0.66\n",
      "Epoch: 63 Loss: 0.98, pos_cosine: 0.67, neg_cosine: 0.65\n",
      "Epoch: 64 Loss: 0.97, pos_cosine: 0.69, neg_cosine: 0.67\n",
      "Epoch: 65 Loss: 1.00, pos_cosine: 0.66, neg_cosine: 0.65\n",
      "Epoch: 66 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.65\n",
      "Epoch: 67 Loss: 0.99, pos_cosine: 0.66, neg_cosine: 0.65\n",
      "Epoch: 68 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 69 Loss: 0.98, pos_cosine: 0.66, neg_cosine: 0.64\n",
      "Epoch: 70 Loss: 0.98, pos_cosine: 0.67, neg_cosine: 0.65\n",
      "Epoch: 71 Loss: 0.98, pos_cosine: 0.67, neg_cosine: 0.64\n",
      "Epoch: 72 Loss: 0.98, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 73 Loss: 0.99, pos_cosine: 0.64, neg_cosine: 0.63\n",
      "Epoch: 74 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 75 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 76 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 77 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.64\n",
      "Epoch: 78 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 79 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 80 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 81 Loss: 0.99, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 82 Loss: 0.99, pos_cosine: 0.64, neg_cosine: 0.63\n",
      "Epoch: 83 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 84 Loss: 0.96, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 85 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 86 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 87 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.61\n",
      "Epoch: 88 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 89 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 90 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 91 Loss: 0.99, pos_cosine: 0.63, neg_cosine: 0.62\n",
      "Epoch: 92 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.63\n",
      "Epoch: 93 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 94 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 95 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 96 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 97 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 98 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 99 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 100 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.61\n",
      "Epoch: 101 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 102 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 103 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 104 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 105 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 106 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 107 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 108 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 109 Loss: 0.99, pos_cosine: 0.63, neg_cosine: 0.62\n",
      "Epoch: 110 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 111 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 112 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 113 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 114 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 115 Loss: 0.96, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 116 Loss: 0.95, pos_cosine: 0.65, neg_cosine: 0.60\n",
      "Epoch: 117 Loss: 0.98, pos_cosine: 0.63, neg_cosine: 0.61\n",
      "Epoch: 118 Loss: 0.95, pos_cosine: 0.64, neg_cosine: 0.60\n",
      "Epoch: 119 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 120 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 121 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 122 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 123 Loss: 0.96, pos_cosine: 0.64, neg_cosine: 0.60\n",
      "Epoch: 124 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.63\n",
      "Epoch: 125 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 126 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 127 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 128 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 129 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 130 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 131 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 132 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 133 Loss: 0.98, pos_cosine: 0.63, neg_cosine: 0.62\n",
      "Epoch: 134 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 135 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 136 Loss: 0.98, pos_cosine: 0.66, neg_cosine: 0.64\n",
      "Epoch: 137 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 138 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 139 Loss: 0.98, pos_cosine: 0.64, neg_cosine: 0.62\n",
      "Epoch: 140 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 141 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 142 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 143 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 144 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.64\n",
      "Epoch: 145 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.61\n",
      "Epoch: 147 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 148 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 149 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 150 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 151 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 152 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 153 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 154 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 155 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 156 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.61\n",
      "Epoch: 157 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 158 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 159 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 160 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 161 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 162 Loss: 0.94, pos_cosine: 0.66, neg_cosine: 0.60\n",
      "Epoch: 163 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 164 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 165 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.60\n",
      "Epoch: 166 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 167 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 168 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 169 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 170 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 171 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 172 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 173 Loss: 0.96, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 174 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 175 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 176 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 177 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 178 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 179 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 180 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.61\n",
      "Epoch: 181 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 182 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 183 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 184 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 185 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 186 Loss: 0.97, pos_cosine: 0.64, neg_cosine: 0.61\n",
      "Epoch: 187 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 188 Loss: 0.97, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 189 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 190 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 191 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 192 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 193 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 194 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 195 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 196 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 197 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 198 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 199 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 200 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 201 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 202 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 203 Loss: 0.96, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 204 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 205 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 206 Loss: 0.94, pos_cosine: 0.67, neg_cosine: 0.60\n",
      "Epoch: 207 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 208 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 209 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 210 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 211 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 212 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 213 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 214 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 215 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 216 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 217 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 218 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 219 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 220 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 221 Loss: 0.95, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 222 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 223 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 224 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 225 Loss: 0.98, pos_cosine: 0.66, neg_cosine: 0.64\n",
      "Epoch: 226 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 227 Loss: 0.97, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 228 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 229 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 230 Loss: 0.93, pos_cosine: 0.67, neg_cosine: 0.61\n",
      "Epoch: 231 Loss: 0.94, pos_cosine: 0.66, neg_cosine: 0.60\n",
      "Epoch: 232 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 233 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 234 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 235 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 236 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 237 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 238 Loss: 0.98, pos_cosine: 0.65, neg_cosine: 0.63\n",
      "Epoch: 239 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 240 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 241 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 242 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 243 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 244 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 245 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.65\n",
      "Epoch: 246 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.61\n",
      "Epoch: 247 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 248 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 249 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 250 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 251 Loss: 0.97, pos_cosine: 0.65, neg_cosine: 0.62\n",
      "Epoch: 252 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 253 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 254 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 255 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 256 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 257 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 258 Loss: 0.93, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 259 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 260 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 261 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 262 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 263 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.64\n",
      "Epoch: 264 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 265 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 266 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 267 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 268 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 269 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 270 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 271 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 272 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 273 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 274 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 275 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 276 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 277 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 278 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 279 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 280 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 281 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 282 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 283 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 284 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 285 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 286 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 287 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 289 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 290 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 291 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 292 Loss: 0.97, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 293 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 294 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 295 Loss: 0.94, pos_cosine: 0.67, neg_cosine: 0.61\n",
      "Epoch: 296 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 297 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 298 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 299 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 300 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 301 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 302 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 303 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 304 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 305 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 306 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 307 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 308 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 309 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 310 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 311 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 312 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 313 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 314 Loss: 0.97, pos_cosine: 0.66, neg_cosine: 0.63\n",
      "Epoch: 315 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 316 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 317 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 318 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.65\n",
      "Epoch: 319 Loss: 0.96, pos_cosine: 0.70, neg_cosine: 0.66\n",
      "Epoch: 320 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 321 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 322 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 323 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 324 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 325 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 326 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 327 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 328 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 329 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 330 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.62\n",
      "Epoch: 331 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 332 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 333 Loss: 0.96, pos_cosine: 0.66, neg_cosine: 0.62\n",
      "Epoch: 334 Loss: 0.97, pos_cosine: 0.70, neg_cosine: 0.67\n",
      "Epoch: 335 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 336 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 337 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 338 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 339 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 340 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 341 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 342 Loss: 0.93, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 343 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 344 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 345 Loss: 0.96, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 346 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 347 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 348 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 349 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 350 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 351 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 352 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 353 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 354 Loss: 0.96, pos_cosine: 0.70, neg_cosine: 0.66\n",
      "Epoch: 355 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 356 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 357 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 358 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.63\n",
      "Epoch: 359 Loss: 0.92, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 360 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 361 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 362 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 363 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 364 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 365 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 366 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 367 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 368 Loss: 0.93, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 369 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 370 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 371 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 372 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 373 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 374 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 375 Loss: 0.93, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 376 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 377 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 378 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 379 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 380 Loss: 0.93, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 381 Loss: 0.96, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 382 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 383 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 384 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 385 Loss: 0.95, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 386 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 387 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 388 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 389 Loss: 0.96, pos_cosine: 0.68, neg_cosine: 0.64\n",
      "Epoch: 390 Loss: 0.97, pos_cosine: 0.68, neg_cosine: 0.65\n",
      "Epoch: 391 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 392 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 393 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 394 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 395 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 396 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.63\n",
      "Epoch: 397 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 398 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 399 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 400 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 401 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 402 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 403 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 404 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 405 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 406 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 407 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 408 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 409 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 410 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 411 Loss: 0.94, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 412 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 413 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 414 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 415 Loss: 0.96, pos_cosine: 0.71, neg_cosine: 0.67\n",
      "Epoch: 416 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 417 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 418 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 419 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 420 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 421 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 422 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 423 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 424 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 425 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 426 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 427 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 428 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 429 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 430 Loss: 0.95, pos_cosine: 0.73, neg_cosine: 0.68\n",
      "Epoch: 431 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 432 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 433 Loss: 0.91, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 434 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 435 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 436 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 437 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 438 Loss: 0.93, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 439 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 440 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 441 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 442 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 443 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 444 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 445 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 446 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 447 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 448 Loss: 0.96, pos_cosine: 0.70, neg_cosine: 0.66\n",
      "Epoch: 449 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.63\n",
      "Epoch: 450 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 451 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 452 Loss: 0.92, pos_cosine: 0.74, neg_cosine: 0.66\n",
      "Epoch: 453 Loss: 0.94, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 454 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 455 Loss: 0.92, pos_cosine: 0.70, neg_cosine: 0.62\n",
      "Epoch: 456 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 457 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 458 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 459 Loss: 0.95, pos_cosine: 0.67, neg_cosine: 0.62\n",
      "Epoch: 460 Loss: 0.95, pos_cosine: 0.73, neg_cosine: 0.68\n",
      "Epoch: 461 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 462 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 463 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 464 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 465 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 466 Loss: 0.96, pos_cosine: 0.71, neg_cosine: 0.68\n",
      "Epoch: 467 Loss: 0.90, pos_cosine: 0.72, neg_cosine: 0.63\n",
      "Epoch: 468 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 469 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 470 Loss: 0.94, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 471 Loss: 0.95, pos_cosine: 0.74, neg_cosine: 0.69\n",
      "Epoch: 472 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 473 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 474 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 475 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 476 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 477 Loss: 0.91, pos_cosine: 0.72, neg_cosine: 0.63\n",
      "Epoch: 478 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 479 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 480 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 481 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 482 Loss: 0.96, pos_cosine: 0.69, neg_cosine: 0.65\n",
      "Epoch: 483 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 484 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 485 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 486 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.63\n",
      "Epoch: 487 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 488 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 489 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 490 Loss: 0.93, pos_cosine: 0.69, neg_cosine: 0.62\n",
      "Epoch: 491 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 492 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 493 Loss: 0.92, pos_cosine: 0.70, neg_cosine: 0.62\n",
      "Epoch: 494 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 495 Loss: 0.90, pos_cosine: 0.72, neg_cosine: 0.62\n",
      "Epoch: 496 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 497 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 498 Loss: 0.94, pos_cosine: 0.68, neg_cosine: 0.62\n",
      "Epoch: 499 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.62\n",
      "Epoch: 500 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 501 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 502 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 503 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 504 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 505 Loss: 0.95, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 506 Loss: 0.95, pos_cosine: 0.72, neg_cosine: 0.67\n",
      "Epoch: 507 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 508 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 509 Loss: 0.94, pos_cosine: 0.75, neg_cosine: 0.69\n",
      "Epoch: 510 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 511 Loss: 0.91, pos_cosine: 0.72, neg_cosine: 0.63\n",
      "Epoch: 512 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 513 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 514 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 515 Loss: 0.92, pos_cosine: 0.74, neg_cosine: 0.65\n",
      "Epoch: 516 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 517 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 518 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 519 Loss: 0.94, pos_cosine: 0.74, neg_cosine: 0.68\n",
      "Epoch: 520 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 521 Loss: 0.93, pos_cosine: 0.74, neg_cosine: 0.67\n",
      "Epoch: 522 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 523 Loss: 0.92, pos_cosine: 0.74, neg_cosine: 0.65\n",
      "Epoch: 524 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 525 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 526 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 527 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 528 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 529 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 530 Loss: 0.95, pos_cosine: 0.73, neg_cosine: 0.68\n",
      "Epoch: 531 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 532 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 533 Loss: 0.95, pos_cosine: 0.71, neg_cosine: 0.66\n",
      "Epoch: 534 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 535 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.64\n",
      "Epoch: 536 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 537 Loss: 0.91, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 538 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 539 Loss: 0.91, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 540 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 541 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 542 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.63\n",
      "Epoch: 543 Loss: 0.92, pos_cosine: 0.74, neg_cosine: 0.66\n",
      "Epoch: 544 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 545 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 546 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 547 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 548 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 549 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 550 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 551 Loss: 0.91, pos_cosine: 0.73, neg_cosine: 0.64\n",
      "Epoch: 552 Loss: 0.94, pos_cosine: 0.76, neg_cosine: 0.69\n",
      "Epoch: 553 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 554 Loss: 0.93, pos_cosine: 0.74, neg_cosine: 0.67\n",
      "Epoch: 555 Loss: 0.91, pos_cosine: 0.73, neg_cosine: 0.64\n",
      "Epoch: 556 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 557 Loss: 0.97, pos_cosine: 0.70, neg_cosine: 0.67\n",
      "Epoch: 558 Loss: 0.92, pos_cosine: 0.75, neg_cosine: 0.67\n",
      "Epoch: 559 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.66\n",
      "Epoch: 560 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 561 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 562 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 563 Loss: 0.94, pos_cosine: 0.69, neg_cosine: 0.63\n",
      "Epoch: 564 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 565 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 566 Loss: 0.92, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 567 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 568 Loss: 0.96, pos_cosine: 0.71, neg_cosine: 0.67\n",
      "Epoch: 569 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 570 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 571 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 572 Loss: 0.93, pos_cosine: 0.74, neg_cosine: 0.67\n",
      "Epoch: 573 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 574 Loss: 0.94, pos_cosine: 0.74, neg_cosine: 0.68\n",
      "Epoch: 575 Loss: 0.93, pos_cosine: 0.75, neg_cosine: 0.68\n",
      "Epoch: 576 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 577 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 578 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 579 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 580 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 581 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 582 Loss: 0.92, pos_cosine: 0.74, neg_cosine: 0.66\n",
      "Epoch: 583 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.64\n",
      "Epoch: 584 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 585 Loss: 0.94, pos_cosine: 0.74, neg_cosine: 0.68\n",
      "Epoch: 586 Loss: 0.92, pos_cosine: 0.75, neg_cosine: 0.66\n",
      "Epoch: 587 Loss: 0.91, pos_cosine: 0.73, neg_cosine: 0.63\n",
      "Epoch: 588 Loss: 0.93, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 589 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 590 Loss: 0.92, pos_cosine: 0.73, neg_cosine: 0.65\n",
      "Epoch: 591 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.66\n",
      "Epoch: 592 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 593 Loss: 0.94, pos_cosine: 0.70, neg_cosine: 0.64\n",
      "Epoch: 594 Loss: 0.95, pos_cosine: 0.74, neg_cosine: 0.69\n",
      "Epoch: 595 Loss: 0.95, pos_cosine: 0.70, neg_cosine: 0.65\n",
      "Epoch: 596 Loss: 0.94, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 597 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 598 Loss: 0.92, pos_cosine: 0.71, neg_cosine: 0.63\n",
      "Epoch: 599 Loss: 0.93, pos_cosine: 0.74, neg_cosine: 0.67\n",
      "Epoch: 600 Loss: 0.94, pos_cosine: 0.71, neg_cosine: 0.65\n",
      "Epoch: 601 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 602 Loss: 0.94, pos_cosine: 0.74, neg_cosine: 0.68\n",
      "Epoch: 603 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 604 Loss: 0.93, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 605 Loss: 0.94, pos_cosine: 0.73, neg_cosine: 0.67\n",
      "Epoch: 606 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 607 Loss: 0.94, pos_cosine: 0.72, neg_cosine: 0.65\n",
      "Epoch: 608 Loss: 0.91, pos_cosine: 0.75, neg_cosine: 0.66\n",
      "Epoch: 609 Loss: 0.93, pos_cosine: 0.71, neg_cosine: 0.64\n",
      "Epoch: 610 Loss: 0.93, pos_cosine: 0.73, neg_cosine: 0.66\n",
      "Epoch: 611 Loss: 0.94, pos_cosine: 0.75, neg_cosine: 0.69\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1\n",
    "\n",
    "print(\"Batch size \", batch_size)\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "# title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "#desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "master_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_in')\n",
    "master_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_pos')\n",
    "master_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'master_neg')\n",
    "\n",
    "NUMBER_OF_INSTANCES = len(baseline.dup_sets_train)\n",
    "BATCH_SIZE = batch_size\n",
    "EPOCHS = epochs\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, master_positive,\n",
    "                                            NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = batch_iterator(baseline, encoded_anchor, baseline.train_data, baseline.dup_sets_train, bug_train_ids, \n",
    "                                       batch_size, 1, issues_by_buckets)\n",
    "    \n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], train_input_sample['description']['token'], train_input_sample['description']['segment'], train_input_sample['info'],\n",
    "                   train_input_pos['title']['token'], train_input_pos['title']['segment'], train_input_pos['description']['token'], train_input_pos['description']['segment'], train_input_pos['info'], \n",
    "                   train_input_neg['title']['token'], train_input_neg['title']['segment'], train_input_neg['description']['token'], train_input_neg['description']['segment'], train_input_neg['info'],\n",
    "                  train_master_input['title']['token'], train_master_input['title']['segment'], train_master_input['description']['token'], train_master_input['description']['segment'], train_master_input['info'],\n",
    "                  train_master_input['title']['token'], train_master_input['title']['segment'], train_master_input['description']['token'], train_master_input['description']['segment'], train_master_input['info'],\n",
    "                   train_master_neg['title']['token'], train_master_neg['title']['segment'], train_master_neg['description']['token'], train_master_neg['description']['segment'], train_master_neg['info']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}, recall@25: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1], h[2], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, pos_cosine: {:.2f}, neg_cosine: {:.2f}\".format(epoch+1,\n",
    "                                                                                                         h[0],\n",
    "                                                                                                         h[1],\n",
    "                                                                                                         h[2]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}s, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108544:111059,109674,108379,109366|109366:0.15593445301055908,111059:0.12969756126403809,94421:0.10437977313995361,121250:0.08986496925354004,115569:0.08433961868286133,108379:0.07375228404998779,121232:0.07194459438323975,99306:0.06010138988494873,112520:0.05320429801940918,98821:0.046608924865722656,117276:0.039613962173461914,102082:0.03391468524932861,115072:0.03152573108673096,107794:0.023058533668518066,65064:0.017291665077209473,115888:0.014262080192565918,114770:0.014158010482788086,94907:0.01373898983001709,110764:0.011090755462646484,102398:0.010446667671203613,110555:0.007324576377868652,102409:0.006232261657714844,115068:0.0018624067306518555,105993:0.0018515586853027344,115895:0.0014536380767822266,96124:0.001212477684020996,105907:0.0009016990661621094,99982:0.00038933753967285156,97860:0.0',\n",
       " '109674:108544,111059,108379,109366|94815:0.45261454582214355,112693:0.27634429931640625,82495:0.17601275444030762,97860:0.15731406211853027,102497:0.1454828977584839,111059:0.11862444877624512,102495:0.10497057437896729,105757:0.10034501552581787,102712:0.0977778434753418,93696:0.08589696884155273,109366:0.0854792594909668,100665:0.05350971221923828,97978:0.048468947410583496,107794:0.035076022148132324,100666:0.034120798110961914,46957:0.028405189514160156,118846:0.02709174156188965,101268:0.01955258846282959,96880:0.01687610149383545,97179:0.015727996826171875,107764:0.011215567588806152,97178:0.01016533374786377,103056:0.010142803192138672,94421:0.00897073745727539,96894:0.0049582719802856445,110764:0.004076480865478516,114280:0.003566145896911621,114671:0.0023888349533081055,105993:0.0',\n",
       " '111059:108544,109674,108379,109366|109366:0.2422277331352234,107794:0.12289237976074219,109674:0.10445380210876465,108544:0.09509432315826416,98033:0.06740593910217285,108377:0.058586955070495605,102003:0.05670928955078125,115262:0.053682804107666016,115068:0.053192973136901855,101652:0.04428601264953613,114310:0.03444564342498779,115072:0.026202082633972168,98821:0.021424174308776855,120996:0.019275188446044922,110928:0.017106294631958008,99982:0.017055869102478027,113113:0.016239285469055176,110764:0.0157698392868042,114153:0.012609124183654785,92348:0.011330842971801758,112299:0.010764718055725098,107044:0.007830381393432617,95800:0.0073277950286865234,46957:0.006055593490600586,114166:0.005383729934692383,65064:0.005276679992675781,82495:0.005202531814575195,110555:0.0040673017501831055,105087:0.0',\n",
       " '108379:108544,111059,109674,109366|105671:0.437638521194458,107764:0.2454017996788025,93296:0.23396331071853638,114022:0.1828811764717102,46957:0.16408848762512207,101268:0.14642333984375,105082:0.14213359355926514,117276:0.07434821128845215,121551:0.06860160827636719,112520:0.053931236267089844,105272:0.045665860176086426,104698:0.043853163719177246,120258:0.040294528007507324,107794:0.037490248680114746,108544:0.033492326736450195,113724:0.031144380569458008,115068:0.030051708221435547,118169:0.028107404708862305,105087:0.026921629905700684,105961:0.024037480354309082,116229:0.022227883338928223,113881:0.01893293857574463,105993:0.01601123809814453,92690:0.012344121932983398,91310:0.012147068977355957,93177:0.003831148147583008,95414:0.003565073013305664,93288:0.0010845661163330078,109366:0.0',\n",
       " '109366:108544,111059,109674,108379|111059:0.21698147058486938,105993:0.12825262546539307,102082:0.12453973293304443,108544:0.09608495235443115,98821:0.08043324947357178,104956:0.06726205348968506,102497:0.06304371356964111,114719:0.062305450439453125,104266:0.05601847171783447,108943:0.05464494228363037,92537:0.05405533313751221,110764:0.053525567054748535,114671:0.05301165580749512,109674:0.046062350273132324,91310:0.041939496994018555,117472:0.03501474857330322,115491:0.02898859977722168,107044:0.028893351554870605,114280:0.02792191505432129,114770:0.027917861938476562,109951:0.02692568302154541,46957:0.026339411735534668,106191:0.024837970733642578,105086:0.013300657272338867,109081:0.012066841125488281,108311:0.010431528091430664,101268:0.004557371139526367,100666:0.002180933952331543,113149:0.0',\n",
       " '110594:107073,108355,108453,109162,111761,111800|111761:0.28077107667922974,101462:0.24996691942214966,108355:0.23279958963394165,102251:0.2065179944038391,106479:0.1730177402496338,107073:0.1585381031036377,103471:0.12058055400848389,98310:0.1152726411819458,92902:0.11297416687011719,108084:0.09468293190002441,109162:0.06187629699707031,108240:0.05947387218475342,101545:0.053543806076049805,108453:0.05231034755706787,98206:0.0483626127243042,95926:0.04473114013671875,95494:0.037129998207092285,122776:0.02749764919281006,103278:0.025778770446777344,95269:0.019495487213134766,102755:0.01943826675415039,96633:0.017624258995056152,98331:0.01730525493621826,103827:0.011102557182312012,99842:0.01011192798614502,97880:0.008169054985046387,102023:0.007064342498779297,101504:0.00626826286315918,104278:0.0',\n",
       " '111800:107073,110594,108355,108453,109162,111761|92771:0.33090949058532715,91826:0.32932865619659424,104268:0.2918418049812317,99603:0.27936702966690063,111626:0.26741981506347656,100801:0.2666364908218384,121756:0.2608810067176819,110442:0.24127942323684692,92000:0.20200371742248535,98964:0.18632948398590088,111347:0.16828513145446777,109162:0.16526436805725098,109662:0.1411592960357666,110341:0.11552798748016357,107073:0.08929145336151123,91324:0.07406604290008545,107223:0.07189071178436279,89379:0.06559717655181885,102744:0.06425786018371582,121632:0.04082596302032471,107843:0.036374807357788086,92759:0.023986339569091797,92532:0.022179722785949707,122158:0.015159130096435547,57538:0.014483094215393066,92998:0.014253616333007812,102068:0.009648561477661133,110374:0.003933429718017578,102522:0.0',\n",
       " '111761:107073,110594,108355,108453,109162,111800|107073:0.3265615701675415,109162:0.26902860403060913,110594:0.24941211938858032,101462:0.22362536191940308,108355:0.21781694889068604,102251:0.1611114740371704,103827:0.122397780418396,108084:0.12086319923400879,98310:0.1123582124710083,108453:0.08837604522705078,103471:0.08475303649902344,106479:0.07916390895843506,108240:0.07339179515838623,95926:0.03534817695617676,101545:0.030498385429382324,95494:0.027716636657714844,108439:0.02713632583618164,110118:0.02399134635925293,114431:0.02396833896636963,120075:0.02188849449157715,122776:0.014126062393188477,98206:0.01351940631866455,98331:0.011695146560668945,111521:0.010702133178710938,110712:0.00449061393737793,111745:0.002848386764526367,104068:0.0016628503799438477,120554:0.0011894702911376953,105377:0.0',\n",
       " '109162:107073,110594,108355,108453,111761,111800|107073:0.40450239181518555,111761:0.3095540404319763,108453:0.22677117586135864,103827:0.2021477222442627,108355:0.18209266662597656,111800:0.1628183126449585,103471:0.1271883249282837,111347:0.11874306201934814,122776:0.09366333484649658,101462:0.0792304277420044,110594:0.07104277610778809,92000:0.0659782886505127,96633:0.0591273307800293,98331:0.055243492126464844,106479:0.05276799201965332,110341:0.05119311809539795,123542:0.042621731758117676,95494:0.04104042053222656,98310:0.03834998607635498,108439:0.03734135627746582,98206:0.029327392578125,109909:0.01858222484588623,108240:0.014924883842468262,95968:0.012642383575439453,102251:0.011452913284301758,104278:0.0075953006744384766,111521:0.006245970726013184,92759:0.003530144691467285,107319:0.0',\n",
       " '108355:107073,110594,108453,109162,111761,111800|101462:0.22342902421951294,111761:0.20511317253112793,110594:0.18873685598373413,106479:0.1643182635307312,107073:0.1557149887084961,109162:0.12886345386505127,103471:0.11172688007354736,90800:0.09411060810089111,103827:0.07720947265625,108439:0.06205034255981445,123048:0.05590248107910156,98310:0.05550217628479004,95494:0.05060160160064697,105251:0.04582560062408447,108453:0.04497253894805908,102251:0.04403948783874512,115456:0.03424942493438721,98206:0.03402352333068848,103278:0.03209125995635986,106817:0.031360626220703125,111521:0.02621781826019287,114123:0.024350285530090332,122776:0.021214008331298828,96633:0.017366409301757812,123542:0.01658785343170166,123003:0.013103485107421875,96944:0.011065244674682617,121504:0.009454488754272461,108084:0.0',\n",
       " '108453:107073,110594,108355,109162,111761,111800|103827:0.277840793132782,109162:0.22108954191207886,107073:0.1659334897994995,111761:0.12321984767913818,108439:0.12063634395599365,108355:0.09252011775970459,106479:0.09193003177642822,122776:0.09098207950592041,101462:0.06809186935424805,98331:0.06126761436462402,108084:0.059085965156555176,110594:0.05579519271850586,95494:0.04803109169006348,98206:0.041808485984802246,98310:0.036202192306518555,108240:0.027439475059509277,98563:0.020057320594787598,105251:0.016239285469055176,109909:0.015335559844970703,101545:0.013898015022277832,113252:0.013215184211730957,103471:0.01303255558013916,90800:0.01288139820098877,102251:0.009745359420776367,95968:0.009125471115112305,92759:0.006757020950317383,110712:0.005710721015930176,104278:0.002374887466430664,107319:0.0',\n",
       " '114705:114676|105931:0.23029762506484985,108012:0.2061995267868042,94463:0.16154682636260986,97969:0.11119723320007324,101937:0.1026226282119751,115404:0.0732429027557373,95171:0.07246720790863037,116139:0.06476593017578125,116993:0.049636006355285645,122331:0.04864621162414551,120728:0.03716075420379639,120979:0.02225804328918457,98827:0.020618677139282227,92564:0.019483327865600586,116991:0.018201589584350586,108454:0.018009543418884277,120908:0.016173481941223145,94054:0.012584447860717773,100282:0.011838555335998535,100324:0.00915682315826416,99703:0.006845235824584961,105747:0.004777193069458008,109393:0.0038521289825439453,114072:0.0037450790405273438,110876:0.002240896224975586,106542:0.002031683921813965,113639:0.0019501447677612305,121250:0.001831650733947754,101799:0.0',\n",
       " '114676:114705|112930:0.2470502257347107,112931:0.2470502257347107,113669:0.22485220432281494,111908:0.2179989218711853,101867:0.20312190055847168,113492:0.18250572681427002,99186:0.1821916103363037,112843:0.17363858222961426,112844:0.17363858222961426,112523:0.1581127643585205,110900:0.15655028820037842,111881:0.14065766334533691,103562:0.1214454174041748,108401:0.11065101623535156,110879:0.10931968688964844,103513:0.08392727375030518,91407:0.0650787353515625,115350:0.06139504909515381,102648:0.06088411808013916,101448:0.051282644271850586,108061:0.03468775749206543,115349:0.0320131778717041,93827:0.02819371223449707,93828:0.02819371223449707,106268:0.020946860313415527,113591:0.017627716064453125,101337:0.013244152069091797,91245:0.006371855735778809,120564:0.0',\n",
       " '110593:110618|110618:0.27422332763671875,106861:0.20607876777648926,91404:0.1837315559387207,117396:0.16944873332977295,110346:0.149397611618042,100465:0.13005363941192627,108454:0.11282062530517578,94900:0.11151158809661865,94569:0.10475361347198486,90511:0.08952844142913818,98753:0.0874871015548706,122420:0.08717405796051025,95999:0.0841597318649292,96115:0.0805128812789917,109850:0.06954562664031982,120613:0.06953561305999756,104124:0.06720662117004395,122842:0.047012925148010254,95237:0.04550027847290039,121895:0.02718377113342285,94253:0.02673482894897461,115612:0.01757633686065674,98983:0.016803860664367676,110329:0.014078736305236816,123080:0.013761043548583984,98948:0.008765459060668945,121020:0.001695394515991211,112630:0.0003720521926879883,103757:0.0',\n",
       " '110618:110593|99082:0.24566984176635742,110593:0.24291372299194336,100465:0.2194603681564331,104270:0.21453315019607544,115825:0.15632927417755127,90511:0.10340237617492676,116649:0.10084819793701172,110346:0.09327352046966553,110329:0.09141290187835693,96115:0.0781710147857666,102506:0.0769338607788086,108088:0.06150650978088379,122420:0.05961203575134277,94900:0.05608010292053223,106625:0.05078566074371338,112862:0.046539306640625,122004:0.04126322269439697,94253:0.04116475582122803,99560:0.041109442710876465,91404:0.03998851776123047,109850:0.03588378429412842,102582:0.03263676166534424,98753:0.025248050689697266,122842:0.011939287185668945,102370:0.007821202278137207,116490:0.004378199577331543,96223:0.0023075342178344727,106165:0.0004960298538208008,106939:0.0',\n",
       " '102409:102053|104044:0.15099382400512695,99306:0.1004105806350708,101652:0.09613847732543945,115888:0.08325719833374023,115262:0.06261301040649414,104460:0.055224061012268066,94909:0.05441153049468994,110764:0.0486527681350708,104447:0.045432090759277344,102557:0.04213833808898926,106234:0.041385769844055176,113149:0.04022026062011719,107044:0.03975045680999756,115895:0.03812110424041748,118144:0.02914261817932129,102450:0.029022932052612305,105993:0.027721881866455078,103056:0.023234128952026367,90533:0.021085619926452637,113552:0.01536858081817627,91703:0.012598156929016113,105087:0.009218573570251465,92202:0.006447196006774902,89910:0.005552768707275391,93165:0.004476428031921387,106462:0.0032881498336791992,106191:0.0025936365127563477,102398:0.0008771419525146484,96894:0.0',\n",
       " '102053:102409|105371:0.4922822117805481,96880:0.44191426038742065,89620:0.42303985357284546,98549:0.41658055782318115,102106:0.3976496458053589,97178:0.39614707231521606,91703:0.36527717113494873,97179:0.33455055952072144,97168:0.3236006498336792,96894:0.32168376445770264,96888:0.3006795048713684,96947:0.2647027373313904,99110:0.26047879457473755,96471:0.20013463497161865,86960:0.18627464771270752,108190:0.09893155097961426,96945:0.09479343891143799,109853:0.06904947757720947,116196:0.06569766998291016,95489:0.05530083179473877,110609:0.03551650047302246,103714:0.03154027462005615,100223:0.028053998947143555,113633:0.026184439659118652,102736:0.02129971981048584,96944:0.015346646308898926,107859:0.008474946022033691,100517:0.003988385200500488,97095:0.0',\n",
       " '110604:110612|115767:0.1774653196334839,116944:0.15825462341308594,99469:0.12495541572570801,114724:0.10803937911987305,117356:0.10132753849029541,102786:0.0765542984008789,122550:0.06793081760406494,115376:0.06750679016113281,96133:0.05580008029937744,93439:0.05571126937866211,108745:0.04860055446624756,104969:0.04390287399291992,99376:0.03654134273529053,115793:0.03336679935455322,92514:0.027127861976623535,98659:0.022405266761779785,97546:0.019362807273864746,110612:0.014008164405822754,96372:0.013920307159423828,95413:0.011096596717834473,114062:0.010454177856445312,92178:0.009595394134521484,108126:0.009032130241394043,100365:0.008712172508239746,95414:0.008603930473327637,114596:0.005584836006164551,101486:0.005530118942260742,92823:0.0012073516845703125,115543:0.0',\n",
       " '110612:110604|109833:0.12061536312103271,113632:0.07972061634063721,92820:0.07967960834503174,103736:0.07654643058776855,91393:0.06962072849273682,108896:0.06514501571655273,98545:0.06431341171264648,99610:0.05564522743225098,113145:0.05414474010467529,107061:0.042258381843566895,95617:0.03755629062652588,101480:0.033821702003479004,77515:0.03332841396331787,111557:0.028989315032958984,111650:0.026064753532409668,99217:0.02477562427520752,92822:0.020562291145324707,113591:0.018076300621032715,87969:0.016690969467163086,110900:0.014474153518676758,94947:0.01051485538482666,99909:0.0055321455001831055,94935:0.004815578460693359,101486:0.0028219223022460938,93289:0.0018314123153686523,108745:0.0014365911483764648,97503:0.0007119178771972656,103284:0.0003694295883178711,110604:0.0',\n",
       " '116738:116938,117027|93311:0.09643280506134033,93306:0.09452974796295166,106542:0.08952963352203369,120979:0.07057976722717285,113106:0.06894874572753906,106589:0.06457722187042236,105862:0.05214118957519531,99106:0.0507352352142334,92166:0.04800224304199219,99737:0.04745817184448242,118749:0.040329933166503906,92713:0.03142988681793213,122331:0.0270998477935791,109630:0.026200175285339355,90796:0.025465846061706543,92977:0.022129297256469727,109485:0.021910905838012695,91795:0.020577073097229004,115376:0.02012181282043457,107738:0.018723130226135254,110740:0.01589977741241455,94580:0.011848092079162598,92178:0.006447434425354004,107833:0.0034192800521850586,102851:0.002656221389770508,92823:0.002144455909729004,115822:0.0011703968048095703,98664:0.0011620521545410156,93192:0.0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoded_anchor\n",
    "# model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, \n",
    "                                                                   bug_train_ids, method='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.67,\n",
       " '2 - recall_at_10': 0.74,\n",
       " '3 - recall_at_15': 0.77,\n",
       " '4 - recall_at_20': 0.79,\n",
       " '5 - recall_at_25': 0.81}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
