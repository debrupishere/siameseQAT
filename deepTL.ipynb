{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# DeepQL with tensorflow\n",
    "\n",
    "https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'deepTL_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72234"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b18579505fb4705a2b6526fea1a1ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebc6721f125463483e5a6c36f0ce7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.74 s, sys: 629 ms, total: 7.36 s\n",
      "Wall time: 7.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c925b02b44bb478c81c3c029c5f0149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_train='train_chronological', path_test='test_chronological'\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28170, 23787],\n",
       " [12823, 22223],\n",
       " [23765, 21850],\n",
       " [102722, 89006],\n",
       " [43930, 46308],\n",
       " [32405, 14334],\n",
       " [39496, 24193],\n",
       " [15393, 17482],\n",
       " [8323, 6808],\n",
       " [36773, 55415]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '4\\n',\n",
       " 'bug_status': '2\\n',\n",
       " 'component': '8\\n',\n",
       " 'creation_ts': '2006-03-06 09:55:00 +0000',\n",
       " 'delta_ts': '2013-08-07 14:40:21 +0000',\n",
       " 'description': '[CLS] load bug doc . there are examples for situations when the promotion / demo ##tion of chapters does not work or does not work correctly . [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  7170, 11829,  9986,  1012,  2045,  2024,  4973,  2005,\n",
       "         8146,  2043,  1996,  4712,  1013,  9703,  3508,  1997,  9159,\n",
       "         2515,   102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 62806,\n",
       " 'priority': '0\\n',\n",
       " 'product': '27\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_token': array([  101,  5326,  1013,  9703,  2618,  1997,  9159, 12491,  2011,\n",
       "         7251,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  7170, 11829,  9986,  1012,  2045,  2024,\n",
       "         4973,  2005,  8146,  2043,  1996,  4712,  1013,  9703,  3508,\n",
       "         1997,  9159,  2515,   102]),\n",
       " 'title': '[CLS] promote / demo ##te of chapters disturbed by tables [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  5326,  1013,  9703,  2618,  1997,  9159, 12491,  2011,\n",
       "         7251,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '259\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 13626)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    title_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_T), 0)\n",
    "    description_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "    for i, bug_id in enumerate(batch_bugs):\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug, index=i, title_ids=title_ids, description_ids=description_ids)\n",
    "\n",
    "    batch_features['title'] = { 'token' : np.array(batch_features['title']), 'segment' : title_ids }\n",
    "    batch_features['desc'] = { 'token' : np.array(batch_features['desc']), 'segment' : description_ids }\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title']['token'], valid_input_sample['title']['segment'], \n",
    "                   valid_input_sample['description']['token'], valid_input_sample['description']['segment'],\n",
    "                   valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 20), (384, 20), (384, 729), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, \\\n",
    "valid_input_sample['description']['token'].shape, \\\n",
    "valid_input_sample['title']['segment'].shape, \\\n",
    "valid_input_sample['description']['segment'].shape, \\\n",
    "valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 7986)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 18562'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    f = open(embed_path, 'rb')\n",
    "    #num_lines = sum(1 for line in open(embed_path, 'rb'))\n",
    "\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embeddings_index[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    i = 0\n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4047e0a501ce4933907a0c5b8addf7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e06b3a200a49ae85d474266e109dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18562), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 20s, sys: 2.69 s, total: 1min 23s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#     )\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=True,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "#     model = get_model(\n",
    "#         token_num=len(token_dict),\n",
    "#         head_num=10,\n",
    "#         transformer_num=layer_num,\n",
    "#         embed_dim=100,\n",
    "#         feed_forward_dim=100,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#         pos_num=MAX_SEQUENCE_LENGTH,\n",
    "#         dropout_rate=0.05,\n",
    "#     )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    outputs = model.get_layer('Encoder-{}-FeedForward-Norm'.format(layer_num)).output\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "    outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "\n",
    "def quintet_loss(y_true, y_pred):\n",
    "    return triplet_loss(y_pred)\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    #del y_true\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    # optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "    \n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=quintet_loss) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepTL_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 768)          80346736    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 768)          80346736    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1836)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1837)         0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 160,912,472\n",
      "Trainable params: 440,296\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 1 Loss: 0.50\n",
      "Epoch: 2 Loss: 0.54\n",
      "Epoch: 3 Loss: 0.49\n",
      "Epoch: 4 Loss: 0.47\n",
      "Epoch: 5 Loss: 0.53\n",
      "Epoch: 6 Loss: 0.50\n",
      "Epoch: 7 Loss: 0.53\n",
      "Epoch: 8 Loss: 0.50\n",
      "Epoch: 9 Loss: 0.47\n",
      "Epoch: 10 Loss: 0.48\n",
      "Epoch: 11 Loss: 0.46\n",
      "Epoch: 12 Loss: 0.49\n",
      "Epoch: 13 Loss: 0.50\n",
      "Epoch: 14 Loss: 0.39\n",
      "Epoch: 15 Loss: 0.47\n",
      "Epoch: 16 Loss: 0.45\n",
      "Epoch: 17 Loss: 0.44\n",
      "Epoch: 18 Loss: 0.36\n",
      "Epoch: 19 Loss: 0.38\n",
      "Epoch: 20 Loss: 0.25\n",
      "Epoch: 21 Loss: 0.25\n",
      "Epoch: 22 Loss: 0.29\n",
      "Epoch: 23 Loss: 0.25\n",
      "Epoch: 24 Loss: 0.20\n",
      "Epoch: 25 Loss: 0.22\n",
      "Epoch: 26 Loss: 0.27\n",
      "Epoch: 27 Loss: 0.18\n",
      "Epoch: 28 Loss: 0.19\n",
      "Epoch: 29 Loss: 0.16\n",
      "Epoch: 30 Loss: 0.11\n",
      "Epoch: 31 Loss: 0.12\n",
      "Epoch: 32 Loss: 0.21\n",
      "Epoch: 33 Loss: 0.16\n",
      "Epoch: 34 Loss: 0.11\n",
      "Epoch: 35 Loss: 0.11\n",
      "Epoch: 36 Loss: 0.21\n",
      "Epoch: 37 Loss: 0.14\n",
      "Epoch: 38 Loss: 0.12\n",
      "Epoch: 39 Loss: 0.07\n",
      "Epoch: 40 Loss: 0.10\n",
      "Epoch: 41 Loss: 0.10\n",
      "Epoch: 42 Loss: 0.09\n",
      "Epoch: 43 Loss: 0.10\n",
      "Epoch: 44 Loss: 0.06\n",
      "Epoch: 45 Loss: 0.11\n",
      "Epoch: 46 Loss: 0.05\n",
      "Epoch: 47 Loss: 0.03\n",
      "Epoch: 48 Loss: 0.06\n",
      "Epoch: 49 Loss: 0.05\n",
      "Epoch: 50 Loss: 0.11\n",
      "Epoch: 51 Loss: 0.07\n",
      "Epoch: 52 Loss: 0.07\n",
      "Epoch: 53 Loss: 0.07\n",
      "Epoch: 54 Loss: 0.06\n",
      "Epoch: 55 Loss: 0.05\n",
      "Epoch: 56 Loss: 0.09\n",
      "Epoch: 57 Loss: 0.03\n",
      "Epoch: 58 Loss: 0.03\n",
      "Epoch: 59 Loss: 0.02\n",
      "Epoch: 60 Loss: 0.04\n",
      "Epoch: 61 Loss: 0.05\n",
      "Epoch: 62 Loss: 0.05\n",
      "Epoch: 63 Loss: 0.04\n",
      "Epoch: 64 Loss: 0.07\n",
      "Epoch: 65 Loss: 0.09\n",
      "Epoch: 66 Loss: 0.07\n",
      "Epoch: 67 Loss: 0.05\n",
      "Epoch: 68 Loss: 0.05\n",
      "Epoch: 69 Loss: 0.05\n",
      "Epoch: 70 Loss: 0.08\n",
      "Epoch: 71 Loss: 0.07\n",
      "Epoch: 72 Loss: 0.04\n",
      "Epoch: 73 Loss: 0.05\n",
      "Epoch: 74 Loss: 0.03\n",
      "Epoch: 75 Loss: 0.02\n",
      "Epoch: 76 Loss: 0.05\n",
      "Epoch: 77 Loss: 0.05\n",
      "Epoch: 78 Loss: 0.08\n",
      "Epoch: 79 Loss: 0.06\n",
      "Epoch: 80 Loss: 0.04\n",
      "Epoch: 81 Loss: 0.03\n",
      "Epoch: 82 Loss: 0.05\n",
      "Epoch: 83 Loss: 0.06\n",
      "Epoch: 84 Loss: 0.05\n",
      "Epoch: 85 Loss: 0.02\n",
      "Epoch: 86 Loss: 0.05\n",
      "Epoch: 87 Loss: 0.05\n",
      "Epoch: 88 Loss: 0.05\n",
      "Epoch: 89 Loss: 0.07\n",
      "Epoch: 90 Loss: 0.05\n",
      "Epoch: 91 Loss: 0.02\n",
      "Epoch: 92 Loss: 0.02\n",
      "Epoch: 93 Loss: 0.06\n",
      "Epoch: 94 Loss: 0.07\n",
      "Epoch: 95 Loss: 0.02\n",
      "Epoch: 96 Loss: 0.05\n",
      "Epoch: 97 Loss: 0.02\n",
      "Epoch: 98 Loss: 0.06\n",
      "Epoch: 99 Loss: 0.03\n",
      "Epoch: 100 Loss: 0.04\n",
      "Epoch: 101 Loss: 0.02\n",
      "Epoch: 102 Loss: 0.05\n",
      "Epoch: 103 Loss: 0.03\n",
      "Epoch: 104 Loss: 0.06\n",
      "Epoch: 105 Loss: 0.04\n",
      "Epoch: 106 Loss: 0.04\n",
      "Epoch: 107 Loss: 0.05\n",
      "Epoch: 108 Loss: 0.06\n",
      "Epoch: 109 Loss: 0.03\n",
      "Epoch: 110 Loss: 0.02\n",
      "Epoch: 111 Loss: 0.03\n",
      "Epoch: 112 Loss: 0.04\n",
      "Epoch: 113 Loss: 0.04\n",
      "Epoch: 114 Loss: 0.03\n",
      "Epoch: 115 Loss: 0.03\n",
      "Epoch: 116 Loss: 0.02\n",
      "Epoch: 117 Loss: 0.02\n",
      "Epoch: 118 Loss: 0.01\n",
      "Epoch: 119 Loss: 0.04\n",
      "Epoch: 120 Loss: 0.05\n",
      "Epoch: 121 Loss: 0.10\n",
      "Epoch: 122 Loss: 0.02\n",
      "Epoch: 123 Loss: 0.02\n",
      "Epoch: 124 Loss: 0.02\n",
      "Epoch: 125 Loss: 0.03\n",
      "Epoch: 126 Loss: 0.02\n",
      "Epoch: 127 Loss: 0.05\n",
      "Epoch: 128 Loss: 0.03\n",
      "Epoch: 129 Loss: 0.04\n",
      "Epoch: 130 Loss: 0.03\n",
      "Epoch: 131 Loss: 0.03\n",
      "Epoch: 132 Loss: 0.04\n",
      "Epoch: 133 Loss: 0.02\n",
      "Epoch: 134 Loss: 0.02\n",
      "Epoch: 135 Loss: 0.02\n",
      "Epoch: 136 Loss: 0.06\n",
      "Epoch: 137 Loss: 0.02\n",
      "Epoch: 138 Loss: 0.02\n",
      "Epoch: 139 Loss: 0.04\n",
      "Epoch: 140 Loss: 0.03\n",
      "Epoch: 141 Loss: 0.05\n",
      "Epoch: 142 Loss: 0.02\n",
      "Epoch: 143 Loss: 0.02\n",
      "Epoch: 144 Loss: 0.01\n",
      "Epoch: 145 Loss: 0.01\n",
      "Epoch: 146 Loss: 0.03\n",
      "Epoch: 147 Loss: 0.05\n",
      "Epoch: 148 Loss: 0.02\n",
      "Epoch: 149 Loss: 0.01\n",
      "Epoch: 150 Loss: 0.03\n",
      "Epoch: 151 Loss: 0.04\n",
      "Epoch: 152 Loss: 0.03\n",
      "Epoch: 153 Loss: 0.01\n",
      "Epoch: 154 Loss: 0.03\n",
      "Epoch: 155 Loss: 0.01\n",
      "Epoch: 156 Loss: 0.05\n",
      "Epoch: 157 Loss: 0.04\n",
      "Epoch: 158 Loss: 0.11\n",
      "Epoch: 159 Loss: 0.02\n",
      "Epoch: 160 Loss: 0.02\n",
      "Epoch: 161 Loss: 0.04\n",
      "Epoch: 162 Loss: 0.03\n",
      "Epoch: 163 Loss: 0.03\n",
      "Epoch: 164 Loss: 0.02\n",
      "Epoch: 165 Loss: 0.01\n",
      "Epoch: 166 Loss: 0.03\n",
      "Epoch: 167 Loss: 0.02\n",
      "Epoch: 168 Loss: 0.02\n",
      "Epoch: 169 Loss: 0.03\n",
      "Epoch: 170 Loss: 0.02\n",
      "Epoch: 171 Loss: 0.01\n",
      "Epoch: 172 Loss: 0.03\n",
      "Epoch: 173 Loss: 0.04\n",
      "Epoch: 174 Loss: 0.01\n",
      "Epoch: 175 Loss: 0.03\n",
      "Epoch: 176 Loss: 0.01\n",
      "Epoch: 177 Loss: 0.04\n",
      "Epoch: 178 Loss: 0.02\n",
      "Epoch: 179 Loss: 0.03\n",
      "Epoch: 180 Loss: 0.02\n",
      "Epoch: 181 Loss: 0.03\n",
      "Epoch: 182 Loss: 0.04\n",
      "Epoch: 183 Loss: 0.04\n",
      "Epoch: 184 Loss: 0.03\n",
      "Epoch: 185 Loss: 0.03\n",
      "Epoch: 186 Loss: 0.01\n",
      "Epoch: 187 Loss: 0.01\n",
      "Epoch: 188 Loss: 0.02\n",
      "Epoch: 189 Loss: 0.01\n",
      "Epoch: 190 Loss: 0.02\n",
      "Epoch: 191 Loss: 0.02\n",
      "Epoch: 192 Loss: 0.03\n",
      "Epoch: 193 Loss: 0.03\n",
      "Epoch: 194 Loss: 0.07\n",
      "Epoch: 195 Loss: 0.01\n",
      "Epoch: 196 Loss: 0.08\n",
      "Epoch: 197 Loss: 0.02\n",
      "Epoch: 198 Loss: 0.01\n",
      "Epoch: 199 Loss: 0.02\n",
      "Epoch: 200 Loss: 0.04\n",
      "Epoch: 201 Loss: 0.02\n",
      "Epoch: 202 Loss: 0.03\n",
      "Epoch: 203 Loss: 0.01\n",
      "Epoch: 204 Loss: 0.04\n",
      "Epoch: 205 Loss: 0.02\n",
      "Epoch: 206 Loss: 0.03\n",
      "Epoch: 207 Loss: 0.01\n",
      "Epoch: 208 Loss: 0.01\n",
      "Epoch: 209 Loss: 0.03\n",
      "Epoch: 210 Loss: 0.01\n",
      "Epoch: 211 Loss: 0.02\n",
      "Epoch: 212 Loss: 0.04\n",
      "Epoch: 213 Loss: 0.01\n",
      "Epoch: 214 Loss: 0.01\n",
      "Epoch: 215 Loss: 0.01\n",
      "Epoch: 216 Loss: 0.03\n",
      "Epoch: 217 Loss: 0.02\n",
      "Epoch: 218 Loss: 0.01\n",
      "Epoch: 219 Loss: 0.02\n",
      "Epoch: 220 Loss: 0.03\n",
      "Epoch: 221 Loss: 0.03\n",
      "Epoch: 222 Loss: 0.05\n",
      "Epoch: 223 Loss: 0.02\n",
      "Epoch: 224 Loss: 0.02\n",
      "Epoch: 225 Loss: 0.02\n",
      "Epoch: 226 Loss: 0.01\n",
      "Epoch: 227 Loss: 0.02\n",
      "Epoch: 228 Loss: 0.01\n",
      "Epoch: 229 Loss: 0.03\n",
      "Epoch: 230 Loss: 0.06\n",
      "Epoch: 231 Loss: 0.07\n",
      "Epoch: 232 Loss: 0.05\n",
      "Epoch: 233 Loss: 0.02\n",
      "Epoch: 234 Loss: 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235 Loss: 0.02\n",
      "Epoch: 236 Loss: 0.03\n",
      "Epoch: 237 Loss: 0.02\n",
      "Epoch: 238 Loss: 0.02\n",
      "Epoch: 239 Loss: 0.03\n",
      "Epoch: 240 Loss: 0.02\n",
      "Epoch: 241 Loss: 0.02\n",
      "Epoch: 242 Loss: 0.02\n",
      "Epoch: 243 Loss: 0.02\n",
      "Epoch: 244 Loss: 0.01\n",
      "Epoch: 245 Loss: 0.03\n",
      "Epoch: 246 Loss: 0.01\n",
      "Epoch: 247 Loss: 0.03\n",
      "Epoch: 248 Loss: 0.02\n",
      "Epoch: 249 Loss: 0.02\n",
      "Epoch: 250 Loss: 0.01\n",
      "Epoch: 251 Loss: 0.04\n",
      "Epoch: 252 Loss: 0.01\n",
      "Epoch: 253 Loss: 0.04\n",
      "Epoch: 254 Loss: 0.01\n",
      "Epoch: 255 Loss: 0.02\n",
      "Epoch: 256 Loss: 0.02\n",
      "Epoch: 257 Loss: 0.02\n",
      "Epoch: 258 Loss: 0.01\n",
      "Epoch: 259 Loss: 0.02\n",
      "Epoch: 260 Loss: 0.02\n",
      "Epoch: 261 Loss: 0.04\n",
      "Epoch: 262 Loss: 0.02\n",
      "Epoch: 263 Loss: 0.02\n",
      "Epoch: 264 Loss: 0.02\n",
      "Epoch: 265 Loss: 0.03\n",
      "Epoch: 266 Loss: 0.02\n",
      "Epoch: 267 Loss: 0.02\n",
      "Epoch: 268 Loss: 0.02\n",
      "Epoch: 269 Loss: 0.05\n",
      "Epoch: 270 Loss: 0.03\n",
      "Epoch: 271 Loss: 0.01\n",
      "Epoch: 272 Loss: 0.01\n",
      "Epoch: 273 Loss: 0.02\n",
      "Epoch: 274 Loss: 0.01\n",
      "Epoch: 275 Loss: 0.02\n",
      "Epoch: 276 Loss: 0.02\n",
      "Epoch: 277 Loss: 0.03\n",
      "Epoch: 278 Loss: 0.06\n",
      "Epoch: 279 Loss: 0.01\n",
      "Epoch: 280 Loss: 0.03\n",
      "Epoch: 281 Loss: 0.03\n",
      "Epoch: 282 Loss: 0.03\n",
      "Epoch: 283 Loss: 0.04\n",
      "Epoch: 284 Loss: 0.01\n",
      "Epoch: 285 Loss: 0.02\n",
      "Epoch: 286 Loss: 0.02\n",
      "Epoch: 287 Loss: 0.03\n",
      "Epoch: 288 Loss: 0.02\n",
      "Epoch: 289 Loss: 0.01\n",
      "Epoch: 290 Loss: 0.04\n",
      "Epoch: 291 Loss: 0.01\n",
      "Epoch: 292 Loss: 0.01\n",
      "Epoch: 293 Loss: 0.03\n",
      "Epoch: 294 Loss: 0.02\n",
      "Epoch: 295 Loss: 0.02\n",
      "Epoch: 296 Loss: 0.01\n",
      "Epoch: 297 Loss: 0.04\n",
      "Epoch: 298 Loss: 0.01\n",
      "Epoch: 299 Loss: 0.01\n",
      "Epoch: 300 Loss: 0.03\n",
      "Epoch: 301 Loss: 0.01\n",
      "Epoch: 302 Loss: 0.01\n",
      "Epoch: 303 Loss: 0.02\n",
      "Epoch: 304 Loss: 0.04\n",
      "Epoch: 305 Loss: 0.02\n",
      "Epoch: 306 Loss: 0.02\n",
      "Epoch: 307 Loss: 0.01\n",
      "Epoch: 308 Loss: 0.03\n",
      "Epoch: 309 Loss: 0.01\n",
      "Epoch: 310 Loss: 0.02\n",
      "Epoch: 311 Loss: 0.02\n",
      "Epoch: 312 Loss: 0.01\n",
      "Epoch: 313 Loss: 0.02\n",
      "Epoch: 314 Loss: 0.03\n",
      "Epoch: 315 Loss: 0.03\n",
      "Epoch: 316 Loss: 0.01\n",
      "Epoch: 317 Loss: 0.01\n",
      "Epoch: 318 Loss: 0.02\n",
      "Epoch: 319 Loss: 0.01\n",
      "Epoch: 320 Loss: 0.01\n",
      "Epoch: 321 Loss: 0.01\n",
      "Epoch: 322 Loss: 0.06\n",
      "Epoch: 323 Loss: 0.02\n",
      "Epoch: 324 Loss: 0.05\n",
      "Epoch: 325 Loss: 0.01\n",
      "Epoch: 326 Loss: 0.02\n",
      "Epoch: 327 Loss: 0.02\n",
      "Epoch: 328 Loss: 0.04\n",
      "Epoch: 329 Loss: 0.01\n",
      "Epoch: 330 Loss: 0.01\n",
      "Epoch: 331 Loss: 0.03\n",
      "Epoch: 332 Loss: 0.02\n",
      "Epoch: 333 Loss: 0.01\n",
      "Epoch: 334 Loss: 0.02\n",
      "Epoch: 335 Loss: 0.01\n",
      "Epoch: 336 Loss: 0.00\n",
      "Epoch: 337 Loss: 0.02\n",
      "Epoch: 338 Loss: 0.02\n",
      "Epoch: 339 Loss: 0.01\n",
      "Epoch: 340 Loss: 0.03\n",
      "Epoch: 341 Loss: 0.02\n",
      "Epoch: 342 Loss: 0.04\n",
      "Epoch: 343 Loss: 0.02\n",
      "Epoch: 344 Loss: 0.03\n",
      "Epoch: 345 Loss: 0.02\n",
      "Epoch: 346 Loss: 0.01\n",
      "Epoch: 347 Loss: 0.03\n",
      "Epoch: 348 Loss: 0.01\n",
      "Epoch: 349 Loss: 0.01\n",
      "Epoch: 350 Loss: 0.01\n",
      "Epoch: 351 Loss: 0.03\n",
      "Epoch: 352 Loss: 0.03\n",
      "Epoch: 353 Loss: 0.02\n",
      "Epoch: 354 Loss: 0.04\n",
      "Epoch: 355 Loss: 0.02\n",
      "Epoch: 356 Loss: 0.01\n",
      "Epoch: 357 Loss: 0.01\n",
      "Epoch: 358 Loss: 0.02\n",
      "Epoch: 359 Loss: 0.04\n",
      "Epoch: 360 Loss: 0.01\n",
      "Epoch: 361 Loss: 0.02\n",
      "Epoch: 362 Loss: 0.02\n",
      "Epoch: 363 Loss: 0.02\n",
      "Epoch: 364 Loss: 0.02\n",
      "Epoch: 365 Loss: 0.03\n",
      "Epoch: 366 Loss: 0.03\n",
      "Epoch: 367 Loss: 0.03\n",
      "Epoch: 368 Loss: 0.02\n",
      "Epoch: 369 Loss: 0.05\n",
      "Epoch: 370 Loss: 0.01\n",
      "Epoch: 371 Loss: 0.02\n",
      "Epoch: 372 Loss: 0.01\n",
      "Epoch: 373 Loss: 0.01\n",
      "Epoch: 374 Loss: 0.02\n",
      "Epoch: 375 Loss: 0.01\n",
      "Epoch: 376 Loss: 0.01\n",
      "Epoch: 377 Loss: 0.01\n",
      "Epoch: 378 Loss: 0.01\n",
      "Epoch: 379 Loss: 0.01\n",
      "Epoch: 380 Loss: 0.02\n",
      "Epoch: 381 Loss: 0.02\n",
      "Epoch: 382 Loss: 0.01\n",
      "Epoch: 383 Loss: 0.01\n",
      "Epoch: 384 Loss: 0.03\n",
      "Epoch: 385 Loss: 0.01\n",
      "Epoch: 386 Loss: 0.00\n",
      "Epoch: 387 Loss: 0.02\n",
      "Epoch: 388 Loss: 0.02\n",
      "Epoch: 389 Loss: 0.01\n",
      "Epoch: 390 Loss: 0.02\n",
      "Epoch: 391 Loss: 0.02\n",
      "Epoch: 392 Loss: 0.01\n",
      "Epoch: 393 Loss: 0.02\n",
      "Epoch: 394 Loss: 0.02\n",
      "Epoch: 395 Loss: 0.00\n",
      "Epoch: 396 Loss: 0.01\n",
      "Epoch: 397 Loss: 0.01\n",
      "Epoch: 398 Loss: 0.02\n",
      "Epoch: 399 Loss: 0.01\n",
      "Epoch: 400 Loss: 0.01\n",
      "Epoch: 401 Loss: 0.01\n",
      "Epoch: 402 Loss: 0.02\n",
      "Epoch: 403 Loss: 0.01\n",
      "Epoch: 404 Loss: 0.01\n",
      "Epoch: 405 Loss: 0.01\n",
      "Epoch: 406 Loss: 0.01\n",
      "Epoch: 407 Loss: 0.01\n",
      "Epoch: 408 Loss: 0.01\n",
      "Epoch: 409 Loss: 0.01\n",
      "Epoch: 410 Loss: 0.02\n",
      "Epoch: 411 Loss: 0.03\n",
      "Epoch: 412 Loss: 0.01\n",
      "Epoch: 413 Loss: 0.02\n",
      "Epoch: 414 Loss: 0.02\n",
      "Epoch: 415 Loss: 0.05\n",
      "Epoch: 416 Loss: 0.03\n",
      "Epoch: 417 Loss: 0.01\n",
      "Epoch: 418 Loss: 0.01\n",
      "Epoch: 419 Loss: 0.01\n",
      "Epoch: 420 Loss: 0.01\n",
      "Epoch: 421 Loss: 0.02\n",
      "Epoch: 422 Loss: 0.02\n",
      "Epoch: 423 Loss: 0.02\n",
      "Epoch: 424 Loss: 0.01\n",
      "Epoch: 425 Loss: 0.03\n",
      "Epoch: 426 Loss: 0.01\n",
      "Epoch: 427 Loss: 0.02\n",
      "Epoch: 428 Loss: 0.02\n",
      "Epoch: 429 Loss: 0.01\n",
      "Epoch: 430 Loss: 0.02\n",
      "Epoch: 431 Loss: 0.01\n",
      "Epoch: 432 Loss: 0.07\n",
      "Epoch: 433 Loss: 0.01\n",
      "Epoch: 434 Loss: 0.01\n",
      "Epoch: 435 Loss: 0.01\n",
      "Epoch: 436 Loss: 0.02\n",
      "Epoch: 437 Loss: 0.02\n",
      "Epoch: 438 Loss: 0.01\n",
      "Epoch: 439 Loss: 0.01\n",
      "Epoch: 440 Loss: 0.03\n",
      "Epoch: 441 Loss: 0.01\n",
      "Epoch: 442 Loss: 0.02\n",
      "Epoch: 443 Loss: 0.01\n",
      "Epoch: 444 Loss: 0.05\n",
      "Epoch: 445 Loss: 0.03\n",
      "Epoch: 446 Loss: 0.01\n",
      "Epoch: 447 Loss: 0.03\n",
      "Epoch: 448 Loss: 0.03\n",
      "Epoch: 449 Loss: 0.05\n",
      "Epoch: 450 Loss: 0.02\n",
      "Epoch: 451 Loss: 0.02\n",
      "Epoch: 452 Loss: 0.01\n",
      "Epoch: 453 Loss: 0.01\n",
      "Epoch: 454 Loss: 0.01\n",
      "Epoch: 455 Loss: 0.02\n",
      "Epoch: 456 Loss: 0.01\n",
      "Epoch: 457 Loss: 0.01\n",
      "Epoch: 458 Loss: 0.03\n",
      "Epoch: 459 Loss: 0.02\n",
      "Epoch: 460 Loss: 0.01\n",
      "Epoch: 461 Loss: 0.01\n",
      "Epoch: 462 Loss: 0.01\n",
      "Epoch: 463 Loss: 0.01\n",
      "Epoch: 464 Loss: 0.05\n",
      "Epoch: 465 Loss: 0.01\n",
      "Epoch: 466 Loss: 0.02\n",
      "Epoch: 467 Loss: 0.02\n",
      "Epoch: 468 Loss: 0.03\n",
      "Epoch: 469 Loss: 0.01\n",
      "Epoch: 470 Loss: 0.01\n",
      "Epoch: 471 Loss: 0.04\n",
      "Epoch: 472 Loss: 0.02\n",
      "Epoch: 473 Loss: 0.02\n",
      "Epoch: 474 Loss: 0.02\n",
      "Epoch: 475 Loss: 0.02\n",
      "Epoch: 476 Loss: 0.02\n",
      "Epoch: 477 Loss: 0.03\n",
      "Epoch: 478 Loss: 0.02\n",
      "Epoch: 479 Loss: 0.02\n",
      "Epoch: 480 Loss: 0.01\n",
      "Epoch: 481 Loss: 0.02\n",
      "Epoch: 482 Loss: 0.01\n",
      "Epoch: 483 Loss: 0.02\n",
      "Epoch: 484 Loss: 0.01\n",
      "Epoch: 485 Loss: 0.01\n",
      "Epoch: 486 Loss: 0.03\n",
      "Epoch: 487 Loss: 0.01\n",
      "Epoch: 488 Loss: 0.02\n",
      "Epoch: 489 Loss: 0.01\n",
      "Epoch: 490 Loss: 0.01\n",
      "Epoch: 491 Loss: 0.01\n",
      "Epoch: 492 Loss: 0.01\n",
      "Epoch: 493 Loss: 0.01\n",
      "Epoch: 494 Loss: 0.00\n",
      "Epoch: 495 Loss: 0.01\n",
      "Epoch: 496 Loss: 0.02\n",
      "Epoch: 497 Loss: 0.01\n",
      "Epoch: 498 Loss: 0.03\n",
      "Epoch: 499 Loss: 0.03\n",
      "Epoch: 500 Loss: 0.01\n",
      "Epoch: 501 Loss: 0.02\n",
      "Epoch: 502 Loss: 0.02\n",
      "Epoch: 503 Loss: 0.04\n",
      "Epoch: 504 Loss: 0.02\n",
      "Epoch: 505 Loss: 0.02\n",
      "Epoch: 506 Loss: 0.01\n",
      "Epoch: 507 Loss: 0.01\n",
      "Epoch: 508 Loss: 0.02\n",
      "Epoch: 509 Loss: 0.01\n",
      "Epoch: 510 Loss: 0.01\n",
      "Epoch: 511 Loss: 0.02\n",
      "Epoch: 512 Loss: 0.01\n",
      "Epoch: 513 Loss: 0.02\n",
      "Epoch: 514 Loss: 0.02\n",
      "Epoch: 515 Loss: 0.01\n",
      "Epoch: 516 Loss: 0.02\n",
      "Epoch: 517 Loss: 0.01\n",
      "Epoch: 518 Loss: 0.02\n",
      "Epoch: 519 Loss: 0.03\n",
      "Epoch: 520 Loss: 0.02\n",
      "Epoch: 521 Loss: 0.01\n",
      "Epoch: 522 Loss: 0.02\n",
      "Epoch: 523 Loss: 0.01\n",
      "Epoch: 524 Loss: 0.02\n",
      "Epoch: 525 Loss: 0.02\n",
      "Epoch: 526 Loss: 0.01\n",
      "Epoch: 527 Loss: 0.02\n",
      "Epoch: 528 Loss: 0.02\n",
      "Epoch: 529 Loss: 0.02\n",
      "Epoch: 530 Loss: 0.02\n",
      "Epoch: 531 Loss: 0.02\n",
      "Epoch: 532 Loss: 0.02\n",
      "Epoch: 533 Loss: 0.01\n",
      "Epoch: 534 Loss: 0.01\n",
      "Epoch: 535 Loss: 0.01\n",
      "Epoch: 536 Loss: 0.04\n",
      "Epoch: 537 Loss: 0.01\n",
      "Epoch: 538 Loss: 0.04\n",
      "Epoch: 539 Loss: 0.02\n",
      "Epoch: 540 Loss: 0.02\n",
      "Epoch: 541 Loss: 0.01\n",
      "Epoch: 542 Loss: 0.00\n",
      "Epoch: 543 Loss: 0.02\n",
      "Epoch: 544 Loss: 0.04\n",
      "Epoch: 545 Loss: 0.01\n",
      "Epoch: 546 Loss: 0.00\n",
      "Epoch: 547 Loss: 0.05\n",
      "Epoch: 548 Loss: 0.01\n",
      "Epoch: 549 Loss: 0.02\n",
      "Epoch: 550 Loss: 0.02\n",
      "Epoch: 551 Loss: 0.01\n",
      "Epoch: 552 Loss: 0.01\n",
      "Epoch: 553 Loss: 0.01\n",
      "Epoch: 554 Loss: 0.01\n",
      "Epoch: 555 Loss: 0.01\n",
      "Epoch: 556 Loss: 0.02\n",
      "Epoch: 557 Loss: 0.01\n",
      "Epoch: 558 Loss: 0.03\n",
      "Epoch: 559 Loss: 0.01\n",
      "Epoch: 560 Loss: 0.02\n",
      "Epoch: 561 Loss: 0.03\n",
      "Epoch: 562 Loss: 0.00\n",
      "Epoch: 563 Loss: 0.01\n",
      "Epoch: 564 Loss: 0.01\n",
      "Epoch: 565 Loss: 0.02\n",
      "Epoch: 566 Loss: 0.01\n",
      "Epoch: 567 Loss: 0.00\n",
      "Epoch: 568 Loss: 0.04\n",
      "Epoch: 569 Loss: 0.02\n",
      "Epoch: 570 Loss: 0.01\n",
      "Epoch: 571 Loss: 0.01\n",
      "Epoch: 572 Loss: 0.01\n",
      "Epoch: 573 Loss: 0.03\n",
      "Epoch: 574 Loss: 0.05\n",
      "Epoch: 575 Loss: 0.01\n",
      "Epoch: 576 Loss: 0.01\n",
      "Epoch: 577 Loss: 0.01\n",
      "Epoch: 578 Loss: 0.05\n",
      "Epoch: 579 Loss: 0.01\n",
      "Epoch: 580 Loss: 0.01\n",
      "Epoch: 581 Loss: 0.02\n",
      "Epoch: 582 Loss: 0.01\n",
      "Epoch: 583 Loss: 0.01\n",
      "Epoch: 584 Loss: 0.03\n",
      "Epoch: 585 Loss: 0.02\n",
      "Epoch: 586 Loss: 0.02\n",
      "Epoch: 587 Loss: 0.03\n",
      "Epoch: 588 Loss: 0.01\n",
      "Epoch: 589 Loss: 0.01\n",
      "Epoch: 590 Loss: 0.03\n",
      "Epoch: 591 Loss: 0.01\n",
      "Epoch: 592 Loss: 0.02\n",
      "Epoch: 593 Loss: 0.02\n",
      "Epoch: 594 Loss: 0.01\n",
      "Epoch: 595 Loss: 0.01\n",
      "Epoch: 596 Loss: 0.01\n",
      "Epoch: 597 Loss: 0.02\n",
      "Epoch: 598 Loss: 0.01\n",
      "Epoch: 599 Loss: 0.01\n",
      "Epoch: 600 Loss: 0.01\n",
      "Epoch: 601 Loss: 0.01\n",
      "Epoch: 602 Loss: 0.04\n",
      "Epoch: 603 Loss: 0.02\n",
      "Epoch: 604 Loss: 0.01\n",
      "Epoch: 605 Loss: 0.02\n",
      "Epoch: 606 Loss: 0.02\n",
      "Epoch: 607 Loss: 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 608 Loss: 0.01\n",
      "Epoch: 609 Loss: 0.01\n",
      "Epoch: 610 Loss: 0.00\n",
      "Epoch: 611 Loss: 0.02\n",
      "Epoch: 612 Loss: 0.01\n",
      "Epoch: 613 Loss: 0.01\n",
      "Epoch: 614 Loss: 0.03\n",
      "Epoch: 615 Loss: 0.02\n",
      "Epoch: 616 Loss: 0.01\n",
      "Epoch: 617 Loss: 0.01\n",
      "Epoch: 618 Loss: 0.01\n",
      "Epoch: 619 Loss: 0.01\n",
      "Epoch: 620 Loss: 0.02\n",
      "Epoch: 621 Loss: 0.01\n",
      "Epoch: 622 Loss: 0.01\n",
      "Epoch: 623 Loss: 0.00\n",
      "Epoch: 624 Loss: 0.01\n",
      "Epoch: 625 Loss: 0.01\n",
      "Epoch: 626 Loss: 0.00\n",
      "Epoch: 627 Loss: 0.01\n",
      "Epoch: 628 Loss: 0.02\n",
      "Epoch: 629 Loss: 0.01\n",
      "Epoch: 630 Loss: 0.01\n",
      "Epoch: 631 Loss: 0.02\n",
      "Epoch: 632 Loss: 0.02\n",
      "Epoch: 633 Loss: 0.00\n",
      "Epoch: 634 Loss: 0.01\n",
      "Epoch: 635 Loss: 0.01\n",
      "Epoch: 636 Loss: 0.02\n",
      "Epoch: 637 Loss: 0.01\n",
      "Epoch: 638 Loss: 0.01\n",
      "Epoch: 639 Loss: 0.01\n",
      "Epoch: 640 Loss: 0.01\n",
      "Epoch: 641 Loss: 0.01\n",
      "Epoch: 642 Loss: 0.01\n",
      "Epoch: 643 Loss: 0.01\n",
      "Epoch: 644 Loss: 0.01\n",
      "Epoch: 645 Loss: 0.01\n",
      "Epoch: 646 Loss: 0.01\n",
      "Epoch: 647 Loss: 0.01\n",
      "Epoch: 648 Loss: 0.01\n",
      "Epoch: 649 Loss: 0.01\n",
      "Epoch: 650 Loss: 0.01\n",
      "Epoch: 651 Loss: 0.01\n",
      "Epoch: 652 Loss: 0.01\n",
      "Epoch: 653 Loss: 0.01\n",
      "Epoch: 654 Loss: 0.02\n",
      "Epoch: 655 Loss: 0.01\n",
      "Epoch: 656 Loss: 0.01\n",
      "Epoch: 657 Loss: 0.01\n",
      "Epoch: 658 Loss: 0.02\n",
      "Epoch: 659 Loss: 0.01\n",
      "Epoch: 660 Loss: 0.01\n",
      "Epoch: 661 Loss: 0.01\n",
      "Epoch: 662 Loss: 0.02\n",
      "Epoch: 663 Loss: 0.01\n",
      "Epoch: 664 Loss: 0.01\n",
      "Epoch: 665 Loss: 0.01\n",
      "Epoch: 666 Loss: 0.00\n",
      "Epoch: 667 Loss: 0.02\n",
      "Epoch: 668 Loss: 0.02\n",
      "Epoch: 669 Loss: 0.01\n",
      "Epoch: 670 Loss: 0.03\n",
      "Epoch: 671 Loss: 0.01\n",
      "Epoch: 672 Loss: 0.01\n",
      "Epoch: 673 Loss: 0.02\n",
      "Epoch: 674 Loss: 0.02\n",
      "Epoch: 675 Loss: 0.02\n",
      "Epoch: 676 Loss: 0.02\n",
      "Epoch: 677 Loss: 0.01\n",
      "Epoch: 678 Loss: 0.01\n",
      "Epoch: 679 Loss: 0.01\n",
      "Epoch: 680 Loss: 0.02\n",
      "Epoch: 681 Loss: 0.01\n",
      "Epoch: 682 Loss: 0.01\n",
      "Epoch: 683 Loss: 0.01\n",
      "Epoch: 684 Loss: 0.01\n",
      "Epoch: 685 Loss: 0.01\n",
      "Epoch: 686 Loss: 0.01\n",
      "Epoch: 687 Loss: 0.01\n",
      "Epoch: 688 Loss: 0.01\n",
      "Epoch: 689 Loss: 0.02\n",
      "Epoch: 690 Loss: 0.01\n",
      "Epoch: 691 Loss: 0.01\n",
      "Epoch: 692 Loss: 0.02\n",
      "Epoch: 693 Loss: 0.02\n",
      "Epoch: 694 Loss: 0.01\n",
      "Epoch: 695 Loss: 0.02\n",
      "Epoch: 696 Loss: 0.01\n",
      "Epoch: 697 Loss: 0.01\n",
      "Epoch: 698 Loss: 0.01\n",
      "Epoch: 699 Loss: 0.01\n",
      "Epoch: 700 Loss: 0.02\n",
      "Epoch: 701 Loss: 0.01\n",
      "Epoch: 702 Loss: 0.02\n",
      "Epoch: 703 Loss: 0.02\n",
      "Epoch: 704 Loss: 0.01\n",
      "Epoch: 705 Loss: 0.02\n",
      "Epoch: 706 Loss: 0.03\n",
      "Epoch: 707 Loss: 0.02\n",
      "Epoch: 708 Loss: 0.01\n",
      "Epoch: 709 Loss: 0.02\n",
      "Epoch: 710 Loss: 0.02\n",
      "Epoch: 711 Loss: 0.00\n",
      "Epoch: 712 Loss: 0.01\n",
      "Epoch: 713 Loss: 0.01\n",
      "Epoch: 714 Loss: 0.01\n",
      "Epoch: 715 Loss: 0.00\n",
      "Epoch: 716 Loss: 0.01\n",
      "Epoch: 717 Loss: 0.01\n",
      "Epoch: 718 Loss: 0.00\n",
      "Epoch: 719 Loss: 0.02\n",
      "Epoch: 720 Loss: 0.01\n",
      "Epoch: 721 Loss: 0.02\n",
      "Epoch: 722 Loss: 0.01\n",
      "Epoch: 723 Loss: 0.01\n",
      "Epoch: 724 Loss: 0.01\n",
      "Epoch: 725 Loss: 0.01\n",
      "Epoch: 726 Loss: 0.01\n",
      "Epoch: 727 Loss: 0.01\n",
      "Epoch: 728 Loss: 0.03\n",
      "Epoch: 729 Loss: 0.00\n",
      "Epoch: 730 Loss: 0.02\n",
      "Epoch: 731 Loss: 0.01\n",
      "Epoch: 732 Loss: 0.01\n",
      "Epoch: 733 Loss: 0.01\n",
      "Epoch: 734 Loss: 0.01\n",
      "Epoch: 735 Loss: 0.01\n",
      "Epoch: 736 Loss: 0.01\n",
      "Epoch: 737 Loss: 0.01\n",
      "Epoch: 738 Loss: 0.01\n",
      "Epoch: 739 Loss: 0.02\n",
      "Epoch: 740 Loss: 0.02\n",
      "Epoch: 741 Loss: 0.01\n",
      "Epoch: 742 Loss: 0.00\n",
      "Epoch: 743 Loss: 0.01\n",
      "Epoch: 744 Loss: 0.02\n",
      "Epoch: 745 Loss: 0.01\n",
      "Epoch: 746 Loss: 0.01\n",
      "Epoch: 747 Loss: 0.01\n",
      "Epoch: 748 Loss: 0.02\n",
      "Epoch: 749 Loss: 0.00\n",
      "Epoch: 750 Loss: 0.01\n",
      "Epoch: 751 Loss: 0.01\n",
      "Epoch: 752 Loss: 0.05\n",
      "Epoch: 753 Loss: 0.02\n",
      "Epoch: 754 Loss: 0.01\n",
      "Epoch: 755 Loss: 0.03\n",
      "Epoch: 756 Loss: 0.02\n",
      "Epoch: 757 Loss: 0.02\n",
      "Epoch: 758 Loss: 0.01\n",
      "Epoch: 759 Loss: 0.03\n",
      "Epoch: 760 Loss: 0.02\n",
      "Epoch: 761 Loss: 0.02\n",
      "Epoch: 762 Loss: 0.03\n",
      "Epoch: 763 Loss: 0.01\n",
      "Epoch: 764 Loss: 0.01\n",
      "Epoch: 765 Loss: 0.01\n",
      "Epoch: 766 Loss: 0.01\n",
      "Epoch: 767 Loss: 0.01\n",
      "Epoch: 768 Loss: 0.01\n",
      "Epoch: 769 Loss: 0.01\n",
      "Epoch: 770 Loss: 0.01\n",
      "Epoch: 771 Loss: 0.01\n",
      "Epoch: 772 Loss: 0.01\n",
      "Epoch: 773 Loss: 0.01\n",
      "Epoch: 774 Loss: 0.01\n",
      "Epoch: 775 Loss: 0.01\n",
      "Epoch: 776 Loss: 0.02\n",
      "Epoch: 777 Loss: 0.00\n",
      "Epoch: 778 Loss: 0.01\n",
      "Epoch: 779 Loss: 0.01\n",
      "Epoch: 780 Loss: 0.01\n",
      "Epoch: 781 Loss: 0.01\n",
      "Epoch: 782 Loss: 0.01\n",
      "Epoch: 783 Loss: 0.02\n",
      "Epoch: 784 Loss: 0.02\n",
      "Epoch: 785 Loss: 0.01\n",
      "Epoch: 786 Loss: 0.02\n",
      "Epoch: 787 Loss: 0.01\n",
      "Epoch: 788 Loss: 0.01\n",
      "Epoch: 789 Loss: 0.01\n",
      "Epoch: 790 Loss: 0.01\n",
      "Epoch: 791 Loss: 0.01\n",
      "Epoch: 792 Loss: 0.00\n",
      "Epoch: 793 Loss: 0.01\n",
      "Epoch: 794 Loss: 0.02\n",
      "Epoch: 795 Loss: 0.02\n",
      "Epoch: 796 Loss: 0.01\n",
      "Epoch: 797 Loss: 0.01\n",
      "Epoch: 798 Loss: 0.01\n",
      "Epoch: 799 Loss: 0.02\n",
      "Epoch: 800 Loss: 0.01\n",
      "Epoch: 801 Loss: 0.03\n",
      "Epoch: 802 Loss: 0.01\n",
      "Epoch: 803 Loss: 0.03\n",
      "Epoch: 804 Loss: 0.01\n",
      "Epoch: 805 Loss: 0.01\n",
      "Epoch: 806 Loss: 0.03\n",
      "Epoch: 807 Loss: 0.01\n",
      "Epoch: 808 Loss: 0.02\n",
      "Epoch: 809 Loss: 0.02\n",
      "Epoch: 810 Loss: 0.02\n",
      "Epoch: 811 Loss: 0.00\n",
      "Epoch: 812 Loss: 0.01\n",
      "Epoch: 813 Loss: 0.02\n",
      "Epoch: 814 Loss: 0.00\n",
      "Epoch: 815 Loss: 0.00\n",
      "Epoch: 816 Loss: 0.01\n",
      "Epoch: 817 Loss: 0.02\n",
      "Epoch: 818 Loss: 0.02\n",
      "Epoch: 819 Loss: 0.00\n",
      "Epoch: 820 Loss: 0.01\n",
      "Epoch: 821 Loss: 0.00\n",
      "Epoch: 822 Loss: 0.02\n",
      "Epoch: 823 Loss: 0.01\n",
      "Epoch: 824 Loss: 0.02\n",
      "Epoch: 825 Loss: 0.01\n",
      "Epoch: 826 Loss: 0.03\n",
      "Epoch: 827 Loss: 0.01\n",
      "Epoch: 828 Loss: 0.00\n",
      "Epoch: 829 Loss: 0.02\n",
      "Epoch: 830 Loss: 0.01\n",
      "Epoch: 831 Loss: 0.01\n",
      "Epoch: 832 Loss: 0.02\n",
      "Epoch: 833 Loss: 0.02\n",
      "Epoch: 834 Loss: 0.02\n",
      "Epoch: 835 Loss: 0.00\n",
      "Epoch: 836 Loss: 0.01\n",
      "Epoch: 837 Loss: 0.01\n",
      "Epoch: 838 Loss: 0.00\n",
      "Epoch: 839 Loss: 0.01\n",
      "Epoch: 840 Loss: 0.01\n",
      "Epoch: 841 Loss: 0.01\n",
      "Epoch: 842 Loss: 0.01\n",
      "Epoch: 843 Loss: 0.01\n",
      "Epoch: 844 Loss: 0.01\n",
      "Epoch: 845 Loss: 0.01\n",
      "Epoch: 846 Loss: 0.01\n",
      "Epoch: 847 Loss: 0.01\n",
      "Epoch: 848 Loss: 0.01\n",
      "Epoch: 849 Loss: 0.00\n",
      "Epoch: 850 Loss: 0.01\n",
      "Epoch: 851 Loss: 0.01\n",
      "Epoch: 852 Loss: 0.01\n",
      "Epoch: 853 Loss: 0.02\n",
      "Epoch: 854 Loss: 0.04\n",
      "Epoch: 855 Loss: 0.01\n",
      "Epoch: 856 Loss: 0.01\n",
      "Epoch: 857 Loss: 0.00\n",
      "Epoch: 858 Loss: 0.01\n",
      "Epoch: 859 Loss: 0.00\n",
      "Epoch: 860 Loss: 0.01\n",
      "Epoch: 861 Loss: 0.04\n",
      "Epoch: 862 Loss: 0.01\n",
      "Epoch: 863 Loss: 0.01\n",
      "Epoch: 864 Loss: 0.01\n",
      "Epoch: 865 Loss: 0.01\n",
      "Epoch: 866 Loss: 0.01\n",
      "Epoch: 867 Loss: 0.02\n",
      "Epoch: 868 Loss: 0.01\n",
      "Epoch: 869 Loss: 0.01\n",
      "Epoch: 870 Loss: 0.01\n",
      "Epoch: 871 Loss: 0.01\n",
      "Epoch: 872 Loss: 0.01\n",
      "Epoch: 873 Loss: 0.01\n",
      "Epoch: 874 Loss: 0.02\n",
      "Epoch: 875 Loss: 0.02\n",
      "Epoch: 876 Loss: 0.01\n",
      "Epoch: 877 Loss: 0.01\n",
      "Epoch: 878 Loss: 0.02\n",
      "Epoch: 879 Loss: 0.00\n",
      "Epoch: 880 Loss: 0.04\n",
      "Epoch: 881 Loss: 0.01\n",
      "Epoch: 882 Loss: 0.01\n",
      "Epoch: 883 Loss: 0.01\n",
      "Epoch: 884 Loss: 0.01\n",
      "Epoch: 885 Loss: 0.01\n",
      "Epoch: 886 Loss: 0.01\n",
      "Epoch: 887 Loss: 0.01\n",
      "Epoch: 888 Loss: 0.01\n",
      "Epoch: 889 Loss: 0.01\n",
      "Epoch: 890 Loss: 0.02\n",
      "Epoch: 891 Loss: 0.02\n",
      "Epoch: 892 Loss: 0.02\n",
      "Epoch: 893 Loss: 0.01\n",
      "Epoch: 894 Loss: 0.01\n",
      "Epoch: 895 Loss: 0.01\n",
      "Epoch: 896 Loss: 0.01\n",
      "Epoch: 897 Loss: 0.00\n",
      "Epoch: 898 Loss: 0.01\n",
      "Epoch: 899 Loss: 0.02\n",
      "Epoch: 900 Loss: 0.02\n",
      "Epoch: 901 Loss: 0.01\n",
      "Epoch: 902 Loss: 0.03\n",
      "Epoch: 903 Loss: 0.00\n",
      "Epoch: 904 Loss: 0.01\n",
      "Epoch: 905 Loss: 0.02\n",
      "Epoch: 906 Loss: 0.02\n",
      "Epoch: 907 Loss: 0.01\n",
      "Epoch: 908 Loss: 0.01\n",
      "Epoch: 909 Loss: 0.01\n",
      "Epoch: 910 Loss: 0.02\n",
      "Epoch: 911 Loss: 0.00\n",
      "Epoch: 912 Loss: 0.00\n",
      "Epoch: 913 Loss: 0.01\n",
      "Epoch: 914 Loss: 0.02\n",
      "Epoch: 915 Loss: 0.01\n",
      "Epoch: 916 Loss: 0.01\n",
      "Epoch: 917 Loss: 0.01\n",
      "Epoch: 918 Loss: 0.03\n",
      "Epoch: 919 Loss: 0.00\n",
      "Epoch: 920 Loss: 0.01\n",
      "Epoch: 921 Loss: 0.00\n",
      "Epoch: 922 Loss: 0.01\n",
      "Epoch: 923 Loss: 0.01\n",
      "Epoch: 924 Loss: 0.01\n",
      "Epoch: 925 Loss: 0.01\n",
      "Epoch: 926 Loss: 0.01\n",
      "Epoch: 927 Loss: 0.01\n",
      "Epoch: 928 Loss: 0.01\n",
      "Epoch: 929 Loss: 0.02\n",
      "Epoch: 930 Loss: 0.01\n",
      "Epoch: 931 Loss: 0.01\n",
      "Epoch: 932 Loss: 0.01\n",
      "Epoch: 933 Loss: 0.00\n",
      "Epoch: 934 Loss: 0.00\n",
      "Epoch: 935 Loss: 0.00\n",
      "Epoch: 936 Loss: 0.02\n",
      "Epoch: 937 Loss: 0.01\n",
      "Epoch: 938 Loss: 0.01\n",
      "Epoch: 939 Loss: 0.01\n",
      "Epoch: 940 Loss: 0.01\n",
      "Epoch: 941 Loss: 0.00\n",
      "Epoch: 942 Loss: 0.01\n",
      "Epoch: 943 Loss: 0.02\n",
      "Epoch: 944 Loss: 0.01\n",
      "Epoch: 945 Loss: 0.00\n",
      "Epoch: 946 Loss: 0.01\n",
      "Epoch: 947 Loss: 0.02\n",
      "Epoch: 948 Loss: 0.01\n",
      "Epoch: 949 Loss: 0.01\n",
      "Epoch: 950 Loss: 0.01\n",
      "Epoch: 951 Loss: 0.01\n",
      "Epoch: 952 Loss: 0.01\n",
      "Epoch: 953 Loss: 0.01\n",
      "Epoch: 954 Loss: 0.00\n",
      "Epoch: 955 Loss: 0.01\n",
      "Epoch: 956 Loss: 0.01\n",
      "Epoch: 957 Loss: 0.00\n",
      "Epoch: 958 Loss: 0.01\n",
      "Epoch: 959 Loss: 0.01\n",
      "Epoch: 960 Loss: 0.01\n",
      "Epoch: 961 Loss: 0.01\n",
      "Epoch: 962 Loss: 0.01\n",
      "Epoch: 963 Loss: 0.01\n",
      "Epoch: 964 Loss: 0.00\n",
      "Epoch: 965 Loss: 0.01\n",
      "Epoch: 966 Loss: 0.01\n",
      "Epoch: 967 Loss: 0.01\n",
      "Epoch: 968 Loss: 0.01\n",
      "Epoch: 969 Loss: 0.01\n",
      "Epoch: 970 Loss: 0.00\n",
      "Epoch: 971 Loss: 0.00\n",
      "Epoch: 972 Loss: 0.01\n",
      "Epoch: 973 Loss: 0.01\n",
      "Epoch: 974 Loss: 0.00\n",
      "Epoch: 975 Loss: 0.01\n",
      "Epoch: 976 Loss: 0.01\n",
      "Epoch: 977 Loss: 0.01\n",
      "Epoch: 978 Loss: 0.01\n",
      "Epoch: 979 Loss: 0.01\n",
      "Epoch: 980 Loss: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 981 Loss: 0.01\n",
      "Epoch: 982 Loss: 0.02\n",
      "Epoch: 983 Loss: 0.01\n",
      "Epoch: 984 Loss: 0.01\n",
      "Epoch: 985 Loss: 0.01\n",
      "Epoch: 986 Loss: 0.01\n",
      "Epoch: 987 Loss: 0.01\n",
      "Epoch: 988 Loss: 0.01\n",
      "Epoch: 989 Loss: 0.01\n",
      "Epoch: 990 Loss: 0.02\n",
      "Epoch: 991 Loss: 0.01\n",
      "Epoch: 992 Loss: 0.02\n",
      "Epoch: 993 Loss: 0.01\n",
      "Epoch: 994 Loss: 0.01\n",
      "Epoch: 995 Loss: 0.02\n",
      "Epoch: 996 Loss: 0.01\n",
      "Epoch: 997 Loss: 0.01\n",
      "Epoch: 998 Loss: 0.01\n",
      "Epoch: 999 Loss: 0.00\n",
      "Epoch: 1000 Loss: 0.01, recall@25: 0.69\n",
      "Saved model 'modelos/model_bert_preprocessing_deepTL_1000_feature_1000epochs_64batch(openoffice).h5' to disk\n",
      "Best_epoch=903, Best_loss=0.00, Recall@25=0.69\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append([h])\n",
    "    result['test'].append([h_validation])\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == epochs) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))\n",
    "    loss = h\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = similarity_model.get_layer('concatenate_1')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=quintet_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepTL_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98306:88871,50853,33630,90791|88871:0.6935537159442902,62750:0.6791682839393616,42169:0.6724690794944763,78716:0.669046014547348,59051:0.6671613156795502,113851:0.6655094027519226,35644:0.6607243418693542,37651:0.6605731546878815,69738:0.6599004864692688,95954:0.6581278741359711,66329:0.6570902466773987,47227:0.656943291425705,33257:0.6566915214061737,54812:0.6565796136856079,99507:0.6562356650829315,92237:0.6561053693294525,1685:0.655159205198288,54357:0.6550473868846893,97116:0.6549574732780457,48861:0.6536872684955597,107843:0.6528923213481903,38090:0.6516199707984924,77976:0.6513862013816833,56216:0.6508936285972595,61627:0.6506760716438293,75024:0.6504048705101013,87639:0.6501749753952026,75171:0.6497997939586639,96878:0.6495416462421417',\n",
       " '32771:32490,33548,32560,33879,32699|5412:0.69867804646492,7354:0.6884078085422516,33506:0.6871865391731262,9917:0.6863522231578827,71665:0.6858185827732086,14831:0.685621827840805,6789:0.6847501695156097,4516:0.6821760833263397,67340:0.6817118227481842,39209:0.68075892329216,86473:0.6806116700172424,19220:0.6800373494625092,3967:0.6789360046386719,13469:0.6766364276409149,5080:0.6764003336429596,98930:0.6761834621429443,4315:0.6757412850856781,12561:0.6747519373893738,9780:0.6746562421321869,19554:0.6743006408214569,26935:0.6740541756153107,12726:0.6730936169624329,17105:0.6730782687664032,81789:0.6729641556739807,17409:0.6719174683094025,16740:0.6707650423049927,24579:0.6702190041542053,12918:0.6701034009456635,18023:0.6696237921714783',\n",
       " '32772:22694,36970,39820,34488,33298,36760,34911|30077:0.7086216509342194,88582:0.6927149593830109,8048:0.6825744211673737,32:0.6822222769260406,91855:0.6821570098400116,55629:0.68135866522789,17674:0.6809585988521576,92772:0.678002268075943,96549:0.6773810088634491,78411:0.6761604249477386,254:0.6751996576786041,89502:0.6748615503311157,81069:0.6741319596767426,31934:0.6723959147930145,35727:0.6696476936340332,42122:0.6693532168865204,3745:0.6688456535339355,18338:0.6683657765388489,65767:0.6664306819438934,92697:0.6662166118621826,108200:0.6658237874507904,19520:0.665193110704422,17972:0.6645689606666565,39420:0.6642955541610718,94546:0.6639027297496796,17401:0.6638642847537994,3428:0.6637772917747498,3077:0.6636682450771332,94393:0.6625686883926392',\n",
       " '32776:30241,33762,31134,33183|33183:0.6963117718696594,31134:0.6892265379428864,23347:0.6713125109672546,26324:0.6664586067199707,51734:0.6662413775920868,39362:0.6650488674640656,9382:0.6619728803634644,15558:0.6611583232879639,9275:0.6543013155460358,33762:0.6517555415630341,3145:0.6454138159751892,3074:0.6454070806503296,70534:0.6446571052074432,9046:0.6446563899517059,12369:0.644529789686203,7721:0.6445208191871643,30241:0.6444954872131348,75171:0.6434627175331116,35670:0.6415214836597443,23923:0.6383650302886963,39221:0.6370326280593872,90738:0.6366576850414276,54812:0.6362473964691162,5103:0.6362217366695404,31775:0.6360727250576019,50559:0.6360443532466888,3807:0.6351710259914398,14805:0.6351076662540436,97309:0.63460972905159',\n",
       " '32780:27332,37094|27604:0.6914290487766266,19989:0.690120667219162,52718:0.6896592080593109,35863:0.6872207522392273,13411:0.6865543723106384,77850:0.6857912242412567,38090:0.6827811598777771,3651:0.6827089190483093,15054:0.6815524101257324,22109:0.6811341643333435,25623:0.6811204254627228,56191:0.6808889210224152,41838:0.6802987456321716,56106:0.6779732406139374,14805:0.6774766743183136,19070:0.6768203973770142,12575:0.6765954196453094,15055:0.6761182248592377,24068:0.6749567687511444,21951:0.6748982965946198,60778:0.6736931800842285,16077:0.6722933053970337,71356:0.6720860302448273,27957:0.6716643571853638,43048:0.6709447205066681,19414:0.6702813506126404,20659:0.6701582968235016,26622:0.6696701645851135,20028:0.6696649491786957',\n",
       " '65549:26122,62365,3411,74204,94365|100529:0.732043594121933,90074:0.7186279892921448,53250:0.7107073068618774,84548:0.7102688848972321,77177:0.7089254558086395,13699:0.7080450356006622,102428:0.7070486843585968,26122:0.7060862481594086,102028:0.7051903009414673,52800:0.7048872709274292,102507:0.7037465870380402,71696:0.7016509473323822,103949:0.7015011310577393,9888:0.7010104060173035,81722:0.699972540140152,80626:0.6979072093963623,62538:0.696997344493866,82400:0.6962997913360596,71259:0.6948212385177612,21097:0.6947061717510223,114835:0.6935175657272339,29025:0.6934773921966553,34563:0.6930317282676697,84855:0.6930269598960876,25706:0.69287970662117,27533:0.6927515864372253,50655:0.6921118795871735,84392:0.6913880705833435,101968:0.6909906566143036',\n",
       " '32781:35467,35941|49152:0.6911932528018951,93756:0.6903458535671234,59021:0.6838387548923492,39087:0.6831216514110565,17683:0.6769667267799377,80125:0.6760289669036865,108978:0.6744520664215088,41312:0.6716583073139191,10613:0.6714436411857605,65027:0.6711513698101044,33061:0.6710166931152344,22723:0.6709859073162079,13787:0.6696867942810059,20254:0.6687652170658112,50593:0.6686079204082489,20938:0.6680441498756409,9046:0.6678359508514404,11493:0.6658728718757629,20557:0.6657317578792572,69738:0.6644541323184967,13886:0.6642912030220032,22735:0.66363325715065,35467:0.6631883680820465,1742:0.6630770564079285,35214:0.6630635261535645,18544:0.663044810295105,6833:0.6628498733043671,4540:0.662775844335556,25825:0.6625906825065613',\n",
       " '32788:31681,32789,31726|32789:1.0,40114:0.6569353342056274,41120:0.6566007137298584,36343:0.6536506414413452,95718:0.6516896188259125,58209:0.6510813236236572,89444:0.645113617181778,34243:0.6443572044372559,31458:0.6436282098293304,62792:0.6423830986022949,42942:0.6423439979553223,73267:0.6414148807525635,76662:0.6407313644886017,117018:0.6403849124908447,33702:0.6391681134700775,100537:0.6387994587421417,29469:0.6379350423812866,77549:0.6377498209476471,101633:0.6373369693756104,105654:0.6371303200721741,76795:0.6366848945617676,34168:0.6364370882511139,17714:0.6359775960445404,75100:0.6357260346412659,47406:0.6350659728050232,80281:0.6345212459564209,66347:0.6341438591480255,74005:0.6338642537593842,31726:0.6338034272193909',\n",
       " '32789:31681,32788,31726|32788:1.0,40114:0.6569353342056274,41120:0.6566007137298584,36343:0.6536506414413452,95718:0.6516896188259125,58209:0.6510813236236572,89444:0.645113617181778,34243:0.6443572044372559,31458:0.6436282098293304,62792:0.6423830986022949,42942:0.6423439979553223,73267:0.6414148807525635,76662:0.6407313644886017,117018:0.6403849124908447,33702:0.6391681134700775,100537:0.6387994587421417,29469:0.6379350423812866,77549:0.6377498209476471,101633:0.6373369693756104,105654:0.6371303200721741,76795:0.6366848945617676,34168:0.6364370882511139,17714:0.6359775960445404,75100:0.6357260346412659,47406:0.6350659728050232,80281:0.6345212459564209,66347:0.6341438591480255,74005:0.6338642537593842,31726:0.6338034272193909',\n",
       " '65561:27520,50753,21280,85187,51168,35298,83046,27630,81041,46738,46739,46740,103094,48951,55995,76895|55995:0.7335552275180817,46349:0.6937848925590515,21773:0.69074347615242,22719:0.6853509247303009,27630:0.6831497550010681,35298:0.6808750629425049,60285:0.6800770163536072,5399:0.6797383427619934,6348:0.676704615354538,10162:0.6753499507904053,27794:0.6738375723361969,74432:0.6720751821994781,11481:0.6715120077133179,50514:0.6687592566013336,17090:0.6676735877990723,23263:0.6676005125045776,31795:0.6667894423007965,32996:0.6653810143470764,81041:0.6647962629795074,45010:0.6620016098022461,12313:0.6615274548530579,95936:0.6614226698875427,116770:0.6610228419303894,34815:0.6603089869022369,79995:0.6600544154644012,94024:0.6592812836170197,88482:0.6586791574954987,111127:0.6586208641529083,42008:0.658358097076416',\n",
       " '65562:69481,65597|45040:0.6908776164054871,115068:0.6900697350502014,18233:0.6900162994861603,48349:0.6888015866279602,38898:0.6883516609668732,45216:0.682060033082962,45215:0.682060033082962,68190:0.6818194389343262,27069:0.6814583837985992,49687:0.681309163570404,52652:0.6813038289546967,45217:0.6792930066585541,45218:0.6792930066585541,45219:0.6792930066585541,113149:0.679082453250885,48119:0.6788297295570374,55743:0.6788058280944824,36777:0.6787387430667877,105907:0.6769103705883026,42177:0.6763060092926025,74226:0.676196813583374,87125:0.6733534336090088,60122:0.6732623875141144,58370:0.6724874377250671,81147:0.6723477244377136,45468:0.6712690889835358,78295:0.6705181002616882,45214:0.67007976770401,67448:0.6689764857292175',\n",
       " '27:59,92|59:0.7148560583591461,8444:0.714823305606842,3359:0.7129833400249481,51824:0.7124328315258026,55317:0.7100666761398315,18118:0.7099215984344482,104421:0.7085428833961487,7917:0.7068174779415131,97746:0.7044991254806519,8836:0.704336553812027,4688:0.7026894092559814,51722:0.7002882063388824,51937:0.6994282603263855,14156:0.6969905495643616,4471:0.6969205439090729,20912:0.6968637704849243,22906:0.6961580812931061,17003:0.6955852210521698,8718:0.6951194703578949,17638:0.6948677003383636,8689:0.6942586302757263,17306:0.6939807832241058,105337:0.6926716566085815,17095:0.6924048364162445,8360:0.6923575103282928,753:0.6916522979736328,55402:0.6898616552352905,724:0.6890843510627747,5542:0.6888872683048248',\n",
       " '65566:56174,58491,18620,61775|56174:0.6613360643386841,16705:0.6515644490718842,103790:0.6506132781505585,61489:0.6478297710418701,104124:0.6472488939762115,92349:0.6448694467544556,82201:0.6448688507080078,43292:0.6437334716320038,49842:0.6426302492618561,67064:0.6408354640007019,41430:0.6406390070915222,49538:0.6406206488609314,42024:0.6402082145214081,102457:0.6397961378097534,61272:0.6393936276435852,86893:0.6388283371925354,39653:0.6380704045295715,97263:0.6375209987163544,84909:0.6367190778255463,27335:0.6366943418979645,56146:0.6366393268108368,57689:0.6359941959381104,63269:0.6357700526714325,59214:0.6357355117797852,102144:0.635632187128067,39645:0.635323703289032,71274:0.6351600289344788,41618:0.6344329118728638,85648:0.6343771517276764',\n",
       " '32:42944,6342,3207,18886,32905,54854,54855,67697,4756,23861,18648,58138,40286|12148:0.6853598654270172,32772:0.6822222769260406,52952:0.6723802387714386,31688:0.6694413721561432,34243:0.6679456830024719,92794:0.6673358976840973,85762:0.6665959358215332,85794:0.6656450927257538,92772:0.6649834513664246,94393:0.6642332673072815,19712:0.6628640592098236,37411:0.6612040102481842,108200:0.659509927034378,35062:0.6581449508666992,3077:0.6573991477489471,41252:0.6566391885280609,57664:0.6565562784671783,72688:0.6559597551822662,17674:0.6557736098766327,254:0.6554023027420044,31550:0.6553208529949188,108803:0.6547991633415222,94546:0.6545063555240631,31929:0.6541278660297394,15207:0.6540379822254181,27296:0.6538308560848236,35029:0.6530950665473938,24991:0.6529304683208466,35830:0.6523886322975159',\n",
       " '32815:32033,31362,31874,33062,31550|44093:0.7270595133304596,46452:0.7227218449115753,46021:0.7165417671203613,48194:0.7105952501296997,61607:0.7099769711494446,45885:0.6937443315982819,40258:0.6928044259548187,53179:0.6917499005794525,41469:0.6900320649147034,41836:0.683965414762497,46724:0.6819341778755188,41587:0.6818923056125641,43715:0.6818437874317169,76662:0.6818346381187439,53303:0.6816944479942322,73267:0.6812622249126434,42051:0.6800765693187714,39967:0.6799960434436798,50613:0.6786356270313263,52382:0.6769197881221771,37485:0.6761659383773804,41605:0.6755312085151672,56506:0.6752097606658936,44085:0.6748642325401306,44095:0.6706776916980743,76795:0.6697596907615662,42202:0.6694688200950623,43780:0.6687090992927551,44664:0.6679716110229492',\n",
       " '65586:71849,67371,67476,83252,88157,67642,71099,69789,81630|71849:0.6771771907806396,17440:0.6663947105407715,19615:0.6656108498573303,3558:0.6626451909542084,15434:0.6615456640720367,25792:0.6593655645847321,2330:0.6587333679199219,31384:0.6569417417049408,82290:0.6552177667617798,100003:0.6545491814613342,14216:0.6543011665344238,55515:0.6537778675556183,55516:0.6537778675556183,55517:0.6537778675556183,62353:0.6524760723114014,20326:0.6517272591590881,48682:0.651565819978714,33626:0.6505184173583984,42607:0.6496476233005524,45074:0.6488657295703888,56384:0.6478337347507477,58456:0.647810310125351,16449:0.6477981805801392,12104:0.6467950642108917,5550:0.6464603841304779,24228:0.6461034119129181,15801:0.6458119750022888,18657:0.645785003900528,108389:0.6457155644893646',\n",
       " '32819:30138,28434,30621|406:0.6863075494766235,45406:0.6720703542232513,13128:0.6644590497016907,6141:0.6636515259742737,21519:0.6587567031383514,39549:0.6583812534809113,31668:0.656134158372879,20113:0.6535807251930237,8833:0.653258204460144,89669:0.6524294018745422,1762:0.6523450613021851,22251:0.6516135632991791,5603:0.6507924199104309,4720:0.6502858996391296,5578:0.6499114036560059,65366:0.6498935222625732,8910:0.6491226553916931,39359:0.649066299200058,10954:0.6485941410064697,35621:0.6480326950550079,32319:0.6478886604309082,24448:0.6478070914745331,4912:0.6476239562034607,28618:0.6475093066692352,5575:0.6474306285381317,32444:0.6473357379436493,44041:0.6465522348880768,13592:0.6465110778808594,41636:0.6463901996612549',\n",
       " '32826:60934,69709,115278,22961,35219|4148:0.6771924197673798,35219:0.6697832643985748,75219:0.6648828387260437,28721:0.6569839417934418,19316:0.6535477340221405,55562:0.6519406139850616,69832:0.6511796414852142,1272:0.649305522441864,21131:0.6481565237045288,25818:0.6466568410396576,92380:0.644101470708847,9493:0.6439521610736847,27673:0.6438963711261749,16647:0.6434173285961151,67802:0.6428765952587128,21480:0.6419674456119537,23588:0.6413086354732513,15429:0.6411081850528717,11265:0.6410991251468658,16979:0.6408216655254364,2068:0.6404056251049042,55354:0.6392029523849487,63826:0.6389811933040619,51310:0.6387036442756653,16055:0.6382246613502502,78239:0.6379814445972443,63185:0.6379109919071198,13793:0.6378035247325897,11266:0.637565404176712',\n",
       " '63:84|17095:0.7102222740650177,17372:0.7094912230968475,8689:0.7090936601161957,12629:0.7078662812709808,12788:0.7062506079673767,55402:0.7025092244148254,109630:0.7002666294574738,12344:0.7001968920230865,69370:0.6995633244514465,17663:0.6984508335590363,14146:0.6964299082756042,2115:0.6961274743080139,51937:0.6933101117610931,2574:0.6896265149116516,17311:0.6891694962978363,17581:0.6885052919387817,15152:0.6871498227119446,70697:0.6861015856266022,51824:0.6860560178756714,113106:0.6859562695026398,1693:0.6857110857963562,36197:0.685368686914444,14851:0.6828857064247131,52697:0.6827086508274078,2108:0.6817558109760284,83939:0.6816522181034088,19160:0.6815859079360962,55317:0.6813870668411255,36109:0.6813850104808807',\n",
       " '65604:92163,76676,67332,16519,79752,3851,6157,19089,61329,55954,66193,94612,93204,58903,5658,11041,69158,75433,31407,101306,29115,106944,40513,88775,85970,105427,50772,6233,58589,35809,56034,63203,43116,107375,6768,8950,25977,85242|56034:0.7493804097175598,76676:0.7213184535503387,6768:0.7166413962841034,93204:0.7156637609004974,58903:0.7138650119304657,55758:0.7137015163898468,63049:0.7116631269454956,51278:0.7084854245185852,3314:0.7079233825206757,63203:0.7070238590240479,46057:0.7058164179325104,111627:0.7049430310726166,38741:0.7046281099319458,69158:0.7045839130878448,480:0.7032329142093658,94612:0.7024610936641693,43116:0.7023202478885651,79328:0.7022018730640411,26004:0.7020865082740784,48470:0.7020229399204254,87442:0.7019166350364685,67399:0.7016398906707764,79752:0.7012617588043213,16519:0.7003988921642303,99376:0.6990072131156921,56936:0.6985904276371002,103861:0.6976974010467529,3851:0.697310596704483,50772:0.6971896886825562']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 7986\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_deepTL_1000_feature_1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 729)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          219000      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 768)          80346736    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 768)          80346736    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1836)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "==================================================================================================\n",
      "Total params: 160,912,472\n",
      "Trainable params: 440,296\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exported_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/bert/exported_rank_deepTL_1000.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.56,\n",
       " '2 - recall_at_10': 0.62,\n",
       " '3 - recall_at_15': 0.65,\n",
       " '4 - recall_at_20': 0.67,\n",
       " '5 - recall_at_25': 0.69}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some ideas to visualizate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
