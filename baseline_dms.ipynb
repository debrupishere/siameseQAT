{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Bug triage with Deep Learning\n",
    "\n",
    "https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'openoffice'\n",
    "METHOD = 'baseline_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98070"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed3f35514f5480da1a3a24fb73f35f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4269ca09b2d5453f8142f2b86d399d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 17.8 s, sys: 5.67 s, total: 23.4 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b7ac948d904074a3e10281ffd6959c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_train='train_chronological', path_test='test_chronological'\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6036, 35039],\n",
       " [26311, 15091],\n",
       " [5142, 34523],\n",
       " [8323, 9594],\n",
       " [76676, 88775],\n",
       " [59725, 71546],\n",
       " [121756, 116964],\n",
       " [32936, 30911],\n",
       " [15257, 16297],\n",
       " [99719, 106395]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '69\\n',\n",
       " 'creation_ts': '2006-09-04 15:15:00 +0000',\n",
       " 'delta_ts': '2007-01-12 14:26:09 +0000',\n",
       " 'description': '[CLS] so i had one or two reports of crashes on o ##oo startup , and traced it ( https : / / bug ##zi ##lla . red ##hat . com / bug ##zi ##lla / show _ bug . c ##gi ? id = 2021 ##65 ) to having a different li ##bs ##t ##dc + + installed in / us ##r / local causing pain and suffering . i suspect some other simi ##lia ##r looking reports in o ##oo issue ##zi ##lla may be the same thing . the following little patch just adds a check to see if creating a simpler ##eg ##ist ##ry failed , which was the immediate sy ##mpt ##om and which will instead cause the \" corrupt installation \" pop ##up to appear . feel free to throw this out as won ##tf ##ix if you want to skip it . but i \\' ll include the stack ##tra ##ce here in the hope that que ##ries for simi ##lia ##r crashes like this might lead people here , in which case . . . ld ##d / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / libre ##g . so . 3 might explain the problem is some random li ##bs ##t ##dc + + is getting used . program received signal si ##gs ##eg ##v , segment ##ation fault . [ switching to thread - 121 ##7 ##75 ##54 ##56 ( l ##w ##p 284 ##0 ##7 ) ] nest ##re ##gist ##ries ( based ##ir = @ 0 ##x ##bf ##8 ##a ##0 ##c ##6 ##c , x ##si ##m ##re ##gf ##ac = @ 0 ##x ##bf ##8 ##a ##0 ##c ##30 , x ##nes ##re ##gf ##ac = @ 0 ##x ##bf ##8 ##a ##0 ##c ##2 ##c , cs ##l _ rd ##bs = @ 0 ##x ##bf ##8 ##a ##0 ##c ##6 ##8 , write _ rd ##b = @ 0 ##x ##bf ##8 ##a ##0 ##c ##64 , force ##write _ rd ##b = 0 \\' \\\\ 0 \\' , bf ##alle ##nb ##ack = 0 \\' \\\\ 0 \\' , boots ##tra ##p = @ 0 ##x ##ba ##c ##53 ##8 ) at / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx : 334 334 / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx : no such file or directory . in / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx current language : auto ; currently c + + ( g ##db ) bt # 0 nest ##re ##gist ##ries ( based ##ir = @ 0 ##x ##bf ##8 ##a ##0 ##c ##6 ##c , x ##si ##m ##re ##gf ##ac = @ 0 ##x ##bf ##8 ##a ##0 ##c ##30 , x ##nes ##re ##gf ##ac = @ 0 ##x ##bf ##8 ##a ##0 ##c ##2 ##c , cs ##l _ rd ##bs = @ 0 ##x ##bf ##8 ##a ##0 ##c ##6 ##8 , write _ rd ##b = @ 0 ##x ##bf ##8 ##a ##0 ##c ##64 , force ##write _ rd ##b = 0 \\' \\\\ 0 \\' , bf ##alle ##nb ##ack = 0 \\' \\\\ 0 \\' , boots ##tra ##p = @ 0 ##x ##ba ##c ##53 ##8 ) at / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx : 334 # 1 0 ##x ##00 ##b ##7 ##3 ##b ##2 ##d in default ##boot ##stra ##p _ initial ##com ##pone ##nt ##con ##text ( boots ##tra ##p = @ 0 ##x ##ba ##c ##53 ##8 ) at / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx : 41 ##9 # 2 0 ##x ##00 ##b ##7 ##40 ##42 in cp ##pu : : default ##boot ##stra ##p _ initial ##com ##pone ##nt ##con ##text ( ) at / us ##r / sr ##c / red ##hat / build / o ##ob ##6 ##80 _ m ##5 / cp ##pu ##hel ##per / source / boots ##tra ##p . c ##xx : 47 ##2 # 3 0 ##x ##00 ##d ##42 ##bc ##c in desktop : : desktop : : create ##app ##lica ##tions ##er ##vic ##eman ##ager ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##bs ##off ##ice . so # 4 0 ##x ##00 ##d ##35 ##2 ##a ##5 in desktop : : desktop : : in ##it ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##bs ##off ##ice . so # 5 0 ##x ##00 ##3 ##b ##26 ##df in in ##it ##vc ##l ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##b ##vc ##l ##6 ##80 ##li . so # 6 0 ##x ##00 ##3 ##b ##28 ##32 in in ##it ##vc ##l ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##b ##vc ##l ##6 ##80 ##li . so # 7 0 ##x ##00 ##3 ##b ##28 ##f ##d in sv ##main ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##b ##vc ##l ##6 ##80 ##li . so # 8 0 ##x ##00 ##d ##30 ##e ##53 in sal _ main ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##bs ##off ##ice . so # 9 0 ##x ##00 ##d ##30 ##e ##9 ##f in main ( ) from / us ##r / li ##b / open ##off ##ice . org ##2 . 0 / program / li ##bs ##off ##ice . so # 10 0 ##x ##00 ##20 ##e ##7 ##24 in _ _ li ##bc _ start _ main ( ) from / li ##b / li ##bc . so . 6 - - - type < return > to continue , or q < return > to quit - - - # 11 0 ##x ##0 ##80 ##48 ##4 ##b ##1 in _ start ( ) [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  2061,  1045,  2018,  2028,  2030,  2048,  4311,  1997,\n",
       "        19119,  2006,  1051,  9541, 22752,  1010,  1998,  9551,  2009,\n",
       "         1006,   102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 69240,\n",
       " 'priority': '3\\n',\n",
       " 'product': '9\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_token': array([  101,  7526,  2005,  2070, 19119,  2464,  2006, 22752,  1999,\n",
       "         5622,  8569,  3630,  1035, 18133, 14289, 16001,  4842, 18195,\n",
       "         2278,   102,   101,  2061,  1045,  2018,  2028,  2030,  2048,\n",
       "         4311,  1997, 19119,  2006,  1051,  9541, 22752,  1010,  1998,\n",
       "         9551,  2009,  1006,   102]),\n",
       " 'title': '[CLS] explanation for some crashes seen on startup in li ##bu ##no _ cp ##pu ##hel ##per ##gc ##c ##3 . so . 3 [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  7526,  2005,  2070, 19119,  2464,  2006, 22752,  1999,\n",
       "         5622,  8569,  3630,  1035, 18133, 14289, 16001,  4842, 18195,\n",
       "         2278,   102]),\n",
       " 'version': '536\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 14508)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    \n",
    "    for bug_id in batch_bugs:\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug)\n",
    "\n",
    "    batch_features['title'] = np.array(batch_features['title'])\n",
    "    batch_features['desc'] = np.array(batch_features['desc'])\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 717 ms, sys: 0 ns, total: 717 ms\n",
      "Wall time: 717 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title'], \n",
    "             valid_input_sample['description'],\n",
    "            valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 738), (384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 8265)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 20031'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27094c23e6d249a4bb66e21d9da78062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e64fea48d54d768d69fb77ff8875ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20031), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 33s, sys: 3.96 s, total: 1min 37s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "\n",
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, TimeDistributed\n",
    "\n",
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 75\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Concatenate()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(50))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "\n",
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "    del y_true\n",
    "    margin = 1.\n",
    "    labels = y_pred[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = y_pred[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    # optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=triplet_loss_adapted_from_tf) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'baseline_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-34-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-35-3672f63e3254>:59: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.95, Loss_test: 0.94\n",
      "Epoch: 2 Loss: 0.94, Loss_test: 0.94\n",
      "Epoch: 3 Loss: 0.94, Loss_test: 0.94\n",
      "Epoch: 4 Loss: 0.95, Loss_test: 0.94\n",
      "Epoch: 5 Loss: 0.95, Loss_test: 0.93\n",
      "Epoch: 6 Loss: 0.94, Loss_test: 0.93\n",
      "Epoch: 7 Loss: 0.93, Loss_test: 0.92\n",
      "Epoch: 8 Loss: 0.94, Loss_test: 0.91\n",
      "Epoch: 9 Loss: 0.92, Loss_test: 0.91\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.92, Loss_test: 0.90\n",
      "Epoch: 11 Loss: 0.92, Loss_test: 0.88\n",
      "Epoch: 12 Loss: 0.90, Loss_test: 0.85\n",
      "Epoch: 13 Loss: 0.91, Loss_test: 0.82\n",
      "Epoch: 14 Loss: 0.87, Loss_test: 0.77\n",
      "Epoch: 15 Loss: 0.88, Loss_test: 0.70\n",
      "Epoch: 16 Loss: 0.83, Loss_test: 0.67\n",
      "Epoch: 17 Loss: 0.78, Loss_test: 0.67\n",
      "Epoch: 18 Loss: 0.74, Loss_test: 0.64\n",
      "Epoch: 19 Loss: 0.78, Loss_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.81, Loss_test: 0.60\n",
      "Epoch: 21 Loss: 0.74, Loss_test: 0.59\n",
      "Epoch: 22 Loss: 0.73, Loss_test: 0.57\n",
      "Epoch: 23 Loss: 0.70, Loss_test: 0.58\n",
      "Epoch: 24 Loss: 0.74, Loss_test: 0.56\n",
      "Epoch: 25 Loss: 0.72, Loss_test: 0.52\n",
      "Epoch: 26 Loss: 0.60, Loss_test: 0.49\n",
      "Epoch: 27 Loss: 0.71, Loss_test: 0.45\n",
      "Epoch: 28 Loss: 0.53, Loss_test: 0.42\n",
      "Epoch: 29 Loss: 0.59, Loss_test: 0.39\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.58, Loss_test: 0.38\n",
      "Epoch: 31 Loss: 0.52, Loss_test: 0.36\n",
      "Epoch: 32 Loss: 0.49, Loss_test: 0.34\n",
      "Epoch: 33 Loss: 0.28, Loss_test: 0.31\n",
      "Epoch: 34 Loss: 0.55, Loss_test: 0.27\n",
      "Epoch: 35 Loss: 0.47, Loss_test: 0.24\n",
      "Epoch: 36 Loss: 0.39, Loss_test: 0.21\n",
      "Epoch: 37 Loss: 0.59, Loss_test: 0.18\n",
      "Epoch: 38 Loss: 0.30, Loss_test: 0.17\n",
      "Epoch: 39 Loss: 0.47, Loss_test: 0.16\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.44, Loss_test: 0.15\n",
      "Epoch: 41 Loss: 0.14, Loss_test: 0.14\n",
      "Epoch: 42 Loss: 0.10, Loss_test: 0.13\n",
      "Epoch: 43 Loss: 0.22, Loss_test: 0.12\n",
      "Epoch: 44 Loss: 0.20, Loss_test: 0.11\n",
      "Epoch: 45 Loss: 0.31, Loss_test: 0.10\n",
      "Epoch: 46 Loss: 0.20, Loss_test: 0.09\n",
      "Epoch: 47 Loss: 0.12, Loss_test: 0.09\n",
      "Epoch: 48 Loss: 0.10, Loss_test: 0.09\n",
      "Epoch: 49 Loss: 0.24, Loss_test: 0.08\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.09, Loss_test: 0.08\n",
      "Epoch: 51 Loss: 0.38, Loss_test: 0.08\n",
      "Epoch: 52 Loss: 0.15, Loss_test: 0.08\n",
      "Epoch: 53 Loss: 0.07, Loss_test: 0.07\n",
      "Epoch: 54 Loss: 0.22, Loss_test: 0.07\n",
      "Epoch: 55 Loss: 0.05, Loss_test: 0.07\n",
      "Epoch: 56 Loss: 0.10, Loss_test: 0.07\n",
      "Epoch: 57 Loss: 0.09, Loss_test: 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Loss: 0.05, Loss_test: 0.07\n",
      "Epoch: 59 Loss: 0.28, Loss_test: 0.07\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.06, Loss_test: 0.07\n",
      "Epoch: 61 Loss: 0.31, Loss_test: 0.07\n",
      "Epoch: 62 Loss: 0.09, Loss_test: 0.07\n",
      "Epoch: 63 Loss: 0.08, Loss_test: 0.07\n",
      "Epoch: 64 Loss: 0.07, Loss_test: 0.07\n",
      "Epoch: 65 Loss: 0.49, Loss_test: 0.07\n",
      "Epoch: 66 Loss: 0.13, Loss_test: 0.07\n",
      "Epoch: 67 Loss: 0.16, Loss_test: 0.07\n",
      "Epoch: 68 Loss: 0.38, Loss_test: 0.07\n",
      "Epoch: 69 Loss: 0.27, Loss_test: 0.07\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.18, Loss_test: 0.07\n",
      "Epoch: 71 Loss: 0.25, Loss_test: 0.07\n",
      "Epoch: 72 Loss: 0.36, Loss_test: 0.06\n",
      "Epoch: 73 Loss: 0.22, Loss_test: 0.06\n",
      "Epoch: 74 Loss: 0.17, Loss_test: 0.06\n",
      "Epoch: 75 Loss: 0.09, Loss_test: 0.07\n",
      "Epoch: 76 Loss: 0.05, Loss_test: 0.07\n",
      "Epoch: 77 Loss: 0.24, Loss_test: 0.06\n",
      "Epoch: 78 Loss: 0.09, Loss_test: 0.07\n",
      "Epoch: 79 Loss: 0.08, Loss_test: 0.07\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.10, Loss_test: 0.06\n",
      "Epoch: 81 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 82 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 83 Loss: 0.18, Loss_test: 0.06\n",
      "Epoch: 84 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 85 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 86 Loss: 0.14, Loss_test: 0.06\n",
      "Epoch: 87 Loss: 0.09, Loss_test: 0.06\n",
      "Epoch: 88 Loss: 0.20, Loss_test: 0.06\n",
      "Epoch: 89 Loss: 0.07, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 91 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 92 Loss: 0.18, Loss_test: 0.06\n",
      "Epoch: 93 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 94 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 95 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 96 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 97 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 98 Loss: 0.06, Loss_test: 0.07\n",
      "Epoch: 99 Loss: 0.15, Loss_test: 0.07\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.07, Loss_test: 0.07, recall@25: 0.36\n",
      "Best_epoch=76, Best_loss=0.05, Recall@25=0.36\n",
      "CPU times: user 8min 22s, sys: 1min 17s, total: 9min 39s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "     # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))\n",
    "    loss = h\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/bert/exported_rank_baseline_100.txt'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_100_feature_100epochs_64batch(openoffice).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=triplet_loss_adapted_from_tf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'baseline_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 102 Loss: 0.05, Loss_test: 0.07\n",
      "Epoch: 103 Loss: 0.07, Loss_test: 0.08\n",
      "Epoch: 104 Loss: 0.07, Loss_test: 0.07\n",
      "Epoch: 105 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 106 Loss: 0.10, Loss_test: 0.06\n",
      "Epoch: 107 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 108 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 109 Loss: 0.08, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 111 Loss: 0.10, Loss_test: 0.06\n",
      "Epoch: 112 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 113 Loss: 0.09, Loss_test: 0.06\n",
      "Epoch: 114 Loss: 0.14, Loss_test: 0.06\n",
      "Epoch: 115 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 116 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 117 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 118 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 119 Loss: 0.10, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 121 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 122 Loss: 0.15, Loss_test: 0.06\n",
      "Epoch: 123 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 124 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 125 Loss: 0.28, Loss_test: 0.06\n",
      "Epoch: 126 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 127 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 128 Loss: 0.10, Loss_test: 0.06\n",
      "Epoch: 129 Loss: 0.05, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 131 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 132 Loss: 0.12, Loss_test: 0.06\n",
      "Epoch: 133 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 134 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 135 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 136 Loss: 0.08, Loss_test: 0.06\n",
      "Epoch: 137 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 138 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 139 Loss: 0.12, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.09, Loss_test: 0.06\n",
      "Epoch: 141 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 142 Loss: 0.10, Loss_test: 0.06\n",
      "Epoch: 143 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 144 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 145 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 146 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 147 Loss: 0.09, Loss_test: 0.06\n",
      "Epoch: 148 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 149 Loss: 0.04, Loss_test: 0.06\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.11, Loss_test: 0.05\n",
      "Epoch: 151 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 152 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 153 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 154 Loss: 0.06, Loss_test: 0.06\n",
      "Epoch: 155 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 156 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 157 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 158 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 159 Loss: 0.05, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 161 Loss: 0.08, Loss_test: 0.05\n",
      "Epoch: 162 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 163 Loss: 0.09, Loss_test: 0.05\n",
      "Epoch: 164 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 165 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 166 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 167 Loss: 0.09, Loss_test: 0.05\n",
      "Epoch: 168 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 169 Loss: 0.04, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 171 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 172 Loss: 0.05, Loss_test: 0.06\n",
      "Epoch: 173 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 174 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 175 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 176 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 177 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 178 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 179 Loss: 0.07, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 181 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 182 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 183 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 184 Loss: 0.08, Loss_test: 0.05\n",
      "Epoch: 185 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 186 Loss: 0.28, Loss_test: 0.05\n",
      "Epoch: 187 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 188 Loss: 0.10, Loss_test: 0.05\n",
      "Epoch: 189 Loss: 0.05, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 191 Loss: 0.07, Loss_test: 0.06\n",
      "Epoch: 192 Loss: 0.11, Loss_test: 0.06\n",
      "Epoch: 193 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 194 Loss: 0.04, Loss_test: 0.06\n",
      "Epoch: 195 Loss: 0.03, Loss_test: 0.06\n",
      "Epoch: 196 Loss: 0.07, Loss_test: 0.05\n",
      "Epoch: 197 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 198 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 199 Loss: 0.03, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.08, Loss_test: 0.05\n",
      "Epoch: 201 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 202 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 203 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 204 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 205 Loss: 0.09, Loss_test: 0.05\n",
      "Epoch: 206 Loss: 0.16, Loss_test: 0.05\n",
      "Epoch: 207 Loss: 0.07, Loss_test: 0.05\n",
      "Epoch: 208 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 209 Loss: 0.03, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 211 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 212 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 213 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 214 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 215 Loss: 0.12, Loss_test: 0.05\n",
      "Epoch: 216 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 217 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 218 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 219 Loss: 0.04, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.02, Loss_test: 0.05\n",
      "Epoch: 221 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 222 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 223 Loss: 0.08, Loss_test: 0.05\n",
      "Epoch: 224 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 225 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 226 Loss: 0.01, Loss_test: 0.05\n",
      "Epoch: 227 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 228 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 229 Loss: 0.04, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.02, Loss_test: 0.05\n",
      "Epoch: 231 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 232 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 233 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 234 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 235 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 236 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 237 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 238 Loss: 0.07, Loss_test: 0.05\n",
      "Epoch: 239 Loss: 0.03, Loss_test: 0.05\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 241 Loss: 0.02, Loss_test: 0.05\n",
      "Epoch: 242 Loss: 0.06, Loss_test: 0.05\n",
      "Epoch: 243 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 244 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 245 Loss: 0.03, Loss_test: 0.05\n",
      "Epoch: 246 Loss: 0.04, Loss_test: 0.05\n",
      "Epoch: 247 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 248 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 249 Loss: 0.03, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.11, Loss_test: 0.04\n",
      "Epoch: 251 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 252 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 253 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 254 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 255 Loss: 0.06, Loss_test: 0.04\n",
      "Epoch: 256 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 257 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 258 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 259 Loss: 0.03, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 261 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 262 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 263 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 264 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 265 Loss: 0.07, Loss_test: 0.04\n",
      "Epoch: 266 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 267 Loss: 0.06, Loss_test: 0.04\n",
      "Epoch: 268 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 269 Loss: 0.03, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 271 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 272 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 273 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 274 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 275 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 276 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 277 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 278 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 279 Loss: 0.02, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.07, Loss_test: 0.04\n",
      "Epoch: 281 Loss: 0.06, Loss_test: 0.04\n",
      "Epoch: 282 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 283 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 284 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 285 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 286 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 287 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 288 Loss: 0.09, Loss_test: 0.04\n",
      "Epoch: 289 Loss: 0.02, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 291 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 292 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 293 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 294 Loss: 0.27, Loss_test: 0.04\n",
      "Epoch: 295 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 296 Loss: 0.05, Loss_test: 0.05\n",
      "Epoch: 297 Loss: 0.07, Loss_test: 0.05\n",
      "Epoch: 298 Loss: 0.08, Loss_test: 0.04\n",
      "Epoch: 299 Loss: 0.02, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 301 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 302 Loss: 0.01, Loss_test: 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 304 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 305 Loss: 0.12, Loss_test: 0.04\n",
      "Epoch: 306 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 307 Loss: 0.02, Loss_test: 0.04\n",
      "Epoch: 308 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 309 Loss: 0.04, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 311 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 312 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 313 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 314 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 315 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 316 Loss: 0.06, Loss_test: 0.04\n",
      "Epoch: 317 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 318 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 319 Loss: 0.04, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 321 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 322 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 323 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 324 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 325 Loss: 0.05, Loss_test: 0.04\n",
      "Epoch: 326 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 327 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 328 Loss: 0.03, Loss_test: 0.04\n",
      "Epoch: 329 Loss: 0.05, Loss_test: 0.04\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 331 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 332 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 333 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 334 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 335 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 336 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 337 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 338 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 339 Loss: 0.03, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 341 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 342 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 343 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 344 Loss: 0.17, Loss_test: 0.03\n",
      "Epoch: 345 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 346 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 347 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 348 Loss: 0.06, Loss_test: 0.03\n",
      "Epoch: 349 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 351 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 352 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 353 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 354 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 355 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 356 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 357 Loss: 0.11, Loss_test: 0.03\n",
      "Epoch: 358 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 359 Loss: 0.03, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.04, Loss_test: 0.04\n",
      "Epoch: 361 Loss: 0.07, Loss_test: 0.03\n",
      "Epoch: 362 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 363 Loss: 0.06, Loss_test: 0.03\n",
      "Epoch: 364 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 365 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 366 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 367 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 368 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 369 Loss: 0.06, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 371 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 372 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 373 Loss: 0.06, Loss_test: 0.03\n",
      "Epoch: 374 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 375 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 376 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 377 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 378 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 379 Loss: 0.03, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 381 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 382 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 383 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 384 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 385 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 386 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 387 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 388 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 389 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 391 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 392 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 393 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 394 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 395 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 396 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 397 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 398 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 399 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 401 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 402 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 403 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 404 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 405 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 406 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 407 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 408 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 409 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 411 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 412 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 413 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 414 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 415 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 416 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 417 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 418 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 419 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 421 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 422 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 423 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 424 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 425 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 426 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 427 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 428 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 429 Loss: 0.05, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 431 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 432 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 433 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 434 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 435 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 436 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 437 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 438 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 439 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 441 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 442 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 443 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 444 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 445 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 446 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 447 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 448 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 449 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 451 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 452 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 453 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 454 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 455 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 456 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 457 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 458 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 459 Loss: 0.01, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 461 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 462 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 463 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 464 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 465 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 466 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 467 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 468 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 469 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 471 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 472 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 473 Loss: 0.05, Loss_test: 0.03\n",
      "Epoch: 474 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 475 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 476 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 477 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 478 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 479 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 481 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 482 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 483 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 484 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 485 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 486 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 487 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 488 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 489 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 491 Loss: 0.04, Loss_test: 0.03\n",
      "Epoch: 492 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 493 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 494 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 495 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 496 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 497 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 498 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 499 Loss: 0.02, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 501 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 502 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 503 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 504 Loss: 0.03, Loss_test: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 505 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 506 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 507 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 508 Loss: 0.03, Loss_test: 0.03\n",
      "Epoch: 509 Loss: 0.05, Loss_test: 0.03\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.02, Loss_test: 0.03\n",
      "Epoch: 511 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 512 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 513 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 514 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 515 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 516 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 517 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 518 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 519 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 521 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 522 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 523 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 524 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 525 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 526 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 527 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 528 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 529 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 531 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 532 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 533 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 534 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 535 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 536 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 537 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 538 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 539 Loss: 0.04, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 541 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 542 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 543 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 544 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 545 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 546 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 547 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 548 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 549 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 551 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 552 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 553 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 554 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 555 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 556 Loss: 0.05, Loss_test: 0.02\n",
      "Epoch: 557 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 558 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 559 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.06, Loss_test: 0.02\n",
      "Epoch: 561 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 562 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 563 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 564 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 565 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 566 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 567 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 568 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 569 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 571 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 572 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 573 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 574 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 575 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 576 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 577 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 578 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 579 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 581 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 582 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 583 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 584 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 585 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 586 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 587 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 588 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 589 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 591 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 592 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 593 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 594 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 595 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 596 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 597 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 598 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 599 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 601 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 602 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 603 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 604 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 605 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 606 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 607 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 608 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 609 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 611 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 612 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 613 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 614 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 615 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 616 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 617 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 618 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 619 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 621 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 622 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 623 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 624 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 625 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 626 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 627 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 628 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 629 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 631 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 632 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 633 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 634 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 635 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 636 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 637 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 638 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 639 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 641 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 642 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 643 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 644 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 645 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 646 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 647 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 648 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 649 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 651 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 652 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 653 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 654 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 655 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 656 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 657 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 658 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 659 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 661 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 662 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 663 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 664 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 665 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 666 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 667 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 668 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 669 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 671 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 672 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 673 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 674 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 675 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 676 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 677 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 678 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 679 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 681 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 682 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 683 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 684 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 685 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 686 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 687 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 688 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 689 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 691 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 692 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 693 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 694 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 695 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 696 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 697 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 698 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 699 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 701 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 702 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 703 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 704 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 705 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 706 Loss: 0.03, Loss_test: 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 707 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 708 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 709 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 711 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 712 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 713 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 714 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 715 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 716 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 717 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 718 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 719 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 721 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 722 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 723 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 724 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 725 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 726 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 727 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 728 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 729 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 731 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 732 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 733 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 734 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 735 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 736 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 737 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 738 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 739 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 741 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 742 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 743 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 744 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 745 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 746 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 747 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 748 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 749 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 751 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 752 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 753 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 754 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 755 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 756 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 757 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 758 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 759 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 761 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 762 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 763 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 764 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 765 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 766 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 767 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 768 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 769 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 771 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 772 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 773 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 774 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 775 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 776 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 777 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 778 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 779 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 781 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 782 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 783 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 784 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 785 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 786 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 787 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 788 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 789 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 791 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 792 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 793 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 794 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 795 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 796 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 797 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 798 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 799 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 801 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 802 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 803 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 804 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 805 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 806 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 807 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 808 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 809 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 811 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 812 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 813 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 814 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 815 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 816 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 817 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 818 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 819 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 821 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 822 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 823 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 824 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 825 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 826 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 827 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 828 Loss: 0.01, Loss_test: 0.03\n",
      "Epoch: 829 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 831 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 832 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 833 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 834 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 835 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 836 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 837 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 838 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 839 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 841 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 842 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 843 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 844 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 845 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 846 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 847 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 848 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 849 Loss: 0.02, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 851 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 852 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 853 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 854 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 855 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 856 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 857 Loss: 0.04, Loss_test: 0.02\n",
      "Epoch: 858 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 859 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 861 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 862 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 863 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 864 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 865 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 866 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 867 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 868 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 869 Loss: 0.01, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 871 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 872 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 873 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 874 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 875 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 876 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 877 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 878 Loss: 0.01, Loss_test: 0.01\n",
      "Epoch: 879 Loss: 0.02, Loss_test: 0.01\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 881 Loss: 0.02, Loss_test: 0.02\n",
      "Epoch: 882 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 883 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 884 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 885 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 886 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 887 Loss: 0.03, Loss_test: 0.02\n",
      "Epoch: 888 Loss: 0.01, Loss_test: 0.01\n",
      "Epoch: 889 Loss: 0.01, Loss_test: 0.01\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.01, Loss_test: 0.01\n",
      "Epoch: 891 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 892 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 893 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 894 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 895 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 896 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 897 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 898 Loss: 0.01, Loss_test: 0.02\n",
      "Epoch: 899 Loss: 0.03, Loss_test: 0.02\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.01, Loss_test: 0.02\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_1000_feature1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_1000_feature_1000epochs_64batch(openoffice).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807c58edb0df442eb8e21f1bd559ca60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8265), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910e2a34b1194b3cae74e33e6923d388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11757), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9131746b51dc4904af8f8284102ab0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12837), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b81f5f363e444c9fdb5aab84439510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d9097c8a98436c80d1a6ba6093ea0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40801e07cba2456d89f8440aa5929d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4d6da2a3434d8a8f753b1fffc7a374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11757), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 900 Loss: 0.01, Loss_test: 0.02, recall@25: 0.50\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98306:88871,50853,33630,90791|69738:0.297765851020813,95954:0.27611619234085083,29075:0.2591426968574524,69519:0.25366532802581787,41017:0.2156497836112976,34122:0.19280236959457397,70162:0.1826537847518921,74089:0.18176430463790894,63806:0.1776655912399292,90792:0.17582958936691284,70060:0.16435980796813965,74532:0.1588779091835022,39878:0.1577959656715393,175:0.15182214975357056,41107:0.1507135033607483,73023:0.15034401416778564,43048:0.14737051725387573,58842:0.14433437585830688,58843:0.14433437585830688,94925:0.14420253038406372,43033:0.14334112405776978,71296:0.1408063769340515,43420:0.13998961448669434,47227:0.1394703984260559,95394:0.13905566930770874,56091:0.13880789279937744,57485:0.13808166980743408,76388:0.13668972253799438,38157:0.12998270988464355',\n",
       " '32771:32490,33548,32560,33879,32699|30642:0.3690817952156067,26003:0.3575056195259094,20959:0.33695292472839355,9780:0.33060187101364136,4189:0.31432896852493286,39343:0.31054621934890747,17105:0.30526626110076904,18023:0.3038383722305298,13572:0.294452965259552,27034:0.29410988092422485,14128:0.29222261905670166,12806:0.2920394539833069,12557:0.2896062731742859,13446:0.287333607673645,7276:0.2870153784751892,2693:0.2828837037086487,12726:0.2803504467010498,17469:0.27907586097717285,105222:0.2767888903617859,2331:0.2709270715713501,9917:0.2708736062049866,1888:0.26936841011047363,28339:0.2686048150062561,67283:0.26804888248443604,5412:0.26727229356765747,7120:0.26509183645248413,6158:0.26341134309768677,16905:0.2633625268936157,13188:0.26012933254241943',\n",
       " '32772:22694,36970,39820,34488,33298,36760,34911|9910:0.2947119474411011,25606:0.28973960876464844,19705:0.2737452983856201,51760:0.2729921340942383,10052:0.26949435472488403,25911:0.2606777548789978,94030:0.250990629196167,35031:0.23744714260101318,114724:0.2277548909187317,45447:0.22659331560134888,51117:0.21214145421981812,14895:0.21196389198303223,13995:0.2113356590270996,27604:0.21000730991363525,44818:0.20734059810638428,26653:0.20722943544387817,21056:0.20619595050811768,28429:0.2047075629234314,94936:0.2040964961051941,60555:0.20197534561157227,74268:0.19665908813476562,10023:0.1954408884048462,30037:0.19463390111923218,27321:0.18943476676940918,22602:0.18590497970581055,44105:0.18400579690933228,19629:0.17485791444778442,12983:0.16997772455215454,20838:0.1698799729347229',\n",
       " '32776:30241,33762,31134,33183|32997:0.423641562461853,32703:0.42007410526275635,32088:0.31743431091308594,29407:0.3058028221130371,34754:0.30442219972610474,67388:0.29408520460128784,20878:0.2929497957229614,27403:0.29065150022506714,20717:0.29058563709259033,18239:0.28993332386016846,54307:0.289839506149292,67389:0.28774410486221313,46446:0.28744977712631226,26962:0.28676652908325195,13361:0.28342461585998535,25223:0.2831263542175293,55692:0.282163143157959,35463:0.2808973789215088,87058:0.28065919876098633,87057:0.2765939235687256,56249:0.2750570774078369,18508:0.2744407057762146,50964:0.2724609971046448,40440:0.2721431851387024,34585:0.2705920934677124,69927:0.2703733444213867,65269:0.27032727003097534,9850:0.26953959465026855,24119:0.2680819630622864',\n",
       " '65545:55967|65107:0.3851437568664551,50903:0.3768705129623413,41429:0.37157565355300903,63107:0.3376288414001465,52325:0.3276277780532837,87813:0.3163057565689087,91709:0.3055180311203003,61840:0.2921804189682007,17678:0.28920257091522217,13396:0.2770656943321228,98015:0.27538377046585083,59598:0.2741658687591553,64490:0.27171874046325684,55874:0.2714153528213501,35006:0.2710389494895935,103810:0.270799458026886,63653:0.2679362893104553,96581:0.2673983573913574,63176:0.2673800587654114,84585:0.2669593095779419,55105:0.2663074731826782,20597:0.2631787657737732,67310:0.2620803117752075,95348:0.2568257451057434,54989:0.2552165985107422,23715:0.25345635414123535,78829:0.25338804721832275,16573:0.25040411949157715,49775:0.24769920110702515',\n",
       " '65549:26122,62365,3411,74204,94365|90943:0.5262443423271179,13699:0.502111166715622,71719:0.4805613160133362,4826:0.46796196699142456,80225:0.4621773958206177,36741:0.437234103679657,46892:0.43163883686065674,37792:0.43087953329086304,71695:0.4132019281387329,6699:0.40992337465286255,23431:0.39813780784606934,50607:0.3953455686569214,3823:0.390666127204895,61150:0.3869180679321289,59207:0.384601891040802,29916:0.38413041830062866,47081:0.3832908272743225,29692:0.38276195526123047,10323:0.38247019052505493,24164:0.37547624111175537,8436:0.3753576874732971,21654:0.37071192264556885,68510:0.3691878318786621,23694:0.3581100106239319,29890:0.35605084896087646,107242:0.34783869981765747,21445:0.34634798765182495,21063:0.34566813707351685,28670:0.344882607460022',\n",
       " '98318:98570|102506:0.4713616967201233,102419:0.43282026052474976,102195:0.4182271361351013,103643:0.41306883096694946,17877:0.41085273027420044,102984:0.40775954723358154,66262:0.39385247230529785,100166:0.3799811601638794,79139:0.36937928199768066,57760:0.3676273226737976,107690:0.3620424270629883,51445:0.36114656925201416,46485:0.3610786199569702,99728:0.35752272605895996,108088:0.35503244400024414,83456:0.35429811477661133,41984:0.3434581160545349,55261:0.3347562551498413,100320:0.33388346433639526,18626:0.3322635889053345,76514:0.32858967781066895,52342:0.3277434706687927,98570:0.3262895941734314,101869:0.32338958978652954,45590:0.3229738473892212,15578:0.3226594924926758,100003:0.32250744104385376,18148:0.3167634606361389,24459:0.31673526763916016',\n",
       " '98319:107242,102941|84419:0.5488477647304535,49328:0.5455146431922913,75104:0.5422944724559784,112435:0.5285430550575256,33883:0.5252002775669098,115858:0.5084112584590912,20551:0.5058055222034454,20552:0.5058055222034454,78334:0.5009729266166687,96999:0.49973100423812866,82363:0.4977872371673584,7074:0.493904709815979,27448:0.49113231897354126,71697:0.47863101959228516,102941:0.47792136669158936,8660:0.47469013929367065,27421:0.4737060070037842,64702:0.46971046924591064,39345:0.46939122676849365,9279:0.4663488268852234,71712:0.46290814876556396,71730:0.4622962474822998,80626:0.4608727693557739,366:0.45872217416763306,39526:0.4579845070838928,84855:0.4547634720802307,64752:0.4529315233230591,68033:0.45141953229904175,28670:0.4513244032859802',\n",
       " '18:100,271|6619:0.6280434131622314,59399:0.619701087474823,1271:0.6194825768470764,6937:0.5929917693138123,23993:0.5632641315460205,63192:0.5627181231975555,27096:0.5141505002975464,10540:0.5132420361042023,73887:0.5063776671886444,5314:0.4837285876274109,6790:0.4820672869682312,10122:0.47933757305145264,2148:0.4762791395187378,7789:0.4741143584251404,57393:0.46736085414886475,6397:0.46700936555862427,14159:0.4543430209159851,7122:0.4504430294036865,5543:0.43374103307724,12204:0.42305874824523926,4649:0.4032489061355591,65006:0.39756155014038086,73159:0.3868088722229004,7303:0.3729073405265808,2287:0.3726850748062134,6575:0.36906272172927856,13343:0.36497950553894043,22867:0.3576211929321289,598:0.35410743951797485',\n",
       " '32788:31681,32789,31726|32789:0.9997879709699191,32677:0.4434373378753662,32655:0.3526046872138977,32675:0.3426949977874756,32541:0.28895294666290283,35189:0.28767138719558716,19976:0.2594492435455322,31623:0.2591656446456909,88333:0.25165265798568726,114525:0.2369336485862732,59131:0.2334800362586975,59178:0.2318892478942871,20883:0.22762000560760498,36327:0.22696036100387573,43557:0.22669047117233276,65359:0.22468900680541992,33532:0.2221638560295105,44824:0.2214786410331726,37265:0.21870630979537964,95162:0.21390634775161743,35435:0.21363019943237305,31944:0.21167701482772827,37804:0.21097725629806519,50991:0.21085602045059204,114596:0.20986133813858032,60856:0.20876431465148926,46471:0.20633888244628906,59989:0.20579200983047485,45913:0.20571517944335938',\n",
       " '32789:31681,32788,31726|32788:0.9997879709699191,32677:0.4434373378753662,32655:0.3526046872138977,32675:0.3426949977874756,32541:0.28895294666290283,35189:0.28767138719558716,19976:0.2594492435455322,31623:0.2591656446456909,88333:0.25165265798568726,114525:0.2369336485862732,59131:0.2334800362586975,59178:0.2318892478942871,20883:0.22762000560760498,36327:0.22696036100387573,43557:0.22669047117233276,65359:0.22468900680541992,33532:0.2221638560295105,44824:0.2214786410331726,37265:0.21870630979537964,95162:0.21390634775161743,35435:0.21363019943237305,31944:0.21167701482772827,37804:0.21097725629806519,50991:0.21085602045059204,114596:0.20986133813858032,60856:0.20876431465148926,46471:0.20633888244628906,59989:0.20579200983047485,45913:0.20571517944335938',\n",
       " '65561:27520,50753,21280,85187,51168,35298,83046,27630,81041,46738,46739,46740,103094,48951,55995,76895|20856:0.47587597370147705,22719:0.44471317529678345,4597:0.4434521198272705,76735:0.43076908588409424,4725:0.41941142082214355,6252:0.4057493209838867,66432:0.3964712619781494,15949:0.38527917861938477,8301:0.38086313009262085,4904:0.3735271692276001,65643:0.37084823846817017,5399:0.3696494698524475,6348:0.3687850832939148,4863:0.3686354160308838,4742:0.3686050772666931,20292:0.359902024269104,27794:0.3582281470298767,15752:0.3548808693885803,14941:0.35376977920532227,5036:0.3526459336280823,13283:0.35169804096221924,8363:0.34951889514923096,97789:0.3481391668319702,21880:0.3444700837135315,35832:0.3427199721336365,42788:0.3403748869895935,8201:0.3366268277168274,8690:0.33340197801589966,32405:0.3319058418273926',\n",
       " '27:59,92|473:0.4648228883743286,474:0.4648228883743286,475:0.4648228883743286,59:0.35100167989730835,28578:0.2663017511367798,28541:0.24834930896759033,41000:0.24704819917678833,4822:0.2200636863708496,50729:0.21082687377929688,7032:0.20659005641937256,18940:0.2035655975341797,33449:0.1997009515762329,16321:0.19601351022720337,39342:0.1919090747833252,14823:0.18984830379486084,113194:0.18350040912628174,19339:0.1829131841659546,35702:0.18156665563583374,17096:0.17708849906921387,34414:0.17674559354782104,22137:0.1760154366493225,4471:0.1747949719429016,22710:0.16620159149169922,22711:0.16620159149169922,22712:0.16620159149169922,7917:0.16491246223449707,8360:0.16415905952453613,52369:0.16384190320968628,32399:0.16381347179412842',\n",
       " '32:42944,6342,3207,18886,32905,54854,54855,67697,4756,23861,18648,58138,40286|1585:0.5119549334049225,24540:0.33723777532577515,21160:0.32085907459259033,70573:0.32054340839385986,17017:0.30937284231185913,44415:0.2939406633377075,19429:0.2902703881263733,19771:0.2808794379234314,20141:0.2683904767036438,32241:0.26822853088378906,52531:0.2674359083175659,20140:0.26643019914627075,85786:0.26599031686782837,116112:0.2643059492111206,66321:0.2604231834411621,75757:0.25920772552490234,60782:0.2589799761772156,22286:0.25895047187805176,12148:0.25847363471984863,41292:0.25662291049957275,100022:0.25362157821655273,43707:0.2518773674964905,73480:0.25131601095199585,31572:0.2510797381401062,32678:0.24772846698760986,41469:0.2456721067428589,86866:0.24527376890182495,67903:0.24358290433883667,4946:0.2424144744873047',\n",
       " '32813:33041|32675:0.3954532742500305,32618:0.3798453211784363,26660:0.3637610077857971,32766:0.3566579222679138,29026:0.33336472511291504,73600:0.33093971014022827,13824:0.328823983669281,2952:0.3195769786834717,72444:0.31416982412338257,12755:0.30498987436294556,42362:0.3033760190010071,51488:0.30255281925201416,55920:0.30220848321914673,11658:0.30155277252197266,4397:0.2928827404975891,13782:0.29173511266708374,16891:0.2892305254936218,16805:0.2845505475997925,16806:0.2845505475997925,59668:0.27940833568573,34329:0.2788658142089844,17286:0.27881985902786255,17085:0.27536439895629883,32677:0.2740269899368286,753:0.2740258574485779,13873:0.27328985929489136,8055:0.2719345688819885,19160:0.27165257930755615,49405:0.26935142278671265',\n",
       " '32814:37274,38852,37357|37274:0.4615306258201599,21919:0.365442156791687,7494:0.3166455030441284,110136:0.3124867081642151,62918:0.29109299182891846,112879:0.28463608026504517,14282:0.2832515835762024,70900:0.2704448699951172,54444:0.26814430952072144,51409:0.2681161165237427,43438:0.26295387744903564,77422:0.2621176242828369,62068:0.255700945854187,21920:0.2537846565246582,10000:0.23867332935333252,52711:0.238530695438385,42355:0.23230820894241333,99063:0.2254999876022339,62224:0.22397559881210327,110609:0.20662981271743774,25787:0.20585626363754272,26749:0.20335739850997925,42152:0.2031218409538269,82101:0.20165324211120605,22498:0.19502365589141846,72036:0.19041860103607178,99610:0.17622679471969604,31658:0.16415023803710938,57544:0.15567779541015625',\n",
       " '32815:32033,31362,31874,33062,31550|32601:0.4162134528160095,39029:0.29396486282348633,45605:0.26962029933929443,45602:0.2669605612754822,57664:0.26664096117019653,71938:0.26207077503204346,32543:0.24829518795013428,31554:0.2465001344680786,69858:0.23875296115875244,17057:0.23328548669815063,35719:0.23249632120132446,85080:0.23099285364151,17132:0.2214241623878479,32662:0.21904194355010986,17060:0.21346360445022583,20120:0.21291494369506836,44824:0.21264415979385376,43621:0.21078574657440186,35739:0.20541012287139893,32981:0.2049022912979126,45621:0.20330309867858887,45625:0.20328110456466675,40286:0.20148366689682007,32033:0.20005959272384644,7432:0.19717073440551758,52491:0.19589459896087646,32788:0.1942077875137329,32789:0.1942077875137329,17059:0.19344604015350342',\n",
       " '65586:71849,67371,67476,83252,88157,67642,71099,69789,81630|69352:0.49027949571609497,55181:0.4178040027618408,61499:0.40643441677093506,68954:0.40236252546310425,48951:0.3961632251739502,19438:0.39449888467788696,58264:0.38689088821411133,63730:0.3750177025794983,24504:0.3724186420440674,16241:0.3669138550758362,6504:0.36072999238967896,5186:0.357183575630188,8262:0.35576462745666504,16303:0.35566461086273193,63524:0.35402435064315796,14541:0.3517962694168091,22177:0.3485810160636902,15972:0.34774696826934814,15973:0.34774696826934814,15975:0.34774696826934814,20496:0.3460236191749573,65073:0.3451724052429199,10453:0.34083855152130127,55758:0.34041541814804077,39521:0.33805108070373535,6996:0.33772653341293335,36410:0.3360362648963928,69180:0.3350304961204529,20023:0.33486849069595337',\n",
       " '51:52|52:0.9218330308794975,9734:0.4243229627609253,1423:0.41651952266693115,1561:0.400671124458313,10123:0.38129955530166626,28839:0.3394574522972107,57817:0.331301748752594,60041:0.29862260818481445,88479:0.29712438583374023,24945:0.2862734794616699,1457:0.2837916612625122,82229:0.21211665868759155,18973:0.20877385139465332,23641:0.20641344785690308,22882:0.20252960920333862,66032:0.19968384504318237,230:0.19928807020187378,28408:0.19452589750289917,82336:0.17826414108276367,33531:0.16940587759017944,19330:0.16544109582901,580:0.15963971614837646,31330:0.159481942653656,25168:0.15706676244735718,83513:0.15362608432769775,82242:0.13298553228378296,23248:0.12908095121383667,27096:0.11395531892776489,115335:0.1023600697517395',\n",
       " '52:51|51:0.9218330308794975,9734:0.42976343631744385,1423:0.42597317695617676,1561:0.400321900844574,10123:0.3759547472000122,28839:0.35627567768096924,57817:0.3409847021102905,88479:0.28962892293930054,1457:0.2800058126449585,60041:0.27583813667297363,24945:0.2563854455947876,66032:0.18584996461868286,22882:0.1815187931060791,23641:0.17488831281661987,82229:0.1734611988067627,18973:0.16947823762893677,230:0.16537129878997803,28408:0.1547393798828125,33531:0.14118820428848267,82336:0.1364864706993103,19330:0.1328350305557251,83513:0.12747079133987427,25168:0.12627893686294556,23248:0.12200719118118286,31330:0.11601865291595459,580:0.1115950345993042,35628:0.10454678535461426,82242:0.09523952007293701,27096:0.08333718776702881']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_1000_feature_1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11757"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exported_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/openoffice/bert/exported_rank_baseline_1000.txt'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.38,\n",
       " '2 - recall_at_10': 0.43,\n",
       " '3 - recall_at_15': 0.46,\n",
       " '4 - recall_at_20': 0.49,\n",
       " '5 - recall_at_25': 0.5}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some ideas to visualizate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
