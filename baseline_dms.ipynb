{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Baseline DMS\n",
    "\n",
    "https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "# %matplotlib inline\n",
    "\n",
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras import optimizers\n",
    "\n",
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval\n",
    "\n",
    "import os\n",
    "from keras_bert import load_vocabulary\n",
    "import random\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional, GlobalAveragePooling1D, TimeDistributed\n",
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: epochs=1000\n",
      "env: base=openoffice\n"
     ]
    }
   ],
   "source": [
    "%env epochs 1000\n",
    "%env base openoffice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 100 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 100 # 500\n",
    "'''\n",
    "    Sequence length\n",
    "    # Eclipse\n",
    "    # 100   recall@25 = \n",
    "    # 20    recall@25 = \n",
    "    # Netbeans\n",
    "    # 100   recall@25 = 0.37\n",
    "    # 20    recall@25 = 0.40\n",
    "    # Open office\n",
    "    # 100   recall@25 = 0.36\n",
    "    # 20    recall@25 = 0.38\n",
    "'''\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = int(os.environ['epochs'])\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = os.environ['base']\n",
    "METHOD = 'baseline_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "baseline_1000 for 1000 epochs in openoffice\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "print(\"*********\")\n",
    "print(\"{} for {} epochs in {}\".format(METHOD, epochs, DOMAIN))\n",
    "print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98070"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2de1b40e554368b2970e8787056985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c55b9a82b24ad89fed9874df5c9ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 23 s, sys: 1.39 s, total: 24.4 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef165b79edb4505a2109e974e43629b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58572), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_train='train_chronological', path_test='test_chronological'\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7499, 14267],\n",
       " [7499, 14451],\n",
       " [7499, 14335],\n",
       " [14267, 14451],\n",
       " [14267, 14335],\n",
       " [14451, 14335],\n",
       " [109857, 109858],\n",
       " [55321, 45346],\n",
       " [55321, 25797],\n",
       " [55321, 39359]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "# Generating tiple of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '1\\n',\n",
       " 'component': '69\\n',\n",
       " 'creation_ts': '2005-11-15 08:56:00 +0000',\n",
       " 'delta_ts': '2005-11-15 12:24:43 +0000',\n",
       " 'description': '[CLS] when saving a document with equations as a mn ##s word document , there are errors in the equations , inc ##l . : alpha sign in write becomes little square in word plus sign in write becomes little square in word [SEP]',\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  2043,  7494,  1037,  6254,  2007, 11380,  2004,  1037,\n",
       "        24098,  2015,  2773,  6254,  1010,  2045,  2024, 10697,  1999,\n",
       "         1996, 11380,  1010,  4297,  2140,  1012,  1024,  6541,  3696,\n",
       "         1999,  4339,  4150,  2210,  2675,  1999,  2773,  4606,  3696,\n",
       "         1999,  4339,  4150,  2210,  2675,  1999,  2773,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          102]),\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 57911,\n",
       " 'priority': '3\\n',\n",
       " 'product': '14\\n',\n",
       " 'resolution': 'NDUPLICATE',\n",
       " 'textual_token': array([  101,  7561,  1999,  8522,  2043,  7494,  4339,  6254,  2004,\n",
       "        24098,  2015,  2773,  6254,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          102,   101,  2043,  7494,  1037,  6254,  2007, 11380,  2004,\n",
       "         1037, 24098,  2015,  2773,  6254,  1010,  2045,  2024, 10697,\n",
       "         1999,  1996, 11380,  1010,  4297,  2140,  1012,  1024,  6541,\n",
       "         3696,  1999,  4339,  4150,  2210,  2675,  1999,  2773,  4606,\n",
       "         3696,  1999,  4339,  4150,  2210,  2675,  1999,  2773,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'title': '[CLS] error in equation when saving write document as mn ##s word document [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101,  7561,  1999,  8522,  2043,  7494,  4339,  6254,  2004,\n",
       "        24098,  2015,  2773,  6254,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          102]),\n",
       " 'topic': 27,\n",
       " 'topic_30': 27,\n",
       " 'topic_50': 44,\n",
       " 'topic_index': 26,\n",
       " 'topic_index_30': 26,\n",
       " 'topic_index_50': 43,\n",
       " 'topics': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.35, 0.  , 0.09, 0.39, 0.  , 0.  ]),\n",
       " 'topics_30': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.35, 0.  , 0.09, 0.39, 0.  , 0.  ]),\n",
       " 'topics_50': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.05,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.06, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.21, 0.  , 0.  , 0.07, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.45, 0.  , 0.  , 0.  , 0.  , 0.  ]),\n",
       " 'version': '245\\n'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 5502)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = bug_ids #list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    \n",
    "    for bug_id in batch_bugs:\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug)\n",
    "\n",
    "    batch_features['title'] = np.array(batch_features['title'])\n",
    "    batch_features['desc'] = np.array(batch_features['desc'])\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.93 ms, sys: 3.92 ms, total: 10.9 ms\n",
      "Wall time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "batch_size_test = 128\n",
    "\n",
    "bug_test_ids = experiment.get_test_ids(baseline.test_data)\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.test_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_test_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title'], \n",
    "             valid_input_sample['description'],\n",
    "            valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 100), (384, 100), (384, 738), (384,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title'].shape, valid_input_sample['description'].shape, valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 4116)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPrsLs4Kg4Pa"
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9eE5TWoH7p"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "vocab = baseline.load_vocabulary(os.path.join(DIR, 'vocab_embed.pkl'))\n",
    "#print(np.random.choice(vocab, 10))\n",
    "# for token in vocab:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 20031'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_embed(baseline, GLOVE_DIR, EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    embed_path = os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')\n",
    "    \n",
    "    f2 = open(embed_path, 'rb')\n",
    "    num_lines = sum(1 for line in f2)\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(embed_path, 'rb')\n",
    "    vocab = baseline.load_vocabulary(os.path.join(baseline.DIR, 'vocab_embed.pkl'))\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Initialize uniform the vector considering the Tanh activation\n",
    "    embedding_matrix = np.random.uniform(-1.0, 1.0, (num_lines + vocab_size, EMBEDDING_DIM))\n",
    "    embedding_matrix[0, :] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    loop = tqdm(f)\n",
    "    loop.set_description(\"Loading Glove\")\n",
    "    i = 0\n",
    "    for line in loop:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embed = np.asarray(tokens[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "        embedding_matrix[i] = embed\n",
    "        i+=1\n",
    "        loop.update(1)\n",
    "    f.close()\n",
    "    loop.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 42B 300d.' % len(embeddings_index))\n",
    "\n",
    "    loop = tqdm(total=vocab_size)\n",
    "    loop.set_description('Loading embedding from dataset pretrained')\n",
    "    \n",
    "    for word, embed in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.asarray(embed, dtype='float32')\n",
    "        loop.update(1)\n",
    "        i+=1\n",
    "    loop.close()\n",
    "    baseline.embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QY-ef3OGoIiq",
    "outputId": "55f4c93c-98bb-4bac-92f2-76bd3b777605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38f4ef1527e4032870e3946715f4107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 1917494 word vectors in Glove 42B 300d.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed02a8300f4c29a06525d4e6f8f94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20031), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 34s, sys: 3.41 s, total: 1min 37s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generating_embed(baseline, GLOVE_DIR=GLOVE_DIR, EMBEDDING_DIM=EMBEDDING_DIM) # MAX_NB_WORDS=MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is missing the padding_idx used in pytorch\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html\n",
    "# https://stackoverflow.com/questions/54824768/rnn-model-gru-of-word2vec-to-regression-not-learning\n",
    "def embedding_layer(embeddings, num_words, embedding_dim, max_sequence_length, trainable):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                  embedding_dim,\n",
    "                                  name='embedding_layer',\n",
    "                                  weights=[embeddings],\n",
    "                                  embeddings_constraint=MaxNorm(max_value=1, axis=0),\n",
    "                                  #input_length=max_sequence_length,\n",
    "                                  input_length=None,\n",
    "                                  trainable=trainable)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI074wU4Y13y"
   },
   "source": [
    "### CNN with filter 3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "h6YJU9GtFTyq",
    "outputId": "f85cf105-1fd6-491d-d969-7e6936f32739",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cnn_model(embedding_layer, max_sequence_length):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None,), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    # best combination filter (3, 4, 5) e 128 e 256\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    n_filters = 64\n",
    "\n",
    "    for index, filter_size in enumerate(filter_sizes):\n",
    "        l_conv = Conv1D(filters=n_filters, kernel_size=filter_size)(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=filter_size)(l_conv) # index+1\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=n_filters * 3, kernel_size=3)(l_merge)\n",
    "    layer = GlobalAveragePooling1D()(l_merge)\n",
    "    #layer = Flatten()(l_merge)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "    #layer = LeakyReLU()(layer)\n",
    "\n",
    "    cnn_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureCNNGenerationModel') # inputs=visible\n",
    "\n",
    "    return cnn_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6ObTXiaALH"
   },
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vC7MQXEsaCeG",
    "outputId": "65e647a9-c5d3-4009-b8a4-2e2d97b52684"
   },
   "outputs": [],
   "source": [
    "def lstm_model(embedding_layer, max_sequence_length):\n",
    "    number_lstm_units = 75\n",
    "    rate_drop_lstm = 0\n",
    "    recurrent_dropout = 0\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), name='Feature_BugInput')\n",
    "    #sequence_input = Input(shape=(None, ), name='Feature_BugInput')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    left_layer = LSTM(number_lstm_units, return_sequences=True)(embedded_sequences)\n",
    "    right_layer = LSTM(number_lstm_units, return_sequences=True, go_backwards=True)(left_layer)\n",
    "    \n",
    "    lstm_layer = Concatenate()([left_layer, right_layer])\n",
    "    \n",
    "    #lstm_layer = TimeDistributed(Dense(50))(lstm_layer)\n",
    "    #layer = Flatten()(lstm_layer)\n",
    "    layer = GlobalAveragePooling1D()(lstm_layer)\n",
    "    layer = Dense(300, activation='tanh')(layer)\n",
    "\n",
    "    lstm_feature_model = Model(inputs=[sequence_input], outputs=[layer], name = 'FeatureLstmGenerationModel') # inputs=visible\n",
    "\n",
    "    return lstm_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "    del y_true\n",
    "    margin = 1.\n",
    "    labels = y_pred[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = y_pred[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    bug_t = Input(shape = (sequence_length_t, ), name = 'title_{}'.format(name))\n",
    "    bug_d = Input(shape = (sequence_length_d, ), name = 'desc_{}'.format(name))\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model(bug_t)\n",
    "    bug_d_feat = desc_feature_model(bug_d)\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    #     bug_feature_output = Activation('tanh')(bug_feature_output)\n",
    "    \n",
    "    # Bug representation layer\n",
    "    # bug_feature_output = Dense(300, activation='tanh')(bug_feature_output)\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t, bug_d, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    # optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=triplet_loss_adapted_from_tf) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "METHOD = 'baseline_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-32-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-33-5cf822f9ffea>:53: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.93, Loss_test: 0.91\n",
      "Epoch: 2 Loss: 0.91, Loss_test: 0.91\n",
      "Epoch: 3 Loss: 0.91, Loss_test: 0.91\n",
      "Epoch: 4 Loss: 0.92, Loss_test: 0.91\n",
      "Epoch: 5 Loss: 0.94, Loss_test: 0.91\n",
      "Epoch: 6 Loss: 0.93, Loss_test: 0.91\n",
      "Epoch: 7 Loss: 0.89, Loss_test: 0.91\n",
      "Epoch: 8 Loss: 0.92, Loss_test: 0.91\n",
      "Epoch: 9 Loss: 0.92, Loss_test: 0.90\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.91, Loss_test: 0.91\n",
      "Epoch: 11 Loss: 0.91, Loss_test: 0.90\n",
      "Epoch: 12 Loss: 0.90, Loss_test: 0.90\n",
      "Epoch: 13 Loss: 0.89, Loss_test: 0.90\n",
      "Epoch: 14 Loss: 0.86, Loss_test: 0.90\n",
      "Epoch: 15 Loss: 0.93, Loss_test: 0.90\n",
      "Epoch: 16 Loss: 0.90, Loss_test: 0.90\n",
      "Epoch: 17 Loss: 0.90, Loss_test: 0.90\n",
      "Epoch: 18 Loss: 0.91, Loss_test: 0.89\n",
      "Epoch: 19 Loss: 0.90, Loss_test: 0.89\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.90, Loss_test: 0.89\n",
      "Epoch: 21 Loss: 0.88, Loss_test: 0.88\n",
      "Epoch: 22 Loss: 0.90, Loss_test: 0.88\n",
      "Epoch: 23 Loss: 0.91, Loss_test: 0.89\n",
      "Epoch: 24 Loss: 0.88, Loss_test: 0.87\n",
      "Epoch: 25 Loss: 0.87, Loss_test: 0.86\n",
      "Epoch: 26 Loss: 0.83, Loss_test: 0.87\n",
      "Epoch: 27 Loss: 0.86, Loss_test: 0.87\n",
      "Epoch: 28 Loss: 0.88, Loss_test: 0.85\n",
      "Epoch: 29 Loss: 0.85, Loss_test: 0.85\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.85, Loss_test: 0.84\n",
      "Epoch: 31 Loss: 0.83, Loss_test: 0.83\n",
      "Epoch: 32 Loss: 0.80, Loss_test: 0.83\n",
      "Epoch: 33 Loss: 0.71, Loss_test: 0.81\n",
      "Epoch: 34 Loss: 0.78, Loss_test: 0.81\n",
      "Epoch: 35 Loss: 0.80, Loss_test: 0.82\n",
      "Epoch: 36 Loss: 0.79, Loss_test: 0.85\n",
      "Epoch: 37 Loss: 0.76, Loss_test: 0.82\n",
      "Epoch: 38 Loss: 0.62, Loss_test: 0.80\n",
      "Epoch: 39 Loss: 0.81, Loss_test: 0.79\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.63, Loss_test: 0.82\n",
      "Epoch: 41 Loss: 0.65, Loss_test: 0.82\n",
      "Epoch: 42 Loss: 0.69, Loss_test: 0.79\n",
      "Epoch: 43 Loss: 0.49, Loss_test: 0.78\n",
      "Epoch: 44 Loss: 0.58, Loss_test: 0.79\n",
      "Epoch: 45 Loss: 0.55, Loss_test: 0.84\n",
      "Epoch: 46 Loss: 0.71, Loss_test: 0.78\n",
      "Epoch: 47 Loss: 0.70, Loss_test: 0.77\n",
      "Epoch: 48 Loss: 0.74, Loss_test: 0.82\n",
      "Epoch: 49 Loss: 0.63, Loss_test: 0.83\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.53, Loss_test: 0.78\n",
      "Epoch: 51 Loss: 0.54, Loss_test: 0.76\n",
      "Epoch: 52 Loss: 0.63, Loss_test: 0.78\n",
      "Epoch: 53 Loss: 0.43, Loss_test: 0.79\n",
      "Epoch: 54 Loss: 0.52, Loss_test: 0.76\n",
      "Epoch: 55 Loss: 0.61, Loss_test: 0.76\n",
      "Epoch: 56 Loss: 0.52, Loss_test: 0.77\n",
      "Epoch: 57 Loss: 0.37, Loss_test: 0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Loss: 0.54, Loss_test: 0.78\n",
      "Epoch: 59 Loss: 0.51, Loss_test: 0.75\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.66, Loss_test: 0.74\n",
      "Epoch: 61 Loss: 0.45, Loss_test: 0.80\n",
      "Epoch: 62 Loss: 0.52, Loss_test: 0.85\n",
      "Epoch: 63 Loss: 0.79, Loss_test: 0.83\n",
      "Epoch: 64 Loss: 0.56, Loss_test: 0.75\n",
      "Epoch: 65 Loss: 0.27, Loss_test: 0.75\n",
      "Epoch: 66 Loss: 0.68, Loss_test: 0.74\n",
      "Epoch: 67 Loss: 0.62, Loss_test: 0.73\n",
      "Epoch: 68 Loss: 0.54, Loss_test: 0.78\n",
      "Epoch: 69 Loss: 0.24, Loss_test: 0.80\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.67, Loss_test: 0.72\n",
      "Epoch: 71 Loss: 0.46, Loss_test: 0.71\n",
      "Epoch: 72 Loss: 0.36, Loss_test: 0.71\n",
      "Epoch: 73 Loss: 0.37, Loss_test: 0.70\n",
      "Epoch: 74 Loss: 0.41, Loss_test: 0.70\n",
      "Epoch: 75 Loss: 0.34, Loss_test: 0.74\n",
      "Epoch: 76 Loss: 0.61, Loss_test: 0.77\n",
      "Epoch: 77 Loss: 0.45, Loss_test: 0.76\n",
      "Epoch: 78 Loss: 0.28, Loss_test: 0.73\n",
      "Epoch: 79 Loss: 0.31, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.30, Loss_test: 0.72\n",
      "Epoch: 81 Loss: 0.47, Loss_test: 0.73\n",
      "Epoch: 82 Loss: 0.30, Loss_test: 0.73\n",
      "Epoch: 83 Loss: 0.30, Loss_test: 0.77\n",
      "Epoch: 84 Loss: 0.42, Loss_test: 0.73\n",
      "Epoch: 85 Loss: 0.26, Loss_test: 0.73\n",
      "Epoch: 86 Loss: 0.23, Loss_test: 0.73\n",
      "Epoch: 87 Loss: 0.40, Loss_test: 0.73\n",
      "Epoch: 88 Loss: 0.29, Loss_test: 0.75\n",
      "Epoch: 89 Loss: 0.62, Loss_test: 0.78\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.65, Loss_test: 0.75\n",
      "Epoch: 91 Loss: 0.35, Loss_test: 0.73\n",
      "Epoch: 92 Loss: 0.23, Loss_test: 0.72\n",
      "Epoch: 93 Loss: 0.47, Loss_test: 0.71\n",
      "Epoch: 94 Loss: 0.62, Loss_test: 0.73\n",
      "Epoch: 95 Loss: 0.43, Loss_test: 0.74\n",
      "Epoch: 96 Loss: 0.34, Loss_test: 0.74\n",
      "Epoch: 97 Loss: 0.36, Loss_test: 0.73\n",
      "Epoch: 98 Loss: 0.34, Loss_test: 0.73\n",
      "Epoch: 99 Loss: 0.40, Loss_test: 0.73\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.18, Loss_test: 0.72, recall@25: 0.57\n",
      "Best_epoch=100, Best_loss=0.18, Recall@25=0.57\n",
      "CPU times: user 11min 46s, sys: 1min 50s, total: 13min 36s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Embeddings\n",
    "desc_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_D, \n",
    "                              trainable=False)\n",
    "title_embedding_layer = embedding_layer(embeddings=baseline.embedding_matrix, \n",
    "                              num_words=len(baseline.embedding_matrix), \n",
    "                              embedding_dim=EMBEDDING_DIM, \n",
    "                              max_sequence_length=MAX_SEQUENCE_LENGTH_T, \n",
    "                              trainable=False)\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "title_feature_model = lstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "     # save results\n",
    "    result['train'].append([h])\n",
    "    result['test'].append([h_validation])\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))\n",
    "    loss = h\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "#experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "#experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[42812, 42877],\n",
       " [16798, 11775],\n",
       " [40765, 39895],\n",
       " [24224, 50899],\n",
       " [36379, 21693],\n",
       " [108355, 111761],\n",
       " [102304, 105795],\n",
       " [56502, 58872],\n",
       " [113258, 116063],\n",
       " [105337, 107738],\n",
       " [31456, 75467],\n",
       " [12038, 56670],\n",
       " [10033, 4380],\n",
       " [79600, 79601],\n",
       " [82605, 109040],\n",
       " [19429, 19431],\n",
       " [69921, 84596],\n",
       " [35214, 54387],\n",
       " [39374, 44313],\n",
       " [102370, 102582],\n",
       " [68629, 69820],\n",
       " [86559, 70582],\n",
       " [55131, 14040],\n",
       " [34141, 28023],\n",
       " [50477, 53118],\n",
       " [11597, 63332],\n",
       " [93147, 28670],\n",
       " [59598, 62710],\n",
       " [79244, 51118],\n",
       " [77431, 11775],\n",
       " [7580, 9777],\n",
       " [71946, 69983],\n",
       " [20448, 16798],\n",
       " [77794, 24780],\n",
       " [5906, 78401],\n",
       " [40809, 45727],\n",
       " [103507, 100472],\n",
       " [37411, 37521],\n",
       " [81160, 50651],\n",
       " [14941, 15047],\n",
       " [22109, 26091],\n",
       " [69721, 69449],\n",
       " [5906, 56670],\n",
       " [50477, 44597],\n",
       " [19139, 7913],\n",
       " [68629, 70455],\n",
       " [50477, 3142],\n",
       " [58697, 59353],\n",
       " [58857, 62078],\n",
       " [41733, 43465],\n",
       " [58138, 40286],\n",
       " [40224, 38093],\n",
       " [44988, 39374],\n",
       " [4678, 10033],\n",
       " [69269, 76405],\n",
       " [16204, 54726],\n",
       " [3142, 49488],\n",
       " [10736, 39003],\n",
       " [50333, 3142],\n",
       " [49722, 49605],\n",
       " [14182, 2330],\n",
       " [21240, 44889],\n",
       " [94124, 95085],\n",
       " [102370, 102419],\n",
       " [102506, 101869],\n",
       " [67697, 18648],\n",
       " [26091, 23347],\n",
       " [51656, 11395],\n",
       " [16609, 21009],\n",
       " [62146, 62691],\n",
       " [52211, 49541],\n",
       " [98764, 98577],\n",
       " [20733, 23774],\n",
       " [107738, 109630],\n",
       " [103654, 101239],\n",
       " [86559, 70623],\n",
       " [70590, 18654],\n",
       " [26502, 11597],\n",
       " [72706, 71663],\n",
       " [15431, 54387],\n",
       " [79344, 80346],\n",
       " [47687, 110101],\n",
       " [24215, 48250],\n",
       " [52377, 52551],\n",
       " [46922, 52823],\n",
       " [18397, 54483],\n",
       " [6952, 7550],\n",
       " [109040, 100855],\n",
       " [50477, 83320],\n",
       " [22335, 7983],\n",
       " [96142, 93749],\n",
       " [94124, 96724],\n",
       " [74732, 75444],\n",
       " [83001, 60586],\n",
       " [23761, 14805],\n",
       " [50333, 83320],\n",
       " [54857, 53498],\n",
       " [40615, 41579],\n",
       " [9899, 13421],\n",
       " [47847, 47868],\n",
       " [5648, 4537],\n",
       " [23577, 19323],\n",
       " [54854, 67697],\n",
       " [722, 733],\n",
       " [56670, 83320],\n",
       " [35616, 59483],\n",
       " [47568, 83708],\n",
       " [53901, 25855],\n",
       " [3142, 19415],\n",
       " [71296, 56670],\n",
       " [102419, 103643],\n",
       " [38810, 75406],\n",
       " [59434, 72850],\n",
       " [102484, 102195],\n",
       " [22723, 19642],\n",
       " [83814, 83101],\n",
       " [106889, 106877],\n",
       " [94924, 94935],\n",
       " [103846, 102762],\n",
       " [84819, 71870],\n",
       " [52250, 3910],\n",
       " [43210, 41085],\n",
       " [37706, 36862],\n",
       " [55465, 52155],\n",
       " [5906, 49488],\n",
       " [2883, 5648],\n",
       " [49220, 48589],\n",
       " [80312, 76543],\n",
       " [15431, 49488],\n",
       " [21688, 23577],\n",
       " [43406, 42812],\n",
       " [36327, 37706],\n",
       " [64664, 56931],\n",
       " [102435, 103507],\n",
       " [14972, 12755],\n",
       " [39690, 40765],\n",
       " [18886, 58138],\n",
       " [8289, 9777],\n",
       " [31456, 19401],\n",
       " [77794, 41457],\n",
       " [59490, 58921],\n",
       " [96142, 96184],\n",
       " [18654, 54387],\n",
       " [16188, 78401],\n",
       " [95571, 93749],\n",
       " [93035, 111603],\n",
       " [1601, 41457],\n",
       " [82336, 82314],\n",
       " [59118, 75671],\n",
       " [88224, 89114],\n",
       " [13787, 18654],\n",
       " [39374, 42812],\n",
       " [35340, 50333],\n",
       " [50743, 19415],\n",
       " [39003, 16798],\n",
       " [43465, 43038],\n",
       " [4854, 21240],\n",
       " [6342, 58138],\n",
       " [36425, 36433],\n",
       " [72872, 80174],\n",
       " [26502, 35214],\n",
       " [20088, 42341],\n",
       " [18886, 67697],\n",
       " [11266, 16068],\n",
       " [15431, 63332],\n",
       " [75872, 76157],\n",
       " [18693, 33787],\n",
       " [84819, 74588],\n",
       " [86476, 10525],\n",
       " [26502, 7913],\n",
       " [23861, 58138],\n",
       " [79723, 101871],\n",
       " [82539, 53252],\n",
       " [56670, 7913],\n",
       " [23347, 20600],\n",
       " [102419, 102195],\n",
       " [70590, 56670],\n",
       " [25224, 11597],\n",
       " [69820, 69983],\n",
       " [60647, 61018],\n",
       " [96498, 96184],\n",
       " [23864, 60702],\n",
       " [55131, 19401],\n",
       " [19989, 20733],\n",
       " [38594, 38925],\n",
       " [60804, 62229],\n",
       " [1601, 66910],\n",
       " [59725, 71546],\n",
       " [9493, 33787],\n",
       " [103846, 102984],\n",
       " [2786, 3334],\n",
       " [75314, 72708],\n",
       " [15541, 18966],\n",
       " [51233, 76039],\n",
       " [27332, 32780],\n",
       " [40615, 7986],\n",
       " [60181, 75671],\n",
       " [75467, 48250],\n",
       " [63653, 111788],\n",
       " [99431, 96445],\n",
       " [39776, 40810],\n",
       " [10417, 26750],\n",
       " [60646, 61175],\n",
       " [56931, 86275],\n",
       " [95854, 94622],\n",
       " [98827, 96498],\n",
       " [41448, 41449],\n",
       " [78401, 26750],\n",
       " [28896, 54773],\n",
       " [16688, 16404],\n",
       " [105862, 109937],\n",
       " [54568, 111788],\n",
       " [19139, 63332],\n",
       " [4854, 44889],\n",
       " [22713, 22709],\n",
       " [16068, 9966],\n",
       " [28063, 67451],\n",
       " [9493, 17494],\n",
       " [11597, 13787],\n",
       " [78898, 78891],\n",
       " [29698, 59490],\n",
       " [9217, 9253],\n",
       " [9899, 10736],\n",
       " [1636, 1590],\n",
       " [50866, 17949],\n",
       " [40898, 40894],\n",
       " [89572, 89496],\n",
       " [41800, 63331],\n",
       " [46990, 68747],\n",
       " [23347, 19323],\n",
       " [62362, 62518],\n",
       " [11983, 11981],\n",
       " [18345, 6925],\n",
       " [10036, 10085],\n",
       " [19138, 7913],\n",
       " [59251, 60182],\n",
       " [54854, 23861],\n",
       " [105862, 113630],\n",
       " [57725, 56071],\n",
       " [97232, 97231],\n",
       " [47232, 48661],\n",
       " [54273, 77431],\n",
       " [62394, 61150],\n",
       " [5376, 7176],\n",
       " [54632, 60702],\n",
       " [35534, 33959],\n",
       " [5906, 67174],\n",
       " [57593, 58205],\n",
       " [37026, 37805],\n",
       " [51035, 68845],\n",
       " [90499, 20342],\n",
       " [46178, 47326],\n",
       " [13787, 63332],\n",
       " [55499, 91260],\n",
       " [35214, 67451],\n",
       " [34374, 38447],\n",
       " [60196, 61018],\n",
       " [67539, 74681],\n",
       " [50743, 19139],\n",
       " [67048, 67218],\n",
       " [5376, 7281],\n",
       " [39262, 21009],\n",
       " [24215, 19915],\n",
       " [76487, 74732],\n",
       " [78401, 3142],\n",
       " [89572, 90085],\n",
       " [51688, 51239],\n",
       " [29698, 72600],\n",
       " [32905, 58138],\n",
       " [60182, 75671],\n",
       " [18886, 32905],\n",
       " [98827, 95571],\n",
       " [71356, 13787],\n",
       " [71356, 83320],\n",
       " [102304, 99215],\n",
       " [15216, 15215],\n",
       " [90925, 89487],\n",
       " [43360, 44595],\n",
       " [14040, 39003],\n",
       " [63332, 26750],\n",
       " [62691, 61175],\n",
       " [8320, 8289],\n",
       " [35592, 25173],\n",
       " [38304, 25446],\n",
       " [26502, 70590],\n",
       " [24215, 39003],\n",
       " [78545, 75102],\n",
       " [44597, 18654],\n",
       " [19401, 77431],\n",
       " [34178, 61471],\n",
       " [102370, 102984],\n",
       " [21443, 44479],\n",
       " [2726, 4380],\n",
       " [24969, 56458],\n",
       " [102984, 102195],\n",
       " [79678, 79940],\n",
       " [70110, 69983],\n",
       " [10720, 10721],\n",
       " [5068, 33787],\n",
       " [32905, 40286],\n",
       " [54867, 57973],\n",
       " [51285, 51286],\n",
       " [59725, 72600],\n",
       " [60586, 90231],\n",
       " [41579, 50093],\n",
       " [19323, 20733],\n",
       " [26502, 25224],\n",
       " [89572, 73946],\n",
       " [41800, 70756],\n",
       " [60196, 61177],\n",
       " [75852, 75444],\n",
       " [102370, 102768],\n",
       " [6108, 6381],\n",
       " [78401, 49488],\n",
       " [23861, 40286],\n",
       " [44456, 44454],\n",
       " [99640, 99902],\n",
       " [41579, 36379],\n",
       " [24268, 20533],\n",
       " [47849, 55465],\n",
       " [56817, 54612],\n",
       " [50093, 21693],\n",
       " [22240, 8086],\n",
       " [25818, 33787],\n",
       " [5171, 9447],\n",
       " [41579, 21693],\n",
       " [63653, 88488],\n",
       " [26531, 17494],\n",
       " [102768, 102419],\n",
       " [25309, 34374],\n",
       " [62792, 86924],\n",
       " [13421, 16798],\n",
       " [19139, 83320],\n",
       " [119648, 119730],\n",
       " [5074, 5405],\n",
       " [6342, 32905],\n",
       " [63848, 78902],\n",
       " [24421, 23774],\n",
       " [28063, 44597],\n",
       " [105862, 105337],\n",
       " [19138, 63332],\n",
       " [56458, 71870],\n",
       " [28721, 33787],\n",
       " [38657, 38622],\n",
       " [26825, 80853],\n",
       " [44988, 44313],\n",
       " [50866, 56502],\n",
       " [54811, 54012],\n",
       " [52250, 19139],\n",
       " [38834, 38835],\n",
       " [78401, 54387],\n",
       " [63185, 62451],\n",
       " [60647, 62710],\n",
       " [49488, 19415],\n",
       " [6464, 5171],\n",
       " [19274, 28721],\n",
       " [68629, 70110],\n",
       " [5328, 32282],\n",
       " [48720, 54867],\n",
       " [62659, 88249],\n",
       " [70582, 70623],\n",
       " [8488, 8751],\n",
       " [19989, 19323],\n",
       " [70590, 19415],\n",
       " [25224, 50333],\n",
       " [48682, 50925],\n",
       " [41221, 39886],\n",
       " [19987, 16972],\n",
       " [61175, 61177],\n",
       " [100385, 67912],\n",
       " [10417, 15431],\n",
       " [98577, 104697],\n",
       " [18886, 54855],\n",
       " [3910, 53118],\n",
       " [76110, 75444],\n",
       " [55131, 9899],\n",
       " [19429, 17804],\n",
       " [46476, 46035],\n",
       " [28063, 49488],\n",
       " [41296, 45989],\n",
       " [60647, 59598],\n",
       " [96162, 96445],\n",
       " [18693, 17715],\n",
       " [83897, 83898],\n",
       " [54387, 67451],\n",
       " [102984, 102768],\n",
       " [3547, 14605],\n",
       " [60743, 62078],\n",
       " [72600, 71870],\n",
       " [12038, 71356],\n",
       " [61176, 61150],\n",
       " [19274, 17715],\n",
       " [28063, 83320],\n",
       " [21545, 11983],\n",
       " [102304, 100273],\n",
       " [103846, 102195],\n",
       " [108061, 107893],\n",
       " [76555, 76110],\n",
       " [23761, 37498],\n",
       " [37337, 37826],\n",
       " [19429, 14729],\n",
       " [102984, 102419],\n",
       " [50743, 49488],\n",
       " [60196, 60647],\n",
       " [26890, 13492],\n",
       " [54466, 47208],\n",
       " [12682, 12703],\n",
       " [54273, 19401],\n",
       " [22723, 19431],\n",
       " [20448, 24299],\n",
       " [102506, 102582],\n",
       " [10417, 54387],\n",
       " [44597, 70590],\n",
       " [90086, 89305],\n",
       " [82605, 84826],\n",
       " [13154, 76255],\n",
       " [9217, 6952],\n",
       " [3558, 4380],\n",
       " [44889, 62078],\n",
       " [22723, 15541],\n",
       " [19139, 18654],\n",
       " [83980, 53533],\n",
       " [44597, 15418],\n",
       " [42883, 49485],\n",
       " [53901, 101871],\n",
       " [64377, 66533],\n",
       " [24299, 77431],\n",
       " [54387, 26750],\n",
       " [95571, 94622],\n",
       " [54632, 23864],\n",
       " [22109, 24346],\n",
       " [111193, 111692],\n",
       " [54811, 53733],\n",
       " [70754, 89572],\n",
       " [28721, 17715],\n",
       " [15418, 15431],\n",
       " [122048, 122047],\n",
       " [3910, 54387],\n",
       " [103256, 104669],\n",
       " [4678, 7176],\n",
       " [45374, 41319],\n",
       " [50333, 15431],\n",
       " [50093, 11728],\n",
       " [71296, 35340],\n",
       " [112113, 110846],\n",
       " [31456, 11775],\n",
       " [46548, 46876],\n",
       " [83086, 88567],\n",
       " [97026, 92020],\n",
       " [26502, 78401],\n",
       " [32, 54854],\n",
       " [13787, 7913],\n",
       " [50477, 7913],\n",
       " [7172, 2853],\n",
       " [102768, 103643],\n",
       " [102419, 108088],\n",
       " [27360, 68033],\n",
       " [35340, 35214],\n",
       " [96162, 95085],\n",
       " [78401, 83320],\n",
       " [70455, 70110],\n",
       " [5906, 19139],\n",
       " [3447, 2330],\n",
       " [61175, 62394],\n",
       " [26502, 15418],\n",
       " [7014, 10904],\n",
       " [79940, 79446],\n",
       " [10417, 44597],\n",
       " [111761, 111800],\n",
       " [34343, 43025],\n",
       " [40615, 11728],\n",
       " [36176, 39262],\n",
       " [9630, 6775],\n",
       " [62691, 61177],\n",
       " [50899, 51796],\n",
       " [41017, 37498],\n",
       " [71296, 28063],\n",
       " [91432, 91681],\n",
       " [59337, 57535],\n",
       " [29698, 34517],\n",
       " [61018, 62393],\n",
       " [19401, 39003],\n",
       " [60586, 75330],\n",
       " [67912, 99445],\n",
       " [17726, 16303],\n",
       " [71946, 69820],\n",
       " [84, 63],\n",
       " [56670, 32880],\n",
       " [54466, 55465],\n",
       " [41579, 7986],\n",
       " [102484, 108088],\n",
       " [67697, 40286],\n",
       " [92224, 95854],\n",
       " [68747, 71870],\n",
       " [49488, 83320],\n",
       " [38820, 38833],\n",
       " [84904, 83649],\n",
       " [28063, 3910],\n",
       " [28593, 44508],\n",
       " [54855, 18648],\n",
       " [102435, 103256],\n",
       " [26146, 26846],\n",
       " [65860, 48720],\n",
       " [5906, 15431],\n",
       " [69291, 68383],\n",
       " [41319, 47156],\n",
       " [15431, 67174],\n",
       " [102768, 104051],\n",
       " [71296, 32880],\n",
       " [78401, 15431],\n",
       " [19401, 13421],\n",
       " [73960, 70582],\n",
       " [1272, 1994],\n",
       " [36176, 16609],\n",
       " [73736, 73735],\n",
       " [101869, 103665],\n",
       " [80232, 80233],\n",
       " [34517, 71870],\n",
       " [103665, 103643],\n",
       " [104421, 107738],\n",
       " [65057, 63901],\n",
       " [28063, 53118],\n",
       " [44508, 52419],\n",
       " [102762, 102195],\n",
       " [5068, 4686],\n",
       " [75745, 78902],\n",
       " [46990, 34517],\n",
       " [44595, 43038],\n",
       " [47142, 52155],\n",
       " [60743, 58857],\n",
       " [102370, 102762],\n",
       " [45410, 46693],\n",
       " [28063, 3142],\n",
       " [28063, 11597],\n",
       " [38625, 38616],\n",
       " [7754, 9366],\n",
       " [52250, 3142],\n",
       " [12038, 32880],\n",
       " [46944, 50222],\n",
       " [68033, 64500],\n",
       " [68103, 10525],\n",
       " [108547, 104462],\n",
       " [107073, 110594],\n",
       " [82605, 100046],\n",
       " [92817, 93092],\n",
       " [44988, 42812],\n",
       " [62793, 86924],\n",
       " [47847, 48936],\n",
       " [19138, 54387],\n",
       " [35340, 5906],\n",
       " [26890, 34122],\n",
       " [87969, 96647],\n",
       " [5578, 5444],\n",
       " [37026, 36862],\n",
       " [5906, 15418],\n",
       " [24421, 23577],\n",
       " [10720, 10717],\n",
       " [12038, 70590],\n",
       " [64996, 57914],\n",
       " [40013, 38366],\n",
       " [40940, 40055],\n",
       " [58696, 61847],\n",
       " [73749, 73750],\n",
       " [28232, 25444],\n",
       " [50743, 56670],\n",
       " [5906, 18654],\n",
       " [101869, 102582],\n",
       " [74956, 18477],\n",
       " [7913, 26750],\n",
       " [52250, 19138],\n",
       " [110306, 113258],\n",
       " [56670, 54387],\n",
       " [71296, 35214],\n",
       " [13267, 13492],\n",
       " [60139, 61718],\n",
       " [25224, 35214],\n",
       " [16068, 9493],\n",
       " [65860, 57973],\n",
       " [12507, 12703],\n",
       " [42944, 58138],\n",
       " [25224, 63332],\n",
       " [26502, 50743],\n",
       " [9966, 7091],\n",
       " [76039, 59118],\n",
       " [23347, 21688],\n",
       " [78401, 11597],\n",
       " [93035, 98577],\n",
       " [10417, 3910],\n",
       " [19138, 19415],\n",
       " [4756, 23861],\n",
       " [90881, 90086],\n",
       " [5906, 3142],\n",
       " [46990, 71546],\n",
       " [23729, 25685],\n",
       " [28063, 63332],\n",
       " [7091, 9493],\n",
       " [11597, 67174],\n",
       " [22109, 23774],\n",
       " [51699, 51854],\n",
       " [5648, 9332],\n",
       " [16068, 5068],\n",
       " [30626, 31770],\n",
       " [43547, 61847],\n",
       " [46081, 46082],\n",
       " [35214, 10417],\n",
       " [55232, 55322],\n",
       " [54854, 4756],\n",
       " [68560, 59434],\n",
       " [21545, 11981],\n",
       " [16188, 67174],\n",
       " [71296, 13787],\n",
       " [67483, 75102],\n",
       " [63653, 54568],\n",
       " [50333, 19415],\n",
       " [67697, 23861],\n",
       " [88488, 59483],\n",
       " [8309, 8310],\n",
       " [15418, 70590],\n",
       " [54568, 59483],\n",
       " [86476, 39092],\n",
       " [90085, 90086],\n",
       " [1601, 60702],\n",
       " [59598, 61177],\n",
       " [74242, 65462],\n",
       " [28063, 50743],\n",
       " [103846, 103643],\n",
       " [19138, 32880],\n",
       " [10417, 32880],\n",
       " [37026, 37663],\n",
       " [54568, 51118],\n",
       " [48917, 50925],\n",
       " [54632, 61339],\n",
       " [35340, 49488],\n",
       " [38833, 38834],\n",
       " [119648, 119756],\n",
       " [6108, 6103],\n",
       " [41319, 45105],\n",
       " [79088, 72321],\n",
       " [16068, 18693],\n",
       " [14729, 19642],\n",
       " [54568, 79244],\n",
       " [16188, 15431],\n",
       " [32905, 54854],\n",
       " [8320, 7580],\n",
       " [19712, 17804],\n",
       " [93035, 98764],\n",
       " [62451, 52155],\n",
       " [50333, 70590],\n",
       " [37521, 36862],\n",
       " [19274, 7091],\n",
       " [16188, 83320],\n",
       " [26531, 28721],\n",
       " [55131, 16798],\n",
       " [25224, 53118],\n",
       " [51796, 50903],\n",
       " [9331, 23895],\n",
       " [58312, 52823],\n",
       " [75852, 76157],\n",
       " [63603, 78902],\n",
       " [61018, 61177],\n",
       " [65249, 65340],\n",
       " [1601, 61339],\n",
       " [71296, 19138],\n",
       " [47687, 84826],\n",
       " [25224, 67451],\n",
       " [75872, 75741],\n",
       " [42883, 46846],\n",
       " [3910, 49488],\n",
       " [75872, 75852],\n",
       " [93035, 104697],\n",
       " [60787, 88249],\n",
       " [112113, 112375],\n",
       " [35436, 33029],\n",
       " [76487, 76110],\n",
       " [54632, 66910],\n",
       " [108961, 113258],\n",
       " [95975, 98764],\n",
       " [70170, 59651],\n",
       " [4854, 59359],\n",
       " [13787, 54387],\n",
       " [20483, 20302],\n",
       " [60646, 61177],\n",
       " [26091, 23577],\n",
       " [54133, 54471],\n",
       " [7539, 10358],\n",
       " [103654, 100472],\n",
       " [75745, 75757],\n",
       " [56707, 42883],\n",
       " [75872, 75444],\n",
       " [5906, 19138],\n",
       " [9253, 6952],\n",
       " [112083, 112579],\n",
       " [24840, 40055],\n",
       " [13492, 16410],\n",
       " [102762, 101869],\n",
       " [40809, 39774],\n",
       " [70165, 68949],\n",
       " [60646, 61176],\n",
       " [9366, 5944],\n",
       " [9136, 10904],\n",
       " [41546, 54811],\n",
       " [98592, 84826],\n",
       " [3003, 3039],\n",
       " [2446, 88190],\n",
       " [95085, 96724],\n",
       " [56817, 55463],\n",
       " [38616, 38652],\n",
       " [62531, 60182],\n",
       " [101869, 102484],\n",
       " [21366, 21367],\n",
       " [44597, 83320],\n",
       " [41733, 43038],\n",
       " [110594, 111800],\n",
       " [104051, 102419],\n",
       " [53396, 83469],\n",
       " [15418, 7913],\n",
       " [12587, 90499],\n",
       " [27599, 21693],\n",
       " [59725, 34517],\n",
       " [68103, 39092],\n",
       " [77793, 77962],\n",
       " [50093, 36379],\n",
       " [16188, 53118],\n",
       " [61176, 62393],\n",
       " [109464, 36425],\n",
       " [100385, 79723],\n",
       " [75757, 72019],\n",
       " [16188, 19138],\n",
       " [47208, 63185],\n",
       " [37706, 37805],\n",
       " [34343, 38833],\n",
       " [49488, 32880],\n",
       " [2726, 7281],\n",
       " [92645, 101863],\n",
       " [57048, 57219],\n",
       " [28593, 52419],\n",
       " [20040, 21024],\n",
       " [35340, 70590],\n",
       " [56972, 58205],\n",
       " [66072, 70170],\n",
       " [110120, 105337],\n",
       " [96162, 99431],\n",
       " [4678, 2726],\n",
       " [82020, 75956],\n",
       " [8289, 7580],\n",
       " [8450, 2883],\n",
       " [36425, 28670],\n",
       " [49722, 49723],\n",
       " [105337, 109630],\n",
       " [11728, 54483],\n",
       " [35056, 33782],\n",
       " [12038, 19138],\n",
       " [71356, 3142],\n",
       " [114339, 113579],\n",
       " [78256, 65057],\n",
       " [34122, 13492],\n",
       " [53557, 53535],\n",
       " [76487, 75444],\n",
       " [95854, 96184],\n",
       " [61749, 61750],\n",
       " [10417, 67451],\n",
       " [62793, 60787],\n",
       " [52956, 53813],\n",
       " [71296, 7913],\n",
       " [51233, 60181],\n",
       " [38355, 39095],\n",
       " [65860, 30132],\n",
       " [35214, 83320],\n",
       " [20448, 39003],\n",
       " [40615, 21693],\n",
       " [38355, 46846],\n",
       " [15334, 60743],\n",
       " [94451, 93092],\n",
       " [81248, 110101],\n",
       " [57760, 26825],\n",
       " [9366, 6873],\n",
       " [92224, 96142],\n",
       " [115335, 98764],\n",
       " [106589, 109630],\n",
       " [58921, 74588],\n",
       " [30288, 4967],\n",
       " [52558, 50866],\n",
       " [25224, 13787],\n",
       " [75218, 60702],\n",
       " [51492, 51550],\n",
       " [4678, 7281],\n",
       " [11395, 54344],\n",
       " [64603, 62606],\n",
       " [3706, 2943],\n",
       " [102506, 102768],\n",
       " [38624, 38652],\n",
       " [94545, 96184],\n",
       " [54616, 55636],\n",
       " [48720, 23864],\n",
       " [10736, 77431],\n",
       " [16188, 26750],\n",
       " [22700, 27885],\n",
       " [77794, 66910],\n",
       " [13267, 16410],\n",
       " [120037, 120360],\n",
       " [99201, 99456],\n",
       " [73074, 63750],\n",
       " [10417, 13787],\n",
       " [75467, 24299],\n",
       " [24780, 54867],\n",
       " [7281, 2330],\n",
       " [47687, 109040],\n",
       " [18693, 19274],\n",
       " [71356, 7913],\n",
       " [16188, 71356],\n",
       " [100840, 101930],\n",
       " [50333, 28063],\n",
       " [70809, 74207],\n",
       " [83608, 90499],\n",
       " [29698, 59725],\n",
       " [5906, 70590],\n",
       " [3910, 26750],\n",
       " [7681, 4828],\n",
       " [16609, 39262],\n",
       " [69921, 96937],\n",
       " [88488, 111788],\n",
       " [46990, 84819],\n",
       " [7929, 9366],\n",
       " [27332, 37094],\n",
       " [3207, 54854],\n",
       " [109937, 109630],\n",
       " [44597, 19138],\n",
       " [54273, 13421],\n",
       " [28063, 19415],\n",
       " [7281, 4380],\n",
       " [92224, 94545],\n",
       " [12038, 35214],\n",
       " [59280, 59345],\n",
       " [82848, 53498],\n",
       " [12038, 26750],\n",
       " [102768, 102484],\n",
       " [75420, 71300],\n",
       " [24215, 16798],\n",
       " [76323, 77850],\n",
       " [110097, 110709],\n",
       " [41457, 61339],\n",
       " [58394, 57403],\n",
       " [60743, 4854],\n",
       " [31456, 55131],\n",
       " [34122, 13267],\n",
       " [53118, 26750],\n",
       " [62531, 59118],\n",
       " [90085, 89305],\n",
       " [26890, 22287],\n",
       " [20448, 77431],\n",
       " [107073, 111800],\n",
       " [80939, 79940],\n",
       " [70756, 51212],\n",
       " [44595, 47156],\n",
       " [47436, 46975],\n",
       " [80417, 74203],\n",
       " [71296, 18654],\n",
       " [62078, 59359],\n",
       " [59745, 72872],\n",
       " [3142, 83320],\n",
       " [9899, 16798],\n",
       " [4686, 33787],\n",
       " [34589, 33791],\n",
       " [89114, 88799],\n",
       " [45320, 43881],\n",
       " [10576, 9549],\n",
       " [36425, 93147],\n",
       " [14729, 15541],\n",
       " [50093, 20533],\n",
       " [100273, 99215],\n",
       " [67539, 69819],\n",
       " [35720, 35724],\n",
       " [26091, 19323],\n",
       " [7176, 14182],\n",
       " [102582, 102195],\n",
       " [30132, 60702],\n",
       " [58758, 59353],\n",
       " [1528, 1507],\n",
       " [102762, 102506],\n",
       " [42883, 18392],\n",
       " [41157, 37623],\n",
       " [46057, 32342],\n",
       " [10417, 11597],\n",
       " [100046, 110101],\n",
       " [3849, 4010],\n",
       " [84819, 34517],\n",
       " [35192, 35518],\n",
       " [35214, 53118],\n",
       " [112850, 113579],\n",
       " [56458, 72600],\n",
       " [1298, 1807],\n",
       " [13421, 11775],\n",
       " [88897, 94724],\n",
       " [35214, 15431],\n",
       " [38624, 38625],\n",
       " [35340, 50743],\n",
       " [36176, 21009],\n",
       " [54273, 16798],\n",
       " [74956, 62078],\n",
       " [103665, 102419],\n",
       " [38657, 38534],\n",
       " [103256, 100472],\n",
       " [3142, 56670],\n",
       " [44597, 7913],\n",
       " [19122, 9357],\n",
       " [76469, 29790],\n",
       " [20533, 21693],\n",
       " [44889, 59359],\n",
       " [86477, 66142],\n",
       " [103665, 102582],\n",
       " [24969, 34517],\n",
       " [94545, 95571],\n",
       " [58696, 43547],\n",
       " [7768, 9393],\n",
       " [101012, 102879],\n",
       " [37411, 36862],\n",
       " [60196, 62393],\n",
       " [3447, 4380],\n",
       " [103665, 104051],\n",
       " [21924, 44033],\n",
       " [77794, 61339],\n",
       " [98577, 98546],\n",
       " [37411, 37805],\n",
       " [89496, 89305],\n",
       " [97568, 97559],\n",
       " [57973, 61339],\n",
       " [87732, 88357],\n",
       " [76487, 75852],\n",
       " [5376, 2726],\n",
       " [103400, 102603],\n",
       " [52764, 52789],\n",
       " [66072, 59651],\n",
       " [79446, 78358],\n",
       " [103846, 102484],\n",
       " [82848, 54857],\n",
       " [61018, 62710],\n",
       " [75343, 76528],\n",
       " [70590, 19138],\n",
       " [56707, 46846],\n",
       " [50477, 11597],\n",
       " [102435, 103654],\n",
       " [87860, 77901],\n",
       " [48682, 48918],\n",
       " [47208, 47849],\n",
       " [7176, 7281],\n",
       " [32921, 36630],\n",
       " [28063, 16188],\n",
       " [78401, 56670],\n",
       " [94172, 94519],\n",
       " [68747, 72600],\n",
       " [70590, 78401],\n",
       " [15418, 63332],\n",
       " [13787, 83320],\n",
       " [41546, 54012],\n",
       " [56458, 74588],\n",
       " [102768, 102195],\n",
       " [97313, 100046],\n",
       " [61339, 66910],\n",
       " [9253, 4394],\n",
       " [728, 63192],\n",
       " [65927, 55794],\n",
       " [35340, 71356],\n",
       " [76380, 76255],\n",
       " [15418, 19415],\n",
       " [10736, 11775],\n",
       " [15431, 32880],\n",
       " [15418, 18654],\n",
       " [12038, 11597],\n",
       " [52584, 54771],\n",
       " [34343, 38835],\n",
       " [75218, 61339],\n",
       " [77127, 28023],\n",
       " [43557, 43038],\n",
       " [94947, 94935],\n",
       " [78401, 67451],\n",
       " [78281, 68747],\n",
       " [47650, 47868],\n",
       " [20448, 10736],\n",
       " [36440, 36630],\n",
       " [49488, 53118],\n",
       " [80016, 80335],\n",
       " [56817, 54358],\n",
       " [49225, 28593],\n",
       " [41457, 23864],\n",
       " [24268, 7986],\n",
       " [18648, 40286],\n",
       " [26502, 35340],\n",
       " [77664, 79308],\n",
       " [71296, 52250],\n",
       " [50743, 63332],\n",
       " [60743, 74956],\n",
       " [73803, 79653],\n",
       " [74021, 73974],\n",
       " [67174, 7913],\n",
       " [50093, 7986],\n",
       " [36440, 35363],\n",
       " [7539, 12212],\n",
       " [71356, 49488],\n",
       " [89444, 88799],\n",
       " [82642, 60702],\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/openoffice/bert/exported_rank_baseline_100.txt\n"
     ]
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "print(EXPORT_RANK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_100_feature_100epochs_64batch(openoffice).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_3')\n",
    "output = model.output\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=triplet_loss_adapted_from_tf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'baseline_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.18, Loss_test: 0.79\n",
      "Epoch: 102 Loss: 0.68, Loss_test: 0.72\n",
      "Epoch: 103 Loss: 0.38, Loss_test: 0.73\n",
      "Epoch: 104 Loss: 0.53, Loss_test: 0.80\n",
      "Epoch: 105 Loss: 0.53, Loss_test: 0.75\n",
      "Epoch: 106 Loss: 0.38, Loss_test: 0.70\n",
      "Epoch: 107 Loss: 0.26, Loss_test: 0.75\n",
      "Epoch: 108 Loss: 0.36, Loss_test: 0.78\n",
      "Epoch: 109 Loss: 0.39, Loss_test: 0.78\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.48, Loss_test: 0.74\n",
      "Epoch: 111 Loss: 0.29, Loss_test: 0.71\n",
      "Epoch: 112 Loss: 0.40, Loss_test: 0.70\n",
      "Epoch: 113 Loss: 0.39, Loss_test: 0.71\n",
      "Epoch: 114 Loss: 0.42, Loss_test: 0.71\n",
      "Epoch: 115 Loss: 0.33, Loss_test: 0.71\n",
      "Epoch: 116 Loss: 0.57, Loss_test: 0.72\n",
      "Epoch: 117 Loss: 0.30, Loss_test: 0.72\n",
      "Epoch: 118 Loss: 0.24, Loss_test: 0.75\n",
      "Epoch: 119 Loss: 0.32, Loss_test: 0.77\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.33, Loss_test: 0.78\n",
      "Epoch: 121 Loss: 0.41, Loss_test: 0.78\n",
      "Epoch: 122 Loss: 0.34, Loss_test: 0.76\n",
      "Epoch: 123 Loss: 0.26, Loss_test: 0.73\n",
      "Epoch: 124 Loss: 0.34, Loss_test: 0.72\n",
      "Epoch: 125 Loss: 0.24, Loss_test: 0.71\n",
      "Epoch: 126 Loss: 0.35, Loss_test: 0.71\n",
      "Epoch: 127 Loss: 0.33, Loss_test: 0.71\n",
      "Epoch: 128 Loss: 0.33, Loss_test: 0.71\n",
      "Epoch: 129 Loss: 0.31, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.27, Loss_test: 0.73\n",
      "Epoch: 131 Loss: 0.22, Loss_test: 0.75\n",
      "Epoch: 132 Loss: 0.35, Loss_test: 0.76\n",
      "Epoch: 133 Loss: 0.35, Loss_test: 0.75\n",
      "Epoch: 134 Loss: 0.40, Loss_test: 0.73\n",
      "Epoch: 135 Loss: 0.28, Loss_test: 0.72\n",
      "Epoch: 136 Loss: 0.35, Loss_test: 0.73\n",
      "Epoch: 137 Loss: 0.19, Loss_test: 0.72\n",
      "Epoch: 138 Loss: 0.29, Loss_test: 0.73\n",
      "Epoch: 139 Loss: 0.26, Loss_test: 0.73\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.43, Loss_test: 0.73\n",
      "Epoch: 141 Loss: 0.29, Loss_test: 0.73\n",
      "Epoch: 142 Loss: 0.26, Loss_test: 0.72\n",
      "Epoch: 143 Loss: 0.14, Loss_test: 0.72\n",
      "Epoch: 144 Loss: 0.36, Loss_test: 0.71\n",
      "Epoch: 145 Loss: 0.34, Loss_test: 0.72\n",
      "Epoch: 146 Loss: 0.37, Loss_test: 0.72\n",
      "Epoch: 147 Loss: 0.16, Loss_test: 0.72\n",
      "Epoch: 148 Loss: 0.32, Loss_test: 0.72\n",
      "Epoch: 149 Loss: 0.29, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.29, Loss_test: 0.72\n",
      "Epoch: 151 Loss: 0.30, Loss_test: 0.73\n",
      "Epoch: 152 Loss: 0.19, Loss_test: 0.72\n",
      "Epoch: 153 Loss: 0.45, Loss_test: 0.72\n",
      "Epoch: 154 Loss: 0.19, Loss_test: 0.72\n",
      "Epoch: 155 Loss: 0.24, Loss_test: 0.72\n",
      "Epoch: 156 Loss: 0.22, Loss_test: 0.71\n",
      "Epoch: 157 Loss: 0.30, Loss_test: 0.71\n",
      "Epoch: 158 Loss: 0.35, Loss_test: 0.71\n",
      "Epoch: 159 Loss: 0.34, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.39, Loss_test: 0.71\n",
      "Epoch: 161 Loss: 0.21, Loss_test: 0.71\n",
      "Epoch: 162 Loss: 0.33, Loss_test: 0.70\n",
      "Epoch: 163 Loss: 0.19, Loss_test: 0.70\n",
      "Epoch: 164 Loss: 0.19, Loss_test: 0.71\n",
      "Epoch: 165 Loss: 0.40, Loss_test: 0.71\n",
      "Epoch: 166 Loss: 0.38, Loss_test: 0.71\n",
      "Epoch: 167 Loss: 0.21, Loss_test: 0.70\n",
      "Epoch: 168 Loss: 0.20, Loss_test: 0.70\n",
      "Epoch: 169 Loss: 0.22, Loss_test: 0.70\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.28, Loss_test: 0.70\n",
      "Epoch: 171 Loss: 0.32, Loss_test: 0.70\n",
      "Epoch: 172 Loss: 0.52, Loss_test: 0.71\n",
      "Epoch: 173 Loss: 0.22, Loss_test: 0.70\n",
      "Epoch: 174 Loss: 0.24, Loss_test: 0.70\n",
      "Epoch: 175 Loss: 0.30, Loss_test: 0.71\n",
      "Epoch: 176 Loss: 0.30, Loss_test: 0.71\n",
      "Epoch: 177 Loss: 0.28, Loss_test: 0.71\n",
      "Epoch: 178 Loss: 0.27, Loss_test: 0.71\n",
      "Epoch: 179 Loss: 0.24, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.27, Loss_test: 0.72\n",
      "Epoch: 181 Loss: 0.24, Loss_test: 0.71\n",
      "Epoch: 182 Loss: 0.27, Loss_test: 0.71\n",
      "Epoch: 183 Loss: 0.36, Loss_test: 0.71\n",
      "Epoch: 184 Loss: 0.29, Loss_test: 0.72\n",
      "Epoch: 185 Loss: 0.35, Loss_test: 0.72\n",
      "Epoch: 186 Loss: 0.15, Loss_test: 0.71\n",
      "Epoch: 187 Loss: 0.39, Loss_test: 0.72\n",
      "Epoch: 188 Loss: 0.33, Loss_test: 0.73\n",
      "Epoch: 189 Loss: 0.24, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.15, Loss_test: 0.72\n",
      "Epoch: 191 Loss: 0.40, Loss_test: 0.73\n",
      "Epoch: 192 Loss: 0.21, Loss_test: 0.72\n",
      "Epoch: 193 Loss: 0.23, Loss_test: 0.72\n",
      "Epoch: 194 Loss: 0.16, Loss_test: 0.73\n",
      "Epoch: 195 Loss: 0.20, Loss_test: 0.72\n",
      "Epoch: 196 Loss: 0.28, Loss_test: 0.72\n",
      "Epoch: 197 Loss: 0.29, Loss_test: 0.72\n",
      "Epoch: 198 Loss: 0.16, Loss_test: 0.72\n",
      "Epoch: 199 Loss: 0.15, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.49, Loss_test: 0.72\n",
      "Epoch: 201 Loss: 0.24, Loss_test: 0.72\n",
      "Epoch: 202 Loss: 0.34, Loss_test: 0.72\n",
      "Epoch: 203 Loss: 0.23, Loss_test: 0.72\n",
      "Epoch: 204 Loss: 0.17, Loss_test: 0.73\n",
      "Epoch: 205 Loss: 0.24, Loss_test: 0.73\n",
      "Epoch: 206 Loss: 0.30, Loss_test: 0.73\n",
      "Epoch: 207 Loss: 0.33, Loss_test: 0.72\n",
      "Epoch: 208 Loss: 0.24, Loss_test: 0.72\n",
      "Epoch: 209 Loss: 0.24, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.29, Loss_test: 0.71\n",
      "Epoch: 211 Loss: 0.18, Loss_test: 0.72\n",
      "Epoch: 212 Loss: 0.27, Loss_test: 0.71\n",
      "Epoch: 213 Loss: 0.22, Loss_test: 0.71\n",
      "Epoch: 214 Loss: 0.26, Loss_test: 0.70\n",
      "Epoch: 215 Loss: 0.17, Loss_test: 0.69\n",
      "Epoch: 216 Loss: 0.14, Loss_test: 0.70\n",
      "Epoch: 217 Loss: 0.30, Loss_test: 0.69\n",
      "Epoch: 218 Loss: 0.34, Loss_test: 0.70\n",
      "Epoch: 219 Loss: 0.19, Loss_test: 0.70\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.18, Loss_test: 0.70\n",
      "Epoch: 221 Loss: 0.15, Loss_test: 0.71\n",
      "Epoch: 222 Loss: 0.20, Loss_test: 0.70\n",
      "Epoch: 223 Loss: 0.37, Loss_test: 0.70\n",
      "Epoch: 224 Loss: 0.15, Loss_test: 0.71\n",
      "Epoch: 225 Loss: 0.27, Loss_test: 0.72\n",
      "Epoch: 226 Loss: 0.17, Loss_test: 0.71\n",
      "Epoch: 227 Loss: 0.26, Loss_test: 0.70\n",
      "Epoch: 228 Loss: 0.30, Loss_test: 0.69\n",
      "Epoch: 229 Loss: 0.24, Loss_test: 0.69\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.35, Loss_test: 0.67\n",
      "Epoch: 231 Loss: 0.27, Loss_test: 0.69\n",
      "Epoch: 232 Loss: 0.26, Loss_test: 0.69\n",
      "Epoch: 233 Loss: 0.23, Loss_test: 0.69\n",
      "Epoch: 234 Loss: 0.25, Loss_test: 0.69\n",
      "Epoch: 235 Loss: 0.28, Loss_test: 0.69\n",
      "Epoch: 236 Loss: 0.30, Loss_test: 0.69\n",
      "Epoch: 237 Loss: 0.24, Loss_test: 0.68\n",
      "Epoch: 238 Loss: 0.29, Loss_test: 0.68\n",
      "Epoch: 239 Loss: 0.27, Loss_test: 0.68\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.17, Loss_test: 0.68\n",
      "Epoch: 241 Loss: 0.22, Loss_test: 0.67\n",
      "Epoch: 242 Loss: 0.17, Loss_test: 0.68\n",
      "Epoch: 243 Loss: 0.14, Loss_test: 0.68\n",
      "Epoch: 244 Loss: 0.22, Loss_test: 0.68\n",
      "Epoch: 245 Loss: 0.14, Loss_test: 0.68\n",
      "Epoch: 246 Loss: 0.12, Loss_test: 0.68\n",
      "Epoch: 247 Loss: 0.22, Loss_test: 0.68\n",
      "Epoch: 248 Loss: 0.34, Loss_test: 0.68\n",
      "Epoch: 249 Loss: 0.22, Loss_test: 0.66\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.21, Loss_test: 0.67\n",
      "Epoch: 251 Loss: 0.17, Loss_test: 0.69\n",
      "Epoch: 252 Loss: 0.14, Loss_test: 0.69\n",
      "Epoch: 253 Loss: 0.18, Loss_test: 0.69\n",
      "Epoch: 254 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 255 Loss: 0.13, Loss_test: 0.69\n",
      "Epoch: 256 Loss: 0.20, Loss_test: 0.68\n",
      "Epoch: 257 Loss: 0.19, Loss_test: 0.69\n",
      "Epoch: 258 Loss: 0.29, Loss_test: 0.69\n",
      "Epoch: 259 Loss: 0.29, Loss_test: 0.69\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.24, Loss_test: 0.70\n",
      "Epoch: 261 Loss: 0.11, Loss_test: 0.69\n",
      "Epoch: 262 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 263 Loss: 0.23, Loss_test: 0.69\n",
      "Epoch: 264 Loss: 0.36, Loss_test: 0.69\n",
      "Epoch: 265 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 266 Loss: 0.17, Loss_test: 0.70\n",
      "Epoch: 267 Loss: 0.16, Loss_test: 0.69\n",
      "Epoch: 268 Loss: 0.21, Loss_test: 0.69\n",
      "Epoch: 269 Loss: 0.15, Loss_test: 0.68\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.22, Loss_test: 0.67\n",
      "Epoch: 271 Loss: 0.20, Loss_test: 0.67\n",
      "Epoch: 272 Loss: 0.28, Loss_test: 0.68\n",
      "Epoch: 273 Loss: 0.16, Loss_test: 0.68\n",
      "Epoch: 274 Loss: 0.19, Loss_test: 0.67\n",
      "Epoch: 275 Loss: 0.21, Loss_test: 0.68\n",
      "Epoch: 276 Loss: 0.16, Loss_test: 0.68\n",
      "Epoch: 277 Loss: 0.20, Loss_test: 0.67\n",
      "Epoch: 278 Loss: 0.21, Loss_test: 0.67\n",
      "Epoch: 279 Loss: 0.10, Loss_test: 0.67\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.18, Loss_test: 0.66\n",
      "Epoch: 281 Loss: 0.21, Loss_test: 0.67\n",
      "Epoch: 282 Loss: 0.28, Loss_test: 0.68\n",
      "Epoch: 283 Loss: 0.15, Loss_test: 0.69\n",
      "Epoch: 284 Loss: 0.23, Loss_test: 0.69\n",
      "Epoch: 285 Loss: 0.12, Loss_test: 0.69\n",
      "Epoch: 286 Loss: 0.23, Loss_test: 0.68\n",
      "Epoch: 287 Loss: 0.27, Loss_test: 0.69\n",
      "Epoch: 288 Loss: 0.13, Loss_test: 0.68\n",
      "Epoch: 289 Loss: 0.17, Loss_test: 0.68\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.22, Loss_test: 0.68\n",
      "Epoch: 291 Loss: 0.25, Loss_test: 0.68\n",
      "Epoch: 292 Loss: 0.13, Loss_test: 0.68\n",
      "Epoch: 293 Loss: 0.19, Loss_test: 0.68\n",
      "Epoch: 294 Loss: 0.31, Loss_test: 0.68\n",
      "Epoch: 295 Loss: 0.22, Loss_test: 0.68\n",
      "Epoch: 296 Loss: 0.13, Loss_test: 0.68\n",
      "Epoch: 297 Loss: 0.15, Loss_test: 0.68\n",
      "Epoch: 298 Loss: 0.15, Loss_test: 0.68\n",
      "Epoch: 299 Loss: 0.20, Loss_test: 0.67\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.17, Loss_test: 0.68\n",
      "Epoch: 301 Loss: 0.19, Loss_test: 0.68\n",
      "Epoch: 302 Loss: 0.17, Loss_test: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 Loss: 0.11, Loss_test: 0.69\n",
      "Epoch: 304 Loss: 0.20, Loss_test: 0.68\n",
      "Epoch: 305 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 306 Loss: 0.28, Loss_test: 0.67\n",
      "Epoch: 307 Loss: 0.15, Loss_test: 0.67\n",
      "Epoch: 308 Loss: 0.10, Loss_test: 0.68\n",
      "Epoch: 309 Loss: 0.18, Loss_test: 0.68\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.13, Loss_test: 0.68\n",
      "Epoch: 311 Loss: 0.32, Loss_test: 0.68\n",
      "Epoch: 312 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 313 Loss: 0.08, Loss_test: 0.70\n",
      "Epoch: 314 Loss: 0.23, Loss_test: 0.69\n",
      "Epoch: 315 Loss: 0.41, Loss_test: 0.70\n",
      "Epoch: 316 Loss: 0.26, Loss_test: 0.70\n",
      "Epoch: 317 Loss: 0.17, Loss_test: 0.70\n",
      "Epoch: 318 Loss: 0.15, Loss_test: 0.71\n",
      "Epoch: 319 Loss: 0.18, Loss_test: 0.73\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.23, Loss_test: 0.71\n",
      "Epoch: 321 Loss: 0.22, Loss_test: 0.70\n",
      "Epoch: 322 Loss: 0.16, Loss_test: 0.67\n",
      "Epoch: 323 Loss: 0.17, Loss_test: 0.68\n",
      "Epoch: 324 Loss: 0.26, Loss_test: 0.69\n",
      "Epoch: 325 Loss: 0.13, Loss_test: 0.69\n",
      "Epoch: 326 Loss: 0.24, Loss_test: 0.69\n",
      "Epoch: 327 Loss: 0.22, Loss_test: 0.70\n",
      "Epoch: 328 Loss: 0.17, Loss_test: 0.72\n",
      "Epoch: 329 Loss: 0.16, Loss_test: 0.72\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.21, Loss_test: 0.70\n",
      "Epoch: 331 Loss: 0.21, Loss_test: 0.69\n",
      "Epoch: 332 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 333 Loss: 0.22, Loss_test: 0.70\n",
      "Epoch: 334 Loss: 0.17, Loss_test: 0.70\n",
      "Epoch: 335 Loss: 0.16, Loss_test: 0.72\n",
      "Epoch: 336 Loss: 0.24, Loss_test: 0.71\n",
      "Epoch: 337 Loss: 0.24, Loss_test: 0.70\n",
      "Epoch: 338 Loss: 0.20, Loss_test: 0.69\n",
      "Epoch: 339 Loss: 0.14, Loss_test: 0.69\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.15, Loss_test: 0.69\n",
      "Epoch: 341 Loss: 0.19, Loss_test: 0.68\n",
      "Epoch: 342 Loss: 0.11, Loss_test: 0.68\n",
      "Epoch: 343 Loss: 0.22, Loss_test: 0.70\n",
      "Epoch: 344 Loss: 0.10, Loss_test: 0.69\n",
      "Epoch: 345 Loss: 0.15, Loss_test: 0.68\n",
      "Epoch: 346 Loss: 0.15, Loss_test: 0.68\n",
      "Epoch: 347 Loss: 0.17, Loss_test: 0.67\n",
      "Epoch: 348 Loss: 0.11, Loss_test: 0.65\n",
      "Epoch: 349 Loss: 0.13, Loss_test: 0.66\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.11, Loss_test: 0.65\n",
      "Epoch: 351 Loss: 0.22, Loss_test: 0.67\n",
      "Epoch: 352 Loss: 0.16, Loss_test: 0.69\n",
      "Epoch: 353 Loss: 0.25, Loss_test: 0.69\n",
      "Epoch: 354 Loss: 0.24, Loss_test: 0.69\n",
      "Epoch: 355 Loss: 0.21, Loss_test: 0.69\n",
      "Epoch: 356 Loss: 0.14, Loss_test: 0.69\n",
      "Epoch: 357 Loss: 0.16, Loss_test: 0.68\n",
      "Epoch: 358 Loss: 0.14, Loss_test: 0.66\n",
      "Epoch: 359 Loss: 0.28, Loss_test: 0.66\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.24, Loss_test: 0.68\n",
      "Epoch: 361 Loss: 0.15, Loss_test: 0.68\n",
      "Epoch: 362 Loss: 0.23, Loss_test: 0.65\n",
      "Epoch: 363 Loss: 0.18, Loss_test: 0.65\n",
      "Epoch: 364 Loss: 0.27, Loss_test: 0.66\n",
      "Epoch: 365 Loss: 0.19, Loss_test: 0.65\n",
      "Epoch: 366 Loss: 0.23, Loss_test: 0.65\n",
      "Epoch: 367 Loss: 0.19, Loss_test: 0.67\n",
      "Epoch: 368 Loss: 0.13, Loss_test: 0.69\n",
      "Epoch: 369 Loss: 0.16, Loss_test: 0.69\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.11, Loss_test: 0.67\n",
      "Epoch: 371 Loss: 0.11, Loss_test: 0.65\n",
      "Epoch: 372 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 373 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 374 Loss: 0.16, Loss_test: 0.65\n",
      "Epoch: 375 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 376 Loss: 0.17, Loss_test: 0.65\n",
      "Epoch: 377 Loss: 0.21, Loss_test: 0.65\n",
      "Epoch: 378 Loss: 0.07, Loss_test: 0.67\n",
      "Epoch: 379 Loss: 0.28, Loss_test: 0.66\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.14, Loss_test: 0.66\n",
      "Epoch: 381 Loss: 0.17, Loss_test: 0.67\n",
      "Epoch: 382 Loss: 0.30, Loss_test: 0.65\n",
      "Epoch: 383 Loss: 0.17, Loss_test: 0.65\n",
      "Epoch: 384 Loss: 0.09, Loss_test: 0.66\n",
      "Epoch: 385 Loss: 0.14, Loss_test: 0.65\n",
      "Epoch: 386 Loss: 0.21, Loss_test: 0.64\n",
      "Epoch: 387 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 388 Loss: 0.19, Loss_test: 0.66\n",
      "Epoch: 389 Loss: 0.17, Loss_test: 0.68\n",
      "=> result saved!\n",
      "Epoch: 390 Loss: 0.18, Loss_test: 0.68\n",
      "Epoch: 391 Loss: 0.17, Loss_test: 0.66\n",
      "Epoch: 392 Loss: 0.10, Loss_test: 0.67\n",
      "Epoch: 393 Loss: 0.17, Loss_test: 0.66\n",
      "Epoch: 394 Loss: 0.20, Loss_test: 0.64\n",
      "Epoch: 395 Loss: 0.15, Loss_test: 0.65\n",
      "Epoch: 396 Loss: 0.18, Loss_test: 0.65\n",
      "Epoch: 397 Loss: 0.14, Loss_test: 0.66\n",
      "Epoch: 398 Loss: 0.23, Loss_test: 0.66\n",
      "Epoch: 399 Loss: 0.18, Loss_test: 0.67\n",
      "=> result saved!\n",
      "Epoch: 400 Loss: 0.29, Loss_test: 0.63\n",
      "Epoch: 401 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 402 Loss: 0.14, Loss_test: 0.65\n",
      "Epoch: 403 Loss: 0.24, Loss_test: 0.63\n",
      "Epoch: 404 Loss: 0.27, Loss_test: 0.64\n",
      "Epoch: 405 Loss: 0.13, Loss_test: 0.65\n",
      "Epoch: 406 Loss: 0.16, Loss_test: 0.67\n",
      "Epoch: 407 Loss: 0.13, Loss_test: 0.70\n",
      "Epoch: 408 Loss: 0.23, Loss_test: 0.69\n",
      "Epoch: 409 Loss: 0.20, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 410 Loss: 0.19, Loss_test: 0.64\n",
      "Epoch: 411 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 412 Loss: 0.23, Loss_test: 0.66\n",
      "Epoch: 413 Loss: 0.15, Loss_test: 0.66\n",
      "Epoch: 414 Loss: 0.12, Loss_test: 0.67\n",
      "Epoch: 415 Loss: 0.14, Loss_test: 0.67\n",
      "Epoch: 416 Loss: 0.14, Loss_test: 0.65\n",
      "Epoch: 417 Loss: 0.13, Loss_test: 0.63\n",
      "Epoch: 418 Loss: 0.17, Loss_test: 0.63\n",
      "Epoch: 419 Loss: 0.08, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 420 Loss: 0.17, Loss_test: 0.62\n",
      "Epoch: 421 Loss: 0.13, Loss_test: 0.63\n",
      "Epoch: 422 Loss: 0.14, Loss_test: 0.64\n",
      "Epoch: 423 Loss: 0.23, Loss_test: 0.63\n",
      "Epoch: 424 Loss: 0.12, Loss_test: 0.64\n",
      "Epoch: 425 Loss: 0.18, Loss_test: 0.64\n",
      "Epoch: 426 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 427 Loss: 0.17, Loss_test: 0.63\n",
      "Epoch: 428 Loss: 0.16, Loss_test: 0.63\n",
      "Epoch: 429 Loss: 0.17, Loss_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 430 Loss: 0.22, Loss_test: 0.63\n",
      "Epoch: 431 Loss: 0.17, Loss_test: 0.62\n",
      "Epoch: 432 Loss: 0.15, Loss_test: 0.65\n",
      "Epoch: 433 Loss: 0.13, Loss_test: 0.66\n",
      "Epoch: 434 Loss: 0.10, Loss_test: 0.67\n",
      "Epoch: 435 Loss: 0.16, Loss_test: 0.67\n",
      "Epoch: 436 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 437 Loss: 0.10, Loss_test: 0.63\n",
      "Epoch: 438 Loss: 0.12, Loss_test: 0.64\n",
      "Epoch: 439 Loss: 0.17, Loss_test: 0.65\n",
      "=> result saved!\n",
      "Epoch: 440 Loss: 0.15, Loss_test: 0.65\n",
      "Epoch: 441 Loss: 0.09, Loss_test: 0.64\n",
      "Epoch: 442 Loss: 0.10, Loss_test: 0.64\n",
      "Epoch: 443 Loss: 0.13, Loss_test: 0.65\n",
      "Epoch: 444 Loss: 0.13, Loss_test: 0.67\n",
      "Epoch: 445 Loss: 0.24, Loss_test: 0.64\n",
      "Epoch: 446 Loss: 0.08, Loss_test: 0.64\n",
      "Epoch: 447 Loss: 0.12, Loss_test: 0.65\n",
      "Epoch: 448 Loss: 0.11, Loss_test: 0.64\n",
      "Epoch: 449 Loss: 0.14, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 450 Loss: 0.19, Loss_test: 0.64\n",
      "Epoch: 451 Loss: 0.11, Loss_test: 0.64\n",
      "Epoch: 452 Loss: 0.14, Loss_test: 0.64\n",
      "Epoch: 453 Loss: 0.10, Loss_test: 0.65\n",
      "Epoch: 454 Loss: 0.18, Loss_test: 0.66\n",
      "Epoch: 455 Loss: 0.12, Loss_test: 0.66\n",
      "Epoch: 456 Loss: 0.17, Loss_test: 0.63\n",
      "Epoch: 457 Loss: 0.07, Loss_test: 0.63\n",
      "Epoch: 458 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 459 Loss: 0.14, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 460 Loss: 0.11, Loss_test: 0.64\n",
      "Epoch: 461 Loss: 0.13, Loss_test: 0.65\n",
      "Epoch: 462 Loss: 0.08, Loss_test: 0.65\n",
      "Epoch: 463 Loss: 0.15, Loss_test: 0.65\n",
      "Epoch: 464 Loss: 0.17, Loss_test: 0.64\n",
      "Epoch: 465 Loss: 0.15, Loss_test: 0.64\n",
      "Epoch: 466 Loss: 0.16, Loss_test: 0.64\n",
      "Epoch: 467 Loss: 0.09, Loss_test: 0.65\n",
      "Epoch: 468 Loss: 0.11, Loss_test: 0.64\n",
      "Epoch: 469 Loss: 0.10, Loss_test: 0.65\n",
      "=> result saved!\n",
      "Epoch: 470 Loss: 0.15, Loss_test: 0.64\n",
      "Epoch: 471 Loss: 0.20, Loss_test: 0.64\n",
      "Epoch: 472 Loss: 0.09, Loss_test: 0.65\n",
      "Epoch: 473 Loss: 0.09, Loss_test: 0.64\n",
      "Epoch: 474 Loss: 0.13, Loss_test: 0.65\n",
      "Epoch: 475 Loss: 0.10, Loss_test: 0.66\n",
      "Epoch: 476 Loss: 0.15, Loss_test: 0.66\n",
      "Epoch: 477 Loss: 0.15, Loss_test: 0.66\n",
      "Epoch: 478 Loss: 0.06, Loss_test: 0.64\n",
      "Epoch: 479 Loss: 0.09, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 480 Loss: 0.12, Loss_test: 0.64\n",
      "Epoch: 481 Loss: 0.14, Loss_test: 0.64\n",
      "Epoch: 482 Loss: 0.10, Loss_test: 0.64\n",
      "Epoch: 483 Loss: 0.10, Loss_test: 0.65\n",
      "Epoch: 484 Loss: 0.11, Loss_test: 0.66\n",
      "Epoch: 485 Loss: 0.12, Loss_test: 0.66\n",
      "Epoch: 486 Loss: 0.13, Loss_test: 0.66\n",
      "Epoch: 487 Loss: 0.09, Loss_test: 0.64\n",
      "Epoch: 488 Loss: 0.07, Loss_test: 0.64\n",
      "Epoch: 489 Loss: 0.13, Loss_test: 0.65\n",
      "=> result saved!\n",
      "Epoch: 490 Loss: 0.09, Loss_test: 0.65\n",
      "Epoch: 491 Loss: 0.14, Loss_test: 0.65\n",
      "Epoch: 492 Loss: 0.10, Loss_test: 0.64\n",
      "Epoch: 493 Loss: 0.15, Loss_test: 0.64\n",
      "Epoch: 494 Loss: 0.12, Loss_test: 0.64\n",
      "Epoch: 495 Loss: 0.11, Loss_test: 0.64\n",
      "Epoch: 496 Loss: 0.12, Loss_test: 0.64\n",
      "Epoch: 497 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 498 Loss: 0.07, Loss_test: 0.64\n",
      "Epoch: 499 Loss: 0.08, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 500 Loss: 0.20, Loss_test: 0.63\n",
      "Epoch: 501 Loss: 0.10, Loss_test: 0.63\n",
      "Epoch: 502 Loss: 0.13, Loss_test: 0.63\n",
      "Epoch: 503 Loss: 0.10, Loss_test: 0.63\n",
      "Epoch: 504 Loss: 0.18, Loss_test: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 505 Loss: 0.09, Loss_test: 0.63\n",
      "Epoch: 506 Loss: 0.11, Loss_test: 0.62\n",
      "Epoch: 507 Loss: 0.06, Loss_test: 0.63\n",
      "Epoch: 508 Loss: 0.07, Loss_test: 0.63\n",
      "Epoch: 509 Loss: 0.24, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 510 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 511 Loss: 0.10, Loss_test: 0.62\n",
      "Epoch: 512 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 513 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 514 Loss: 0.17, Loss_test: 0.63\n",
      "Epoch: 515 Loss: 0.12, Loss_test: 0.63\n",
      "Epoch: 516 Loss: 0.11, Loss_test: 0.63\n",
      "Epoch: 517 Loss: 0.11, Loss_test: 0.62\n",
      "Epoch: 518 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 519 Loss: 0.06, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 520 Loss: 0.15, Loss_test: 0.61\n",
      "Epoch: 521 Loss: 0.16, Loss_test: 0.60\n",
      "Epoch: 522 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 523 Loss: 0.15, Loss_test: 0.60\n",
      "Epoch: 524 Loss: 0.09, Loss_test: 0.63\n",
      "Epoch: 525 Loss: 0.15, Loss_test: 0.63\n",
      "Epoch: 526 Loss: 0.26, Loss_test: 0.60\n",
      "Epoch: 527 Loss: 0.09, Loss_test: 0.64\n",
      "Epoch: 528 Loss: 0.16, Loss_test: 0.61\n",
      "Epoch: 529 Loss: 0.11, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 530 Loss: 0.18, Loss_test: 0.62\n",
      "Epoch: 531 Loss: 0.20, Loss_test: 0.60\n",
      "Epoch: 532 Loss: 0.19, Loss_test: 0.63\n",
      "Epoch: 533 Loss: 0.14, Loss_test: 0.63\n",
      "Epoch: 534 Loss: 0.15, Loss_test: 0.61\n",
      "Epoch: 535 Loss: 0.06, Loss_test: 0.62\n",
      "Epoch: 536 Loss: 0.17, Loss_test: 0.61\n",
      "Epoch: 537 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 538 Loss: 0.16, Loss_test: 0.61\n",
      "Epoch: 539 Loss: 0.08, Loss_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 540 Loss: 0.08, Loss_test: 0.64\n",
      "Epoch: 541 Loss: 0.23, Loss_test: 0.60\n",
      "Epoch: 542 Loss: 0.14, Loss_test: 0.64\n",
      "Epoch: 543 Loss: 0.15, Loss_test: 0.65\n",
      "Epoch: 544 Loss: 0.10, Loss_test: 0.62\n",
      "Epoch: 545 Loss: 0.17, Loss_test: 0.61\n",
      "Epoch: 546 Loss: 0.15, Loss_test: 0.67\n",
      "Epoch: 547 Loss: 0.18, Loss_test: 0.60\n",
      "Epoch: 548 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 549 Loss: 0.11, Loss_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 550 Loss: 0.11, Loss_test: 0.65\n",
      "Epoch: 551 Loss: 0.08, Loss_test: 0.64\n",
      "Epoch: 552 Loss: 0.06, Loss_test: 0.62\n",
      "Epoch: 553 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 554 Loss: 0.15, Loss_test: 0.62\n",
      "Epoch: 555 Loss: 0.12, Loss_test: 0.63\n",
      "Epoch: 556 Loss: 0.09, Loss_test: 0.64\n",
      "Epoch: 557 Loss: 0.13, Loss_test: 0.64\n",
      "Epoch: 558 Loss: 0.06, Loss_test: 0.64\n",
      "Epoch: 559 Loss: 0.09, Loss_test: 0.64\n",
      "=> result saved!\n",
      "Epoch: 560 Loss: 0.08, Loss_test: 0.63\n",
      "Epoch: 561 Loss: 0.15, Loss_test: 0.61\n",
      "Epoch: 562 Loss: 0.14, Loss_test: 0.60\n",
      "Epoch: 563 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 564 Loss: 0.14, Loss_test: 0.61\n",
      "Epoch: 565 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 566 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 567 Loss: 0.09, Loss_test: 0.63\n",
      "Epoch: 568 Loss: 0.15, Loss_test: 0.62\n",
      "Epoch: 569 Loss: 0.10, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 570 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 571 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 572 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 573 Loss: 0.12, Loss_test: 0.60\n",
      "Epoch: 574 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 575 Loss: 0.18, Loss_test: 0.63\n",
      "Epoch: 576 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 577 Loss: 0.11, Loss_test: 0.63\n",
      "Epoch: 578 Loss: 0.14, Loss_test: 0.62\n",
      "Epoch: 579 Loss: 0.08, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 580 Loss: 0.16, Loss_test: 0.61\n",
      "Epoch: 581 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 582 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 583 Loss: 0.12, Loss_test: 0.59\n",
      "Epoch: 584 Loss: 0.10, Loss_test: 0.58\n",
      "Epoch: 585 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 586 Loss: 0.11, Loss_test: 0.62\n",
      "Epoch: 587 Loss: 0.13, Loss_test: 0.61\n",
      "Epoch: 588 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 589 Loss: 0.09, Loss_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 590 Loss: 0.16, Loss_test: 0.60\n",
      "Epoch: 591 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 592 Loss: 0.10, Loss_test: 0.59\n",
      "Epoch: 593 Loss: 0.10, Loss_test: 0.59\n",
      "Epoch: 594 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 595 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 596 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 597 Loss: 0.12, Loss_test: 0.60\n",
      "Epoch: 598 Loss: 0.14, Loss_test: 0.61\n",
      "Epoch: 599 Loss: 0.08, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 600 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 601 Loss: 0.04, Loss_test: 0.61\n",
      "Epoch: 602 Loss: 0.15, Loss_test: 0.60\n",
      "Epoch: 603 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 604 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 605 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 606 Loss: 0.12, Loss_test: 0.60\n",
      "Epoch: 607 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 608 Loss: 0.13, Loss_test: 0.59\n",
      "Epoch: 609 Loss: 0.13, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 610 Loss: 0.10, Loss_test: 0.59\n",
      "Epoch: 611 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 612 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 613 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 614 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 615 Loss: 0.12, Loss_test: 0.60\n",
      "Epoch: 616 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 617 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 618 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 619 Loss: 0.07, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 620 Loss: 0.07, Loss_test: 0.63\n",
      "Epoch: 621 Loss: 0.17, Loss_test: 0.60\n",
      "Epoch: 622 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 623 Loss: 0.08, Loss_test: 0.67\n",
      "Epoch: 624 Loss: 0.16, Loss_test: 0.60\n",
      "Epoch: 625 Loss: 0.13, Loss_test: 0.59\n",
      "Epoch: 626 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 627 Loss: 0.06, Loss_test: 0.62\n",
      "Epoch: 628 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 629 Loss: 0.14, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 630 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 631 Loss: 0.15, Loss_test: 0.62\n",
      "Epoch: 632 Loss: 0.06, Loss_test: 0.64\n",
      "Epoch: 633 Loss: 0.07, Loss_test: 0.63\n",
      "Epoch: 634 Loss: 0.11, Loss_test: 0.63\n",
      "Epoch: 635 Loss: 0.08, Loss_test: 0.62\n",
      "Epoch: 636 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 637 Loss: 0.04, Loss_test: 0.63\n",
      "Epoch: 638 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 639 Loss: 0.09, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 640 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 641 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 642 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 643 Loss: 0.05, Loss_test: 0.60\n",
      "Epoch: 644 Loss: 0.14, Loss_test: 0.60\n",
      "Epoch: 645 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 646 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 647 Loss: 0.10, Loss_test: 0.62\n",
      "Epoch: 648 Loss: 0.08, Loss_test: 0.62\n",
      "Epoch: 649 Loss: 0.09, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 650 Loss: 0.14, Loss_test: 0.61\n",
      "Epoch: 651 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 652 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 653 Loss: 0.14, Loss_test: 0.62\n",
      "Epoch: 654 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 655 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 656 Loss: 0.10, Loss_test: 0.62\n",
      "Epoch: 657 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 658 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 659 Loss: 0.10, Loss_test: 0.63\n",
      "=> result saved!\n",
      "Epoch: 660 Loss: 0.09, Loss_test: 0.63\n",
      "Epoch: 661 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 662 Loss: 0.11, Loss_test: 0.62\n",
      "Epoch: 663 Loss: 0.14, Loss_test: 0.62\n",
      "Epoch: 664 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 665 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 666 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 667 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 668 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 669 Loss: 0.08, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 670 Loss: 0.13, Loss_test: 0.62\n",
      "Epoch: 671 Loss: 0.05, Loss_test: 0.62\n",
      "Epoch: 672 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 673 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 674 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 675 Loss: 0.13, Loss_test: 0.61\n",
      "Epoch: 676 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 677 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 678 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 679 Loss: 0.09, Loss_test: 0.62\n",
      "=> result saved!\n",
      "Epoch: 680 Loss: 0.07, Loss_test: 0.61\n",
      "Epoch: 681 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 682 Loss: 0.10, Loss_test: 0.59\n",
      "Epoch: 683 Loss: 0.14, Loss_test: 0.61\n",
      "Epoch: 684 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 685 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 686 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 687 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 688 Loss: 0.11, Loss_test: 0.62\n",
      "Epoch: 689 Loss: 0.09, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 690 Loss: 0.07, Loss_test: 0.61\n",
      "Epoch: 691 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 692 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 693 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 694 Loss: 0.11, Loss_test: 0.59\n",
      "Epoch: 695 Loss: 0.06, Loss_test: 0.59\n",
      "Epoch: 696 Loss: 0.08, Loss_test: 0.59\n",
      "Epoch: 697 Loss: 0.07, Loss_test: 0.59\n",
      "Epoch: 698 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 699 Loss: 0.07, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 700 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 701 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 702 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 703 Loss: 0.08, Loss_test: 0.61\n",
      "Epoch: 704 Loss: 0.13, Loss_test: 0.61\n",
      "Epoch: 705 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 706 Loss: 0.05, Loss_test: 0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 707 Loss: 0.14, Loss_test: 0.61\n",
      "Epoch: 708 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 709 Loss: 0.13, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 710 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 711 Loss: 0.10, Loss_test: 0.60\n",
      "Epoch: 712 Loss: 0.05, Loss_test: 0.60\n",
      "Epoch: 713 Loss: 0.06, Loss_test: 0.59\n",
      "Epoch: 714 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 715 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 716 Loss: 0.10, Loss_test: 0.62\n",
      "Epoch: 717 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 718 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 719 Loss: 0.09, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 720 Loss: 0.12, Loss_test: 0.60\n",
      "Epoch: 721 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 722 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 723 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 724 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 725 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 726 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 727 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 728 Loss: 0.09, Loss_test: 0.62\n",
      "Epoch: 729 Loss: 0.08, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 730 Loss: 0.07, Loss_test: 0.61\n",
      "Epoch: 731 Loss: 0.05, Loss_test: 0.61\n",
      "Epoch: 732 Loss: 0.06, Loss_test: 0.61\n",
      "Epoch: 733 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 734 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 735 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 736 Loss: 0.08, Loss_test: 0.59\n",
      "Epoch: 737 Loss: 0.05, Loss_test: 0.60\n",
      "Epoch: 738 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 739 Loss: 0.08, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 740 Loss: 0.04, Loss_test: 0.60\n",
      "Epoch: 741 Loss: 0.13, Loss_test: 0.58\n",
      "Epoch: 742 Loss: 0.13, Loss_test: 0.58\n",
      "Epoch: 743 Loss: 0.12, Loss_test: 0.58\n",
      "Epoch: 744 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 745 Loss: 0.03, Loss_test: 0.59\n",
      "Epoch: 746 Loss: 0.12, Loss_test: 0.58\n",
      "Epoch: 747 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 748 Loss: 0.10, Loss_test: 0.59\n",
      "Epoch: 749 Loss: 0.04, Loss_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 750 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 751 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 752 Loss: 0.08, Loss_test: 0.57\n",
      "Epoch: 753 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 754 Loss: 0.08, Loss_test: 0.58\n",
      "Epoch: 755 Loss: 0.04, Loss_test: 0.57\n",
      "Epoch: 756 Loss: 0.11, Loss_test: 0.57\n",
      "Epoch: 757 Loss: 0.11, Loss_test: 0.57\n",
      "Epoch: 758 Loss: 0.11, Loss_test: 0.57\n",
      "Epoch: 759 Loss: 0.05, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 760 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 761 Loss: 0.05, Loss_test: 0.61\n",
      "Epoch: 762 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 763 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 764 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 765 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 766 Loss: 0.07, Loss_test: 0.62\n",
      "Epoch: 767 Loss: 0.10, Loss_test: 0.61\n",
      "Epoch: 768 Loss: 0.11, Loss_test: 0.61\n",
      "Epoch: 769 Loss: 0.07, Loss_test: 0.61\n",
      "=> result saved!\n",
      "Epoch: 770 Loss: 0.12, Loss_test: 0.62\n",
      "Epoch: 771 Loss: 0.09, Loss_test: 0.61\n",
      "Epoch: 772 Loss: 0.13, Loss_test: 0.60\n",
      "Epoch: 773 Loss: 0.08, Loss_test: 0.60\n",
      "Epoch: 774 Loss: 0.07, Loss_test: 0.59\n",
      "Epoch: 775 Loss: 0.14, Loss_test: 0.58\n",
      "Epoch: 776 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 777 Loss: 0.14, Loss_test: 0.57\n",
      "Epoch: 778 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 779 Loss: 0.10, Loss_test: 0.60\n",
      "=> result saved!\n",
      "Epoch: 780 Loss: 0.08, Loss_test: 0.58\n",
      "Epoch: 781 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 782 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 783 Loss: 0.13, Loss_test: 0.56\n",
      "Epoch: 784 Loss: 0.08, Loss_test: 0.56\n",
      "Epoch: 785 Loss: 0.13, Loss_test: 0.57\n",
      "Epoch: 786 Loss: 0.15, Loss_test: 0.58\n",
      "Epoch: 787 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 788 Loss: 0.12, Loss_test: 0.59\n",
      "Epoch: 789 Loss: 0.13, Loss_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 790 Loss: 0.08, Loss_test: 0.57\n",
      "Epoch: 791 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 792 Loss: 0.10, Loss_test: 0.56\n",
      "Epoch: 793 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 794 Loss: 0.10, Loss_test: 0.55\n",
      "Epoch: 795 Loss: 0.04, Loss_test: 0.56\n",
      "Epoch: 796 Loss: 0.06, Loss_test: 0.55\n",
      "Epoch: 797 Loss: 0.06, Loss_test: 0.55\n",
      "Epoch: 798 Loss: 0.10, Loss_test: 0.56\n",
      "Epoch: 799 Loss: 0.08, Loss_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 800 Loss: 0.08, Loss_test: 0.55\n",
      "Epoch: 801 Loss: 0.05, Loss_test: 0.55\n",
      "Epoch: 802 Loss: 0.08, Loss_test: 0.55\n",
      "Epoch: 803 Loss: 0.12, Loss_test: 0.55\n",
      "Epoch: 804 Loss: 0.11, Loss_test: 0.57\n",
      "Epoch: 805 Loss: 0.05, Loss_test: 0.57\n",
      "Epoch: 806 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 807 Loss: 0.06, Loss_test: 0.56\n",
      "Epoch: 808 Loss: 0.08, Loss_test: 0.58\n",
      "Epoch: 809 Loss: 0.08, Loss_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 810 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 811 Loss: 0.03, Loss_test: 0.60\n",
      "Epoch: 812 Loss: 0.11, Loss_test: 0.60\n",
      "Epoch: 813 Loss: 0.08, Loss_test: 0.57\n",
      "Epoch: 814 Loss: 0.09, Loss_test: 0.57\n",
      "Epoch: 815 Loss: 0.03, Loss_test: 0.57\n",
      "Epoch: 816 Loss: 0.09, Loss_test: 0.57\n",
      "Epoch: 817 Loss: 0.11, Loss_test: 0.56\n",
      "Epoch: 818 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 819 Loss: 0.07, Loss_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 820 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 821 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 822 Loss: 0.08, Loss_test: 0.58\n",
      "Epoch: 823 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 824 Loss: 0.08, Loss_test: 0.57\n",
      "Epoch: 825 Loss: 0.08, Loss_test: 0.57\n",
      "Epoch: 826 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 827 Loss: 0.04, Loss_test: 0.58\n",
      "Epoch: 828 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 829 Loss: 0.08, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 830 Loss: 0.06, Loss_test: 0.59\n",
      "Epoch: 831 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 832 Loss: 0.04, Loss_test: 0.60\n",
      "Epoch: 833 Loss: 0.07, Loss_test: 0.58\n",
      "Epoch: 834 Loss: 0.04, Loss_test: 0.59\n",
      "Epoch: 835 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 836 Loss: 0.07, Loss_test: 0.59\n",
      "Epoch: 837 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 838 Loss: 0.12, Loss_test: 0.59\n",
      "Epoch: 839 Loss: 0.05, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 840 Loss: 0.04, Loss_test: 0.60\n",
      "Epoch: 841 Loss: 0.04, Loss_test: 0.59\n",
      "Epoch: 842 Loss: 0.03, Loss_test: 0.60\n",
      "Epoch: 843 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 844 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 845 Loss: 0.03, Loss_test: 0.59\n",
      "Epoch: 846 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 847 Loss: 0.07, Loss_test: 0.60\n",
      "Epoch: 848 Loss: 0.08, Loss_test: 0.59\n",
      "Epoch: 849 Loss: 0.07, Loss_test: 0.59\n",
      "=> result saved!\n",
      "Epoch: 850 Loss: 0.09, Loss_test: 0.59\n",
      "Epoch: 851 Loss: 0.09, Loss_test: 0.60\n",
      "Epoch: 852 Loss: 0.08, Loss_test: 0.59\n",
      "Epoch: 853 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 854 Loss: 0.06, Loss_test: 0.59\n",
      "Epoch: 855 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 856 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 857 Loss: 0.13, Loss_test: 0.59\n",
      "Epoch: 858 Loss: 0.06, Loss_test: 0.59\n",
      "Epoch: 859 Loss: 0.05, Loss_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 860 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 861 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 862 Loss: 0.12, Loss_test: 0.58\n",
      "Epoch: 863 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 864 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 865 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 866 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 867 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 868 Loss: 0.09, Loss_test: 0.56\n",
      "Epoch: 869 Loss: 0.08, Loss_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 870 Loss: 0.04, Loss_test: 0.57\n",
      "Epoch: 871 Loss: 0.05, Loss_test: 0.57\n",
      "Epoch: 872 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 873 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 874 Loss: 0.04, Loss_test: 0.57\n",
      "Epoch: 875 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 876 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 877 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 878 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 879 Loss: 0.13, Loss_test: 0.58\n",
      "=> result saved!\n",
      "Epoch: 880 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 881 Loss: 0.05, Loss_test: 0.59\n",
      "Epoch: 882 Loss: 0.05, Loss_test: 0.58\n",
      "Epoch: 883 Loss: 0.07, Loss_test: 0.58\n",
      "Epoch: 884 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 885 Loss: 0.06, Loss_test: 0.60\n",
      "Epoch: 886 Loss: 0.07, Loss_test: 0.59\n",
      "Epoch: 887 Loss: 0.07, Loss_test: 0.59\n",
      "Epoch: 888 Loss: 0.06, Loss_test: 0.58\n",
      "Epoch: 889 Loss: 0.08, Loss_test: 0.57\n",
      "=> result saved!\n",
      "Epoch: 890 Loss: 0.06, Loss_test: 0.56\n",
      "Epoch: 891 Loss: 0.07, Loss_test: 0.57\n",
      "Epoch: 892 Loss: 0.10, Loss_test: 0.58\n",
      "Epoch: 893 Loss: 0.09, Loss_test: 0.58\n",
      "Epoch: 894 Loss: 0.09, Loss_test: 0.57\n",
      "Epoch: 895 Loss: 0.04, Loss_test: 0.57\n",
      "Epoch: 896 Loss: 0.05, Loss_test: 0.57\n",
      "Epoch: 897 Loss: 0.06, Loss_test: 0.57\n",
      "Epoch: 898 Loss: 0.05, Loss_test: 0.57\n",
      "Epoch: 899 Loss: 0.05, Loss_test: 0.56\n",
      "=> result saved!\n",
      "Epoch: 900 Loss: 0.06, Loss_test: 0.57\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title'], train_input_sample['description'], train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append([h])\n",
    "    result['test'].append([h_validation])\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\".format(epoch+1, h, h_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 900)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_preprocessing_baseline_1000_feature1000epochs_64batch(openoffice)\n"
     ]
    }
   ],
   "source": [
    "print(SAVE_PATH.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_baseline_1000_feature_1000epochs_64batch(openoffice).h5' to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model saved'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c2394b2f20476ba5bdeb09d43bfe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4116), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534f888a1cfb4cbf83bd9d6e85ff65d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1922), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1c08e476754906be74155b6434bc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1922), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39e76042a1047c782eaa23e832ecb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceea8b0d03b4832926a4dad8b259c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f2a8efb9e4ba894c4d29127bbd170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d497a228fe64f33b18cdaf8bdc2ccce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1922), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 900 Loss: 0.06, Loss_test: 0.57, recall@25: 0.68\n"
     ]
    }
   ],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, bug_train_ids)\n",
    "print(\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h, h_validation, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.678\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['49152:49146|22335:0.4175642132759094,29297:0.4133017063140869,28593:0.3922470808029175,5578:0.3878999948501587,40676:0.3809131383895874,22368:0.3797723054885864,25685:0.37942516803741455,77962:0.3775831460952759,52155:0.3745584487915039,25818:0.36400455236434937,19602:0.3633955717086792,17567:0.3601383566856384,47208:0.3594701886177063,7970:0.3590354323387146,46846:0.3575632572174072,56254:0.35730665922164917,45784:0.35588276386260986,8295:0.35381370782852173,25443:0.3536869287490845,46642:0.35283148288726807,96950:0.3526841402053833,77819:0.3513496518135071,54466:0.35086870193481445,24780:0.3434591293334961,8296:0.3414788246154785,54867:0.3411867618560791,21443:0.34056562185287476,23895:0.3400833010673523,82642:0.338129460811615',\n",
       " '73735:73736|73736:0.9999060991976876,69291:0.46634238958358765,68383:0.46525639295578003,76528:0.43222546577453613,76110:0.4299775958061218,90533:0.42333143949508667,83897:0.417693555355072,75343:0.4142876863479614,54612:0.4137670397758484,62242:0.40867722034454346,32931:0.40762656927108765,83898:0.3994213938713074,69764:0.39461666345596313,75872:0.3857570290565491,93692:0.38012218475341797,30626:0.37771427631378174,19401:0.3767532706260681,74732:0.37427932024002075,13426:0.361258327960968,69661:0.35619980096817017,43556:0.3545514941215515,75852:0.3493920564651489,75444:0.3480339050292969,37026:0.34717869758605957,110555:0.3461799621582031,89537:0.3362952470779419,34556:0.33150649070739746,83896:0.33032017946243286,55131:0.32624244689941406',\n",
       " '73736:73735|73735:0.9999060991976876,69291:0.46634238958358765,68383:0.46525639295578003,76528:0.43222546577453613,76110:0.4299775958061218,90533:0.42333143949508667,83897:0.417693555355072,75343:0.4142876863479614,54612:0.4137670397758484,62242:0.40867722034454346,32931:0.40762656927108765,83898:0.3994213938713074,69764:0.39461666345596313,75872:0.3857570290565491,93692:0.38012218475341797,30626:0.37771427631378174,19401:0.3767532706260681,74732:0.37427932024002075,13426:0.361258327960968,69661:0.35619980096817017,43556:0.3545514941215515,75852:0.3493920564651489,75444:0.3480339050292969,37026:0.34717869758605957,110555:0.3461799621582031,89537:0.3362952470779419,34556:0.33150649070739746,83896:0.33032017946243286,55131:0.32624244689941406',\n",
       " '32780:27332,37094|28063:0.23524552583694458,19139:0.13284516334533691,35340:0.13255345821380615,17486:0.1293441653251648,9253:0.12870079278945923,13787:0.11118525266647339,70590:0.10580569505691528,80346:0.1024327278137207,78401:0.09996891021728516,19415:0.09898412227630615,14805:0.0963934063911438,56670:0.09630095958709717,11597:0.09538549184799194,54387:0.09296208620071411,37498:0.09276705980300903,18662:0.08891290426254272,71296:0.0859992504119873,11760:0.08568370342254639,26825:0.07970911264419556,34589:0.07777762413024902,63332:0.07733315229415894,41017:0.07574683427810669,76273:0.07404249906539917,7983:0.07002228498458862,68845:0.0681963562965393,10417:0.06002318859100342,96498:0.058633506298065186,26750:0.056901752948760986,20965:0.05399751663208008',\n",
       " '16404:16688|16688:0.5104857385158539,22458:0.48561030626296997,23476:0.43038761615753174,104753:0.42389243841171265,102506:0.42042720317840576,108088:0.39856743812561035,54471:0.3915219306945801,14729:0.3867986798286438,109009:0.3722190260887146,102195:0.36820733547210693,102984:0.35851985216140747,15088:0.35739314556121826,54133:0.35680001974105835,21114:0.3526434302330017,14218:0.3520217537879944,14219:0.3520217537879944,104494:0.34906768798828125,991:0.3414129614830017,86845:0.33744192123413086,47568:0.3360069990158081,102419:0.33509528636932373,52211:0.33459532260894775,92474:0.3252719044685364,3641:0.32322901487350464,92473:0.3225758671760559,6833:0.32131439447402954,102768:0.3211422562599182,102762:0.3191910982131958,4967:0.31794875860214233',\n",
       " '73749:73750|73750:1.0,68136:0.6360568106174469,69934:0.5766002535820007,70801:0.5748563408851624,41448:0.5548224449157715,41449:0.5548224449157715,54958:0.4835282564163208,113579:0.3625357151031494,95768:0.3533271551132202,35305:0.34435176849365234,52761:0.33831483125686646,38037:0.3366979956626892,86252:0.3301038146018982,111650:0.3264460563659668,44970:0.3245776891708374,33029:0.3196715712547302,37675:0.31932127475738525,36825:0.3185386061668396,97095:0.3050752282142639,67539:0.2990010976791382,77962:0.29559165239334106,28691:0.29075050354003906,8488:0.28577882051467896,58205:0.28096622228622437,47687:0.27995961904525757,88449:0.2791106700897217,93692:0.2790931463241577,99257:0.27783721685409546,80421:0.27488720417022705',\n",
       " '73750:73749|73749:1.0,68136:0.6360568106174469,69934:0.5766002535820007,70801:0.5748563408851624,41448:0.5548224449157715,41449:0.5548224449157715,54958:0.4835282564163208,113579:0.3625357151031494,95768:0.3533271551132202,35305:0.34435176849365234,52761:0.33831483125686646,38037:0.3366979956626892,86252:0.3301038146018982,111650:0.3264460563659668,44970:0.3245776891708374,33029:0.3196715712547302,37675:0.31932127475738525,36825:0.3185386061668396,97095:0.3050752282142639,67539:0.2990010976791382,77962:0.29559165239334106,28691:0.29075050354003906,8488:0.28577882051467896,58205:0.28096622228622437,47687:0.27995961904525757,88449:0.2791106700897217,93692:0.2790931463241577,99257:0.27783721685409546,80421:0.27488720417022705',\n",
       " '57365:64622|61849:0.4712967872619629,28691:0.4692186117172241,64921:0.4632728695869446,64622:0.4562520384788513,54773:0.42946940660476685,65216:0.42340099811553955,28690:0.4174633026123047,8815:0.4062654972076416,8816:0.4062654972076416,16460:0.3997330665588379,16461:0.3997330665588379,16462:0.3997330665588379,16463:0.3997330665588379,80335:0.39666879177093506,53318:0.39328891038894653,56707:0.3879415988922119,32398:0.37985551357269287,111617:0.37891054153442383,111650:0.3701478838920593,32168:0.3678596615791321,9263:0.36680376529693604,97095:0.3667236566543579,67859:0.366022527217865,67329:0.3655916452407837,54854:0.36556947231292725,95768:0.36508673429489136,59724:0.36494797468185425,51879:0.35938143730163574,28896:0.3573351502418518',\n",
       " '24600:23597|3003:0.43506038188934326,21693:0.4046902060508728,86252:0.3849210739135742,3438:0.3781256675720215,19712:0.3718339204788208,37510:0.37029701471328735,63603:0.35175102949142456,67539:0.34334683418273926,5648:0.3428010940551758,2883:0.34101903438568115,52956:0.3389549255371094,85462:0.3386116027832031,54854:0.3379296064376831,40809:0.33761030435562134,88799:0.33702635765075684,50651:0.33431631326675415,30132:0.331462025642395,113630:0.32854902744293213,19429:0.32170140743255615,71403:0.3214946389198303,111617:0.3204437494277954,62659:0.3198850154876709,112371:0.3176758885383606,88190:0.3164433240890503,86924:0.3153918981552124,113579:0.31438982486724854,33029:0.3131827116012573,54466:0.3121458888053894,9332:0.31178784370422363',\n",
       " '16410:28992,26890,34122,22287,20402,13267,13492,30742|9642:0.3648117780685425,19567:0.36297160387039185,13492:0.3393505811691284,26890:0.3201914429664612,16609:0.3182401657104492,28992:0.3076455593109131,22287:0.29900288581848145,47650:0.28812921047210693,22184:0.28781354427337646,18334:0.27942901849746704,30742:0.27100956439971924,14703:0.2636934518814087,60610:0.26097768545150757,14972:0.23260986804962158,66532:0.22658568620681763,25733:0.2132471799850464,50796:0.20259451866149902,3706:0.20227348804473877,102603:0.19205743074417114,13154:0.18753087520599365,34122:0.1831457018852234,39262:0.1830640435218811,51286:0.1824766993522644,91377:0.18137127161026,91378:0.18137127161026,64702:0.17190194129943848,59651:0.17073023319244385,85141:0.17010444402694702,7432:0.1657845377922058',\n",
       " '32:42944,6342,3207,18886,32905,54854,54855,67697,4756,23861,18648,58138,40286|40810:0.41024571657180786,3587:0.40465235710144043,5068:0.3941423296928406,30132:0.3765564560890198,3438:0.37453293800354004,20120:0.3584911823272705,54854:0.35523420572280884,7432:0.3536149263381958,2581:0.3518486022949219,59712:0.3515446186065674,114339:0.34844160079956055,28721:0.34617775678634644,9966:0.34501147270202637,92224:0.33573371171951294,90096:0.33442920446395874,20483:0.3326157331466675,6342:0.330396831035614,4686:0.33036571741104126,112371:0.3277883529663086,17715:0.326602041721344,94622:0.3218134641647339,113639:0.32127612829208374,21019:0.3210068345069885,7091:0.31734782457351685,3207:0.316581666469574,4718:0.3122706413269043,12188:0.3114631175994873,66097:0.3107612133026123,3334:0.3080231547355652',\n",
       " '73764:72643|73946:0.46262454986572266,72600:0.4334023594856262,48266:0.3928530812263489,73617:0.3841490149497986,74588:0.3815600872039795,45755:0.376470148563385,71870:0.37002891302108765,48179:0.361083447933197,62393:0.35701513290405273,74394:0.3518015146255493,107073:0.35036414861679077,53733:0.3469398021697998,69721:0.3459259271621704,70756:0.34428316354751587,59598:0.34251540899276733,54012:0.34148842096328735,56707:0.340004026889801,62691:0.33017605543136597,69449:0.3256725072860718,67539:0.3234381079673767,24900:0.3226173520088196,62710:0.31824177503585815,72467:0.3162180185317993,65496:0.3148380517959595,65497:0.3148380517959595,73843:0.3146892189979553,75467:0.3123081922531128,72670:0.31161075830459595,74681:0.3094225525856018',\n",
       " '81959:81709,82941|94519:0.3423004150390625,80364:0.3337676525115967,70754:0.33318448066711426,89305:0.326055109500885,90881:0.3250006437301636,54012:0.32330232858657837,84819:0.32327336072921753,24969:0.31916725635528564,29641:0.31819456815719604,73946:0.31801849603652954,72600:0.31781119108200073,112150:0.31276410818099976,38785:0.3126734495162964,90086:0.31232666969299316,70756:0.30276018381118774,59490:0.30145037174224854,71870:0.3014277219772339,89246:0.2969241738319397,100986:0.29662734270095825,86193:0.292644739151001,80693:0.29086315631866455,29790:0.2851142883300781,91612:0.28355926275253296,69697:0.2810201048851013,88007:0.2787367105484009,4010:0.278542160987854,82941:0.27338582277297974,41546:0.2721138000488281,73764:0.2706119418144226',\n",
       " '41012:6705,64659,21924,44033|57760:0.5222095847129822,64659:0.4544554352760315,53901:0.43478482961654663,65057:0.4096692204475403,51656:0.3742995262145996,103472:0.37163078784942627,21924:0.37061214447021484,86643:0.3704894781112671,72976:0.36964333057403564,46057:0.3559229373931885,49541:0.34897881746292114,45692:0.3478826880455017,50925:0.34633296728134155,94124:0.3453880548477173,78140:0.34223973751068115,22402:0.3402133584022522,49542:0.33627140522003174,59745:0.33447301387786865,95380:0.33216947317123413,54344:0.33161091804504395,67912:0.3308197259902954,84622:0.330008327960968,95630:0.3273468613624573,28943:0.32633620500564575,91670:0.32553213834762573,94724:0.3244926333427429,6705:0.3193435072898865,3686:0.31901198625564575,10717:0.31780534982681274',\n",
       " '57397:58003|58003:0.6527427136898041,44836:0.37178683280944824,53318:0.36927056312561035,54857:0.347271203994751,72208:0.3444402813911438,69630:0.34152311086654663,54466:0.3350760340690613,80068:0.3336758017539978,53302:0.3331735134124756,55512:0.3222261667251587,11530:0.32005906105041504,63852:0.3182055354118347,59434:0.31635838747024536,53888:0.3038320541381836,27231:0.30244284868240356,52654:0.3017839193344116,45727:0.30002379417419434,111859:0.2988632321357727,72207:0.2949397563934326,61980:0.2931366562843323,13934:0.2920612096786499,56931:0.29106247425079346,53436:0.2905898690223694,59490:0.2864346504211426,67497:0.28564488887786865,77794:0.2853586673736572,58394:0.28251779079437256,34141:0.28159773349761963,64996:0.2808723449707031',\n",
       " '41017:38304,25446,23761,14805,37498|9915:0.34898608922958374,17949:0.3226255774497986,27383:0.3046911358833313,63332:0.2903950810432434,65927:0.2832000255584717,41157:0.27721405029296875,38833:0.275290310382843,19415:0.2582756280899048,55794:0.2567152976989746,38834:0.2534835934638977,38835:0.2534835934638977,3142:0.25252383947372437,50093:0.25244569778442383,38832:0.2515934705734253,80346:0.247769296169281,7929:0.24600094556808472,3547:0.23168766498565674,64137:0.2279653549194336,67174:0.21590089797973633,37498:0.21468698978424072,64391:0.2129841446876526,26750:0.2093663215637207,61339:0.2086302638053894,52558:0.20683878660202026,5906:0.20478659868240356,90925:0.1991317868232727,12575:0.19856679439544678,36788:0.19597619771957397,24421:0.19123607873916626',\n",
       " '57403:58394|90704:0.42815399169921875,62606:0.4260164499282837,55512:0.3866846561431885,56931:0.38132965564727783,74021:0.3676755428314209,12587:0.36144280433654785,49541:0.3585641384124756,1528:0.35387879610061646,60196:0.35327011346817017,56155:0.3532596826553345,64664:0.3528008460998535,48250:0.35227853059768677,80417:0.33794063329696655,58116:0.33146172761917114,86275:0.33056533336639404,82020:0.3199186325073242,54594:0.31797927618026733,25706:0.31707870960235596,53901:0.3069213628768921,20343:0.30254340171813965,3686:0.29744964838027954,74207:0.29690688848495483,67912:0.29686641693115234,82913:0.29672783613204956,103472:0.2963104844093323,1507:0.2925788164138794,17440:0.29203295707702637,57725:0.29122495651245117,86845:0.2890135645866394',\n",
       " '63:84|84:0.42888838052749634,86812:0.3516196012496948,3003:0.3472467064857483,3039:0.3246726989746094,16464:0.3212014436721802,86836:0.318361759185791,97313:0.31330227851867676,3869:0.3130910396575928,16191:0.3101070523262024,4551:0.3030998706817627,9605:0.3002343773841858,67601:0.29894715547561646,94580:0.2984631061553955,103944:0.29499953985214233,103945:0.29499953985214233,78755:0.294542133808136,1792:0.2861521244049072,59724:0.28599774837493896,1298:0.28201884031295776,52761:0.2803657650947571,59726:0.27141010761260986,14218:0.2671288847923279,14219:0.2671288847923279,17853:0.2605136036872864,88449:0.2583954334259033,67615:0.25743168592453003,13024:0.25678306818008423,86835:0.25625455379486084,12507:0.25557994842529297',\n",
       " '98371:98044|98044:0.5281383097171783,119730:0.37497448921203613,38785:0.3729060888290405,100986:0.361749529838562,53707:0.3579322099685669,29790:0.3544159531593323,88007:0.3487919569015503,62691:0.34339970350265503,58696:0.3415769338607788,62710:0.34039419889450073,22686:0.3306064009666443,94519:0.32537245750427246,44988:0.3233181834220886,98072:0.3199452757835388,58312:0.31555140018463135,97559:0.315240740776062,38355:0.3130856156349182,63185:0.3089369535446167,50901:0.3045339584350586,29641:0.3026244044303894,57451:0.3020282983779907,73617:0.3006938695907593,56707:0.3006448745727539,20302:0.3003488779067993,59490:0.30017775297164917,41546:0.29851073026657104,119682:0.29617995023727417,120360:0.29598039388656616,26531:0.29169440269470215',\n",
       " '49220:48266,48589|51796:0.3974648118019104,43547:0.38831865787506104,48266:0.3869432806968689,75420:0.3842189311981201,71546:0.37900203466415405,53733:0.37500250339508057,44656:0.3711537718772888,51118:0.3707766532897949,57725:0.37044650316238403,35318:0.36877232789993286,35319:0.36877232789993286,71870:0.3660507798194885,48179:0.3626253604888916,43406:0.3610304594039917,45298:0.35963886976242065,42812:0.35588836669921875,45755:0.35015320777893066,57255:0.34647971391677856,57451:0.3443989157676697,44400:0.34423136711120605,50651:0.3402363657951355,62997:0.33998578786849976,41429:0.33770716190338135,51733:0.32893216609954834,80693:0.3227251172065735,34229:0.32123637199401855,66142:0.3162931203842163,82941:0.3091849088668823,43906:0.3060110807418823']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 4116\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_preprocessing_baseline_1000_feature_1000epochs_64batch(openoffice)'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 738)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_in (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_in (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          221700      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureLstmGenerationModel (Mod (None, 300)          581460900   title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureCNNGenerationModel (Mode (None, 300)          581507592   desc_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureLstmGenerationModel[1][0] \n",
      "                                                                 FeatureCNNGenerationModel[1][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,163,190,192\n",
      "Trainable params: 675,192\n",
      "Non-trainable params: 1,162,515,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922\n"
     ]
    }
   ],
   "source": [
    "print(len(exported_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/openoffice/bert/exported_rank_baseline_1000.txt\n"
     ]
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "print(EXPORT_RANK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3 - recall_at_15': 0.621, '5 - recall_at_25': 0.678, '0 - recall_at_1': 0.339, '1 - recall_at_5': 0.511, '2 - recall_at_10': 0.583, '4 - recall_at_20': 0.655}\n"
     ]
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some ideas to visualizate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
