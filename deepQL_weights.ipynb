{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose centroid replacing the masters with BERT siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "# from tensorflow.python import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the use of GPUs\n",
    "# https://datascience.stackexchange.com/questions/23895/multi-gpu-in-keras\n",
    "# https://keras.io/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus\n",
    "# https://stackoverflow.com/questions/56316451/how-to-use-specific-gpus-in-keras-for-multi-gpu-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 20\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 80\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'eclipse'\n",
    "METHOD = 'deepQL_weights_{}'.format(epochs)\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}'.format(DOMAIN)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Path embeddings\n",
    "EMBED_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_feature@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_feature_@number_of_epochs@epochs_64batch({})'.format(METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip -o uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, \n",
    "                    token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buckets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963df14ab90441b0a7acce1252377584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=322339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6ff88fa8984396a38f0ede1a534c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3744d8b17364fdca581045ce4ca8067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=361006), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5812d810328c47a3ac14b9c54fe6d528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 30.3 s, sys: 3.2 s, total: 33.5 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs()\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189a0942bd974765978dc310c9ed7941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=321536), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 2min 36s, sys: 23 ms, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a random bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '2\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '461\\n',\n",
       " 'creation_ts': '2012-12-06 10:56:00 -0500',\n",
       " 'delta_ts': '2012-12-07 11:30:06 -0500',\n",
       " 'description': \"[CLS] created attachment 224 ##38 ##1 screens ##hot of empty theme pop ##up i tried the new editor co ##g but there are no themes offered . i just get some gray pixels ( border of an empty pop ##up ) . note i haven ' t chosen a theme before , so maybe that ' s the problem . we need to ensure that a theme ##less user gets the right behavior since that is the default state . [SEP]\",\n",
       " 'description_bert': \"[CLS] created attachment 224 ##38 ##1 screens ##hot of empty theme pop ##up i tried the new editor co ##g but there are no themes offered . i just get some gray pixels ( border of an empty pop ##up ) . note i haven ' t chosen a theme before , so maybe that ' s the problem . we need to ensure that a theme ##less user gets the right behavior since that is the default state . [SEP]\",\n",
       " 'description_word': array([  101,  2580, 14449, 19711, 22025,  2487, 12117, 12326,  1997,\n",
       "         4064,  4323,  3769,  6279,  1045,  2699,  1996,  2047,  3559,\n",
       "         2522,   102]),\n",
       " 'description_word_bert': [101,\n",
       "  2580,\n",
       "  14449,\n",
       "  19711,\n",
       "  22025,\n",
       "  2487,\n",
       "  12117,\n",
       "  12326,\n",
       "  1997,\n",
       "  4064,\n",
       "  4323,\n",
       "  3769,\n",
       "  6279,\n",
       "  1045,\n",
       "  2699,\n",
       "  1996,\n",
       "  2047,\n",
       "  3559,\n",
       "  2522,\n",
       "  2290,\n",
       "  2021,\n",
       "  2045,\n",
       "  2024,\n",
       "  2053,\n",
       "  6991,\n",
       "  3253,\n",
       "  1012,\n",
       "  1045,\n",
       "  2074,\n",
       "  2131,\n",
       "  2070,\n",
       "  3897,\n",
       "  27725,\n",
       "  1006,\n",
       "  3675,\n",
       "  1997,\n",
       "  2019,\n",
       "  4064,\n",
       "  3769,\n",
       "  6279,\n",
       "  1007,\n",
       "  1012,\n",
       "  3602,\n",
       "  1045,\n",
       "  4033,\n",
       "  1005,\n",
       "  1056,\n",
       "  4217,\n",
       "  1037,\n",
       "  4323,\n",
       "  2077,\n",
       "  1010,\n",
       "  2061,\n",
       "  2672,\n",
       "  2008,\n",
       "  1005,\n",
       "  1055,\n",
       "  1996,\n",
       "  3291,\n",
       "  1012,\n",
       "  2057,\n",
       "  2342,\n",
       "  2000,\n",
       "  5676,\n",
       "  2008,\n",
       "  1037,\n",
       "  4323,\n",
       "  3238,\n",
       "  5310,\n",
       "  4152,\n",
       "  1996,\n",
       "  2157,\n",
       "  5248,\n",
       "  2144,\n",
       "  2008,\n",
       "  2003,\n",
       "  1996,\n",
       "  12398,\n",
       "  2110,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'dup_id': '[]',\n",
       " 'issue_id': 395948,\n",
       " 'priority': '4\\n',\n",
       " 'product': '1\\n',\n",
       " 'resolution': 'FIXED',\n",
       " 'textual_word': array([  101,  3559, 10906,  3769,  6279,  4107,  2053,  4323,  9804,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  2580, 14449, 19711, 22025,  2487, 12117,\n",
       "        12326,  1997,  4064,  4323,  3769,  6279,  1045,  2699,  1996,\n",
       "         2047,  3559,  2522,   102]),\n",
       " 'title': '[CLS] editor settings pop ##up offers no theme choices [SEP]',\n",
       " 'title_bert': '[CLS] editor settings pop ##up offers no theme choices [SEP]',\n",
       " 'title_word': array([  101,  3559, 10906,  3769,  6279,  4107,  2053,  4323,  9804,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'title_word_bert': [101,\n",
       "  3559,\n",
       "  10906,\n",
       "  3769,\n",
       "  6279,\n",
       "  4107,\n",
       "  2053,\n",
       "  4323,\n",
       "  9804,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'version': '21\\n'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 34882)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indexed all train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_idx = bug_train_ids[0]\n",
    "vector = baseline.bug_set[bug_idx]['textual_word']\n",
    "annoy_train = AnnoyIndex(vector.shape[0])\n",
    "for bug_id in bug_train_ids:\n",
    "    annoy_train.add_item(bug_id, baseline.bug_set[bug_id]['textual_word'])\n",
    "annoy_train.build(10) # 10 trees\n",
    "\"Indexed all train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.4 ms, sys: 29 µs, total: 65.4 ms\n",
      "Wall time: 65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_input_pos, valid_input_neg, \\\n",
    "                            valid_master_sample, valid_master_neg, valid_sim = experiment.batch_iterator_bert(None, baseline.train_data, \n",
    "                                                                                          baseline.dup_sets_train,\n",
    "                                                                                          bug_train_ids,\n",
    "                                                                                          batch_size_test, 1, \n",
    "                                                                                              issues_by_buckets, INCLUDE_MASTER=True)\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 20), (128, 20), (128, 1682), (128,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, valid_input_sample['description']['token'].shape, \\\n",
    "    valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "OhTbr3a5nmrh",
    "outputId": "a2d73e0f-e9ce-4d12-a5c8-f0008d2402d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5, batch_iterator, issues_by_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#     )\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=True,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "#     model = get_model(\n",
    "#         token_num=len(token_dict),\n",
    "#         head_num=10,\n",
    "#         transformer_num=layer_num,\n",
    "#         embed_dim=100,\n",
    "#         feed_forward_dim=100,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#         pos_num=MAX_SEQUENCE_LENGTH,\n",
    "#         dropout_rate=0.05,\n",
    "#     )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    outputs = model.get_layer('Encoder-{}-FeedForward-Norm'.format(layer_num)).output\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "#     outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    #layer = GRU(100, activation='tanh')(layer)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "'''\n",
    "    Some loss ideas\n",
    "    hinge loss Kullback-Leibler\n",
    "    https://stackoverflow.com/questions/53581298/custom-combined-hinge-kb-divergence-loss-function-in-siamese-net-fails-to-genera\n",
    "'''\n",
    "\n",
    "def normalize(x, axis):\n",
    "    norm = K.sqrt(K.sum(K.square(x), axis=axis, keepdims=False))\n",
    "    return x, K.maximum(norm, K.epsilon())\n",
    "    \n",
    "# https://github.com/keras-team/keras/issues/3031\n",
    "# https://github.com/keras-team/keras/issues/8335\n",
    "def cosine_distance(inputs):\n",
    "    x, y = inputs\n",
    "    x, x_norm = normalize(x, axis=-1)\n",
    "    y, y_norm = normalize(y, axis=-1)\n",
    "    distance = K.sum( x * y, axis=-1) / (x_norm * y_norm)\n",
    "    distance = (distance + K.constant(1)) / K.constant(2)\n",
    "    # Distance goes from 0 to 2 in theory, but from 0 to 1 if x and y are both\n",
    "    # positive (which is the case after ReLU activation).\n",
    "    return K.mean(distance, axis=-1, keepdims=True)\n",
    "    #return K.mean(distance, axis=-1, keepdims=False)\n",
    "\n",
    "def _triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1.0)\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    return K.mean(K.maximum(0.0, pos - neg + margin))\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    pos = vects[0]\n",
    "    neg = vects[1]\n",
    "    margin = K.constant(1.0)\n",
    "    return K.maximum(0.0, margin - pos + neg)\n",
    "    #return K.mean(K.maximum(0.0, margin - pos + neg), keepdims=False)\n",
    "\n",
    "class QuintetWeights(Layer):\n",
    " \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(QuintetWeights, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.W= self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.built = False \n",
    " \n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.W)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "class Triplet(Layer):\n",
    " \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Triplet, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    " \n",
    "    def call(self, x):\n",
    "        pos, neg  = x\n",
    "        margin = K.constant(1.0)\n",
    "        return K.maximum(0.0, margin - pos + neg)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    return y_pred[0][0]\n",
    "\n",
    "def triplet_bug(y_true, y_pred):\n",
    "    return y_pred[0][1]\n",
    "def triplet_anchor(y_true, y_pred):\n",
    "    return y_pred[0][2]\n",
    "def triplet_pos(y_true, y_pred):\n",
    "    return y_pred[0][3]\n",
    "def triplet_neg(y_true, y_pred):\n",
    "    return y_pred[0][4]\n",
    "\n",
    "def pos_distance(y_true, y_pred):\n",
    "    return y_pred[0]\n",
    "\n",
    "def neg_distance(y_true, y_pred):\n",
    "    return y_pred[1]\n",
    "\n",
    "def stack_tensors(vects):\n",
    "    return K.stack(vects, axis=-1)\n",
    "\n",
    "# https://www.kaggle.com/c/quora-question-pairs/discussion/33631\n",
    "# https://www.researchgate.net/figure/Illustration-of-triplet-loss-contrastive-loss-for-negative-samples-and-binomial_fig2_322060548\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    pos = y_pred[0]\n",
    "    neg = y_pred[1]\n",
    "    margin = 1\n",
    "    return K.mean(pos * K.square(neg) +\n",
    "                  (1 - pos) * K.square(K.maximum(margin - neg, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, Average, Maximum, Subtract, Average, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "    \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model_centroid(sequence_length, name):\n",
    "    \n",
    "    # Bug Centroid Feature\n",
    "    bug_centroid = Input(shape = (sequence_length, ), name = 'bug_centroid_feature_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_centroid], outputs=[bug_centroid], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average, Reshape, Add\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "def max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                             master_anchor, master_negative, master_positive, \n",
    "                         NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1):\n",
    "    \n",
    "    inputs = np.concatenate([encoded_anchor.input, encoded_positive.input, encoded_negative.input], -1).tolist()\n",
    "    \n",
    "    inputs.append(master_anchor.input)\n",
    "    inputs.append(master_positive.input)\n",
    "    inputs.append(master_negative.input)\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    encoded_positive = encoded_positive.output\n",
    "    encoded_negative = encoded_negative.output\n",
    "    master_anchor = master_anchor.output\n",
    "    master_negative = master_negative.output\n",
    "    master_positive = master_positive.output\n",
    "    \n",
    "    # Distance bugs\n",
    "    positive_d = Lambda(cosine_distance, name='pos_cosine_distance')([encoded_anchor, encoded_positive])\n",
    "    negative_d = Lambda(cosine_distance, name='neg_cosine_distance')([encoded_anchor, encoded_negative])\n",
    "    \n",
    "    # Distance masters anchor\n",
    "    master_anchor_positive_d = Lambda(cosine_distance, name='pos_master_cosine_distance')([encoded_anchor, master_positive])\n",
    "    master_anchor_negative_d = Lambda(cosine_distance, name='neg_master_cosine_distance')([encoded_anchor, master_negative])\n",
    "    \n",
    "    # Distance master positive\n",
    "    master_pos_positive_d = Lambda(cosine_distance, name='pos_master_pos_cosine_distance')([encoded_positive, master_positive])\n",
    "    master_pos_negative_d = Lambda(cosine_distance, name='neg_master_pos_cosine_distance')([encoded_positive, master_negative])\n",
    "    \n",
    "    # Distance master negative\n",
    "    master_neg_positive_d = Lambda(cosine_distance, name='pos_master_neg_cosine_distance')([encoded_negative, master_negative])\n",
    "    master_neg_negative_d = Lambda(cosine_distance, name='neg_master_neg_cosine_distance')([encoded_negative, master_positive])\n",
    "     \n",
    "#     output_bug = Triplet(1)([positive_d, negative_d])\n",
    "#     output_bug = QuintetWeights((1,2))(output_bug)\n",
    "    output_TL = Lambda(triplet_loss, name='triplet_pos_neg')([positive_d, negative_d])\n",
    "    output_TL_a = Lambda(triplet_loss, name='triplet_anchor_centroid')([master_anchor_positive_d, master_anchor_negative_d])\n",
    "    output_TL_p = Lambda(triplet_loss, name='triplet_pos_centroid')([master_pos_positive_d, master_pos_negative_d])\n",
    "    output_TL_n = Lambda(triplet_loss, name='triplet_neg_centroid')([master_neg_positive_d, master_neg_negative_d])\n",
    "    \n",
    "    # Weights for each loss\n",
    "    loss_TL = Reshape((1, ))(output_TL)\n",
    "    loss_TL_a = Reshape((1, ))(output_TL_a)\n",
    "    loss_TL_p = Reshape((1, ))(output_TL_p)\n",
    "    loss_TL_n = Reshape((1, ))(output_TL_n)\n",
    "    \n",
    "    sum_all_losses = Add()([loss_TL, loss_TL_a, loss_TL_p, loss_TL_n])\n",
    "    output_TL_w = Dense(1, input_shape=(1,), use_bias=False)(loss_TL)\n",
    "    output_TL_a_w = Dense(1, input_shape=(1,), use_bias=False)(loss_TL_a)\n",
    "    output_TL_p_w = Dense(1, input_shape=(1,), use_bias=False)(loss_TL_p)\n",
    "    output_TL_n_w = Dense(1, input_shape=(1,), use_bias=False)(loss_TL_n)\n",
    "    sum_all_loss_w = Add()([output_TL_w, output_TL_a_w, output_TL_p_w, output_TL_n_w])\n",
    "    # Weighted Median Loss\n",
    "    output_median = Lambda(\n",
    "            lambda inputs: inputs[0] / inputs[1], \n",
    "            name = 'Weighted_Median_Loss')([sum_all_loss_w, sum_all_losses])\n",
    "    \n",
    "    output = concatenate([output_median, loss_TL, loss_TL_a, loss_TL_p, loss_TL_n])\n",
    "\n",
    "    similarity_model = Model(inputs = inputs, outputs = [output], name = 'Similarity_Model')\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam',\n",
    "                             metrics=[triplet_bug, triplet_anchor, triplet_pos, triplet_neg],\n",
    "                             loss=custom_loss)\n",
    "\n",
    "    # metrics=[triplet_bug, triplet_anchor, triplet_pos, triplet_neg],\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size  64\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_pos (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_pos (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_pos (InputLayer)  (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_pos (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_pos (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "info_neg (InputLayer)           (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_neg (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_neg (InputLayer)  (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_neg (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_neg (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "                                                                 info_pos[0][0]                   \n",
      "                                                                 info_neg[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 768)          80346736    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "                                                                 title_token_pos[0][0]            \n",
      "                                                                 title_segment_pos[0][0]          \n",
      "                                                                 title_token_neg[0][0]            \n",
      "                                                                 title_segment_neg[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 768)          80346736    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "                                                                 desc_token_pos[0][0]             \n",
      "                                                                 desc_segment_pos[0][0]           \n",
      "                                                                 desc_token_neg[0][0]             \n",
      "                                                                 desc_segment_neg[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1836)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_pos (Concatenate (None, 1836)         0           FeatureMlpGenerationModel[2][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[2\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "merge_features_neg (Concatenate (None, 1836)         0           FeatureMlpGenerationModel[3][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[3\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "bug_centroid_feature_master_pos (None, 1836)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bug_centroid_feature_master_neg (None, 1836)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_cosine_distance (Lambda)    (1,)                 0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_pos[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "neg_cosine_distance (Lambda)    (1,)                 0           merge_features_in[0][0]          \n",
      "                                                                 merge_features_neg[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pos_master_cosine_distance (Lam (1,)                 0           merge_features_in[0][0]          \n",
      "                                                                 bug_centroid_feature_master_pos[0\n",
      "__________________________________________________________________________________________________\n",
      "neg_master_cosine_distance (Lam (1,)                 0           merge_features_in[0][0]          \n",
      "                                                                 bug_centroid_feature_master_neg[0\n",
      "__________________________________________________________________________________________________\n",
      "pos_master_pos_cosine_distance  (1,)                 0           merge_features_pos[0][0]         \n",
      "                                                                 bug_centroid_feature_master_pos[0\n",
      "__________________________________________________________________________________________________\n",
      "neg_master_pos_cosine_distance  (1,)                 0           merge_features_pos[0][0]         \n",
      "                                                                 bug_centroid_feature_master_neg[0\n",
      "__________________________________________________________________________________________________\n",
      "pos_master_neg_cosine_distance  (1,)                 0           merge_features_neg[0][0]         \n",
      "                                                                 bug_centroid_feature_master_neg[0\n",
      "__________________________________________________________________________________________________\n",
      "neg_master_neg_cosine_distance  (1,)                 0           merge_features_neg[0][0]         \n",
      "                                                                 bug_centroid_feature_master_pos[0\n",
      "__________________________________________________________________________________________________\n",
      "triplet_pos_neg (Lambda)        (1,)                 0           pos_cosine_distance[0][0]        \n",
      "                                                                 neg_cosine_distance[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "triplet_anchor_centroid (Lambda (1,)                 0           pos_master_cosine_distance[0][0] \n",
      "                                                                 neg_master_cosine_distance[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "triplet_pos_centroid (Lambda)   (1,)                 0           pos_master_pos_cosine_distance[0]\n",
      "                                                                 neg_master_pos_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "triplet_neg_centroid (Lambda)   (1,)                 0           pos_master_neg_cosine_distance[0]\n",
      "                                                                 neg_master_neg_cosine_distance[0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (1, 1)               0           triplet_pos_neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (1, 1)               0           triplet_anchor_centroid[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (1, 1)               0           triplet_pos_centroid[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (1, 1)               0           triplet_neg_centroid[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (1, 1)               1           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (1, 1)               1           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1, 1)               1           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, 1)               1           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (1, 1)               0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (1, 1)               0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Weighted_Median_Loss (Lambda)   (1, 1)               0           add_2[0][0]                      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (1, 5)               0           Weighted_Median_Loss[0][0]       \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,198,376\n",
      "Trainable params: 726,200\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.38, TL: 1.02, TL_A: 0.97, TL_P: 0.96, TL_N: 0.97\n",
      "Epoch: 2 Loss: 0.38, TL: 1.02, TL_A: 0.97, TL_P: 0.96, TL_N: 0.97\n",
      "Epoch: 3 Loss: 0.38, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 4 Loss: 0.38, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 5 Loss: 0.38, TL: 1.01, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 6 Loss: 0.38, TL: 1.01, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 7 Loss: 0.37, TL: 1.02, TL_A: 0.97, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 8 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 9 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 10 Loss: 0.37, TL: 1.01, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 11 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 12 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 13 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 14 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 15 Loss: 0.37, TL: 1.03, TL_A: 0.95, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 16 Loss: 0.37, TL: 1.02, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 17 Loss: 0.36, TL: 1.03, TL_A: 0.96, TL_P: 0.94, TL_N: 0.95\n",
      "Epoch: 18 Loss: 0.36, TL: 1.03, TL_A: 0.95, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 19 Loss: 0.36, TL: 1.02, TL_A: 0.95, TL_P: 0.93, TL_N: 0.95\n",
      "Epoch: 20 Loss: 0.36, TL: 1.03, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 21 Loss: 0.36, TL: 1.03, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 22 Loss: 0.36, TL: 1.03, TL_A: 0.95, TL_P: 0.94, TL_N: 0.97\n",
      "Epoch: 23 Loss: 0.36, TL: 1.02, TL_A: 0.95, TL_P: 0.93, TL_N: 0.96\n",
      "Epoch: 24 Loss: 0.36, TL: 1.03, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 25 Loss: 0.35, TL: 1.04, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 26 Loss: 0.35, TL: 1.03, TL_A: 0.96, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 27 Loss: 0.35, TL: 1.03, TL_A: 0.96, TL_P: 0.95, TL_N: 0.96\n",
      "Epoch: 28 Loss: 0.35, TL: 1.02, TL_A: 0.96, TL_P: 0.94, TL_N: 0.95\n",
      "Epoch: 29 Loss: 0.35, TL: 1.04, TL_A: 0.96, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 30 Loss: 0.35, TL: 1.04, TL_A: 0.95, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 31 Loss: 0.35, TL: 1.04, TL_A: 0.96, TL_P: 0.94, TL_N: 0.97\n",
      "Epoch: 32 Loss: 0.35, TL: 1.04, TL_A: 0.96, TL_P: 0.94, TL_N: 0.97\n",
      "Epoch: 33 Loss: 0.35, TL: 1.04, TL_A: 0.97, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 34 Loss: 0.35, TL: 1.05, TL_A: 0.97, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 35 Loss: 0.34, TL: 1.05, TL_A: 0.97, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 36 Loss: 0.35, TL: 1.03, TL_A: 0.97, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 37 Loss: 0.34, TL: 1.04, TL_A: 0.97, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 38 Loss: 0.34, TL: 1.05, TL_A: 0.98, TL_P: 0.94, TL_N: 0.97\n",
      "Epoch: 39 Loss: 0.34, TL: 1.04, TL_A: 0.97, TL_P: 0.96, TL_N: 0.97\n",
      "Epoch: 40 Loss: 0.34, TL: 1.04, TL_A: 0.98, TL_P: 0.97, TL_N: 0.98\n",
      "Epoch: 41 Loss: 0.34, TL: 1.05, TL_A: 0.98, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 42 Loss: 0.34, TL: 1.04, TL_A: 0.98, TL_P: 0.97, TL_N: 0.98\n",
      "Epoch: 43 Loss: 0.34, TL: 1.06, TL_A: 0.97, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 44 Loss: 0.33, TL: 1.07, TL_A: 0.98, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 45 Loss: 0.33, TL: 1.06, TL_A: 0.98, TL_P: 0.96, TL_N: 0.97\n",
      "Epoch: 46 Loss: 0.33, TL: 1.10, TL_A: 0.98, TL_P: 0.95, TL_N: 0.98\n",
      "Epoch: 47 Loss: 0.33, TL: 1.08, TL_A: 0.96, TL_P: 0.92, TL_N: 0.97\n",
      "Epoch: 48 Loss: 0.33, TL: 1.08, TL_A: 0.95, TL_P: 0.92, TL_N: 0.97\n",
      "Epoch: 49 Loss: 0.32, TL: 1.12, TL_A: 0.95, TL_P: 0.90, TL_N: 0.95\n",
      "Epoch: 50 Loss: 0.33, TL: 1.13, TL_A: 0.95, TL_P: 0.87, TL_N: 0.97\n",
      "Epoch: 51 Loss: 0.31, TL: 1.15, TL_A: 0.98, TL_P: 0.90, TL_N: 0.94\n",
      "Epoch: 52 Loss: 0.32, TL: 1.14, TL_A: 0.96, TL_P: 0.88, TL_N: 0.96\n",
      "Epoch: 53 Loss: 0.31, TL: 1.16, TL_A: 0.95, TL_P: 0.87, TL_N: 0.93\n",
      "Epoch: 54 Loss: 0.31, TL: 1.16, TL_A: 0.95, TL_P: 0.89, TL_N: 0.95\n",
      "Epoch: 55 Loss: 0.31, TL: 1.14, TL_A: 0.95, TL_P: 0.87, TL_N: 0.92\n",
      "Epoch: 56 Loss: 0.31, TL: 1.16, TL_A: 0.94, TL_P: 0.90, TL_N: 0.96\n",
      "Epoch: 57 Loss: 0.31, TL: 1.16, TL_A: 0.96, TL_P: 0.89, TL_N: 0.96\n",
      "Epoch: 58 Loss: 0.31, TL: 1.18, TL_A: 0.99, TL_P: 0.86, TL_N: 0.93\n",
      "Epoch: 59 Loss: 0.31, TL: 1.14, TL_A: 0.97, TL_P: 0.88, TL_N: 0.95\n",
      "Epoch: 60 Loss: 0.31, TL: 1.19, TL_A: 0.95, TL_P: 0.83, TL_N: 0.93\n",
      "Epoch: 61 Loss: 0.31, TL: 1.14, TL_A: 0.96, TL_P: 0.89, TL_N: 0.96\n",
      "Epoch: 62 Loss: 0.30, TL: 1.22, TL_A: 0.98, TL_P: 0.85, TL_N: 0.94\n",
      "Epoch: 63 Loss: 0.29, TL: 1.21, TL_A: 0.97, TL_P: 0.93, TL_N: 0.97\n",
      "Epoch: 64 Loss: 0.31, TL: 1.18, TL_A: 0.97, TL_P: 0.88, TL_N: 0.97\n",
      "Epoch: 65 Loss: 0.30, TL: 1.20, TL_A: 1.00, TL_P: 0.84, TL_N: 0.94\n",
      "Epoch: 66 Loss: 0.30, TL: 1.29, TL_A: 0.97, TL_P: 0.86, TL_N: 0.99\n",
      "Epoch: 67 Loss: 0.28, TL: 1.26, TL_A: 1.03, TL_P: 0.86, TL_N: 0.92\n",
      "Epoch: 68 Loss: 0.28, TL: 1.24, TL_A: 1.02, TL_P: 0.83, TL_N: 0.90\n",
      "Epoch: 69 Loss: 0.28, TL: 1.24, TL_A: 1.04, TL_P: 0.81, TL_N: 0.89\n",
      "Epoch: 70 Loss: 0.27, TL: 1.22, TL_A: 1.01, TL_P: 0.93, TL_N: 0.93\n",
      "Epoch: 71 Loss: 0.30, TL: 1.22, TL_A: 0.97, TL_P: 0.87, TL_N: 0.98\n",
      "Epoch: 72 Loss: 0.32, TL: 1.11, TL_A: 0.98, TL_P: 0.87, TL_N: 0.99\n",
      "Epoch: 73 Loss: 0.28, TL: 1.22, TL_A: 1.04, TL_P: 0.86, TL_N: 0.91\n",
      "Epoch: 74 Loss: 0.33, TL: 1.21, TL_A: 0.88, TL_P: 0.85, TL_N: 1.07\n",
      "Epoch: 75 Loss: 0.28, TL: 1.31, TL_A: 1.01, TL_P: 0.79, TL_N: 0.94\n",
      "Epoch: 76 Loss: 0.30, TL: 1.25, TL_A: 0.91, TL_P: 0.90, TL_N: 1.05\n",
      "Epoch: 77 Loss: 0.26, TL: 1.31, TL_A: 1.03, TL_P: 0.85, TL_N: 0.92\n",
      "Epoch: 78 Loss: 0.28, TL: 1.22, TL_A: 1.01, TL_P: 0.83, TL_N: 0.93\n",
      "Epoch: 79 Loss: 0.27, TL: 1.41, TL_A: 0.92, TL_P: 0.86, TL_N: 1.01\n",
      "Epoch: 80 Loss: 0.29, TL: 1.28, TL_A: 0.98, TL_P: 0.81, TL_N: 0.97\n",
      "Epoch: 81 Loss: 0.30, TL: 1.17, TL_A: 0.98, TL_P: 0.85, TL_N: 0.97\n",
      "Epoch: 82 Loss: 0.30, TL: 1.26, TL_A: 0.95, TL_P: 0.75, TL_N: 0.98\n",
      "Epoch: 83 Loss: 0.30, TL: 1.27, TL_A: 0.97, TL_P: 0.78, TL_N: 0.98\n",
      "Epoch: 84 Loss: 0.31, TL: 1.27, TL_A: 0.94, TL_P: 0.76, TL_N: 1.01\n",
      "Epoch: 85 Loss: 0.30, TL: 1.21, TL_A: 0.93, TL_P: 0.85, TL_N: 1.02\n",
      "Epoch: 86 Loss: 0.28, TL: 1.31, TL_A: 1.00, TL_P: 0.81, TL_N: 0.98\n",
      "Epoch: 87 Loss: 0.30, TL: 1.32, TL_A: 0.89, TL_P: 0.78, TL_N: 1.05\n",
      "Epoch: 88 Loss: 0.29, TL: 1.31, TL_A: 0.98, TL_P: 0.77, TL_N: 1.00\n",
      "Epoch: 89 Loss: 0.27, TL: 1.25, TL_A: 1.01, TL_P: 0.84, TL_N: 0.94\n",
      "Epoch: 90 Loss: 0.24, TL: 1.30, TL_A: 1.06, TL_P: 0.87, TL_N: 0.89\n",
      "Epoch: 91 Loss: 0.28, TL: 1.30, TL_A: 0.98, TL_P: 0.80, TL_N: 0.97\n",
      "Epoch: 92 Loss: 0.26, TL: 1.26, TL_A: 0.98, TL_P: 0.79, TL_N: 0.91\n",
      "Epoch: 93 Loss: 0.27, TL: 1.33, TL_A: 1.00, TL_P: 0.76, TL_N: 0.95\n",
      "Epoch: 94 Loss: 0.28, TL: 1.34, TL_A: 0.89, TL_P: 0.81, TL_N: 1.05\n",
      "Epoch: 95 Loss: 0.27, TL: 1.29, TL_A: 0.98, TL_P: 0.84, TL_N: 0.98\n",
      "Epoch: 96 Loss: 0.27, TL: 1.30, TL_A: 0.97, TL_P: 0.83, TL_N: 1.00\n",
      "Epoch: 97 Loss: 0.27, TL: 1.35, TL_A: 0.93, TL_P: 0.89, TL_N: 1.05\n",
      "Epoch: 98 Loss: 0.28, TL: 1.23, TL_A: 0.98, TL_P: 0.79, TL_N: 0.96\n",
      "Epoch: 99 Loss: 0.26, TL: 1.18, TL_A: 1.00, TL_P: 0.91, TL_N: 0.95\n",
      "Epoch: 100 Loss: 0.28, TL: 1.24, TL_A: 0.98, TL_P: 0.84, TL_N: 1.00\n",
      "Epoch: 101 Loss: 0.24, TL: 1.34, TL_A: 1.06, TL_P: 0.75, TL_N: 0.88\n",
      "Epoch: 102 Loss: 0.28, TL: 1.22, TL_A: 0.97, TL_P: 0.79, TL_N: 0.99\n",
      "Epoch: 103 Loss: 0.26, TL: 1.32, TL_A: 1.00, TL_P: 0.76, TL_N: 0.95\n",
      "Epoch: 104 Loss: 0.26, TL: 1.33, TL_A: 0.95, TL_P: 0.82, TL_N: 0.98\n",
      "Epoch: 105 Loss: 0.29, TL: 1.26, TL_A: 0.91, TL_P: 0.78, TL_N: 1.05\n",
      "Epoch: 106 Loss: 0.26, TL: 1.34, TL_A: 1.01, TL_P: 0.74, TL_N: 0.95\n",
      "Epoch: 107 Loss: 0.23, TL: 1.24, TL_A: 1.12, TL_P: 0.80, TL_N: 0.82\n",
      "Epoch: 108 Loss: 0.29, TL: 1.39, TL_A: 0.87, TL_P: 0.73, TL_N: 1.08\n",
      "Epoch: 109 Loss: 0.24, TL: 1.39, TL_A: 1.00, TL_P: 0.77, TL_N: 0.96\n",
      "Epoch: 110 Loss: 0.24, TL: 1.26, TL_A: 1.02, TL_P: 0.86, TL_N: 0.94\n",
      "Epoch: 111 Loss: 0.28, TL: 1.32, TL_A: 0.94, TL_P: 0.77, TL_N: 1.05\n",
      "Epoch: 112 Loss: 0.22, TL: 1.27, TL_A: 1.11, TL_P: 0.80, TL_N: 0.84\n",
      "Epoch: 113 Loss: 0.22, TL: 1.28, TL_A: 1.04, TL_P: 0.90, TL_N: 0.92\n",
      "Epoch: 114 Loss: 0.25, TL: 1.28, TL_A: 1.05, TL_P: 0.77, TL_N: 0.93\n",
      "Epoch: 115 Loss: 0.21, TL: 1.40, TL_A: 1.06, TL_P: 0.80, TL_N: 0.89\n",
      "Epoch: 116 Loss: 0.23, TL: 1.28, TL_A: 0.99, TL_P: 0.87, TL_N: 0.94\n",
      "Epoch: 117 Loss: 0.22, TL: 1.35, TL_A: 1.02, TL_P: 0.84, TL_N: 0.93\n",
      "Epoch: 118 Loss: 0.26, TL: 1.32, TL_A: 0.98, TL_P: 0.76, TL_N: 0.99\n",
      "Epoch: 119 Loss: 0.22, TL: 1.30, TL_A: 1.06, TL_P: 0.80, TL_N: 0.89\n",
      "Epoch: 120 Loss: 0.24, TL: 1.26, TL_A: 1.00, TL_P: 0.84, TL_N: 0.96\n",
      "Epoch: 121 Loss: 0.26, TL: 1.38, TL_A: 0.88, TL_P: 0.80, TL_N: 1.08\n",
      "Epoch: 122 Loss: 0.23, TL: 1.34, TL_A: 1.01, TL_P: 0.81, TL_N: 0.94\n",
      "Epoch: 123 Loss: 0.27, TL: 1.22, TL_A: 0.93, TL_P: 0.87, TL_N: 1.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 Loss: 0.22, TL: 1.27, TL_A: 0.96, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 125 Loss: 0.23, TL: 1.35, TL_A: 0.97, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 126 Loss: 0.24, TL: 1.30, TL_A: 0.97, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 127 Loss: 0.23, TL: 1.32, TL_A: 1.00, TL_P: 0.82, TL_N: 0.97\n",
      "Epoch: 128 Loss: 0.20, TL: 1.28, TL_A: 1.08, TL_P: 0.85, TL_N: 0.88\n",
      "Epoch: 129 Loss: 0.22, TL: 1.38, TL_A: 1.00, TL_P: 0.80, TL_N: 0.96\n",
      "Epoch: 130 Loss: 0.21, TL: 1.41, TL_A: 1.00, TL_P: 0.84, TL_N: 0.97\n",
      "Epoch: 131 Loss: 0.23, TL: 1.30, TL_A: 0.99, TL_P: 0.78, TL_N: 0.95\n",
      "Epoch: 132 Loss: 0.20, TL: 1.40, TL_A: 1.04, TL_P: 0.84, TL_N: 0.92\n",
      "Epoch: 133 Loss: 0.27, TL: 1.30, TL_A: 0.84, TL_P: 0.82, TL_N: 1.11\n",
      "Epoch: 134 Loss: 0.23, TL: 1.31, TL_A: 1.00, TL_P: 0.85, TL_N: 0.98\n",
      "Epoch: 135 Loss: 0.25, TL: 1.30, TL_A: 0.93, TL_P: 0.83, TL_N: 1.05\n",
      "Epoch: 136 Loss: 0.24, TL: 1.26, TL_A: 1.01, TL_P: 0.81, TL_N: 0.99\n",
      "Epoch: 137 Loss: 0.24, TL: 1.32, TL_A: 0.90, TL_P: 0.84, TL_N: 1.04\n",
      "Epoch: 138 Loss: 0.24, TL: 1.31, TL_A: 0.95, TL_P: 0.80, TL_N: 1.02\n",
      "Epoch: 139 Loss: 0.22, TL: 1.33, TL_A: 0.96, TL_P: 0.85, TL_N: 1.00\n",
      "Epoch: 140 Loss: 0.22, TL: 1.18, TL_A: 1.02, TL_P: 0.90, TL_N: 0.95\n",
      "Epoch: 141 Loss: 0.22, TL: 1.28, TL_A: 0.95, TL_P: 0.90, TL_N: 1.02\n",
      "Epoch: 142 Loss: 0.21, TL: 1.24, TL_A: 1.01, TL_P: 0.90, TL_N: 0.94\n",
      "Epoch: 143 Loss: 0.21, TL: 1.29, TL_A: 0.98, TL_P: 0.87, TL_N: 0.97\n",
      "Epoch: 144 Loss: 0.22, TL: 1.26, TL_A: 0.95, TL_P: 0.92, TL_N: 1.00\n",
      "Epoch: 145 Loss: 0.20, TL: 1.31, TL_A: 1.08, TL_P: 0.77, TL_N: 0.89\n",
      "Epoch: 146 Loss: 0.22, TL: 1.38, TL_A: 1.00, TL_P: 0.68, TL_N: 0.94\n",
      "Epoch: 147 Loss: 0.23, TL: 1.31, TL_A: 0.96, TL_P: 0.83, TL_N: 1.03\n",
      "Epoch: 148 Loss: 0.24, TL: 1.31, TL_A: 0.93, TL_P: 0.77, TL_N: 1.05\n",
      "Epoch: 149 Loss: 0.20, TL: 1.35, TL_A: 1.05, TL_P: 0.74, TL_N: 0.91\n",
      "Epoch: 150 Loss: 0.26, TL: 1.32, TL_A: 0.89, TL_P: 0.75, TL_N: 1.09\n",
      "Epoch: 151 Loss: 0.23, TL: 1.36, TL_A: 0.95, TL_P: 0.75, TL_N: 1.01\n",
      "Epoch: 152 Loss: 0.22, TL: 1.23, TL_A: 1.01, TL_P: 0.79, TL_N: 0.94\n",
      "Epoch: 153 Loss: 0.21, TL: 1.33, TL_A: 1.03, TL_P: 0.71, TL_N: 0.93\n",
      "Epoch: 154 Loss: 0.21, TL: 1.27, TL_A: 1.00, TL_P: 0.81, TL_N: 0.95\n",
      "Epoch: 155 Loss: 0.17, TL: 1.40, TL_A: 1.02, TL_P: 0.88, TL_N: 0.93\n",
      "Epoch: 156 Loss: 0.19, TL: 1.29, TL_A: 1.07, TL_P: 0.79, TL_N: 0.89\n",
      "Epoch: 157 Loss: 0.19, TL: 1.30, TL_A: 1.01, TL_P: 0.84, TL_N: 0.92\n",
      "Epoch: 158 Loss: 0.23, TL: 1.19, TL_A: 0.95, TL_P: 0.86, TL_N: 1.02\n",
      "Epoch: 159 Loss: 0.18, TL: 1.31, TL_A: 0.98, TL_P: 0.83, TL_N: 0.93\n",
      "Epoch: 160 Loss: 0.21, TL: 1.34, TL_A: 0.98, TL_P: 0.77, TL_N: 0.99\n",
      "Epoch: 161 Loss: 0.18, TL: 1.36, TL_A: 1.09, TL_P: 0.78, TL_N: 0.89\n",
      "Epoch: 162 Loss: 0.18, TL: 1.35, TL_A: 0.99, TL_P: 0.89, TL_N: 0.97\n",
      "Epoch: 163 Loss: 0.21, TL: 1.31, TL_A: 0.91, TL_P: 0.91, TL_N: 1.06\n",
      "Epoch: 164 Loss: 0.20, TL: 1.31, TL_A: 1.01, TL_P: 0.77, TL_N: 0.96\n",
      "Epoch: 165 Loss: 0.20, TL: 1.33, TL_A: 0.93, TL_P: 0.88, TL_N: 1.04\n",
      "Epoch: 166 Loss: 0.23, TL: 1.23, TL_A: 0.93, TL_P: 0.85, TL_N: 1.06\n",
      "Epoch: 167 Loss: 0.21, TL: 1.23, TL_A: 0.98, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 168 Loss: 0.20, TL: 1.32, TL_A: 0.99, TL_P: 0.85, TL_N: 1.01\n",
      "Epoch: 169 Loss: 0.20, TL: 1.34, TL_A: 0.96, TL_P: 0.80, TL_N: 1.01\n",
      "Epoch: 170 Loss: 0.16, TL: 1.31, TL_A: 1.05, TL_P: 0.92, TL_N: 0.91\n",
      "Epoch: 171 Loss: 0.18, TL: 1.27, TL_A: 0.95, TL_P: 0.90, TL_N: 0.98\n",
      "Epoch: 172 Loss: 0.16, TL: 1.35, TL_A: 1.04, TL_P: 0.91, TL_N: 0.93\n",
      "Epoch: 173 Loss: 0.16, TL: 1.29, TL_A: 1.04, TL_P: 0.88, TL_N: 0.92\n",
      "Epoch: 174 Loss: 0.19, TL: 1.10, TL_A: 1.04, TL_P: 0.95, TL_N: 0.94\n",
      "Epoch: 175 Loss: 0.19, TL: 1.20, TL_A: 0.97, TL_P: 0.94, TL_N: 0.99\n",
      "Epoch: 176 Loss: 0.17, TL: 1.29, TL_A: 0.97, TL_P: 0.94, TL_N: 0.99\n",
      "Epoch: 177 Loss: 0.19, TL: 1.14, TL_A: 1.00, TL_P: 0.94, TL_N: 0.97\n",
      "Epoch: 178 Loss: 0.18, TL: 1.28, TL_A: 1.03, TL_P: 0.85, TL_N: 0.96\n",
      "Epoch: 179 Loss: 0.19, TL: 1.18, TL_A: 1.00, TL_P: 0.95, TL_N: 0.99\n",
      "Epoch: 180 Loss: 0.18, TL: 1.15, TL_A: 0.97, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 181 Loss: 0.23, TL: 1.07, TL_A: 0.91, TL_P: 0.94, TL_N: 1.07\n",
      "Epoch: 182 Loss: 0.19, TL: 1.11, TL_A: 0.99, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 183 Loss: 0.20, TL: 1.03, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 184 Loss: 0.20, TL: 1.05, TL_A: 1.00, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 185 Loss: 0.20, TL: 1.05, TL_A: 0.99, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 186 Loss: 0.19, TL: 1.05, TL_A: 0.99, TL_P: 1.01, TL_N: 0.99\n",
      "Epoch: 187 Loss: 0.20, TL: 1.03, TL_A: 0.98, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 188 Loss: 0.19, TL: 1.03, TL_A: 1.00, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 189 Loss: 0.20, TL: 1.02, TL_A: 0.98, TL_P: 0.99, TL_N: 1.01\n",
      "Epoch: 190 Loss: 0.20, TL: 1.05, TL_A: 1.01, TL_P: 0.96, TL_N: 0.99\n",
      "Epoch: 191 Loss: 0.19, TL: 1.03, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 192 Loss: 0.20, TL: 0.97, TL_A: 0.99, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 193 Loss: 0.19, TL: 1.02, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 194 Loss: 0.18, TL: 1.06, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 195 Loss: 0.18, TL: 1.05, TL_A: 0.98, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 196 Loss: 0.19, TL: 1.02, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 197 Loss: 0.19, TL: 1.02, TL_A: 1.01, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 198 Loss: 0.20, TL: 0.99, TL_A: 0.97, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 199 Loss: 0.19, TL: 1.05, TL_A: 0.99, TL_P: 0.95, TL_N: 1.00\n",
      "Epoch: 200 Loss: 0.18, TL: 1.03, TL_A: 1.00, TL_P: 0.99, TL_N: 0.98\n",
      "Epoch: 201 Loss: 0.17, TL: 1.05, TL_A: 1.01, TL_P: 0.95, TL_N: 0.95\n",
      "Epoch: 202 Loss: 0.18, TL: 1.07, TL_A: 0.98, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 203 Loss: 0.17, TL: 1.09, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 204 Loss: 0.18, TL: 1.00, TL_A: 1.00, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 205 Loss: 0.18, TL: 1.05, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 206 Loss: 0.17, TL: 1.08, TL_A: 0.99, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 207 Loss: 0.18, TL: 1.06, TL_A: 0.97, TL_P: 0.94, TL_N: 0.99\n",
      "Epoch: 208 Loss: 0.18, TL: 1.02, TL_A: 1.02, TL_P: 0.96, TL_N: 0.99\n",
      "Epoch: 209 Loss: 0.16, TL: 1.13, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 210 Loss: 0.17, TL: 1.08, TL_A: 0.99, TL_P: 0.99, TL_N: 1.02\n",
      "Epoch: 211 Loss: 0.18, TL: 1.09, TL_A: 0.99, TL_P: 0.95, TL_N: 1.02\n",
      "Epoch: 212 Loss: 0.16, TL: 1.09, TL_A: 1.00, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 213 Loss: 0.16, TL: 1.08, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 214 Loss: 0.18, TL: 1.16, TL_A: 0.97, TL_P: 0.89, TL_N: 1.04\n",
      "Epoch: 215 Loss: 0.16, TL: 1.19, TL_A: 0.97, TL_P: 0.90, TL_N: 1.00\n",
      "Epoch: 216 Loss: 0.15, TL: 1.15, TL_A: 1.01, TL_P: 0.94, TL_N: 0.96\n",
      "Epoch: 217 Loss: 0.13, TL: 1.30, TL_A: 0.97, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 218 Loss: 0.12, TL: 1.24, TL_A: 1.03, TL_P: 0.93, TL_N: 0.93\n",
      "Epoch: 219 Loss: 0.15, TL: 1.17, TL_A: 0.97, TL_P: 0.99, TL_N: 1.02\n",
      "Epoch: 220 Loss: 0.19, TL: 1.18, TL_A: 0.99, TL_P: 0.81, TL_N: 1.06\n",
      "Epoch: 221 Loss: 0.15, TL: 1.26, TL_A: 0.99, TL_P: 0.93, TL_N: 1.03\n",
      "Epoch: 222 Loss: 0.13, TL: 1.28, TL_A: 0.96, TL_P: 0.93, TL_N: 1.00\n",
      "Epoch: 223 Loss: 0.13, TL: 1.21, TL_A: 0.99, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 224 Loss: 0.13, TL: 1.18, TL_A: 1.06, TL_P: 0.87, TL_N: 0.91\n",
      "Epoch: 225 Loss: 0.16, TL: 1.13, TL_A: 0.96, TL_P: 0.90, TL_N: 1.00\n",
      "Epoch: 226 Loss: 0.14, TL: 1.18, TL_A: 0.98, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 227 Loss: 0.15, TL: 1.05, TL_A: 1.00, TL_P: 0.99, TL_N: 0.98\n",
      "Epoch: 228 Loss: 0.14, TL: 1.18, TL_A: 1.05, TL_P: 0.92, TL_N: 0.97\n",
      "Epoch: 229 Loss: 0.15, TL: 1.09, TL_A: 1.01, TL_P: 0.90, TL_N: 0.95\n",
      "Epoch: 230 Loss: 0.15, TL: 1.14, TL_A: 0.98, TL_P: 0.92, TL_N: 1.00\n",
      "Epoch: 231 Loss: 0.15, TL: 1.12, TL_A: 1.02, TL_P: 0.95, TL_N: 1.01\n",
      "Epoch: 232 Loss: 0.13, TL: 1.12, TL_A: 1.01, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 233 Loss: 0.15, TL: 1.16, TL_A: 0.98, TL_P: 0.93, TL_N: 1.02\n",
      "Epoch: 234 Loss: 0.15, TL: 1.08, TL_A: 0.99, TL_P: 0.99, TL_N: 1.01\n",
      "Epoch: 235 Loss: 0.15, TL: 1.05, TL_A: 0.98, TL_P: 0.94, TL_N: 0.98\n",
      "Epoch: 236 Loss: 0.13, TL: 1.16, TL_A: 0.97, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 237 Loss: 0.14, TL: 1.10, TL_A: 0.97, TL_P: 0.99, TL_N: 1.03\n",
      "Epoch: 238 Loss: 0.13, TL: 1.09, TL_A: 0.98, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 239 Loss: 0.13, TL: 1.13, TL_A: 1.03, TL_P: 0.97, TL_N: 0.98\n",
      "Epoch: 240 Loss: 0.15, TL: 1.07, TL_A: 0.97, TL_P: 0.95, TL_N: 1.01\n",
      "Epoch: 241 Loss: 0.14, TL: 1.07, TL_A: 0.98, TL_P: 0.94, TL_N: 0.98\n",
      "Epoch: 242 Loss: 0.13, TL: 1.10, TL_A: 0.98, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 243 Loss: 0.14, TL: 1.04, TL_A: 1.00, TL_P: 0.95, TL_N: 0.98\n",
      "Epoch: 244 Loss: 0.13, TL: 1.05, TL_A: 0.98, TL_P: 0.99, TL_N: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 Loss: 0.13, TL: 1.11, TL_A: 0.99, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 246 Loss: 0.13, TL: 1.10, TL_A: 0.99, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 247 Loss: 0.14, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 248 Loss: 0.13, TL: 1.08, TL_A: 0.99, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 249 Loss: 0.13, TL: 1.08, TL_A: 1.02, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 250 Loss: 0.13, TL: 1.08, TL_A: 0.98, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 251 Loss: 0.13, TL: 1.06, TL_A: 0.98, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 252 Loss: 0.13, TL: 1.06, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 253 Loss: 0.12, TL: 1.09, TL_A: 0.98, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 254 Loss: 0.12, TL: 1.07, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 255 Loss: 0.12, TL: 1.10, TL_A: 0.99, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 256 Loss: 0.11, TL: 1.08, TL_A: 1.03, TL_P: 0.98, TL_N: 0.97\n",
      "Epoch: 257 Loss: 0.10, TL: 1.17, TL_A: 1.03, TL_P: 0.93, TL_N: 0.95\n",
      "Epoch: 258 Loss: 0.11, TL: 1.14, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 259 Loss: 0.12, TL: 1.08, TL_A: 1.00, TL_P: 0.99, TL_N: 0.99\n",
      "Epoch: 260 Loss: 0.13, TL: 1.05, TL_A: 0.95, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 261 Loss: 0.11, TL: 1.14, TL_A: 0.99, TL_P: 0.92, TL_N: 0.99\n",
      "Epoch: 262 Loss: 0.10, TL: 1.08, TL_A: 1.01, TL_P: 0.97, TL_N: 0.95\n",
      "Epoch: 263 Loss: 0.12, TL: 1.05, TL_A: 0.99, TL_P: 0.95, TL_N: 0.99\n",
      "Epoch: 264 Loss: 0.11, TL: 1.18, TL_A: 0.98, TL_P: 0.89, TL_N: 1.00\n",
      "Epoch: 265 Loss: 0.11, TL: 1.06, TL_A: 0.97, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 266 Loss: 0.10, TL: 1.10, TL_A: 1.02, TL_P: 0.97, TL_N: 0.96\n",
      "Epoch: 267 Loss: 0.11, TL: 1.10, TL_A: 0.98, TL_P: 0.98, TL_N: 1.01\n",
      "Epoch: 268 Loss: 0.09, TL: 1.17, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 269 Loss: 0.11, TL: 1.06, TL_A: 0.99, TL_P: 0.96, TL_N: 0.98\n",
      "Epoch: 270 Loss: 0.10, TL: 1.13, TL_A: 0.99, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 271 Loss: 0.11, TL: 1.09, TL_A: 0.96, TL_P: 0.92, TL_N: 0.99\n",
      "Epoch: 272 Loss: 0.09, TL: 1.12, TL_A: 1.00, TL_P: 0.98, TL_N: 0.97\n",
      "Epoch: 273 Loss: 0.12, TL: 1.14, TL_A: 0.97, TL_P: 0.89, TL_N: 1.02\n",
      "Epoch: 274 Loss: 0.07, TL: 1.23, TL_A: 1.00, TL_P: 0.95, TL_N: 0.95\n",
      "Epoch: 275 Loss: 0.08, TL: 1.17, TL_A: 1.03, TL_P: 0.91, TL_N: 0.94\n",
      "Epoch: 276 Loss: 0.08, TL: 1.29, TL_A: 0.92, TL_P: 0.99, TL_N: 1.06\n",
      "Epoch: 277 Loss: 0.07, TL: 1.27, TL_A: 1.05, TL_P: 0.90, TL_N: 0.95\n",
      "Epoch: 278 Loss: 0.11, TL: 1.22, TL_A: 0.96, TL_P: 0.76, TL_N: 0.99\n",
      "Epoch: 279 Loss: 0.08, TL: 1.27, TL_A: 1.00, TL_P: 0.90, TL_N: 0.98\n",
      "Epoch: 280 Loss: 0.09, TL: 1.16, TL_A: 0.99, TL_P: 0.85, TL_N: 0.94\n",
      "Epoch: 281 Loss: 0.08, TL: 1.28, TL_A: 1.01, TL_P: 0.80, TL_N: 0.96\n",
      "Epoch: 282 Loss: 0.09, TL: 1.20, TL_A: 0.92, TL_P: 0.88, TL_N: 1.00\n",
      "Epoch: 283 Loss: 0.09, TL: 1.21, TL_A: 0.96, TL_P: 0.88, TL_N: 0.99\n",
      "Epoch: 284 Loss: 0.09, TL: 1.31, TL_A: 0.99, TL_P: 0.75, TL_N: 0.97\n",
      "Epoch: 285 Loss: 0.07, TL: 1.26, TL_A: 0.99, TL_P: 0.82, TL_N: 0.93\n",
      "Epoch: 286 Loss: 0.08, TL: 1.30, TL_A: 0.99, TL_P: 0.77, TL_N: 0.94\n",
      "Epoch: 287 Loss: 0.09, TL: 1.28, TL_A: 0.95, TL_P: 0.87, TL_N: 1.03\n",
      "Epoch: 288 Loss: 0.07, TL: 1.36, TL_A: 1.04, TL_P: 0.74, TL_N: 0.94\n",
      "Epoch: 289 Loss: 0.10, TL: 1.34, TL_A: 0.88, TL_P: 0.77, TL_N: 1.07\n",
      "Epoch: 290 Loss: 0.07, TL: 1.31, TL_A: 0.98, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 291 Loss: 0.10, TL: 1.27, TL_A: 0.96, TL_P: 0.82, TL_N: 1.05\n",
      "Epoch: 292 Loss: 0.06, TL: 1.22, TL_A: 0.99, TL_P: 0.87, TL_N: 0.92\n",
      "Epoch: 293 Loss: 0.07, TL: 1.28, TL_A: 1.01, TL_P: 0.79, TL_N: 0.96\n",
      "Epoch: 294 Loss: 0.11, TL: 1.34, TL_A: 0.87, TL_P: 0.75, TL_N: 1.10\n",
      "Epoch: 295 Loss: 0.08, TL: 1.29, TL_A: 0.95, TL_P: 0.83, TL_N: 1.02\n",
      "Epoch: 296 Loss: 0.06, TL: 1.34, TL_A: 0.96, TL_P: 0.86, TL_N: 0.99\n",
      "Epoch: 297 Loss: 0.05, TL: 1.42, TL_A: 1.01, TL_P: 0.69, TL_N: 0.92\n",
      "Epoch: 298 Loss: 0.02, TL: 1.44, TL_A: 1.08, TL_P: 0.76, TL_N: 0.87\n",
      "Epoch: 299 Loss: 0.05, TL: 1.30, TL_A: 0.97, TL_P: 0.89, TL_N: 0.97\n",
      "Epoch: 300 Loss: 0.06, TL: 1.36, TL_A: 0.99, TL_P: 0.78, TL_N: 0.97\n",
      "Epoch: 301 Loss: 0.09, TL: 1.32, TL_A: 0.89, TL_P: 0.80, TL_N: 1.07\n",
      "Epoch: 302 Loss: 0.07, TL: 1.35, TL_A: 0.90, TL_P: 0.84, TL_N: 1.03\n",
      "Epoch: 303 Loss: 0.04, TL: 1.42, TL_A: 1.06, TL_P: 0.74, TL_N: 0.91\n",
      "Epoch: 304 Loss: 0.05, TL: 1.37, TL_A: 0.99, TL_P: 0.84, TL_N: 1.00\n",
      "Epoch: 305 Loss: 0.02, TL: 1.39, TL_A: 1.02, TL_P: 0.87, TL_N: 0.93\n",
      "Epoch: 306 Loss: 0.09, TL: 1.33, TL_A: 0.91, TL_P: 0.72, TL_N: 1.04\n",
      "Epoch: 307 Loss: 0.06, TL: 1.31, TL_A: 0.99, TL_P: 0.80, TL_N: 0.97\n",
      "Epoch: 308 Loss: 0.07, TL: 1.28, TL_A: 0.98, TL_P: 0.86, TL_N: 1.02\n",
      "Epoch: 309 Loss: 0.04, TL: 1.35, TL_A: 1.00, TL_P: 0.86, TL_N: 0.97\n",
      "Epoch: 310 Loss: 0.06, TL: 1.26, TL_A: 0.98, TL_P: 0.86, TL_N: 1.01\n",
      "Epoch: 311 Loss: 0.07, TL: 1.30, TL_A: 0.87, TL_P: 0.86, TL_N: 1.06\n",
      "Epoch: 312 Loss: 0.05, TL: 1.35, TL_A: 1.03, TL_P: 0.71, TL_N: 0.94\n",
      "Epoch: 313 Loss: 0.05, TL: 1.25, TL_A: 0.99, TL_P: 0.84, TL_N: 0.95\n",
      "Epoch: 314 Loss: 0.06, TL: 1.29, TL_A: 0.89, TL_P: 0.91, TL_N: 1.06\n",
      "Epoch: 315 Loss: 0.02, TL: 1.37, TL_A: 0.96, TL_P: 0.88, TL_N: 0.96\n",
      "Epoch: 316 Loss: 0.03, TL: 1.30, TL_A: 1.03, TL_P: 0.83, TL_N: 0.91\n",
      "Epoch: 317 Loss: 0.03, TL: 1.31, TL_A: 1.08, TL_P: 0.78, TL_N: 0.88\n",
      "Epoch: 318 Loss: 0.04, TL: 1.34, TL_A: 1.05, TL_P: 0.71, TL_N: 0.91\n",
      "Epoch: 319 Loss: 0.06, TL: 1.24, TL_A: 0.98, TL_P: 0.82, TL_N: 0.99\n",
      "Epoch: 320 Loss: 0.05, TL: 1.31, TL_A: 1.01, TL_P: 0.71, TL_N: 0.93\n",
      "Epoch: 321 Loss: 0.05, TL: 1.34, TL_A: 0.92, TL_P: 0.80, TL_N: 1.03\n",
      "Epoch: 322 Loss: 0.04, TL: 1.33, TL_A: 1.01, TL_P: 0.74, TL_N: 0.93\n",
      "Epoch: 323 Loss: 0.04, TL: 1.44, TL_A: 0.96, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 324 Loss: 0.05, TL: 1.32, TL_A: 0.92, TL_P: 0.80, TL_N: 1.00\n",
      "Epoch: 325 Loss: 0.06, TL: 1.36, TL_A: 0.86, TL_P: 0.81, TL_N: 1.09\n",
      "Epoch: 326 Loss: 0.04, TL: 1.35, TL_A: 0.98, TL_P: 0.77, TL_N: 0.99\n",
      "Epoch: 327 Loss: 0.04, TL: 1.34, TL_A: 0.96, TL_P: 0.78, TL_N: 0.99\n",
      "Epoch: 328 Loss: 0.05, TL: 1.38, TL_A: 0.91, TL_P: 0.80, TL_N: 1.07\n",
      "Epoch: 329 Loss: 0.05, TL: 1.25, TL_A: 0.92, TL_P: 0.85, TL_N: 1.03\n",
      "Epoch: 330 Loss: 0.06, TL: 1.31, TL_A: 0.94, TL_P: 0.84, TL_N: 1.06\n",
      "Epoch: 331 Loss: 0.03, TL: 1.34, TL_A: 0.92, TL_P: 0.89, TL_N: 1.03\n",
      "Epoch: 332 Loss: 0.03, TL: 1.37, TL_A: 1.01, TL_P: 0.80, TL_N: 1.00\n",
      "Epoch: 333 Loss: 0.02, TL: 1.25, TL_A: 0.99, TL_P: 0.86, TL_N: 0.94\n",
      "Epoch: 334 Loss: 0.03, TL: 1.22, TL_A: 1.02, TL_P: 0.85, TL_N: 0.96\n",
      "Epoch: 335 Loss: 0.02, TL: 1.29, TL_A: 1.03, TL_P: 0.81, TL_N: 0.95\n",
      "Epoch: 336 Loss: 0.01, TL: 1.32, TL_A: 1.06, TL_P: 0.86, TL_N: 0.95\n",
      "Epoch: 337 Loss: 0.04, TL: 1.30, TL_A: 0.92, TL_P: 0.93, TL_N: 1.08\n",
      "Epoch: 338 Loss: 0.04, TL: 1.26, TL_A: 0.97, TL_P: 0.90, TL_N: 1.03\n",
      "Epoch: 339 Loss: 0.02, TL: 1.30, TL_A: 0.99, TL_P: 0.86, TL_N: 0.96\n",
      "Epoch: 340 Loss: 0.03, TL: 1.28, TL_A: 0.93, TL_P: 0.84, TL_N: 1.01\n",
      "Epoch: 341 Loss: 0.01, TL: 1.31, TL_A: 1.02, TL_P: 0.78, TL_N: 0.90\n",
      "Epoch: 342 Loss: 0.02, TL: 1.24, TL_A: 0.99, TL_P: 0.86, TL_N: 0.95\n",
      "Epoch: 343 Loss: 0.00, TL: 1.30, TL_A: 1.07, TL_P: 0.81, TL_N: 0.91\n",
      "Epoch: 344 Loss: 0.04, TL: 1.19, TL_A: 0.91, TL_P: 0.81, TL_N: 0.98\n",
      "Epoch: 345 Loss: 0.01, TL: 1.29, TL_A: 1.04, TL_P: 0.82, TL_N: 0.93\n",
      "Epoch: 346 Loss: 0.04, TL: 1.31, TL_A: 0.96, TL_P: 0.76, TL_N: 1.01\n",
      "Epoch: 347 Loss: 0.02, TL: 1.21, TL_A: 1.01, TL_P: 0.88, TL_N: 0.98\n",
      "Epoch: 348 Loss: 0.01, TL: 1.28, TL_A: 0.98, TL_P: 0.87, TL_N: 0.98\n",
      "Epoch: 349 Loss: -0.00, TL: 1.30, TL_A: 1.01, TL_P: 0.89, TL_N: 0.96\n",
      "Epoch: 350 Loss: 0.05, TL: 1.28, TL_A: 0.90, TL_P: 0.84, TL_N: 1.10\n",
      "Epoch: 351 Loss: -0.02, TL: 1.43, TL_A: 1.02, TL_P: 0.84, TL_N: 0.95\n",
      "Epoch: 352 Loss: 0.02, TL: 1.35, TL_A: 0.96, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 353 Loss: 0.03, TL: 1.29, TL_A: 0.89, TL_P: 0.87, TL_N: 1.07\n",
      "Epoch: 354 Loss: 0.03, TL: 1.28, TL_A: 0.94, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 355 Loss: 0.01, TL: 1.35, TL_A: 1.01, TL_P: 0.73, TL_N: 0.96\n",
      "Epoch: 356 Loss: -0.01, TL: 1.41, TL_A: 0.96, TL_P: 0.86, TL_N: 1.02\n",
      "Epoch: 357 Loss: 0.02, TL: 1.37, TL_A: 0.94, TL_P: 0.79, TL_N: 1.03\n",
      "Epoch: 358 Loss: 0.00, TL: 1.38, TL_A: 1.01, TL_P: 0.76, TL_N: 0.96\n",
      "Epoch: 359 Loss: -0.00, TL: 1.28, TL_A: 1.05, TL_P: 0.79, TL_N: 0.92\n",
      "Epoch: 360 Loss: -0.02, TL: 1.38, TL_A: 1.05, TL_P: 0.74, TL_N: 0.90\n",
      "Epoch: 361 Loss: 0.03, TL: 1.29, TL_A: 0.89, TL_P: 0.86, TL_N: 1.09\n",
      "Epoch: 362 Loss: -0.03, TL: 1.49, TL_A: 1.03, TL_P: 0.76, TL_N: 0.94\n",
      "Epoch: 363 Loss: -0.00, TL: 1.31, TL_A: 1.04, TL_P: 0.77, TL_N: 0.94\n",
      "Epoch: 364 Loss: -0.02, TL: 1.39, TL_A: 1.06, TL_P: 0.71, TL_N: 0.90\n",
      "Epoch: 365 Loss: 0.01, TL: 1.33, TL_A: 0.93, TL_P: 0.79, TL_N: 1.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366 Loss: -0.02, TL: 1.24, TL_A: 0.95, TL_P: 0.97, TL_N: 0.98\n",
      "Epoch: 367 Loss: -0.03, TL: 1.31, TL_A: 1.04, TL_P: 0.88, TL_N: 0.93\n",
      "Epoch: 368 Loss: -0.01, TL: 1.31, TL_A: 1.04, TL_P: 0.78, TL_N: 0.94\n",
      "Epoch: 369 Loss: 0.02, TL: 1.25, TL_A: 0.93, TL_P: 0.88, TL_N: 1.07\n",
      "Epoch: 370 Loss: -0.01, TL: 1.30, TL_A: 1.07, TL_P: 0.80, TL_N: 0.93\n",
      "Epoch: 371 Loss: -0.03, TL: 1.34, TL_A: 0.96, TL_P: 0.80, TL_N: 0.93\n",
      "Epoch: 372 Loss: -0.05, TL: 1.43, TL_A: 1.08, TL_P: 0.76, TL_N: 0.86\n",
      "Epoch: 373 Loss: 0.01, TL: 1.29, TL_A: 0.93, TL_P: 0.77, TL_N: 1.02\n",
      "Epoch: 374 Loss: 0.01, TL: 1.32, TL_A: 1.00, TL_P: 0.76, TL_N: 1.00\n",
      "Epoch: 375 Loss: -0.06, TL: 1.38, TL_A: 1.11, TL_P: 0.78, TL_N: 0.83\n",
      "Epoch: 376 Loss: -0.03, TL: 1.35, TL_A: 1.10, TL_P: 0.70, TL_N: 0.88\n",
      "Epoch: 377 Loss: -0.03, TL: 1.28, TL_A: 0.97, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 378 Loss: -0.00, TL: 1.22, TL_A: 1.03, TL_P: 0.77, TL_N: 0.93\n",
      "Epoch: 379 Loss: -0.00, TL: 1.25, TL_A: 0.90, TL_P: 0.91, TL_N: 1.06\n",
      "Epoch: 380 Loss: -0.03, TL: 1.23, TL_A: 1.07, TL_P: 0.83, TL_N: 0.88\n",
      "Epoch: 381 Loss: 0.01, TL: 1.19, TL_A: 0.98, TL_P: 0.80, TL_N: 1.00\n",
      "Epoch: 382 Loss: -0.02, TL: 1.27, TL_A: 1.04, TL_P: 0.79, TL_N: 0.94\n",
      "Epoch: 383 Loss: -0.00, TL: 1.29, TL_A: 0.92, TL_P: 0.85, TL_N: 1.04\n",
      "Epoch: 384 Loss: -0.01, TL: 1.28, TL_A: 0.94, TL_P: 0.86, TL_N: 1.01\n",
      "Epoch: 385 Loss: -0.02, TL: 1.24, TL_A: 0.97, TL_P: 0.86, TL_N: 0.98\n",
      "Epoch: 386 Loss: -0.03, TL: 1.29, TL_A: 1.01, TL_P: 0.79, TL_N: 0.93\n",
      "Epoch: 387 Loss: -0.03, TL: 1.24, TL_A: 1.08, TL_P: 0.81, TL_N: 0.91\n",
      "Epoch: 388 Loss: 0.00, TL: 1.16, TL_A: 0.97, TL_P: 0.85, TL_N: 1.00\n",
      "Epoch: 389 Loss: -0.02, TL: 1.28, TL_A: 1.00, TL_P: 0.80, TL_N: 0.96\n",
      "Epoch: 390 Loss: -0.03, TL: 1.24, TL_A: 1.04, TL_P: 0.81, TL_N: 0.90\n",
      "Epoch: 391 Loss: -0.01, TL: 1.28, TL_A: 1.00, TL_P: 0.76, TL_N: 0.98\n",
      "Epoch: 392 Loss: 0.01, TL: 1.30, TL_A: 0.85, TL_P: 0.82, TL_N: 1.10\n",
      "Epoch: 393 Loss: -0.03, TL: 1.34, TL_A: 0.99, TL_P: 0.77, TL_N: 0.97\n",
      "Epoch: 394 Loss: -0.04, TL: 1.35, TL_A: 1.02, TL_P: 0.85, TL_N: 0.96\n",
      "Epoch: 395 Loss: -0.02, TL: 1.44, TL_A: 0.93, TL_P: 0.74, TL_N: 1.03\n",
      "Epoch: 396 Loss: -0.05, TL: 1.43, TL_A: 1.03, TL_P: 0.77, TL_N: 0.94\n",
      "Epoch: 397 Loss: -0.02, TL: 1.30, TL_A: 0.99, TL_P: 0.76, TL_N: 0.97\n",
      "Epoch: 398 Loss: -0.08, TL: 1.43, TL_A: 1.05, TL_P: 0.89, TL_N: 0.93\n",
      "Epoch: 399 Loss: -0.04, TL: 1.35, TL_A: 1.06, TL_P: 0.63, TL_N: 0.88\n",
      "Epoch: 400 Loss: 0.04, TL: 1.16, TL_A: 0.87, TL_P: 0.73, TL_N: 1.09\n",
      "Epoch: 401 Loss: -0.02, TL: 1.24, TL_A: 0.88, TL_P: 0.93, TL_N: 1.06\n",
      "Epoch: 402 Loss: -0.04, TL: 1.36, TL_A: 1.01, TL_P: 0.78, TL_N: 0.97\n",
      "Epoch: 403 Loss: -0.03, TL: 1.30, TL_A: 1.00, TL_P: 0.81, TL_N: 0.99\n",
      "Epoch: 404 Loss: -0.04, TL: 1.31, TL_A: 0.99, TL_P: 0.78, TL_N: 0.96\n",
      "Epoch: 405 Loss: -0.04, TL: 1.26, TL_A: 0.97, TL_P: 0.87, TL_N: 0.99\n",
      "Epoch: 406 Loss: -0.03, TL: 1.33, TL_A: 0.94, TL_P: 0.89, TL_N: 1.07\n",
      "Epoch: 407 Loss: -0.06, TL: 1.43, TL_A: 0.98, TL_P: 0.80, TL_N: 0.97\n",
      "Epoch: 408 Loss: -0.04, TL: 1.24, TL_A: 0.99, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 409 Loss: -0.03, TL: 1.37, TL_A: 0.90, TL_P: 0.82, TL_N: 1.06\n",
      "Epoch: 410 Loss: -0.04, TL: 1.22, TL_A: 1.03, TL_P: 0.77, TL_N: 0.92\n",
      "Epoch: 411 Loss: -0.06, TL: 1.31, TL_A: 1.11, TL_P: 0.72, TL_N: 0.86\n",
      "Epoch: 412 Loss: -0.06, TL: 1.29, TL_A: 1.03, TL_P: 0.85, TL_N: 0.93\n",
      "Epoch: 413 Loss: -0.08, TL: 1.30, TL_A: 1.04, TL_P: 0.80, TL_N: 0.88\n",
      "Epoch: 414 Loss: -0.03, TL: 1.25, TL_A: 1.00, TL_P: 0.81, TL_N: 0.99\n",
      "Epoch: 415 Loss: -0.08, TL: 1.33, TL_A: 1.12, TL_P: 0.78, TL_N: 0.86\n",
      "Epoch: 416 Loss: -0.09, TL: 1.26, TL_A: 1.09, TL_P: 0.92, TL_N: 0.88\n",
      "Epoch: 417 Loss: -0.05, TL: 1.31, TL_A: 0.95, TL_P: 0.85, TL_N: 1.02\n",
      "Epoch: 418 Loss: -0.04, TL: 1.37, TL_A: 0.96, TL_P: 0.75, TL_N: 1.02\n",
      "Epoch: 419 Loss: -0.07, TL: 1.27, TL_A: 1.07, TL_P: 0.84, TL_N: 0.90\n",
      "Epoch: 420 Loss: -0.07, TL: 1.31, TL_A: 1.00, TL_P: 0.90, TL_N: 0.98\n",
      "Epoch: 421 Loss: -0.07, TL: 1.37, TL_A: 1.00, TL_P: 0.80, TL_N: 0.97\n",
      "Epoch: 422 Loss: -0.03, TL: 1.33, TL_A: 0.90, TL_P: 0.74, TL_N: 1.06\n",
      "Epoch: 423 Loss: -0.06, TL: 1.25, TL_A: 1.01, TL_P: 0.87, TL_N: 0.98\n",
      "Epoch: 424 Loss: -0.08, TL: 1.33, TL_A: 1.03, TL_P: 0.75, TL_N: 0.90\n",
      "Epoch: 425 Loss: -0.07, TL: 1.31, TL_A: 0.95, TL_P: 0.94, TL_N: 1.02\n",
      "Epoch: 426 Loss: -0.08, TL: 1.40, TL_A: 1.00, TL_P: 0.82, TL_N: 0.97\n",
      "Epoch: 427 Loss: -0.05, TL: 1.30, TL_A: 0.97, TL_P: 0.82, TL_N: 1.01\n",
      "Epoch: 428 Loss: -0.08, TL: 1.42, TL_A: 0.93, TL_P: 0.89, TL_N: 1.02\n",
      "Epoch: 429 Loss: -0.05, TL: 1.25, TL_A: 0.99, TL_P: 0.81, TL_N: 0.99\n",
      "Epoch: 430 Loss: -0.06, TL: 1.27, TL_A: 0.95, TL_P: 0.91, TL_N: 1.02\n",
      "Epoch: 431 Loss: -0.05, TL: 1.27, TL_A: 0.90, TL_P: 0.94, TL_N: 1.08\n",
      "Epoch: 432 Loss: -0.06, TL: 1.34, TL_A: 0.98, TL_P: 0.77, TL_N: 1.00\n",
      "Epoch: 433 Loss: -0.04, TL: 1.27, TL_A: 0.97, TL_P: 0.79, TL_N: 1.05\n",
      "Epoch: 434 Loss: -0.08, TL: 1.25, TL_A: 1.02, TL_P: 0.89, TL_N: 0.96\n",
      "Epoch: 435 Loss: -0.09, TL: 1.38, TL_A: 1.06, TL_P: 0.79, TL_N: 0.92\n",
      "Epoch: 436 Loss: -0.03, TL: 1.24, TL_A: 0.95, TL_P: 0.75, TL_N: 1.02\n",
      "Epoch: 437 Loss: -0.09, TL: 1.31, TL_A: 0.98, TL_P: 0.89, TL_N: 0.96\n",
      "Epoch: 438 Loss: -0.11, TL: 1.40, TL_A: 1.00, TL_P: 0.95, TL_N: 0.97\n",
      "Epoch: 439 Loss: -0.07, TL: 1.27, TL_A: 1.01, TL_P: 0.79, TL_N: 0.95\n",
      "Epoch: 440 Loss: -0.12, TL: 1.38, TL_A: 1.03, TL_P: 0.85, TL_N: 0.89\n",
      "Epoch: 441 Loss: -0.06, TL: 1.27, TL_A: 0.97, TL_P: 0.78, TL_N: 0.99\n",
      "Epoch: 442 Loss: -0.11, TL: 1.39, TL_A: 1.07, TL_P: 0.77, TL_N: 0.87\n",
      "Epoch: 443 Loss: -0.11, TL: 1.34, TL_A: 1.06, TL_P: 0.87, TL_N: 0.91\n",
      "Epoch: 444 Loss: -0.07, TL: 1.36, TL_A: 0.95, TL_P: 0.79, TL_N: 1.03\n",
      "Epoch: 445 Loss: -0.10, TL: 1.26, TL_A: 1.04, TL_P: 0.86, TL_N: 0.92\n",
      "Epoch: 446 Loss: -0.05, TL: 1.23, TL_A: 0.99, TL_P: 0.79, TL_N: 1.01\n",
      "Epoch: 447 Loss: -0.08, TL: 1.39, TL_A: 0.96, TL_P: 0.70, TL_N: 0.96\n",
      "Epoch: 448 Loss: -0.12, TL: 1.41, TL_A: 1.06, TL_P: 0.84, TL_N: 0.91\n",
      "Epoch: 449 Loss: -0.10, TL: 1.39, TL_A: 0.94, TL_P: 0.92, TL_N: 1.04\n",
      "Epoch: 450 Loss: -0.05, TL: 1.23, TL_A: 0.90, TL_P: 0.80, TL_N: 1.04\n",
      "Epoch: 451 Loss: -0.11, TL: 1.37, TL_A: 1.07, TL_P: 0.78, TL_N: 0.91\n",
      "Epoch: 452 Loss: -0.05, TL: 1.33, TL_A: 0.90, TL_P: 0.73, TL_N: 1.07\n",
      "Epoch: 453 Loss: -0.08, TL: 1.37, TL_A: 0.98, TL_P: 0.72, TL_N: 1.00\n",
      "Epoch: 454 Loss: -0.13, TL: 1.44, TL_A: 1.07, TL_P: 0.81, TL_N: 0.89\n",
      "Epoch: 455 Loss: -0.08, TL: 1.29, TL_A: 0.97, TL_P: 0.88, TL_N: 1.03\n",
      "Epoch: 456 Loss: -0.08, TL: 1.34, TL_A: 1.02, TL_P: 0.73, TL_N: 0.97\n",
      "Epoch: 457 Loss: -0.09, TL: 1.33, TL_A: 1.00, TL_P: 0.80, TL_N: 0.98\n",
      "Epoch: 458 Loss: -0.08, TL: 1.32, TL_A: 1.00, TL_P: 0.80, TL_N: 1.01\n",
      "Epoch: 459 Loss: -0.11, TL: 1.36, TL_A: 0.96, TL_P: 0.83, TL_N: 0.97\n",
      "Epoch: 460 Loss: -0.10, TL: 1.24, TL_A: 1.01, TL_P: 0.88, TL_N: 0.97\n",
      "Epoch: 461 Loss: -0.11, TL: 1.28, TL_A: 1.02, TL_P: 0.88, TL_N: 0.94\n",
      "Epoch: 462 Loss: -0.09, TL: 1.27, TL_A: 1.00, TL_P: 0.77, TL_N: 0.95\n",
      "Epoch: 463 Loss: -0.09, TL: 1.23, TL_A: 1.01, TL_P: 0.74, TL_N: 0.93\n",
      "Epoch: 464 Loss: -0.11, TL: 1.37, TL_A: 0.99, TL_P: 0.85, TL_N: 0.99\n",
      "Epoch: 465 Loss: -0.10, TL: 1.24, TL_A: 1.02, TL_P: 0.85, TL_N: 0.95\n",
      "Epoch: 466 Loss: -0.09, TL: 1.30, TL_A: 1.03, TL_P: 0.71, TL_N: 0.94\n",
      "Epoch: 467 Loss: -0.11, TL: 1.27, TL_A: 0.98, TL_P: 0.89, TL_N: 0.99\n",
      "Epoch: 468 Loss: -0.08, TL: 1.25, TL_A: 0.96, TL_P: 0.82, TL_N: 1.02\n",
      "Epoch: 469 Loss: -0.11, TL: 1.32, TL_A: 1.01, TL_P: 0.86, TL_N: 1.00\n",
      "Epoch: 470 Loss: -0.14, TL: 1.36, TL_A: 1.05, TL_P: 0.86, TL_N: 0.90\n",
      "Epoch: 471 Loss: -0.10, TL: 1.23, TL_A: 0.98, TL_P: 0.82, TL_N: 0.96\n",
      "Epoch: 472 Loss: -0.11, TL: 1.29, TL_A: 0.97, TL_P: 0.78, TL_N: 0.95\n",
      "Epoch: 473 Loss: -0.09, TL: 1.28, TL_A: 0.96, TL_P: 0.81, TL_N: 1.03\n",
      "Epoch: 474 Loss: -0.11, TL: 1.27, TL_A: 1.05, TL_P: 0.84, TL_N: 0.95\n",
      "Epoch: 475 Loss: -0.10, TL: 1.34, TL_A: 0.90, TL_P: 0.86, TL_N: 1.06\n",
      "Epoch: 476 Loss: -0.11, TL: 1.34, TL_A: 1.06, TL_P: 0.78, TL_N: 0.96\n",
      "Epoch: 477 Loss: -0.09, TL: 1.33, TL_A: 0.89, TL_P: 0.84, TL_N: 1.08\n",
      "Epoch: 478 Loss: -0.10, TL: 1.24, TL_A: 0.99, TL_P: 0.87, TL_N: 1.02\n",
      "Epoch: 479 Loss: -0.11, TL: 1.33, TL_A: 0.97, TL_P: 0.80, TL_N: 0.99\n",
      "Epoch: 480 Loss: -0.12, TL: 1.38, TL_A: 1.00, TL_P: 0.79, TL_N: 0.99\n",
      "Epoch: 481 Loss: -0.08, TL: 1.36, TL_A: 0.87, TL_P: 0.74, TL_N: 1.10\n",
      "Epoch: 482 Loss: -0.13, TL: 1.34, TL_A: 1.07, TL_P: 0.79, TL_N: 0.92\n",
      "Epoch: 483 Loss: -0.16, TL: 1.27, TL_A: 1.06, TL_P: 0.80, TL_N: 0.82\n",
      "Epoch: 484 Loss: -0.12, TL: 1.33, TL_A: 1.00, TL_P: 0.82, TL_N: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 Loss: -0.13, TL: 1.21, TL_A: 1.00, TL_P: 0.89, TL_N: 0.95\n",
      "Epoch: 486 Loss: -0.11, TL: 1.38, TL_A: 0.94, TL_P: 0.80, TL_N: 1.05\n",
      "Epoch: 487 Loss: -0.15, TL: 1.38, TL_A: 1.05, TL_P: 0.86, TL_N: 0.94\n",
      "Epoch: 488 Loss: -0.12, TL: 1.27, TL_A: 1.03, TL_P: 0.77, TL_N: 0.96\n",
      "Epoch: 489 Loss: -0.13, TL: 1.36, TL_A: 0.98, TL_P: 0.80, TL_N: 0.96\n",
      "Epoch: 490 Loss: -0.12, TL: 1.44, TL_A: 0.91, TL_P: 0.73, TL_N: 1.03\n",
      "Epoch: 491 Loss: -0.12, TL: 1.30, TL_A: 0.95, TL_P: 0.89, TL_N: 1.03\n",
      "Epoch: 492 Loss: -0.06, TL: 1.31, TL_A: 0.83, TL_P: 0.72, TL_N: 1.16\n",
      "Epoch: 493 Loss: -0.12, TL: 1.36, TL_A: 0.97, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 494 Loss: -0.14, TL: 1.37, TL_A: 0.96, TL_P: 0.90, TL_N: 1.03\n",
      "Epoch: 495 Loss: -0.16, TL: 1.42, TL_A: 1.03, TL_P: 0.85, TL_N: 0.94\n",
      "Epoch: 496 Loss: -0.10, TL: 1.24, TL_A: 0.94, TL_P: 0.74, TL_N: 1.00\n",
      "Epoch: 497 Loss: -0.17, TL: 1.35, TL_A: 1.05, TL_P: 0.87, TL_N: 0.91\n",
      "Epoch: 498 Loss: -0.13, TL: 1.40, TL_A: 0.97, TL_P: 0.75, TL_N: 1.00\n",
      "Epoch: 499 Loss: -0.12, TL: 1.25, TL_A: 0.96, TL_P: 0.87, TL_N: 1.01\n",
      "Epoch: 500 Loss: -0.11, TL: 1.35, TL_A: 0.82, TL_P: 0.83, TL_N: 1.12\n",
      "Epoch: 501 Loss: -0.14, TL: 1.26, TL_A: 1.01, TL_P: 0.82, TL_N: 0.94\n",
      "Epoch: 502 Loss: -0.13, TL: 1.24, TL_A: 0.97, TL_P: 0.88, TL_N: 1.01\n",
      "Epoch: 503 Loss: -0.13, TL: 1.31, TL_A: 0.93, TL_P: 0.78, TL_N: 0.99\n",
      "Epoch: 504 Loss: -0.15, TL: 1.26, TL_A: 0.99, TL_P: 0.80, TL_N: 0.90\n",
      "Epoch: 505 Loss: -0.15, TL: 1.31, TL_A: 0.99, TL_P: 0.88, TL_N: 0.99\n",
      "Epoch: 506 Loss: -0.17, TL: 1.27, TL_A: 1.05, TL_P: 0.88, TL_N: 0.90\n",
      "Epoch: 507 Loss: -0.15, TL: 1.22, TL_A: 1.03, TL_P: 0.88, TL_N: 0.94\n",
      "Epoch: 508 Loss: -0.13, TL: 1.19, TL_A: 0.93, TL_P: 0.92, TL_N: 1.04\n",
      "Epoch: 509 Loss: -0.16, TL: 1.31, TL_A: 1.00, TL_P: 0.87, TL_N: 0.97\n",
      "Epoch: 510 Loss: -0.14, TL: 1.22, TL_A: 0.97, TL_P: 0.87, TL_N: 0.96\n",
      "Epoch: 511 Loss: -0.17, TL: 1.23, TL_A: 1.06, TL_P: 0.84, TL_N: 0.87\n",
      "Epoch: 512 Loss: -0.16, TL: 1.25, TL_A: 1.02, TL_P: 0.84, TL_N: 0.93\n",
      "Epoch: 513 Loss: -0.14, TL: 1.23, TL_A: 1.03, TL_P: 0.76, TL_N: 0.92\n",
      "Epoch: 514 Loss: -0.11, TL: 1.19, TL_A: 0.94, TL_P: 0.89, TL_N: 1.09\n",
      "Epoch: 515 Loss: -0.13, TL: 1.16, TL_A: 0.96, TL_P: 0.84, TL_N: 0.98\n",
      "Epoch: 516 Loss: -0.17, TL: 1.26, TL_A: 1.09, TL_P: 0.87, TL_N: 0.91\n",
      "Epoch: 517 Loss: -0.12, TL: 1.27, TL_A: 0.87, TL_P: 0.79, TL_N: 1.06\n",
      "Epoch: 518 Loss: -0.16, TL: 1.34, TL_A: 0.96, TL_P: 0.82, TL_N: 0.99\n",
      "Epoch: 519 Loss: -0.15, TL: 1.38, TL_A: 0.96, TL_P: 0.84, TL_N: 1.04\n",
      "Epoch: 520 Loss: -0.15, TL: 1.27, TL_A: 0.98, TL_P: 0.79, TL_N: 0.97\n",
      "Epoch: 521 Loss: -0.15, TL: 1.36, TL_A: 0.95, TL_P: 0.79, TL_N: 1.03\n",
      "Epoch: 522 Loss: -0.18, TL: 1.29, TL_A: 1.03, TL_P: 0.82, TL_N: 0.90\n",
      "Epoch: 523 Loss: -0.17, TL: 1.34, TL_A: 1.02, TL_P: 0.79, TL_N: 0.94\n",
      "Epoch: 524 Loss: -0.14, TL: 1.34, TL_A: 0.95, TL_P: 0.76, TL_N: 1.03\n",
      "Epoch: 525 Loss: -0.14, TL: 1.25, TL_A: 0.98, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 526 Loss: -0.17, TL: 1.23, TL_A: 0.99, TL_P: 0.85, TL_N: 0.94\n",
      "Epoch: 527 Loss: -0.17, TL: 1.34, TL_A: 1.02, TL_P: 0.71, TL_N: 0.90\n",
      "Epoch: 528 Loss: -0.13, TL: 1.32, TL_A: 0.90, TL_P: 0.78, TL_N: 1.08\n",
      "Epoch: 529 Loss: -0.16, TL: 1.27, TL_A: 0.98, TL_P: 0.82, TL_N: 0.98\n",
      "Epoch: 530 Loss: -0.15, TL: 1.39, TL_A: 0.97, TL_P: 0.77, TL_N: 1.04\n",
      "Epoch: 531 Loss: -0.17, TL: 1.33, TL_A: 1.01, TL_P: 0.76, TL_N: 0.94\n",
      "Epoch: 532 Loss: -0.17, TL: 1.27, TL_A: 0.99, TL_P: 0.86, TL_N: 0.97\n",
      "Epoch: 533 Loss: -0.18, TL: 1.42, TL_A: 1.01, TL_P: 0.78, TL_N: 0.99\n",
      "Epoch: 534 Loss: -0.17, TL: 1.35, TL_A: 0.96, TL_P: 0.82, TL_N: 1.02\n",
      "Epoch: 535 Loss: -0.15, TL: 1.46, TL_A: 0.88, TL_P: 0.73, TL_N: 1.08\n",
      "Epoch: 536 Loss: -0.19, TL: 1.37, TL_A: 1.00, TL_P: 0.88, TL_N: 0.98\n",
      "Epoch: 537 Loss: -0.17, TL: 1.21, TL_A: 1.02, TL_P: 0.84, TL_N: 0.96\n",
      "Epoch: 538 Loss: -0.12, TL: 1.23, TL_A: 0.96, TL_P: 0.70, TL_N: 1.03\n",
      "Epoch: 539 Loss: -0.15, TL: 1.16, TL_A: 0.98, TL_P: 0.84, TL_N: 0.99\n",
      "Epoch: 540 Loss: -0.18, TL: 1.31, TL_A: 0.96, TL_P: 0.81, TL_N: 0.97\n",
      "Epoch: 541 Loss: -0.19, TL: 1.31, TL_A: 1.10, TL_P: 0.81, TL_N: 0.93\n",
      "Epoch: 542 Loss: -0.20, TL: 1.38, TL_A: 1.05, TL_P: 0.83, TL_N: 0.93\n",
      "Epoch: 543 Loss: -0.19, TL: 1.33, TL_A: 1.01, TL_P: 0.77, TL_N: 0.92\n",
      "Epoch: 544 Loss: -0.14, TL: 1.28, TL_A: 0.90, TL_P: 0.75, TL_N: 1.05\n",
      "Epoch: 545 Loss: -0.22, TL: 1.42, TL_A: 1.03, TL_P: 0.86, TL_N: 0.92\n",
      "Epoch: 546 Loss: -0.19, TL: 1.35, TL_A: 1.03, TL_P: 0.78, TL_N: 0.96\n",
      "Epoch: 547 Loss: -0.21, TL: 1.36, TL_A: 1.00, TL_P: 0.94, TL_N: 0.98\n",
      "Epoch: 548 Loss: -0.18, TL: 1.29, TL_A: 1.00, TL_P: 0.80, TL_N: 0.98\n",
      "Epoch: 549 Loss: -0.18, TL: 1.34, TL_A: 0.96, TL_P: 0.81, TL_N: 1.02\n",
      "Epoch: 550 Loss: -0.21, TL: 1.39, TL_A: 1.02, TL_P: 0.88, TL_N: 0.97\n",
      "Epoch: 551 Loss: -0.17, TL: 1.32, TL_A: 0.99, TL_P: 0.78, TL_N: 1.01\n",
      "Epoch: 552 Loss: -0.15, TL: 1.33, TL_A: 0.83, TL_P: 0.84, TL_N: 1.14\n",
      "Epoch: 553 Loss: -0.18, TL: 1.32, TL_A: 1.03, TL_P: 0.71, TL_N: 0.95\n",
      "Epoch: 554 Loss: -0.21, TL: 1.33, TL_A: 1.05, TL_P: 0.85, TL_N: 0.93\n",
      "Epoch: 555 Loss: -0.20, TL: 1.30, TL_A: 0.97, TL_P: 0.91, TL_N: 1.01\n",
      "Epoch: 556 Loss: -0.19, TL: 1.30, TL_A: 1.00, TL_P: 0.81, TL_N: 0.98\n",
      "Epoch: 557 Loss: -0.18, TL: 1.35, TL_A: 0.96, TL_P: 0.75, TL_N: 0.99\n",
      "Epoch: 558 Loss: -0.18, TL: 1.32, TL_A: 1.00, TL_P: 0.72, TL_N: 0.97\n",
      "Epoch: 559 Loss: -0.17, TL: 1.29, TL_A: 1.07, TL_P: 0.70, TL_N: 0.98\n",
      "Epoch: 560 Loss: -0.18, TL: 1.36, TL_A: 0.87, TL_P: 0.85, TL_N: 1.08\n",
      "Epoch: 561 Loss: -0.20, TL: 1.29, TL_A: 0.92, TL_P: 0.81, TL_N: 0.95\n",
      "Epoch: 562 Loss: -0.18, TL: 1.29, TL_A: 0.93, TL_P: 0.83, TL_N: 1.03\n",
      "Epoch: 563 Loss: -0.20, TL: 1.29, TL_A: 1.02, TL_P: 0.81, TL_N: 0.96\n",
      "Epoch: 564 Loss: -0.19, TL: 1.35, TL_A: 0.97, TL_P: 0.79, TL_N: 1.01\n",
      "Epoch: 565 Loss: -0.22, TL: 1.41, TL_A: 0.98, TL_P: 0.88, TL_N: 0.99\n",
      "Epoch: 566 Loss: -0.17, TL: 1.33, TL_A: 0.92, TL_P: 0.81, TL_N: 1.09\n",
      "Epoch: 567 Loss: -0.23, TL: 1.36, TL_A: 1.02, TL_P: 0.81, TL_N: 0.93\n",
      "Epoch: 568 Loss: -0.21, TL: 1.20, TL_A: 0.98, TL_P: 0.91, TL_N: 0.97\n",
      "Epoch: 569 Loss: -0.21, TL: 1.32, TL_A: 0.89, TL_P: 0.91, TL_N: 1.04\n",
      "Epoch: 570 Loss: -0.21, TL: 1.29, TL_A: 1.05, TL_P: 0.85, TL_N: 0.97\n",
      "Epoch: 571 Loss: -0.22, TL: 1.24, TL_A: 1.04, TL_P: 0.87, TL_N: 0.92\n",
      "Epoch: 572 Loss: -0.19, TL: 1.26, TL_A: 0.91, TL_P: 0.85, TL_N: 1.05\n",
      "Epoch: 573 Loss: -0.19, TL: 1.35, TL_A: 0.85, TL_P: 0.95, TL_N: 1.14\n",
      "Epoch: 574 Loss: -0.19, TL: 1.32, TL_A: 0.96, TL_P: 0.85, TL_N: 1.05\n",
      "Epoch: 575 Loss: -0.21, TL: 1.29, TL_A: 1.03, TL_P: 0.81, TL_N: 0.96\n",
      "Epoch: 576 Loss: -0.18, TL: 1.30, TL_A: 0.90, TL_P: 0.84, TL_N: 1.11\n",
      "Epoch: 577 Loss: -0.17, TL: 1.21, TL_A: 0.94, TL_P: 0.86, TL_N: 1.08\n",
      "Epoch: 578 Loss: -0.25, TL: 1.39, TL_A: 1.03, TL_P: 0.87, TL_N: 0.94\n",
      "Epoch: 579 Loss: -0.20, TL: 1.19, TL_A: 0.89, TL_P: 0.89, TL_N: 1.02\n",
      "Epoch: 580 Loss: -0.25, TL: 1.35, TL_A: 1.00, TL_P: 0.90, TL_N: 0.94\n",
      "Epoch: 581 Loss: -0.22, TL: 1.22, TL_A: 0.96, TL_P: 0.87, TL_N: 0.97\n",
      "Epoch: 582 Loss: -0.18, TL: 1.22, TL_A: 1.00, TL_P: 0.80, TL_N: 1.03\n",
      "Epoch: 583 Loss: -0.23, TL: 1.28, TL_A: 0.97, TL_P: 0.84, TL_N: 0.94\n",
      "Epoch: 584 Loss: -0.23, TL: 1.24, TL_A: 1.01, TL_P: 0.90, TL_N: 0.96\n",
      "Epoch: 585 Loss: -0.25, TL: 1.36, TL_A: 1.02, TL_P: 0.81, TL_N: 0.91\n",
      "Epoch: 586 Loss: -0.24, TL: 1.33, TL_A: 0.94, TL_P: 0.83, TL_N: 0.96\n",
      "Epoch: 587 Loss: -0.21, TL: 1.26, TL_A: 0.95, TL_P: 0.83, TL_N: 0.99\n",
      "Epoch: 588 Loss: -0.25, TL: 1.35, TL_A: 1.05, TL_P: 0.78, TL_N: 0.90\n",
      "Epoch: 589 Loss: -0.22, TL: 1.25, TL_A: 0.96, TL_P: 0.86, TL_N: 1.01\n",
      "Epoch: 590 Loss: -0.22, TL: 1.30, TL_A: 0.96, TL_P: 0.82, TL_N: 0.99\n",
      "Epoch: 591 Loss: -0.23, TL: 1.33, TL_A: 0.98, TL_P: 0.82, TL_N: 0.99\n",
      "Epoch: 592 Loss: -0.23, TL: 1.24, TL_A: 1.01, TL_P: 0.84, TL_N: 0.94\n",
      "Epoch: 593 Loss: -0.20, TL: 1.14, TL_A: 0.91, TL_P: 0.89, TL_N: 1.03\n",
      "Epoch: 594 Loss: -0.19, TL: 1.36, TL_A: 0.81, TL_P: 0.78, TL_N: 1.13\n",
      "Epoch: 595 Loss: -0.25, TL: 1.41, TL_A: 1.04, TL_P: 0.76, TL_N: 0.93\n",
      "Epoch: 596 Loss: -0.22, TL: 1.26, TL_A: 0.96, TL_P: 0.88, TL_N: 1.02\n",
      "Epoch: 597 Loss: -0.23, TL: 1.14, TL_A: 1.04, TL_P: 0.90, TL_N: 0.93\n",
      "Epoch: 598 Loss: -0.20, TL: 1.08, TL_A: 0.95, TL_P: 0.90, TL_N: 1.01\n",
      "Epoch: 599 Loss: -0.20, TL: 1.00, TL_A: 1.00, TL_P: 0.93, TL_N: 1.00\n",
      "Epoch: 600 Loss: -0.20, TL: 0.97, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 601 Loss: -0.21, TL: 1.00, TL_A: 1.00, TL_P: 0.98, TL_N: 0.98\n",
      "Epoch: 602 Loss: -0.21, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 603 Loss: -0.21, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 604 Loss: -0.21, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 605 Loss: -0.21, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 606 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 607 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 608 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 609 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 610 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 611 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 612 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 613 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 614 Loss: -0.22, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 615 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 616 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 617 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 618 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 619 Loss: -0.23, TL: 0.99, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 620 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 621 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 622 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 623 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 624 Loss: -0.23, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 625 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 626 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 627 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 628 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 629 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 630 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 631 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 632 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 633 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 634 Loss: -0.24, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 635 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 636 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 637 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 638 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 639 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 640 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 641 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 642 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 643 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 644 Loss: -0.25, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 645 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 646 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 647 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 648 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 649 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 650 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 651 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 652 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 653 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 654 Loss: -0.26, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 655 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 656 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 657 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 658 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 659 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 660 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 661 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 662 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 663 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 664 Loss: -0.27, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 665 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 666 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 667 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 668 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 669 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 670 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 671 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 672 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 673 Loss: -0.28, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 674 Loss: -0.28, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 675 Loss: -0.29, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 676 Loss: -0.29, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 677 Loss: -0.29, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 678 Loss: -0.29, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 679 Loss: -0.29, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 680 Loss: -0.29, TL: 1.02, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 681 Loss: -0.29, TL: 1.02, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 682 Loss: -0.30, TL: 1.03, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 683 Loss: -0.30, TL: 1.03, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 684 Loss: -0.30, TL: 1.03, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 685 Loss: -0.30, TL: 1.03, TL_A: 1.00, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 686 Loss: -0.30, TL: 1.04, TL_A: 1.00, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 687 Loss: -0.30, TL: 1.04, TL_A: 1.00, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 688 Loss: -0.30, TL: 1.02, TL_A: 0.99, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 689 Loss: -0.30, TL: 1.07, TL_A: 0.99, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 690 Loss: -0.31, TL: 1.06, TL_A: 1.00, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 691 Loss: -0.31, TL: 1.07, TL_A: 1.00, TL_P: 0.97, TL_N: 1.00\n",
      "Epoch: 692 Loss: -0.30, TL: 1.07, TL_A: 0.99, TL_P: 0.94, TL_N: 1.01\n",
      "Epoch: 693 Loss: -0.31, TL: 1.07, TL_A: 1.00, TL_P: 0.97, TL_N: 0.99\n",
      "Epoch: 694 Loss: -0.31, TL: 1.05, TL_A: 1.00, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 695 Loss: -0.31, TL: 1.06, TL_A: 1.00, TL_P: 0.96, TL_N: 1.00\n",
      "Epoch: 696 Loss: -0.31, TL: 1.09, TL_A: 0.97, TL_P: 0.95, TL_N: 1.02\n",
      "Epoch: 697 Loss: -0.31, TL: 1.07, TL_A: 1.00, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 698 Loss: -0.31, TL: 1.10, TL_A: 1.01, TL_P: 0.95, TL_N: 1.00\n",
      "Epoch: 699 Loss: -0.32, TL: 1.09, TL_A: 1.00, TL_P: 0.96, TL_N: 1.00\n",
      "Epoch: 700 Loss: -0.32, TL: 1.09, TL_A: 1.01, TL_P: 0.97, TL_N: 1.00\n",
      "Epoch: 701 Loss: -0.32, TL: 1.09, TL_A: 1.01, TL_P: 0.98, TL_N: 0.99\n",
      "Epoch: 702 Loss: -0.32, TL: 1.11, TL_A: 0.98, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 703 Loss: -0.31, TL: 1.14, TL_A: 1.01, TL_P: 0.90, TL_N: 1.02\n",
      "Epoch: 704 Loss: -0.32, TL: 1.12, TL_A: 0.98, TL_P: 0.94, TL_N: 1.01\n",
      "Epoch: 705 Loss: -0.32, TL: 1.11, TL_A: 1.01, TL_P: 0.96, TL_N: 1.01\n",
      "Epoch: 706 Loss: -0.32, TL: 1.14, TL_A: 0.99, TL_P: 0.93, TL_N: 1.01\n",
      "Epoch: 707 Loss: -0.31, TL: 1.09, TL_A: 0.98, TL_P: 0.94, TL_N: 1.02\n",
      "Epoch: 708 Loss: -0.33, TL: 1.08, TL_A: 1.01, TL_P: 0.96, TL_N: 0.99\n",
      "Epoch: 709 Loss: -0.32, TL: 1.08, TL_A: 1.00, TL_P: 0.94, TL_N: 0.99\n",
      "Epoch: 710 Loss: -0.34, TL: 1.15, TL_A: 0.99, TL_P: 0.98, TL_N: 1.01\n",
      "Epoch: 711 Loss: -0.34, TL: 1.14, TL_A: 1.01, TL_P: 0.94, TL_N: 0.99\n",
      "Epoch: 712 Loss: -0.32, TL: 1.10, TL_A: 0.98, TL_P: 0.92, TL_N: 1.01\n",
      "Epoch: 713 Loss: -0.34, TL: 1.19, TL_A: 0.99, TL_P: 0.93, TL_N: 1.02\n",
      "Epoch: 714 Loss: -0.35, TL: 1.23, TL_A: 1.02, TL_P: 0.92, TL_N: 0.98\n",
      "Epoch: 715 Loss: -0.35, TL: 1.20, TL_A: 1.01, TL_P: 0.95, TL_N: 0.99\n",
      "Epoch: 716 Loss: -0.33, TL: 1.18, TL_A: 0.98, TL_P: 0.90, TL_N: 1.02\n",
      "Epoch: 717 Loss: -0.34, TL: 1.14, TL_A: 0.98, TL_P: 0.93, TL_N: 0.99\n",
      "Epoch: 718 Loss: -0.36, TL: 1.20, TL_A: 1.01, TL_P: 0.95, TL_N: 0.99\n",
      "Epoch: 719 Loss: -0.35, TL: 1.19, TL_A: 1.01, TL_P: 0.92, TL_N: 0.99\n",
      "Epoch: 720 Loss: -0.36, TL: 1.22, TL_A: 1.00, TL_P: 0.94, TL_N: 1.00\n",
      "Epoch: 721 Loss: -0.35, TL: 1.18, TL_A: 0.99, TL_P: 0.92, TL_N: 1.00\n",
      "Epoch: 722 Loss: -0.36, TL: 1.22, TL_A: 1.02, TL_P: 0.90, TL_N: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 723 Loss: -0.36, TL: 1.19, TL_A: 0.99, TL_P: 0.97, TL_N: 1.00\n",
      "Epoch: 724 Loss: -0.36, TL: 1.23, TL_A: 1.03, TL_P: 0.91, TL_N: 0.97\n",
      "Epoch: 725 Loss: -0.37, TL: 1.27, TL_A: 1.00, TL_P: 0.92, TL_N: 1.00\n",
      "Epoch: 726 Loss: -0.36, TL: 1.26, TL_A: 0.97, TL_P: 0.95, TL_N: 1.04\n",
      "Epoch: 727 Loss: -0.35, TL: 1.14, TL_A: 1.00, TL_P: 0.89, TL_N: 0.98\n",
      "Epoch: 728 Loss: -0.36, TL: 1.21, TL_A: 1.02, TL_P: 0.89, TL_N: 0.97\n",
      "Epoch: 729 Loss: -0.36, TL: 1.26, TL_A: 0.96, TL_P: 0.93, TL_N: 1.04\n",
      "Epoch: 730 Loss: -0.37, TL: 1.22, TL_A: 1.02, TL_P: 0.94, TL_N: 1.00\n",
      "Epoch: 731 Loss: -0.36, TL: 1.17, TL_A: 1.00, TL_P: 0.93, TL_N: 0.99\n",
      "Epoch: 732 Loss: -0.35, TL: 1.09, TL_A: 0.99, TL_P: 0.93, TL_N: 0.98\n",
      "Epoch: 733 Loss: -0.37, TL: 1.20, TL_A: 1.00, TL_P: 0.95, TL_N: 1.01\n",
      "Epoch: 734 Loss: -0.38, TL: 1.27, TL_A: 1.01, TL_P: 0.93, TL_N: 1.00\n",
      "Epoch: 735 Loss: -0.36, TL: 1.25, TL_A: 0.99, TL_P: 0.89, TL_N: 1.00\n",
      "Epoch: 736 Loss: -0.34, TL: 1.14, TL_A: 0.95, TL_P: 0.88, TL_N: 1.03\n",
      "Epoch: 737 Loss: -0.39, TL: 1.31, TL_A: 1.06, TL_P: 0.84, TL_N: 0.93\n",
      "Epoch: 738 Loss: -0.38, TL: 1.29, TL_A: 1.02, TL_P: 0.90, TL_N: 0.99\n",
      "Epoch: 739 Loss: -0.36, TL: 1.24, TL_A: 0.98, TL_P: 0.91, TL_N: 1.03\n",
      "Epoch: 740 Loss: -0.39, TL: 1.30, TL_A: 0.98, TL_P: 0.91, TL_N: 0.98\n",
      "Epoch: 741 Loss: -0.38, TL: 1.26, TL_A: 1.01, TL_P: 0.87, TL_N: 0.98\n",
      "Epoch: 742 Loss: -0.37, TL: 1.19, TL_A: 0.98, TL_P: 0.90, TL_N: 0.98\n",
      "Epoch: 743 Loss: -0.37, TL: 1.17, TL_A: 1.01, TL_P: 0.88, TL_N: 0.96\n",
      "Epoch: 744 Loss: -0.36, TL: 1.06, TL_A: 1.01, TL_P: 0.98, TL_N: 1.00\n",
      "Epoch: 745 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 746 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 747 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 748 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 749 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 750 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 751 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 752 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 753 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 754 Loss: -0.36, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 755 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 756 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 757 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 758 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 759 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 760 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 761 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 762 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 763 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 764 Loss: -0.37, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 765 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 766 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 767 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 768 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 769 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 770 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 771 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 772 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 773 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 774 Loss: -0.38, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 775 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 776 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 777 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 778 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 779 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 780 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 781 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 782 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 783 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 784 Loss: -0.39, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 785 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 786 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 787 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 788 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 789 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 790 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 791 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 792 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 793 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 794 Loss: -0.40, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 795 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 796 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 797 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 798 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 799 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 800 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 801 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 802 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 803 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 804 Loss: -0.41, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 805 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 806 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 807 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 808 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 809 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 810 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 811 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 812 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 813 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 814 Loss: -0.42, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 815 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 816 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 817 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 818 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 819 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 820 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 821 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 822 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 823 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 824 Loss: -0.43, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 825 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 826 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 827 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 828 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 829 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 830 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 831 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 832 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 833 Loss: -0.44, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 834 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 835 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 836 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 837 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 838 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 839 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 840 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 841 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 842 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 843 Loss: -0.45, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 844 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 845 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 846 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 847 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 848 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 849 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 850 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 851 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 852 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 853 Loss: -0.46, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 854 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 855 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 856 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 857 Loss: -0.47, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 858 Loss: -0.47, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 859 Loss: -0.47, TL: 1.01, TL_A: 0.99, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 860 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 861 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 862 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 863 Loss: -0.47, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 864 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 865 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 866 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 867 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 868 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 869 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 870 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 871 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 872 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 873 Loss: -0.48, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 874 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 875 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 876 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 877 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 878 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 879 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 880 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 881 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 882 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 883 Loss: -0.49, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 884 Loss: -0.50, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 885 Loss: -0.50, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 886 Loss: -0.50, TL: 1.01, TL_A: 0.99, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 887 Loss: -0.50, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 888 Loss: -0.50, TL: 1.04, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 889 Loss: -0.50, TL: 1.09, TL_A: 0.98, TL_P: 0.93, TL_N: 0.99\n",
      "Epoch: 890 Loss: -0.50, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 891 Loss: -0.50, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 892 Loss: -0.50, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 893 Loss: -0.50, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 894 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 895 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 896 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 897 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 898 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 899 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 900 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 901 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 902 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 903 Loss: -0.51, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 904 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 905 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 906 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 907 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 908 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 909 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 910 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 911 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 912 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 913 Loss: -0.52, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 914 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 915 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 916 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 917 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 918 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 919 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 920 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 921 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 922 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 923 Loss: -0.53, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 924 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 925 Loss: -0.54, TL: 1.01, TL_A: 1.00, TL_P: 0.99, TL_N: 1.00\n",
      "Epoch: 926 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 927 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 928 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 929 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 930 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 931 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 932 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 933 Loss: -0.54, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 934 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 935 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 936 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 937 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 938 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 939 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 940 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 941 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 942 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 943 Loss: -0.55, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 944 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 945 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 946 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 947 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 948 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 949 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 950 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 951 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 952 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 953 Loss: -0.56, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 954 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 955 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 956 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 957 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 958 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 959 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 960 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 961 Loss: -0.57, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 962 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 963 Loss: -0.57, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 964 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 965 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 966 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 967 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 968 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 969 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 970 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 971 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 972 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 973 Loss: -0.58, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 974 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 975 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 976 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 977 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 978 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 979 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 980 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 981 Loss: -0.59, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 982 Loss: -0.59, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 983 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 984 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 985 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 986 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 987 Loss: -0.60, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 988 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 989 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 990 Loss: -0.60, TL: 1.00, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 991 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 992 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 993 Loss: -0.60, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 994 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 995 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 996 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 997 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 998 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 999 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00\n",
      "Epoch: 1000 Loss: -0.61, TL: 1.01, TL_A: 1.00, TL_P: 1.00, TL_N: 1.00, recall@25: 0.21\n",
      "Saved model 'modelos/model_deepQL_weights_1000_feature_1000epochs_64batch(eclipse).h5' to disk\n",
      "Best_epoch=1000, Best_loss=-0.61s, Recall@25=0.21\n",
      "CPU times: user 7h 47min 20s, sys: 15h 10min 34s, total: 22h 57min 55s\n",
      "Wall time: 19h 29min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "print(\"Batch size \", batch_size)\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_dilated_model\n",
    "    arcii_model\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    bilstm_model\n",
    "'''\n",
    "# title_feature_model = bilstm_model(title_embedding_layer, MAX_SEQUENCE_LENGTH_T)\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "#desc_feature_model = cnn_model(desc_embedding_layer, MAX_SEQUENCE_LENGTH_D)\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "encoded_positive = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'pos')\n",
    "encoded_negative = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'neg')\n",
    "# Master model\n",
    "embed_size = K.int_shape(title_feature_model.get_output_at(0))[1] + K.int_shape(desc_feature_model.get_output_at(0))[1] + K.int_shape(categorical_feature_model.get_output_at(0))[1] \n",
    "\n",
    "master_anchor = siamese_model_centroid(embed_size, 'master_anchor')\n",
    "master_pos = siamese_model_centroid(embed_size, 'master_pos')\n",
    "master_negative = siamese_model_centroid(embed_size, 'master_neg')\n",
    "\n",
    "NUMBER_OF_INSTANCES = len(baseline.dup_sets_train)\n",
    "BATCH_SIZE = batch_size\n",
    "EPOCHS = epochs\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, \n",
    "                                            master_anchor, master_negative, master_pos,\n",
    "                                            NUMBER_OF_INSTANCES, BATCH_SIZE, EPOCHS, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_input_pos, train_input_neg, train_master_input, train_master_neg, \\\n",
    "            train_sim = experiment.batch_iterator_bert(encoded_anchor, baseline.train_data, baseline.dup_sets_train, \\\n",
    "                                                       bug_train_ids, \n",
    "                                                       batch_size, 1, \n",
    "                                                       issues_by_buckets, \n",
    "                                                       TRIPLET_HARD=True, USE_CENTROID=True)\n",
    "    \n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], train_input_sample['description']['token'], train_input_sample['description']['segment'], train_input_sample['info'],\n",
    "                   train_input_pos['title']['token'], train_input_pos['title']['segment'], train_input_pos['description']['token'], train_input_pos['description']['segment'], train_input_pos['info'], \n",
    "                   train_input_neg['title']['token'], train_input_neg['title']['segment'], train_input_neg['description']['token'], train_input_neg['description']['segment'], train_input_neg['info'],\n",
    "                   train_master_input['centroid_embed'],\n",
    "                   train_master_input['centroid_embed'],\n",
    "                   train_master_neg['centroid_embed']]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    \n",
    "    if (epoch+1 == epochs): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, _, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print(\"Epoch: {} Loss: {:.2f}, TL: {:.2f}, TL_A: {:.2f}, TL_P: {:.2f}, TL_N: {:.2f}, recall@25: {:.2f}\".format(epoch+1, h[0], h[1], h[2], h[3], h[4], recall))\n",
    "    else:\n",
    "        print(\"Epoch: {} Loss: {:.2f}, TL: {:.2f}, TL_A: {:.2f}, TL_P: {:.2f}, TL_N: {:.2f}\".format(epoch+1,h[0], h[1], h[2], h[3], h[4]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}s, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['327681:324658|327985:0.9428757652640343,324056:0.9425551109015942,343513:0.9424966424703598,318846:0.9424221701920033,352297:0.9420154206454754,318845:0.9420088715851307,325924:0.9417591020464897,400354:0.9417415969073772,394703:0.9416719935834408,343328:0.9415632486343384,320640:0.9414856433868408,280457:0.9414570070803165,394104:0.9410785920917988,342369:0.9410055316984653,336435:0.9410013109445572,303904:0.9409882128238678,330650:0.9409813396632671,315120:0.9408961646258831,233093:0.9408209845423698,345982:0.9406372345983982,311065:0.9406181387603283,350844:0.9406060986220837,357210:0.9405073411762714,338351:0.9405035898089409,396638:0.9402814470231533,356356:0.940276350826025,344444:0.9402624070644379,354235:0.9402533769607544,361530:0.9402143992483616',\n",
       " '360457:362252|364552:0.9639727622270584,354118:0.9556278437376022,353222:0.9505909606814384,367431:0.9504093267023563,338252:0.9497708156704903,343568:0.949703074991703,396905:0.9494970850646496,377371:0.9488717056810856,353496:0.9486352391541004,391011:0.9486161470413208,368089:0.9485962502658367,315855:0.9485135115683079,378207:0.948097538203001,320687:0.9480040706694126,406110:0.9469122141599655,297418:0.9465593807399273,318008:0.9464490860700607,359173:0.9463041350245476,338876:0.9460469000041485,344823:0.9458872117102146,386025:0.9458319172263145,356626:0.9458171911537647,350623:0.9457954578101635,419520:0.9457623437047005,320325:0.9456939101219177,317997:0.9454572647809982,321116:0.9453848600387573,319176:0.9452911354601383,402100:0.9452521428465843',\n",
       " '393230:393054|345504:0.9503036923706532,371695:0.9487299062311649,315086:0.9477042518556118,317747:0.9472730234265327,380258:0.9470367394387722,342222:0.9462740682065487,390479:0.94624263048172,345712:0.9462150111794472,379137:0.9459233321249485,333063:0.9456181339919567,398880:0.9455787651240826,328406:0.945522453635931,318845:0.9451248086988926,359344:0.9450218975543976,386581:0.9449852742254734,369540:0.9445581026375294,398274:0.9444081149995327,347772:0.9442115984857082,318822:0.9439710676670074,344358:0.9438256286084652,346806:0.943823330104351,382204:0.9437539093196392,376036:0.9435850642621517,385368:0.9434621036052704,316135:0.9434277974069118,398917:0.9434044770896435,359105:0.943328782916069,348945:0.9431999586522579,305468:0.9429849125444889',\n",
       " '393232:393282,390667,383388|399657:0.9627461321651936,406690:0.9574465453624725,403754:0.9571732170879841,392565:0.9549221694469452,414929:0.9538932591676712,406039:0.9532436095178127,421072:0.952961515635252,415235:0.9527530558407307,412105:0.9522998817265034,395005:0.9521359875798225,408678:0.9521229118108749,413509:0.9520765133202076,400975:0.9520512819290161,409369:0.9519330933690071,411120:0.9515228942036629,406941:0.9514960162341595,397532:0.951303768903017,421036:0.9512179419398308,404887:0.9505838006734848,408733:0.9502637051045895,421343:0.9491295106709003,420245:0.9487075246870518,403900:0.9485283829271793,386608:0.9484011046588421,408095:0.9483277015388012,371821:0.9482164867222309,380848:0.948074322193861,387658:0.9480644389986992,384573:0.9479953572154045',\n",
       " '393247:401563,396542|419072:0.9645472355186939,355970:0.9603112265467644,340898:0.9545061737298965,414924:0.9536901749670506,365614:0.9526735134422779,336140:0.951201006770134,318357:0.9506650865077972,362250:0.950556356459856,384286:0.9504521079361439,383424:0.9502940066158772,377383:0.9495544545352459,376407:0.9494446739554405,399121:0.9493977725505829,419352:0.9490939751267433,382388:0.9488208405673504,328926:0.9488037601113319,370995:0.9486021287739277,368283:0.9485489130020142,385286:0.9484886899590492,392911:0.94823944196105,420086:0.9478628523647785,321929:0.947774451225996,321696:0.9476671516895294,363197:0.9475715011358261,361860:0.9475191570818424,326062:0.947495985776186,358296:0.9474683366715908,394929:0.9473706260323524,318782:0.9473592452704906',\n",
       " '360484:359237|389840:0.9495484828948975,324648:0.9483637250959873,350992:0.9480546601116657,400354:0.9480342194437981,353489:0.9479410201311111,388674:0.9477584771811962,401606:0.9477026462554932,294321:0.9476889595389366,330108:0.947362057864666,402592:0.9471041783690453,363913:0.9468668550252914,359316:0.9459785409271717,389199:0.9459065794944763,376460:0.9458132535219193,396638:0.9456446878612041,378102:0.9455794543027878,372230:0.9453730285167694,384676:0.9451843053102493,368255:0.9448897130787373,404357:0.9448763802647591,394104:0.9448745064437389,419196:0.9448685869574547,363917:0.9448428116738796,397698:0.9445077143609524,378666:0.9441278092563152,376808:0.9439882338047028,348945:0.9436244629323483,372824:0.943534430116415,379336:0.9434709213674068',\n",
       " '360489:358624,354593,355714,357219,358267,362534,356295,356871,355722,356717,356973,358774,360214,356122,355803,356413|381539:0.9800518918782473,406956:0.9790693689137697,350122:0.9783081859350204,368796:0.977947635576129,355580:0.977618271484971,350900:0.9774864260107279,355803:0.9773121103644371,360214:0.9772991314530373,350902:0.9766286108642817,367651:0.9765010997653008,352470:0.9757046457380056,370559:0.9753899797797203,370562:0.9750922545790672,373897:0.9748223442584276,368310:0.9744287934154272,356973:0.9743878729641438,349594:0.9741512592881918,337008:0.9738786742091179,354189:0.9738010186702013,375934:0.9737362675368786,355722:0.9734829179942608,369211:0.9734659548848867,380915:0.9733269140124321,356717:0.9730088412761688,370503:0.9729726631194353,369310:0.9726517256349325,360213:0.9723171647638083,375915:0.9719347469508648,352802:0.9718415308743715',\n",
       " '262194:331909|385769:0.9559758305549622,303746:0.9558323882520199,341428:0.9557645581662655,314309:0.955647062510252,418241:0.9553604610264301,344729:0.9550445824861526,285268:0.9547884725034237,293689:0.9546136520802975,380864:0.9542345143854618,373988:0.9536923468112946,415235:0.9532845504581928,69630:0.9532566294074059,341717:0.9531315341591835,382533:0.9529909156262875,403532:0.9526454880833626,271551:0.9526101797819138,384678:0.9525272697210312,386608:0.9525061100721359,355404:0.952459454536438,327893:0.9523779414594173,240048:0.9523414708673954,340599:0.9523173533380032,363630:0.952241986989975,412811:0.9522112794220448,400975:0.9521797969937325,412063:0.9519787840545177,330258:0.9517904445528984,378519:0.9517578110098839,226476:0.9516861401498318',\n",
       " '327731:328902|351254:0.9528194777667522,342792:0.9525092132389545,319056:0.9511078335344791,337392:0.9497980698943138,347116:0.949750728905201,237638:0.9497195780277252,394884:0.9494544118642807,331202:0.9488105103373528,358629:0.9487049952149391,320183:0.9485478401184082,412518:0.9482583925127983,336223:0.9479788020253181,296850:0.9473710246384144,377048:0.9463776871562004,361368:0.946021493524313,338579:0.9459537006914616,344939:0.9450666084885597,297191:0.9447868280112743,376242:0.9441845379769802,178009:0.944097388535738,416296:0.9438628330826759,389775:0.9431095607578754,398963:0.9430683813989162,350323:0.9429863691329956,375396:0.9428379647433758,347691:0.9426914788782597,376143:0.9426517151296139,311039:0.942587286233902,258805:0.942529708147049',\n",
       " '393277:393864,400436|317785:0.9473316632211208,320329:0.9471486620604992,364849:0.9466122500598431,323149:0.94613416492939,356053:0.9455556459724903,373733:0.9450777880847454,317885:0.9447977617383003,348205:0.9442593120038509,347560:0.9437469318509102,336936:0.943718146532774,349541:0.9432767406105995,356414:0.9430825412273407,375327:0.9420474357903004,401416:0.9419226124882698,406848:0.94188541918993,348787:0.9417934268712997,319114:0.941730447113514,317383:0.9416600316762924,354593:0.9416559189558029,333434:0.9413700439035892,336498:0.9413181766867638,411452:0.9412961229681969,374895:0.9411607645452023,340873:0.9409557618200779,384204:0.9409033134579659,411006:0.9407936483621597,344750:0.9407687075436115,356057:0.9407503753900528,349280:0.940436989068985',\n",
       " '393282:393232,390667,383388|402661:0.9580960497260094,382486:0.9528191052377224,323601:0.9527834989130497,405714:0.9523298367857933,340197:0.9500687345862389,319654:0.949862789362669,325519:0.9496368132531643,404719:0.9495365209877491,342271:0.9492107070982456,379101:0.9490357786417007,401390:0.9486247226595879,367268:0.9483763203024864,216784:0.9481143951416016,373578:0.948008581995964,409356:0.9479181617498398,359809:0.9478049166500568,333397:0.9477441310882568,350769:0.9476816803216934,318297:0.9475506469607353,412266:0.9474415481090546,352801:0.9469982087612152,349382:0.9469073675572872,346296:0.9467107579112053,390154:0.9467092603445053,353907:0.946619052439928,395427:0.9463410712778568,347391:0.9463365338742733,406926:0.9461484812200069,277265:0.9459278583526611',\n",
       " '327748:330466|339286:0.9565724693238735,320553:0.9564148411154747,385265:0.9536571055650711,321294:0.9528969191014767,320015:0.9506802409887314,318751:0.9505831301212311,338735:0.9502308368682861,320584:0.949543658643961,360683:0.9487813301384449,410647:0.9479599185287952,343666:0.9476315267384052,318855:0.9474663995206356,353514:0.9473517537117004,368804:0.9472205601632595,383497:0.9469809979200363,371598:0.9467786401510239,408505:0.9467309974133968,354967:0.9467145353555679,318490:0.946485698223114,387932:0.9464668929576874,414371:0.9464544542133808,401801:0.9463887698948383,390511:0.9463780149817467,320021:0.9463712982833385,380603:0.9463579952716827,334093:0.9459997788071632,383815:0.9458609335124493,345555:0.9458369053900242,390713:0.9457710608839989',\n",
       " '327754:330209,331297,332294,327303,327528,329166,329232,330705,326194,329778,331186,332594,333917,327415,327548,333341,333342,332927|350548:0.9472246244549751,325671:0.9471192732453346,369773:0.9461704604327679,402453:0.9454321935772896,373081:0.9437685683369637,366786:0.9435556381940842,392691:0.9434316642582417,363022:0.943190447986126,332551:0.9429463520646095,389840:0.9427898786962032,354057:0.942637961357832,359891:0.9425468742847443,403128:0.942545160651207,335963:0.9425403103232384,325984:0.9422449581325054,331981:0.9420028403401375,355708:0.941795825958252,318679:0.9417331852018833,373174:0.9414260312914848,406182:0.9413991421461105,325820:0.9413871951401234,349478:0.9413712657988071,349352:0.9413668625056744,363879:0.9413584806025028,335075:0.9411389082670212,323107:0.9410556629300117,393918:0.941039364784956,333107:0.9410037733614445,360169:0.94100047275424',\n",
       " '360529:354495,343276,347031|378287:0.9508385770022869,342202:0.9471019469201565,347031:0.9421909004449844,349752:0.9418205507099628,362378:0.9412399865686893,359800:0.94118607416749,346351:0.9399225115776062,318744:0.9391792006790638,377119:0.9382039494812489,351436:0.9376845397055149,318822:0.9360394552350044,377425:0.9356213361024857,371730:0.9355015605688095,405350:0.935353122651577,354066:0.9350085631012917,414668:0.9348957017064095,340714:0.9347941726446152,336087:0.9347433298826218,361542:0.9344100207090378,368876:0.9343954920768738,356487:0.9340991675853729,400439:0.9340528100728989,341288:0.933971144258976,346806:0.933785617351532,406789:0.9337089359760284,408246:0.9335755780339241,326688:0.9335120096802711,316213:0.9334984719753265,328737:0.9332952722907066',\n",
       " '393303:392707|365750:0.9463100992143154,395794:0.9451114013791084,329716:0.943589698523283,320546:0.9427988268435001,351083:0.9423450715839863,406700:0.9423010908067226,375830:0.9418922364711761,396520:0.9416795782744884,374082:0.9416577853262424,325435:0.9415984712541103,322695:0.9413847401738167,345290:0.9413744024932384,340799:0.9412724040448666,322520:0.9411971420049667,333873:0.9411150850355625,325502:0.9411148801445961,364733:0.9409157745540142,389101:0.9407416619360447,420086:0.9407290369272232,333375:0.9407071582973003,367340:0.9406785443425179,335069:0.940594132989645,320735:0.9405670426785946,347328:0.9405650123953819,320084:0.9405520632863045,344410:0.9403581731021404,323824:0.9402578175067902,318782:0.9402479231357574,385636:0.9402167834341526',\n",
       " '327769:177756,329721|396056:0.951971098780632,360935:0.9498672038316727,333776:0.9496069736778736,339736:0.9489301219582558,339036:0.9488363452255726,336563:0.9486609548330307,392416:0.9474256671965122,371089:0.9471846483647823,346768:0.9467488080263138,322917:0.9464637748897076,393906:0.9463810883462429,385080:0.9463336654007435,380401:0.9462046399712563,329336:0.9459966123104095,392798:0.9459827579557896,363019:0.945840023458004,360588:0.9455547481775284,348151:0.9454028382897377,373727:0.9449391290545464,377860:0.9448931813240051,324440:0.9446971118450165,324111:0.9446401633322239,325627:0.9445919431746006,366178:0.9445355907082558,355372:0.9443245865404606,353463:0.944197628647089,320469:0.9441936686635017,347890:0.9441233240067959,394684:0.9440939091145992',\n",
       " '393305:381846|389961:0.937601450830698,350107:0.9366920292377472,341628:0.9365543276071548,421474:0.9365170896053314,323465:0.9363748356699944,316922:0.9362027570605278,363879:0.9357764944434166,390320:0.9356392547488213,353198:0.9355537965893745,400439:0.935439832508564,337630:0.9352911412715912,333808:0.9352392479777336,354066:0.9348366782069206,324536:0.934819795191288,340304:0.9346093907952309,342879:0.9343973621726036,364616:0.9343578815460205,351958:0.9340102672576904,396990:0.9338735267519951,376251:0.9337171092629433,389045:0.9336490705609322,314083:0.9336487129330635,324293:0.93364068120718,323091:0.9336405545473099,326688:0.9335999265313148,382678:0.9335532858967781,339862:0.9332559704780579,347419:0.933127798140049,332729:0.9330647960305214',\n",
       " '360540:355108|360574:0.9553924314677715,378375:0.9539396949112415,368969:0.9497459419071674,382534:0.9469353072345257,362575:0.9467310421168804,371617:0.9460070244967937,389938:0.9459836222231388,367434:0.9441006891429424,369971:0.9436911158263683,380734:0.943608745932579,361447:0.9430901482701302,360980:0.9430461339652538,363108:0.9427677020430565,364398:0.942575816065073,366212:0.9425635375082493,359609:0.9425577260553837,396792:0.9424552619457245,398916:0.9423699341714382,339046:0.9417318217456341,334042:0.9415863677859306,379307:0.941438153386116,391963:0.9414311572909355,382274:0.9414157718420029,368868:0.9411530196666718,382604:0.9410916417837143,368864:0.9410731941461563,382657:0.9410155713558197,340869:0.9409772381186485,327985:0.9408883862197399',\n",
       " '327772:326427|327233:0.9678105898201466,328722:0.9583651311695576,333375:0.9575367160141468,328926:0.9574504643678665,331453:0.9569699950516224,328010:0.9563003033399582,329872:0.9530752114951611,315855:0.9530099593102932,337263:0.952894851565361,317929:0.9528051540255547,375783:0.9522991739213467,319176:0.951997447758913,327476:0.9518988057971001,333962:0.9518469423055649,385286:0.9516591913998127,320546:0.9515026733279228,320768:0.9508626535534859,348806:0.9504636488854885,357559:0.950322013348341,367155:0.9501484856009483,356184:0.9500578790903091,342114:0.9498908296227455,319643:0.9497975446283817,351083:0.9496453106403351,351519:0.9495441317558289,321929:0.9495428055524826,326085:0.9493328332901001,358795:0.9492903612554073,318604:0.9492235854268074',\n",
       " '360547:360548,360549|360548:0.9998749967780896,360549:0.9998749967780896,398091:0.9468395709991455,388299:0.9435712993144989,330122:0.9414561241865158,364849:0.941131878644228,346221:0.9410470612347126,347560:0.9409034177660942,402343:0.9406285546720028,338975:0.940502904355526,378155:0.9404514953494072,348205:0.9401924014091492,330395:0.9400539807975292,398070:0.9399829991161823,372838:0.9395245090126991,354593:0.9392751157283783,323149:0.9385746642947197,338177:0.9384274631738663,356414:0.9383676685392857,406765:0.9381305128335953,394327:0.9380553662776947,349541:0.937993761152029,351021:0.9377066418528557,328795:0.9376535974442959,366092:0.9372697994112968,409916:0.9372463971376419,317885:0.9371809884905815,319962:0.9370211288332939,356057:0.9369224160909653']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, exported_rank, debug = experiment.evaluate_validation_test(experiment, retrieval, verbose, \n",
    "#                                                         encoded_anchor, issues_by_buckets, evaluate_validation_test)\n",
    "# test_vectorized, queries_test_vectorized, annoy, X_test, distance_test, indices_test = debug\n",
    "# \"recall@25 last epoch:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 4641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepQL_weights_1000_feature_1000epochs_64batch(eclipse)'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoded_anchor\n",
    "# model = experiment.get_model_vectorizer(path=SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 1682)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          504900      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 768)          80346736    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 768)          80346736    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 1836)         0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "==================================================================================================\n",
      "Total params: 161,198,372\n",
      "Trainable params: 726,196\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 0, model, issues_by_buckets, \n",
    "                                                                   bug_train_ids, method='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/eclipse/exported_rank_deepQL_weights_1000.txt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.15,\n",
       " '2 - recall_at_10': 0.18,\n",
       " '3 - recall_at_15': 0.19,\n",
       " '4 - recall_at_20': 0.21,\n",
       " '5 - recall_at_25': 0.21}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
