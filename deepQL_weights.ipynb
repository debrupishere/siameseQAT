{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtdA1qs_UQP1"
   },
   "source": [
    "# Propose centroid replacing the masters with BERT siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the use of GPUs\n",
    "# https://datascience.stackexchange.com/questions/23895/multi-gpu-in-keras\n",
    "# https://keras.io/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus\n",
    "# https://stackoverflow.com/questions/56316451/how-to-use-specific-gpus-in-keras-for-multi-gpu-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnSCLmiomFE1"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIha-SERnD72"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from annoy import AnnoyIndex\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c57gQiuAnJAe",
    "outputId": "9eaf2d3f-619a-492d-f40b-6ba2c48426fa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Add, Activation, Dropout, Embedding, MaxPooling1D, \\\n",
    "    GlobalMaxPool1D, Flatten, Dense, Concatenate, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baseline import Baseline\n",
    "from methods.experiments import Experiment\n",
    "from methods.evaluation import Evaluation\n",
    "from methods.retrieval import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VUZ6oG1gb91"
   },
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uQou7m2-bFO"
   },
   "source": [
    "## Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Kn3x_K-aZj"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_T = 20 # 100\n",
    "MAX_SEQUENCE_LENGTH_D = 20 # 500\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "'''\n",
    "    Configuration\n",
    "'''\n",
    "epochs = 1000\n",
    "freeze_train = .1 # 10% with freeze weights\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "verbose = 0\n",
    "loss = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse bugs preproprecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "DOMAIN = 'netbeans'\n",
    "METHOD = 'deepQL_weights_{}'.format(epochs)\n",
    "PREPROCESSING = 'bert'\n",
    "TOKEN = 'bert'\n",
    "# Dataset paths\n",
    "DIR = 'data/processed/{}/{}'.format(DOMAIN, PREPROCESSING)\n",
    "DIR_PAIRS = 'data/normalized/{}'.format(DOMAIN)\n",
    "DATASET = os.path.join('data/normalized/{}'.format(DOMAIN), '{}.csv'.format(DOMAIN))\n",
    "# Glove embeddings\n",
    "GLOVE_DIR='data/embed'\n",
    "# Save model\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "\n",
    "# Extract CORPUs\n",
    "EXTRACT_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_vocabulary\n",
    "\n",
    "token_dict = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total vocabulary: 30522'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total vocabulary: {}\".format(len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(DOMAIN, DIR, DATASET, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D,\n",
    "                   token_dict['[CLS]'], token_dict['[SEP]'])\n",
    "evaluation = Evaluation(verbose=0)\n",
    "retrieval = Retrieval()\n",
    "experiment = Experiment(baseline, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_retrieval(retrieval, baseline, DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading bug ids in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bug ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216715"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.load_ids()\n",
    "len(baseline.bug_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqzt5EKzqzcI"
   },
   "source": [
    "#### Dicionário de títulos e descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50842c71709d4612a738b855a5a32874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd47bb1da33d4b35a993c7b9be1309ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 24.1 s, sys: 4.1 s, total: 28.2 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.load_bugs(TOKEN)\n",
    "len(baseline.sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing bugs by buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c319213fef4d24b3dedc61a8b327c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=216715), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "issues_by_buckets = experiment.get_buckets_for_bugs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6Obtop6UIVD"
   },
   "source": [
    "#### Prepare the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvyMGBD4IhB-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data\n",
      "Reading bug ids\n",
      "CPU times: user 45min 27s, sys: 157 ms, total: 45min 27s\n",
      "Wall time: 45min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment.prepare_dataset(issues_by_buckets, path_train='train_chronological', path_test='test_chronological')\n",
    "# Read and create the test queries duplicates\n",
    "retrieval.create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23370, 26780],\n",
       " [103239, 105067],\n",
       " [61954, 73016],\n",
       " [204317, 202674],\n",
       " [220178, 220147],\n",
       " [195089, 195269],\n",
       " [221186, 219028],\n",
       " [102315, 105397],\n",
       " [196611, 193682],\n",
       " [50448, 50450]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery bug ids from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_train_ids = experiment.get_train_ids(baseline.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the corpus train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_CORPUS:\n",
    "    corpus = []\n",
    "    export_file = open(os.path.join(DIR, 'corpus_train.txt'), 'w')\n",
    "    for bug_id in tqdm(baseline.bug_set):\n",
    "        bug = baseline.bug_set[bug_id]\n",
    "        title = bug['title']\n",
    "        desc = bug['description']\n",
    "        export_file.write(\"{}\\n{}\\n\".format(title, desc))\n",
    "    export_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_severity': '0\\n',\n",
       " 'bug_status': '0\\n',\n",
       " 'component': '298\\n',\n",
       " 'creation_ts': '2007-03-14 19:01:00 +0000',\n",
       " 'delta_ts': '2007-03-16 15:21:07 +0000',\n",
       " 'description': \"[CLS] 07 ##0 ##31 ##4 1 , the 5 . 5 . 1 install ##er of cdc pack has only 5 . 5 in title 2 , it doesn ' t provide 5 . 5 . 1 n ##b in the list of installed n ##b ( only 5 . 5 ) [SEP]\",\n",
       " 'description_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'description_token': array([  101,  5718,  2692, 21486,  2549,  1015,  1010,  1996,  1019,\n",
       "         1012,  1019,  1012,  1015, 16500,  2121,  1997, 26629,  5308,\n",
       "         2038,   102]),\n",
       " 'dup_id': '97997',\n",
       " 'issue_id': 97889,\n",
       " 'priority': '1\\n',\n",
       " 'product': '18\\n',\n",
       " 'resolution': 'DUPLICATE',\n",
       " 'textual_token': array([  101, 26629,  5308, 16500,  2121,  2005,  1019,  1012,  1019,\n",
       "         1012,  1015,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102,   101,  5718,  2692, 21486,  2549,  1015,  1010,\n",
       "         1996,  1019,  1012,  1019,  1012,  1015, 16500,  2121,  1997,\n",
       "        26629,  5308,  2038,   102]),\n",
       " 'title': '[CLS] cdc pack install ##er for 5 . 5 . 1 [SEP]',\n",
       " 'title_segment': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'title_token': array([  101, 26629,  5308, 16500,  2121,  2005,  1019,  1012,  1019,\n",
       "         1012,  1015,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102]),\n",
       " 'version': '6\\n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(baseline.bug_ids, 1)[0]\n",
    "baseline.bug_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train ', 34596)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train \", len(baseline.dup_sets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data - path\n",
    "# batch_size - 128\n",
    "# n_neg - 1\n",
    "def batch_iterator(self, retrieval, model, data, dup_sets, bug_ids, \n",
    "                   batch_size, n_neg, issues_by_buckets, TRIPLET_HARD=False, FLOATING_PADDING=False):\n",
    "    # global train_data\n",
    "    # global self.dup_sets\n",
    "    # global self.bug_ids\n",
    "    # global self.bug_set\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batch_features = {'title' : [], 'desc' : [], 'info' : []}\n",
    "    n_train = len(data)\n",
    "\n",
    "    batch_triplets, batch_bugs_anchor, batch_bugs_pos, batch_bugs_neg, batch_bugs = [], [], [], [], []\n",
    "\n",
    "    all_bugs = list(issues_by_buckets.keys())\n",
    "    buckets = retrieval.buckets\n",
    "\n",
    "    for offset in range(batch_size):\n",
    "        anchor, pos = data[offset][0], data[offset][1]\n",
    "        batch_bugs_anchor.append(anchor)\n",
    "        batch_bugs_pos.append(pos)\n",
    "        batch_bugs.append(anchor)\n",
    "        batch_bugs.append(pos)\n",
    "        #batch_bugs += dup_sets[anchor]\n",
    "\n",
    "    for anchor, pos in zip(batch_bugs_anchor, batch_bugs_pos):\n",
    "        while True:\n",
    "            neg = self.get_neg_bug(anchor, buckets[issues_by_buckets[anchor]], issues_by_buckets, all_bugs)\n",
    "            bug_anchor = self.bug_set[anchor]\n",
    "            bug_pos = self.bug_set[pos]\n",
    "            if neg not in self.bug_set:\n",
    "                continue\n",
    "            batch_bugs.append(neg)\n",
    "            batch_bugs_neg.append(neg)\n",
    "            bug_neg = self.bug_set[neg]\n",
    "            break\n",
    "        \n",
    "        # triplet bug and master\n",
    "        batch_triplets.append([anchor, pos, neg])\n",
    "    \n",
    "    random.shuffle(batch_bugs)\n",
    "    title_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_T), 0)\n",
    "    description_ids = np.full((len(batch_bugs), MAX_SEQUENCE_LENGTH_D), 0)\n",
    "    for i, bug_id in enumerate(batch_bugs):\n",
    "        bug = self.bug_set[bug_id]\n",
    "        self.read_batch_bugs(batch_features, bug, index=i, title_ids=title_ids, description_ids=description_ids)\n",
    "\n",
    "    batch_features['title'] = { 'token' : np.array(batch_features['title']), 'segment' : title_ids }\n",
    "    batch_features['desc'] = { 'token' : np.array(batch_features['desc']), 'segment' : description_ids }\n",
    "    batch_features['info'] = np.array(batch_features['info'])\n",
    "    \n",
    "    sim = np.asarray([issues_by_buckets[bug_id] for bug_id in batch_bugs])\n",
    "\n",
    "    input_sample = {}\n",
    "\n",
    "    input_sample = { 'title' : batch_features['title'], \n",
    "                        'description' : batch_features['desc'], \n",
    "                            'info' : batch_features['info'] }\n",
    "\n",
    "    return batch_triplets, input_sample, sim #sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PA5CIhgz7odW",
    "outputId": "ae98fdec-1d54-4b1f-ee0e-4c5633802a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_test = 128\n",
    "\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "batch_triplets_valid, valid_input_sample, valid_sim = batch_iterator(baseline, retrieval, None, \n",
    "                                                                                      baseline.train_data, \n",
    "                                                                                      baseline.dup_sets_train,\n",
    "                                                                                      bug_train_ids,\n",
    "                                                                                      batch_size_test, 1,\n",
    "                                                                                      issues_by_buckets)\n",
    "\n",
    "validation_sample = [valid_input_sample['title']['token'], valid_input_sample['title']['segment'], \n",
    "                   valid_input_sample['description']['token'], valid_input_sample['description']['segment'],\n",
    "                   valid_input_sample['info'], valid_sim]\n",
    "\n",
    "# Categorical columns\n",
    "number_of_columns_info = valid_input_sample['info'].shape[1]\n",
    "# Max sequence title\n",
    "MAX_SEQUENCE_LENGTH_T = valid_input_sample['title']['token'].shape[1]\n",
    "MAX_SEQUENCE_LENGTH_D = valid_input_sample['description']['token'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 20), (384, 20), (384, 20), (384, 20), (384, 544), (384,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input_sample['title']['token'].shape, \\\n",
    "valid_input_sample['description']['token'].shape, \\\n",
    "valid_input_sample['title']['segment'].shape, \\\n",
    "valid_input_sample['description']['segment'].shape, \\\n",
    "valid_input_sample['info'].shape, valid_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24mY22BGnkqp"
   },
   "source": [
    "### Validar entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "#baseline.display_batch(baseline.train_data, baseline.dup_sets_train, bug_train_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test ', 17002)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test \", len(baseline.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lev5Y7oaFQBd"
   },
   "source": [
    "## Propose\n",
    "\n",
    "https://github.com/tqtg/DuplicateBugFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform, RandomNormal, Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/CyberZHG/keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import compile_model, get_model\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def bert_model(MAX_SEQUENCE_LENGTH, name):\n",
    "    layer_num = 8\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#             config_path,\n",
    "#             model_path,\n",
    "#             training=True,\n",
    "#             trainable=True,\n",
    "#             seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#     )\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        model_path,\n",
    "        training=True,\n",
    "        use_adapter=True,\n",
    "        seq_len=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(12-layer_num, 13)] +\n",
    "        ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(layer_num)],\n",
    "    )\n",
    "#     model = get_model(\n",
    "#         token_num=len(token_dict),\n",
    "#         head_num=10,\n",
    "#         transformer_num=layer_num,\n",
    "#         embed_dim=100,\n",
    "#         feed_forward_dim=100,\n",
    "#         seq_len=MAX_SEQUENCE_LENGTH,\n",
    "#         pos_num=MAX_SEQUENCE_LENGTH,\n",
    "#         dropout_rate=0.05,\n",
    "#     )\n",
    "    compile_model(model)\n",
    "    inputs = model.inputs[:2]\n",
    "    outputs = model.get_layer('Encoder-{}-FeedForward-Norm'.format(layer_num)).output\n",
    "    #outputs = model.get_layer('Extract').output\n",
    "    outputs = GlobalAveragePooling1D()(outputs)\n",
    "    outputs = Dense(300, activation='tanh')(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='FeatureBERTGenerationModel{}'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_size):\n",
    "    info_input = Input(shape=(input_size, ), name='Feature_BugInput')\n",
    "    input_size = 300\n",
    "    \n",
    "    for units in [64, 32]:\n",
    "        layer = Dense(units, activation='tanh', kernel_initializer='random_uniform')(info_input)\n",
    "    \n",
    "    layer = Dense(input_size, activation='tanh')(info_input)\n",
    "    \n",
    "    mlp_feature_model = Model(inputs=[info_input], outputs=[layer], name = 'FeatureMlpGenerationModel')\n",
    "    \n",
    "    return mlp_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEedCg5AaTf2"
   },
   "source": [
    "### Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "VWBkSIYVaXyP",
    "outputId": "ed2a3d37-b8ec-4960-ef45-2909a87c8fa5"
   },
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "#     return mask_positives\n",
    "\n",
    "   # pos \n",
    "    embed_pos = tf.matmul(mask_positives_centroids, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "    # negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "\n",
    "#     return mask_positives_centroids\n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    batch_centroid_matrix_neg = tf.Variable([])\n",
    "    batch_centroid_matrix_all = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all):\n",
    "        # centroid pos\n",
    "        mask_positives_batch = tf.reshape(tf.gather(mask_positives, i), (-1, 1))\n",
    "        centroid_pos = tf.gather(centroid_embed_pos, i)\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_pos], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_pos = mask_positives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_positives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_pos, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid neg\n",
    "        centroid_neg = tf.gather(centroid_embed_neg, i)\n",
    "        mask_negatives_batch = tf.reshape(tf.gather(mask_negatives, i), (-1, 1))\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_neg], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_neg = mask_negatives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_negatives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_neg, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix_neg = tf.concat([batch_centroid_matrix_neg, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid pos and neg\n",
    "        new_batch_centroids = tf.reduce_sum([new_batch_centroid_pos, new_batch_centroid_neg], axis=0, keepdims=True)[0]\n",
    "        vects_new_batch_centroids = tf.concat([labels_pos, new_batch_centroids], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch_centroids)\n",
    "        batch_centroid_matrix_all = tf.concat([batch_centroid_matrix_all, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all]\n",
    "    _, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all = tf.while_loop(lambda i, a, b, c: i<batch_size // 3, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])])\n",
    "\n",
    "    TL_anchor_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    \n",
    "#     tl_weights = tf.truediv(num_of_pos, tf.reduce_max(num_of_pos))\n",
    "#     tl_w = tl_weights # tf.random_uniform_initializer(minval=0.0, maxval=tl_weights)(shape=[1])\n",
    "#     TL_pos_weighted = tf.reshape(batch_centroid_matrix, (-1, 1)) * tl_w\n",
    "#     TL_pos = tf.truediv(tf.reduce_sum(TL_pos_weighted), tf.reduce_sum(tl_w))\n",
    "\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL_neg = tf.reduce_mean(batch_centroid_matrix_neg) #triplet_loss(vects_neg)\n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix_all) # triplet_loss(vects_centroids)\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w]) #  \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, \n",
    "                    TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Add, Lambda, merge, Average, Maximum\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "def siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, sequence_length_info, \n",
    "                  sequence_length_t, sequence_length_d, name):\n",
    "  \n",
    "    # Title\n",
    "    bug_t_token = Input(shape = (sequence_length_t, ), name = 'title_token_{}'.format(name))\n",
    "    bug_t_segment = Input(shape = (sequence_length_t, ), name = 'title_segment_{}'.format(name))\n",
    "    # Description\n",
    "    bug_d_token = Input(shape = (sequence_length_d, ), name = 'desc_token_{}'.format(name))\n",
    "    bug_d_segment = Input(shape = (sequence_length_d, ), name = 'desc_segment_{}'.format(name))\n",
    "    # Categorical\n",
    "    bug_i = Input(shape = (sequence_length_info, ), name = 'info_{}'.format(name))\n",
    "    \n",
    "    bug_t_feat = title_feature_model([bug_t_token, bug_t_segment])\n",
    "    bug_d_feat = desc_feature_model([bug_d_token, bug_d_segment])\n",
    "    bug_i_feat = categorical_feature_model(bug_i)\n",
    "    \n",
    "    #bug_feature_output = Add(name = 'merge_features_{}'.format(name))([bug_i_feat, bug_t_feat, bug_d_feat])\n",
    "    bug_feature_output = concatenate([bug_i_feat, bug_t_feat, bug_d_feat], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    bug_feature_model = Model(inputs=[bug_t_token, bug_t_segment, bug_d_token, bug_d_segment, bug_i], outputs=[bug_feature_output], name = 'merge_features_{}'.format(name))\n",
    "    \n",
    "    return bug_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_objective(encoded_anchor, decay_lr=1):\n",
    "    \n",
    "    input_labels = Input(shape=(1,), name='input_label')    # input layer for labels\n",
    "    inputs = np.concatenate([encoded_anchor.input, [input_labels]], -1).tolist()\n",
    "    \n",
    "    encoded_anchor = encoded_anchor.output\n",
    "    \n",
    "    output = concatenate([input_labels, encoded_anchor])  # concatenating the labels + embeddings\n",
    "    \n",
    "    output = Lambda(quintet_loss, name='quintet_loss')(output)\n",
    "    \n",
    "    similarity_model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "    #optimizer = Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon(), schedule_decay=0.01)\n",
    "    # optimizer = Adam(lr=1e-3 * decay_lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "\n",
    "    # setup the optimization process \n",
    "    similarity_model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                         TL, TL_pos, TL_neg, TL_centroid]) \n",
    "    # metrics=[pos_distance, neg_distance, custom_margin_loss]\n",
    "\n",
    "    return similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def save_loss(result):\n",
    "    with open(os.path.join(DIR,'{}_log.pkl'.format(METHOD)), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(\"=> result saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-31-cd325db98846>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss (Lambda)           (9,)                 0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 161,318,372\n",
      "Trainable params: 846,196\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n",
      "Total of  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.51, Loss_test: 0.45\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.54, TL_pos: 0.53, TL_neg: 0.53, TL_centroid: 0.43\n",
      "Epoch: 2 Loss: 0.50, Loss_test: 0.45\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.45, TL_pos: 0.44, TL_neg: 0.60, TL_centroid: 0.53\n",
      "Epoch: 3 Loss: 0.57, Loss_test: 0.44\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.54, TL_pos: 0.53, TL_neg: 0.70, TL_centroid: 0.51\n",
      "Epoch: 4 Loss: 0.45, Loss_test: 0.43\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.51, TL_pos: 0.50, TL_neg: 0.37, TL_centroid: 0.41\n",
      "Epoch: 5 Loss: 0.48, Loss_test: 0.42\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.48, TL_pos: 0.47, TL_neg: 0.41, TL_centroid: 0.55\n",
      "Epoch: 6 Loss: 0.44, Loss_test: 0.40\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.38, TL_pos: 0.37, TL_neg: 0.43, TL_centroid: 0.58\n",
      "Epoch: 7 Loss: 0.42, Loss_test: 0.40\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.35, TL_pos: 0.34, TL_neg: 0.39, TL_centroid: 0.60\n",
      "Epoch: 8 Loss: 0.50, Loss_test: 0.39\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.44, TL_pos: 0.42, TL_neg: 0.61, TL_centroid: 0.55\n",
      "Epoch: 9 Loss: 0.40, Loss_test: 0.38\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.34, TL_pos: 0.33, TL_neg: 0.42, TL_centroid: 0.50\n",
      "=> result saved!\n",
      "Epoch: 10 Loss: 0.40, Loss_test: 0.38\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.37, TL_pos: 0.37, TL_neg: 0.26, TL_centroid: 0.59\n",
      "Epoch: 11 Loss: 0.42, Loss_test: 0.38\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.37, TL_pos: 0.36, TL_neg: 0.29, TL_centroid: 0.66\n",
      "Epoch: 12 Loss: 0.46, Loss_test: 0.37\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.44, TL_pos: 0.43, TL_neg: 0.27, TL_centroid: 0.72\n",
      "Epoch: 13 Loss: 0.45, Loss_test: 0.38\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.44, TL_pos: 0.43, TL_neg: 0.26, TL_centroid: 0.67\n",
      "Epoch: 14 Loss: 0.39, Loss_test: 0.37\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.30, TL_pos: 0.30, TL_neg: 0.19, TL_centroid: 0.75\n",
      "Epoch: 15 Loss: 0.37, Loss_test: 0.37\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.32, TL_pos: 0.31, TL_neg: 0.18, TL_centroid: 0.68\n",
      "Epoch: 16 Loss: 0.46, Loss_test: 0.37\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.33, TL_pos: 0.32, TL_neg: 0.42, TL_centroid: 0.78\n",
      "Epoch: 17 Loss: 0.46, Loss_test: 0.36\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.36, TL_pos: 0.34, TL_neg: 0.43, TL_centroid: 0.71\n",
      "Epoch: 18 Loss: 0.33, Loss_test: 0.35\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.24, TL_pos: 0.24, TL_neg: 0.16, TL_centroid: 0.70\n",
      "Epoch: 19 Loss: 0.43, Loss_test: 0.34\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.38, TL_pos: 0.37, TL_neg: 0.26, TL_centroid: 0.69\n",
      "=> result saved!\n",
      "Epoch: 20 Loss: 0.35, Loss_test: 0.34\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.28, TL_pos: 0.27, TL_neg: 0.17, TL_centroid: 0.68\n",
      "Epoch: 21 Loss: 0.32, Loss_test: 0.33\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.22, TL_neg: 0.10, TL_centroid: 0.76\n",
      "Epoch: 22 Loss: 0.31, Loss_test: 0.33\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.07, TL_centroid: 0.81\n",
      "Epoch: 23 Loss: 0.38, Loss_test: 0.33\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.28, TL_pos: 0.27, TL_neg: 0.13, TL_centroid: 0.84\n",
      "Epoch: 24 Loss: 0.45, Loss_test: 0.33\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.29, TL_pos: 0.29, TL_neg: 0.24, TL_centroid: 0.99\n",
      "Epoch: 25 Loss: 0.42, Loss_test: 0.33\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.30, TL_pos: 0.29, TL_neg: 0.28, TL_centroid: 0.82\n",
      "Epoch: 26 Loss: 0.42, Loss_test: 0.32\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.32, TL_pos: 0.31, TL_neg: 0.20, TL_centroid: 0.83\n",
      "Epoch: 27 Loss: 0.31, Loss_test: 0.32\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.21, TL_neg: 0.12, TL_centroid: 0.72\n",
      "Epoch: 28 Loss: 0.34, Loss_test: 0.32\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.18, TL_neg: 0.10, TL_centroid: 0.91\n",
      "Epoch: 29 Loss: 0.34, Loss_test: 0.32\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.19, TL_neg: 0.06, TL_centroid: 0.92\n",
      "=> result saved!\n",
      "Epoch: 30 Loss: 0.33, Loss_test: 0.31\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.21, TL_neg: 0.12, TL_centroid: 0.79\n",
      "Epoch: 31 Loss: 0.36, Loss_test: 0.31\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.24, TL_pos: 0.24, TL_neg: 0.17, TL_centroid: 0.79\n",
      "Epoch: 32 Loss: 0.34, Loss_test: 0.31\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.27, TL_pos: 0.27, TL_neg: 0.11, TL_centroid: 0.71\n",
      "Epoch: 33 Loss: 0.40, Loss_test: 0.31\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.30, TL_pos: 0.29, TL_neg: 0.18, TL_centroid: 0.84\n",
      "Epoch: 34 Loss: 0.48, Loss_test: 0.31\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.28, TL_pos: 0.27, TL_neg: 0.13, TL_centroid: 1.25\n",
      "Epoch: 35 Loss: 0.29, Loss_test: 0.30\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.15, TL_neg: 0.06, TL_centroid: 0.77\n",
      "Epoch: 36 Loss: 0.45, Loss_test: 0.30\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.32, TL_pos: 0.32, TL_neg: 0.11, TL_centroid: 1.07\n",
      "Epoch: 37 Loss: 0.49, Loss_test: 0.30\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.19, TL_neg: 0.25, TL_centroid: 1.31\n",
      "Epoch: 38 Loss: 0.34, Loss_test: 0.30\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.07, TL_centroid: 0.93\n",
      "Epoch: 39 Loss: 0.44, Loss_test: 0.29\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.27, TL_pos: 0.26, TL_neg: 0.14, TL_centroid: 1.08\n",
      "=> result saved!\n",
      "Epoch: 40 Loss: 0.41, Loss_test: 0.29\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.29, TL_pos: 0.28, TL_neg: 0.14, TL_centroid: 0.94\n",
      "Epoch: 41 Loss: 0.37, Loss_test: 0.29\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.25, TL_pos: 0.25, TL_neg: 0.09, TL_centroid: 0.90\n",
      "Epoch: 42 Loss: 0.33, Loss_test: 0.29\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.06, TL_centroid: 0.92\n",
      "Epoch: 43 Loss: 0.28, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.15, TL_neg: 0.03, TL_centroid: 0.78\n",
      "Epoch: 44 Loss: 0.35, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.07, TL_centroid: 0.97\n",
      "Epoch: 45 Loss: 0.26, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.79\n",
      "Epoch: 46 Loss: 0.32, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.07, TL_centroid: 0.94\n",
      "Epoch: 47 Loss: 0.41, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.20, TL_neg: 0.14, TL_centroid: 1.09\n",
      "Epoch: 48 Loss: 0.31, Loss_test: 0.28\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.21, TL_neg: 0.05, TL_centroid: 0.76\n",
      "Epoch: 49 Loss: 0.39, Loss_test: 0.27\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.24, TL_pos: 0.24, TL_neg: 0.06, TL_centroid: 1.04\n",
      "=> result saved!\n",
      "Epoch: 50 Loss: 0.40, Loss_test: 0.27\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.22, TL_neg: 0.08, TL_centroid: 1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 Loss: 0.46, Loss_test: 0.27\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.20, TL_neg: 0.11, TL_centroid: 1.33\n",
      "Epoch: 52 Loss: 0.27, Loss_test: 0.27\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.82\n",
      "Epoch: 53 Loss: 0.31, Loss_test: 0.27\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.05, TL_centroid: 0.86\n",
      "Epoch: 54 Loss: 0.36, Loss_test: 0.26\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.21, TL_neg: 0.14, TL_centroid: 0.85\n",
      "Epoch: 55 Loss: 0.32, Loss_test: 0.26\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.05, TL_centroid: 0.86\n",
      "Epoch: 56 Loss: 0.24, Loss_test: 0.26\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.03, TL_centroid: 0.76\n",
      "Epoch: 57 Loss: 0.34, Loss_test: 0.25\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.18, TL_neg: 0.07, TL_centroid: 0.91\n",
      "Epoch: 58 Loss: 0.26, Loss_test: 0.25\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.80\n",
      "Epoch: 59 Loss: 0.23, Loss_test: 0.25\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.05, TL_centroid: 0.58\n",
      "=> result saved!\n",
      "Epoch: 60 Loss: 0.28, Loss_test: 0.25\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.03, TL_centroid: 0.75\n",
      "Epoch: 61 Loss: 0.27, Loss_test: 0.24\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.17, TL_neg: 0.04, TL_centroid: 0.72\n",
      "Epoch: 62 Loss: 0.30, Loss_test: 0.24\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.20, TL_neg: 0.06, TL_centroid: 0.75\n",
      "Epoch: 63 Loss: 0.25, Loss_test: 0.24\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.24, TL_pos: 0.23, TL_neg: 0.04, TL_centroid: 0.50\n",
      "Epoch: 64 Loss: 0.30, Loss_test: 0.24\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.25, TL_pos: 0.25, TL_neg: 0.06, TL_centroid: 0.64\n",
      "Epoch: 65 Loss: 0.31, Loss_test: 0.24\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.19, TL_neg: 0.09, TL_centroid: 0.76\n",
      "Epoch: 66 Loss: 0.24, Loss_test: 0.23\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.03, TL_centroid: 0.66\n",
      "Epoch: 67 Loss: 0.22, Loss_test: 0.23\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.04, TL_centroid: 0.61\n",
      "Epoch: 68 Loss: 0.18, Loss_test: 0.23\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.51\n",
      "Epoch: 69 Loss: 0.27, Loss_test: 0.22\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.22, TL_neg: 0.08, TL_centroid: 0.57\n",
      "=> result saved!\n",
      "Epoch: 70 Loss: 0.29, Loss_test: 0.22\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.29, TL_pos: 0.28, TL_neg: 0.08, TL_centroid: 0.52\n",
      "Epoch: 71 Loss: 0.21, Loss_test: 0.22\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.17, TL_neg: 0.03, TL_centroid: 0.48\n",
      "Epoch: 72 Loss: 0.18, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.04, TL_centroid: 0.54\n",
      "Epoch: 73 Loss: 0.18, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.48\n",
      "Epoch: 74 Loss: 0.19, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.04, TL_centroid: 0.48\n",
      "Epoch: 75 Loss: 0.21, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.04, TL_centroid: 0.53\n",
      "Epoch: 76 Loss: 0.25, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.12, TL_centroid: 0.57\n",
      "Epoch: 77 Loss: 0.18, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.49\n",
      "Epoch: 78 Loss: 0.21, Loss_test: 0.21\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.15, TL_neg: 0.05, TL_centroid: 0.46\n",
      "Epoch: 79 Loss: 0.18, Loss_test: 0.20\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.44\n",
      "=> result saved!\n",
      "Epoch: 80 Loss: 0.23, Loss_test: 0.20\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.21, TL_neg: 0.06, TL_centroid: 0.41\n",
      "Epoch: 81 Loss: 0.25, Loss_test: 0.20\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.08, TL_centroid: 0.59\n",
      "Epoch: 82 Loss: 0.16, Loss_test: 0.19\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.04, TL_centroid: 0.36\n",
      "Epoch: 83 Loss: 0.22, Loss_test: 0.19\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.06, TL_centroid: 0.46\n",
      "Epoch: 84 Loss: 0.14, Loss_test: 0.19\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.31\n",
      "Epoch: 85 Loss: 0.19, Loss_test: 0.18\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.16, TL_neg: 0.06, TL_centroid: 0.35\n",
      "Epoch: 86 Loss: 0.15, Loss_test: 0.18\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.27\n",
      "Epoch: 87 Loss: 0.16, Loss_test: 0.18\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.03, TL_centroid: 0.27\n",
      "Epoch: 88 Loss: 0.21, Loss_test: 0.18\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.25, TL_pos: 0.25, TL_neg: 0.08, TL_centroid: 0.27\n",
      "Epoch: 89 Loss: 0.16, Loss_test: 0.18\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.04, TL_centroid: 0.24\n",
      "=> result saved!\n",
      "Epoch: 90 Loss: 0.10, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.19\n",
      "Epoch: 91 Loss: 0.16, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.16, TL_neg: 0.05, TL_centroid: 0.28\n",
      "Epoch: 92 Loss: 0.18, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.19, TL_neg: 0.05, TL_centroid: 0.28\n",
      "Epoch: 93 Loss: 0.14, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.03, TL_centroid: 0.27\n",
      "Epoch: 94 Loss: 0.13, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.15, TL_neg: 0.02, TL_centroid: 0.20\n",
      "Epoch: 95 Loss: 0.13, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.20\n",
      "Epoch: 96 Loss: 0.18, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.09, TL_centroid: 0.32\n",
      "Epoch: 97 Loss: 0.15, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.18, TL_neg: 0.05, TL_centroid: 0.19\n",
      "Epoch: 98 Loss: 0.14, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.25\n",
      "Epoch: 99 Loss: 0.10, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.18\n",
      "=> result saved!\n",
      "Epoch: 100 Loss: 0.17, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.23, TL_pos: 0.22, TL_neg: 0.06, TL_centroid: 0.17, recall@25: 0.61\n",
      "Best_epoch=90, Best_loss=0.10, Recall@25=0.61\n",
      "CPU times: user 13h 41min 15s, sys: 2h 29min 52s, total: 16h 11min 8s\n",
      "Wall time: 2h 14min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "\n",
    "# Inspired on https://'pastebin.com/TaGFdcBA\n",
    "# TODO: https://stackoverflow.com/questions/49941903/keras-compute-cosine-distance-between-two-flattened-outputs\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Clear GPU memory\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "# Feature models\n",
    "'''\n",
    "    cnn_model\n",
    "    lstm_model\n",
    "    mlp_model\n",
    "'''\n",
    "title_feature_model = bert_model(MAX_SEQUENCE_LENGTH_T, 'Title')\n",
    "desc_feature_model = bert_model(MAX_SEQUENCE_LENGTH_D, 'Description')\n",
    "categorical_feature_model = mlp_model(number_of_columns_info)\n",
    "\n",
    "# Similarity model\n",
    "encoded_anchor = siamese_model(title_feature_model, desc_feature_model, categorical_feature_model, \n",
    "                                     number_of_columns_info, MAX_SEQUENCE_LENGTH_T, MAX_SEQUENCE_LENGTH_D, 'in')\n",
    "\n",
    "similarity_model = max_margin_objective(encoded_anchor, decay_lr=1)\n",
    "\n",
    "# cnn_feature_model.summary()\n",
    "# lstm_feature_model.summary()\n",
    "similarity_model.summary()\n",
    "\n",
    "'''\n",
    "    Experiment\n",
    "'''\n",
    "result = { 'train' : [], 'test' : [] }\n",
    "limit_train = int(epochs * freeze_train) # 10% de 1000 , 100 epocas\n",
    "print(\"Total of \", limit_train)\n",
    "for epoch in range(limit_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, encoded_anchor, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "#     if epoch == 10:\n",
    "#         similarity_model = max_margin_objective(encoded_anchor, encoded_positive, encoded_negative, decay_lr=0.1)\n",
    "    \n",
    "    h = similarity_model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = similarity_model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == limit_train) ):\n",
    "        save_loss(result)\n",
    "    \n",
    "    if (epoch+1 == limit_train): #(epoch > 1 and epoch % 10 == 0) or (epoch+1 == epochs):\n",
    "        recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, verbose, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "                \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "              \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], \n",
    "                                          h[4], h[5], h[6], h[7], h[8], recall))\n",
    "    else:\n",
    "        print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))\n",
    "    loss = h[0]\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "# experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "# experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "print('Best_epoch={}, Best_loss={:.2f}, Recall@25={:.2f}'.format(best_epoch, best_loss, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepQL_weights_{}'.format(limit_train)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/netbeans/bert/exported_rank_deepQL_weights_100.txt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model 'modelos/model_bert_preprocessing_deepQL_weights_100_feature_100epochs_64batch(netbeans).h5' to disk\n"
     ]
    }
   ],
   "source": [
    "experiment.save_model(similarity_model, SAVE_PATH.replace('@number_of_epochs@', str(limit_train)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(limit_train)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "info_in (InputLayer)            (None, 544)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_token_in (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_segment_in (InputLayer)   (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_token_in (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_in (InputLayer)    (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeatureMlpGenerationModel (Mode (None, 300)          163500      info_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelTitle (None, 300)          80577436    title_token_in[0][0]             \n",
      "                                                                 title_segment_in[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FeatureBERTGenerationModelDescr (None, 300)          80577436    desc_token_in[0][0]              \n",
      "                                                                 desc_segment_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_in (Concatenate) (None, 900)          0           FeatureMlpGenerationModel[1][0]  \n",
      "                                                                 FeatureBERTGenerationModelTitle[1\n",
      "                                                                 FeatureBERTGenerationModelDescrip\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 901)          0           input_label[0][0]                \n",
      "                                                                 merge_features_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quintet_loss_unfreezed (Lambda) (9,)                 0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 161,318,372\n",
      "Trainable params: 846,196\n",
      "Non-trainable params: 160,472,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = similarity_model.get_layer('concatenate_1')\n",
    "output = Lambda(quintet_loss, name='quintet_loss_unfreezed')(model.output)\n",
    "inputs = similarity_model.inputs\n",
    "model = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')\n",
    "\n",
    "# setup the optimization process \n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[TL_w_anchor, TL_w_pos, TL_w_neg, TL_w_centroid,\n",
    "                                                                     TL, TL_pos, TL_neg, TL_centroid])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Loss: 0.14, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.02, TL_centroid: 0.18\n",
      "Epoch: 102 Loss: 0.21, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.29, TL_pos: 0.29, TL_neg: 0.08, TL_centroid: 0.19\n",
      "Epoch: 103 Loss: 0.11, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.18\n",
      "Epoch: 104 Loss: 0.07, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.14\n",
      "Epoch: 105 Loss: 0.08, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.13\n",
      "Epoch: 106 Loss: 0.09, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.14\n",
      "Epoch: 107 Loss: 0.10, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.14\n",
      "Epoch: 108 Loss: 0.08, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.10\n",
      "Epoch: 109 Loss: 0.16, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.19, TL_neg: 0.06, TL_centroid: 0.19\n",
      "=> result saved!\n",
      "Epoch: 110 Loss: 0.07, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.12\n",
      "Epoch: 111 Loss: 0.09, Loss_test: 0.17\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.12\n",
      "Epoch: 112 Loss: 0.12, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.02, TL_centroid: 0.14\n",
      "Epoch: 113 Loss: 0.12, Loss_test: 0.16\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.16, TL_neg: 0.04, TL_centroid: 0.12\n",
      "Epoch: 114 Loss: 0.14, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.17, TL_neg: 0.08, TL_centroid: 0.14\n",
      "Epoch: 115 Loss: 0.06, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.10\n",
      "Epoch: 116 Loss: 0.16, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.23, TL_pos: 0.23, TL_neg: 0.06, TL_centroid: 0.12\n",
      "Epoch: 117 Loss: 0.13, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.04, TL_centroid: 0.13\n",
      "Epoch: 118 Loss: 0.14, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.20, TL_neg: 0.04, TL_centroid: 0.11\n",
      "Epoch: 119 Loss: 0.12, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.05, TL_centroid: 0.10\n",
      "=> result saved!\n",
      "Epoch: 120 Loss: 0.07, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.12\n",
      "Epoch: 121 Loss: 0.13, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.19, TL_neg: 0.03, TL_centroid: 0.11\n",
      "Epoch: 122 Loss: 0.09, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.15\n",
      "Epoch: 123 Loss: 0.12, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.05, TL_centroid: 0.12\n",
      "Epoch: 124 Loss: 0.14, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.19, TL_neg: 0.06, TL_centroid: 0.10\n",
      "Epoch: 125 Loss: 0.09, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.12\n",
      "Epoch: 126 Loss: 0.12, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.16, TL_neg: 0.03, TL_centroid: 0.13\n",
      "Epoch: 127 Loss: 0.14, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.22, TL_pos: 0.21, TL_neg: 0.02, TL_centroid: 0.11\n",
      "Epoch: 128 Loss: 0.07, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 129 Loss: 0.08, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.09, TL_neg: 0.03, TL_centroid: 0.10\n",
      "=> result saved!\n",
      "Epoch: 130 Loss: 0.15, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.23, TL_pos: 0.23, TL_neg: 0.04, TL_centroid: 0.10\n",
      "Epoch: 131 Loss: 0.10, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.02, TL_centroid: 0.11\n",
      "Epoch: 132 Loss: 0.12, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.17, TL_pos: 0.17, TL_neg: 0.03, TL_centroid: 0.10\n",
      "Epoch: 133 Loss: 0.06, Loss_test: 0.15\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 134 Loss: 0.06, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 135 Loss: 0.10, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.10\n",
      "Epoch: 136 Loss: 0.09, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 137 Loss: 0.09, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.10\n",
      "Epoch: 138 Loss: 0.10, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 139 Loss: 0.06, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.09\n",
      "=> result saved!\n",
      "Epoch: 140 Loss: 0.09, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.09\n",
      "Epoch: 141 Loss: 0.05, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 142 Loss: 0.12, Loss_test: 0.14\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.18, TL_neg: 0.03, TL_centroid: 0.10\n",
      "Epoch: 143 Loss: 0.10, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 144 Loss: 0.09, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 145 Loss: 0.10, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.09\n",
      "Epoch: 146 Loss: 0.12, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.19, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 147 Loss: 0.11, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.15, TL_neg: 0.04, TL_centroid: 0.09\n",
      "Epoch: 148 Loss: 0.06, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 149 Loss: 0.13, Loss_test: 0.13\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.19, TL_pos: 0.19, TL_neg: 0.04, TL_centroid: 0.09\n",
      "=> result saved!\n",
      "Epoch: 150 Loss: 0.11, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 151 Loss: 0.13, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.20, TL_pos: 0.19, TL_neg: 0.05, TL_centroid: 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152 Loss: 0.08, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 153 Loss: 0.09, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 154 Loss: 0.10, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.04, TL_centroid: 0.08\n",
      "Epoch: 155 Loss: 0.11, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 156 Loss: 0.07, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 157 Loss: 0.06, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 158 Loss: 0.10, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.10\n",
      "Epoch: 159 Loss: 0.11, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.03, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 160 Loss: 0.05, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 161 Loss: 0.12, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.18, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 162 Loss: 0.05, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.00, TL_centroid: 0.09\n",
      "Epoch: 163 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 164 Loss: 0.07, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.06\n",
      "Epoch: 165 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 166 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 167 Loss: 0.07, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 168 Loss: 0.08, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 169 Loss: 0.05, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.00, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 170 Loss: 0.04, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 171 Loss: 0.08, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.01, TL_centroid: 0.10\n",
      "Epoch: 172 Loss: 0.09, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.10\n",
      "Epoch: 173 Loss: 0.07, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 174 Loss: 0.05, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 175 Loss: 0.07, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 176 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 177 Loss: 0.12, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 178 Loss: 0.09, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 179 Loss: 0.15, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.23, TL_pos: 0.22, TL_neg: 0.06, TL_centroid: 0.10\n",
      "=> result saved!\n",
      "Epoch: 180 Loss: 0.06, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 181 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 182 Loss: 0.09, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.12, TL_neg: 0.04, TL_centroid: 0.05\n",
      "Epoch: 183 Loss: 0.08, Loss_test: 0.12\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 184 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 185 Loss: 0.05, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 186 Loss: 0.07, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 187 Loss: 0.09, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 188 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 189 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.09\n",
      "=> result saved!\n",
      "Epoch: 190 Loss: 0.10, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 191 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 192 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 193 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 194 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.10\n",
      "Epoch: 195 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.04, TL_centroid: 0.07\n",
      "Epoch: 196 Loss: 0.08, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 197 Loss: 0.11, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.04, TL_centroid: 0.07\n",
      "Epoch: 198 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 199 Loss: 0.08, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 200 Loss: 0.14, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.21, TL_pos: 0.21, TL_neg: 0.03, TL_centroid: 0.10\n",
      "Epoch: 201 Loss: 0.09, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.11\n",
      "Epoch: 202 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.03, TL_centroid: 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203 Loss: 0.09, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.09\n",
      "Epoch: 204 Loss: 0.08, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 205 Loss: 0.07, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 206 Loss: 0.10, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.16, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 207 Loss: 0.09, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 208 Loss: 0.08, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 209 Loss: 0.10, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.06, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 210 Loss: 0.05, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 211 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 212 Loss: 0.08, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 213 Loss: 0.09, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 214 Loss: 0.07, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 215 Loss: 0.09, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 216 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 217 Loss: 0.06, Loss_test: 0.11\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.00, TL_centroid: 0.09\n",
      "Epoch: 218 Loss: 0.10, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.15, TL_neg: 0.04, TL_centroid: 0.06\n",
      "Epoch: 219 Loss: 0.08, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.04, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 220 Loss: 0.07, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 221 Loss: 0.09, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.04, TL_centroid: 0.07\n",
      "Epoch: 222 Loss: 0.10, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.05, TL_centroid: 0.08\n",
      "Epoch: 223 Loss: 0.05, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 224 Loss: 0.05, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 225 Loss: 0.04, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 226 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.00, TL_centroid: 0.06\n",
      "Epoch: 227 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 228 Loss: 0.10, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 229 Loss: 0.10, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.16, TL_pos: 0.15, TL_neg: 0.04, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 230 Loss: 0.07, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 231 Loss: 0.10, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 232 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 233 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 234 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 235 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 236 Loss: 0.04, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 237 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 238 Loss: 0.09, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.04, TL_centroid: 0.08\n",
      "Epoch: 239 Loss: 0.10, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.14, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 240 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.06, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 241 Loss: 0.03, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.03, TL_pos: 0.03, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 242 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 243 Loss: 0.09, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.04, TL_centroid: 0.07\n",
      "Epoch: 244 Loss: 0.05, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 245 Loss: 0.05, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 246 Loss: 0.06, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.10\n",
      "Epoch: 247 Loss: 0.08, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 248 Loss: 0.06, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 249 Loss: 0.08, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.01, TL_centroid: 0.06\n",
      "=> result saved!\n",
      "Epoch: 250 Loss: 0.04, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 251 Loss: 0.07, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 252 Loss: 0.09, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 253 Loss: 0.04, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 Loss: 0.06, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 255 Loss: 0.08, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 256 Loss: 0.08, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.07\n",
      "Epoch: 257 Loss: 0.06, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 258 Loss: 0.05, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 259 Loss: 0.09, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 260 Loss: 0.05, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 261 Loss: 0.05, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 262 Loss: 0.06, Loss_test: 0.10\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 263 Loss: 0.07, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 264 Loss: 0.07, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 265 Loss: 0.10, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.13, TL_neg: 0.03, TL_centroid: 0.09\n",
      "Epoch: 266 Loss: 0.04, Loss_test: 0.09\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 267 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 268 Loss: 0.08, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 269 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.02, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 270 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.04, TL_centroid: 0.07\n",
      "Epoch: 271 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 272 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 273 Loss: 0.09, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 274 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.06\n",
      "Epoch: 275 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 276 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 277 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 278 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 279 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 280 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 281 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 282 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 283 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 284 Loss: 0.08, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 285 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 286 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 287 Loss: 0.09, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 288 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 289 Loss: 0.07, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 290 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 291 Loss: 0.05, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 292 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 293 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 294 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 295 Loss: 0.08, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 296 Loss: 0.10, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 297 Loss: 0.09, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 298 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 299 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 300 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 301 Loss: 0.07, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 302 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 303 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 304 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 305 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.00, TL_centroid: 0.09\n",
      "Epoch: 306 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 307 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 308 Loss: 0.08, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 309 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.05\n",
      "=> result saved!\n",
      "Epoch: 310 Loss: 0.05, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 311 Loss: 0.10, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.15, TL_pos: 0.15, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 312 Loss: 0.06, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 313 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 314 Loss: 0.04, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.00, TL_centroid: 0.05\n",
      "Epoch: 315 Loss: 0.09, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.14, TL_pos: 0.14, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 316 Loss: 0.03, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.03, TL_pos: 0.03, TL_neg: 0.00, TL_centroid: 0.06\n",
      "Epoch: 317 Loss: 0.05, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 318 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 319 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 320 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.00, TL_centroid: 0.06\n",
      "Epoch: 321 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 322 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 323 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 324 Loss: 0.07, Loss_test: 0.08\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 325 Loss: 0.05, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 326 Loss: 0.12, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.18, TL_pos: 0.17, TL_neg: 0.05, TL_centroid: 0.07\n",
      "Epoch: 327 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 328 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 329 Loss: 0.04, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.00, TL_centroid: 0.05\n",
      "=> result saved!\n",
      "Epoch: 330 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 331 Loss: 0.05, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 332 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 333 Loss: 0.08, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.00, TL_centroid: 0.06\n",
      "Epoch: 334 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 335 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 336 Loss: 0.07, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 337 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 338 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 339 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 340 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 341 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 342 Loss: 0.07, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 343 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 344 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 345 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 346 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.05\n",
      "Epoch: 347 Loss: 0.06, Loss_test: 0.07\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 348 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.05\n",
      "Epoch: 349 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.02, TL_centroid: 0.09\n",
      "=> result saved!\n",
      "Epoch: 350 Loss: 0.03, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.03, TL_pos: 0.03, TL_neg: 0.01, TL_centroid: 0.05\n",
      "Epoch: 351 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.05, TL_centroid: 0.09\n",
      "Epoch: 352 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.02, TL_centroid: 0.09\n",
      "Epoch: 353 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 354 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 355 Loss: 0.03, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.03, TL_pos: 0.03, TL_neg: 0.02, TL_centroid: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 356 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 357 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 358 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 359 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.05, TL_pos: 0.05, TL_neg: 0.01, TL_centroid: 0.08\n",
      "=> result saved!\n",
      "Epoch: 360 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 361 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 362 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.06, TL_pos: 0.06, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 363 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 364 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 365 Loss: 0.04, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 366 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 367 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.09, TL_pos: 0.09, TL_neg: 0.02, TL_centroid: 0.06\n",
      "Epoch: 368 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.03, TL_centroid: 0.08\n",
      "Epoch: 369 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.01, TL_centroid: 0.09\n",
      "=> result saved!\n",
      "Epoch: 370 Loss: 0.05, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.07, TL_pos: 0.07, TL_neg: 0.00, TL_centroid: 0.07\n",
      "Epoch: 371 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.12, TL_pos: 0.12, TL_neg: 0.01, TL_centroid: 0.06\n",
      "Epoch: 372 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.01, TL_centroid: 0.07\n",
      "Epoch: 373 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.08\n",
      "Epoch: 374 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.11, TL_neg: 0.02, TL_centroid: 0.07\n",
      "Epoch: 375 Loss: 0.07, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.10, TL_pos: 0.09, TL_neg: 0.01, TL_centroid: 0.09\n",
      "Epoch: 376 Loss: 0.03, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.03, TL_pos: 0.03, TL_neg: 0.00, TL_centroid: 0.06\n",
      "Epoch: 377 Loss: 0.06, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.08, TL_pos: 0.08, TL_neg: 0.01, TL_centroid: 0.08\n",
      "Epoch: 378 Loss: 0.04, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.04, TL_pos: 0.04, TL_neg: 0.01, TL_centroid: 0.05\n",
      "Epoch: 379 Loss: 0.08, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.11, TL_pos: 0.10, TL_neg: 0.02, TL_centroid: 0.07\n",
      "=> result saved!\n",
      "Epoch: 380 Loss: 0.10, Loss_test: 0.06\n",
      "TL_w: 1.00, TL_pos_w: 1.00, TL_neg_w: 1.00, TL_centroid_w: 1.00\n",
      "TL: 0.13, TL_pos: 0.13, TL_neg: 0.05, TL_centroid: 0.08\n"
     ]
    }
   ],
   "source": [
    "end_train = epochs - limit_train\n",
    "for epoch in range(limit_train, end_train):\n",
    "    batch_triplet_train, \\\n",
    "        train_input_sample, train_sim = batch_iterator(baseline, retrieval, model, baseline.train_data, \n",
    "                                                       baseline.dup_sets_train, bug_train_ids, \n",
    "                                                           batch_size, 1, issues_by_buckets, TRIPLET_HARD=False)\n",
    "    train_batch = [train_input_sample['title']['token'], train_input_sample['title']['segment'], \n",
    "                   train_input_sample['description']['token'], train_input_sample['description']['segment'],\n",
    "                   train_input_sample['info'], train_sim]\n",
    "    \n",
    "\n",
    "    h = model.train_on_batch(x=train_batch, y=train_sim)\n",
    "    h_validation = model.test_on_batch(x=validation_sample, y=valid_sim)\n",
    "    \n",
    "    # save results\n",
    "    result['train'].append(h)\n",
    "    result['test'].append(h_validation)\n",
    "    \n",
    "    if( (epoch+1) % 10 == 0 or (epoch+1 == end_train )):\n",
    "        save_loss(result)\n",
    "    \n",
    "    print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "               \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "              \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}\").format(\n",
    "            epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['train']), len(result['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.get_layer('merge_features_in')\n",
    "output = encoded.output\n",
    "inputs = similarity_model.inputs[:-1]\n",
    "encoded_anchor = Model(inputs = inputs, outputs = output, name = 'Similarity_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain to use\n",
    "METHOD = 'deepQL_weights_{}'.format(epochs)\n",
    "SAVE_PATH = '{}_preprocessing_{}_feature@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)\n",
    "SAVE_PATH_FEATURE = '{}_preprocessing_{}_feature_@number_of_epochs@epochs_64batch({})'.format(PREPROCESSING, METHOD, DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save_model(model, SAVE_PATH.replace('@number_of_epochs@', str(epochs)))\n",
    "experiment.save_model(encoded_anchor, SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs)), verbose=1)\n",
    "\"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, exported_rank, debug = experiment.evaluate_validation_test(retrieval, 1, encoded_anchor, issues_by_buckets, \n",
    "                                                               bug_train_ids, method='bert')\n",
    "print((\"Epoch: {} Loss: {:.2f}, Loss_test: {:.2f}\\n\" +\n",
    "       \"TL_w: {:.2f}, TL_pos_w: {:.2f}, TL_neg_w: {:.2f}, TL_centroid_w: {:.2f}\\n\" + \n",
    "        \"TL: {:.2f}, TL_pos: {:.2f}, TL_neg: {:.2f}, TL_centroid: {:.2f}, \" +\n",
    "      \"recall@25: {:.2f}\").format(epoch+1, h[0], h_validation[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7], h[8], recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['131079:124863|157441:0.4148419499397278,153250:0.38918328285217285,88786:0.35134613513946533,159448:0.33832842111587524,128057:0.3276810050010681,151301:0.3177196979522705,152163:0.31371253728866577,114524:0.30969077348709106,162718:0.3042418956756592,144081:0.2951458692550659,146370:0.29255032539367676,131439:0.29044628143310547,109493:0.2879393696784973,142671:0.28376519680023193,140484:0.2794573903083801,137689:0.27797865867614746,132127:0.27490001916885376,140154:0.27451425790786743,156232:0.27409160137176514,140248:0.27330899238586426,153877:0.2647252678871155,166692:0.25921231508255005,153597:0.25295156240463257,153598:0.25295156240463257,153600:0.25295156240463257,153601:0.25295156240463257,153602:0.25295156240463257,153603:0.25295156240463257,153604:0.25295156240463257',\n",
       " '131082:131085|131085:1.0,171788:0.3017921447753906,82394:0.30098146200180054,117293:0.2969033122062683,118987:0.2951868772506714,114168:0.25983726978302,140788:0.25778019428253174,82208:0.25458401441574097,105529:0.24804610013961792,80035:0.2468584179878235,107282:0.23830318450927734,100619:0.23304200172424316,100621:0.23304200172424316,100623:0.23304200172424316,107916:0.22490090131759644,118637:0.21415472030639648,86847:0.18871921300888062,130184:0.18209725618362427,123473:0.15296781063079834,114132:0.15211904048919678,116090:0.14697813987731934,140718:0.1343151330947876,157300:0.13171309232711792,105333:0.13034802675247192,134304:0.12660956382751465,107239:0.12268418073654175,107403:0.12267124652862549,92347:0.12217950820922852,108718:0.12032502889633179',\n",
       " '131083:145537,124867,144649,131084,132247,129785,129786,145533,145534,143327|131084:0.9997939883905929,145533:0.9997939883905929,132247:0.9079520329833031,145534:0.9079520329833031,144649:0.7852092981338501,147776:0.7852092981338501,136337:0.6193545758724213,124867:0.6113282740116119,129786:0.6113282740116119,129785:0.603154331445694,146754:0.47832244634628296,160439:0.4582001566886902,161539:0.4398413300514221,163324:0.4398413300514221,131086:0.43957579135894775,124857:0.4206452965736389,125020:0.4206452965736389,171109:0.4188724160194397,128350:0.4176982641220093,143610:0.41318607330322266,155409:0.41230249404907227,159010:0.41230249404907227,170095:0.40360480546951294,139805:0.4017406105995178,142299:0.3943029046058655,142296:0.3875722885131836,141578:0.3871152997016907,171278:0.3869863748550415,143344:0.3858270049095154',\n",
       " '131084:145537,124867,144649,131083,132247,129785,129786,145533,145534,143327|131083:0.9997939883905929,145533:0.9997939883905929,132247:0.9079520329833031,145534:0.9079520329833031,144649:0.7852092981338501,147776:0.7852092981338501,136337:0.6193545758724213,124867:0.6113282740116119,129786:0.6113282740116119,129785:0.603154331445694,146754:0.47832244634628296,160439:0.4582001566886902,161539:0.4398413300514221,163324:0.4398413300514221,131086:0.43957579135894775,124857:0.4206452965736389,125020:0.4206452965736389,171109:0.4188724160194397,128350:0.4176982641220093,143610:0.41318607330322266,155409:0.41230249404907227,159010:0.41230249404907227,170095:0.40360480546951294,139805:0.4017406105995178,142299:0.3943029046058655,142296:0.3875722885131836,141578:0.3871152997016907,171278:0.3869863748550415,143344:0.3858270049095154',\n",
       " '131085:131082|131082:1.0,171788:0.3017921447753906,82394:0.30098146200180054,117293:0.2969033122062683,118987:0.2951868772506714,114168:0.25983726978302,140788:0.25778019428253174,82208:0.25458401441574097,105529:0.24804610013961792,80035:0.2468584179878235,107282:0.23830318450927734,100619:0.23304200172424316,100621:0.23304200172424316,100623:0.23304200172424316,107916:0.22490090131759644,118637:0.21415472030639648,86847:0.18871921300888062,130184:0.18209725618362427,123473:0.15296781063079834,114132:0.15211904048919678,116090:0.14697813987731934,140718:0.1343151330947876,157300:0.13171309232711792,105333:0.13034802675247192,134304:0.12660956382751465,107239:0.12268418073654175,107403:0.12267124652862549,92347:0.12217950820922852,108718:0.12032502889633179',\n",
       " '131086:118825,117035|118825:0.49500012397766113,128350:0.45154619216918945,155409:0.4463520050048828,159010:0.4463520050048828,131083:0.43957579135894775,131084:0.43957579135894775,145533:0.43957579135894775,171109:0.42409223318099976,143344:0.416096568107605,132247:0.4140382409095764,145534:0.4140382409095764,139805:0.40376484394073486,170095:0.40327417850494385,161539:0.40046149492263794,163324:0.40046149492263794,146754:0.40003979206085205,144649:0.3908368945121765,147776:0.3908368945121765,173067:0.39082425832748413,142352:0.39046216011047363,142353:0.39046216011047363,143610:0.3887266516685486,136337:0.3795892596244812,171278:0.37868356704711914,156889:0.3737891912460327,142299:0.3690446615219116,143343:0.367892861366272,160439:0.36458927392959595,173988:0.36401307582855225',\n",
       " '131103:129349,131439,130960,130033,132127|130960:0.3269888758659363,129349:0.3227629065513611,135105:0.2490137219429016,131566:0.24394017457962036,117299:0.1940874457359314,151900:0.1917838454246521,89840:0.17931920289993286,76734:0.16352546215057373,114713:0.1633327603340149,163036:0.15839064121246338,158346:0.15612280368804932,140245:0.15577465295791626,114721:0.15270859003067017,161088:0.15072768926620483,127532:0.1500554084777832,129358:0.14523571729660034,147001:0.14433568716049194,158344:0.14363396167755127,136011:0.14057785272598267,107259:0.13916075229644775,166780:0.13827669620513916,114132:0.1365552544593811,168182:0.13615566492080688,145390:0.13491171598434448,101010:0.13155853748321533,170700:0.1314137578010559,104338:0.1293899416923523,141678:0.12795031070709229,106184:0.12304407358169556',\n",
       " '131126:117856,118467,118468,118469,126180,116295,116680,118634,117003,103531,131983,118351,118735,109177,129907,117299,104665,117274,117981|169562:0.3784828782081604,137762:0.3403099775314331,169573:0.33200734853744507,156483:0.32401591539382935,104665:0.3231239318847656,148009:0.32281672954559326,109738:0.3065221309661865,122650:0.30637258291244507,118879:0.3053170442581177,143455:0.3025828003883362,155394:0.29991453886032104,167164:0.298603892326355,120910:0.297268271446228,101020:0.29525208473205566,141178:0.29497963190078735,88755:0.2935065031051636,139767:0.2917066216468811,170431:0.2868006229400635,134548:0.28653955459594727,97579:0.2859645485877991,169270:0.2856626510620117,88360:0.2838131785392761,122681:0.281882107257843,143274:0.2782668471336365,125870:0.27709418535232544,169625:0.27367091178894043,160071:0.2727252244949341,103371:0.27247530221939087,111876:0.26596635580062866',\n",
       " '131145:131146|131146:0.6874652802944183,134835:0.3077518939971924,92551:0.30528461933135986,163131:0.2879285216331482,155317:0.27964866161346436,163767:0.24942141771316528,170733:0.2400137186050415,150256:0.2398105263710022,147572:0.23031175136566162,170734:0.2151244878768921,134494:0.2021893858909607,149558:0.1969316005706787,150698:0.19381451606750488,148727:0.19320541620254517,165046:0.19072610139846802,150118:0.1664271354675293,149769:0.1594642996788025,89870:0.1589146852493286,118502:0.1509028673171997,158747:0.1498551368713379,153476:0.14654791355133057,168058:0.1353217363357544,105081:0.12938493490219116,124979:0.12891525030136108,107409:0.1287493109703064,151373:0.1265096664428711,113884:0.12570065259933472,116998:0.12311327457427979,146248:0.12177306413650513',\n",
       " '131146:131145|131145:0.6874652802944183,134835:0.3565635085105896,163131:0.31739312410354614,155317:0.2944202423095703,163767:0.2638382911682129,92551:0.2560262084007263,150256:0.24755644798278809,147572:0.24484872817993164,170734:0.23705774545669556,170733:0.211500883102417,150698:0.20853179693222046,148727:0.20440673828125,134494:0.20322144031524658,89870:0.19586312770843506,149769:0.19029009342193604,149558:0.1857197880744934,153476:0.18563276529312134,124979:0.1786738634109497,165046:0.1761099100112915,113884:0.17337018251419067,91943:0.17235249280929565,150118:0.16917943954467773,168058:0.16658830642700195,146248:0.16615402698516846,104148:0.16547083854675293,158747:0.15284103155136108,119261:0.15126395225524902,116998:0.14918172359466553,118502:0.1489364504814148',\n",
       " '131148:130987|112035:0.17207324504852295,151855:0.1414167881011963,147883:0.1351538896560669,167283:0.12908345460891724,134584:0.12770581245422363,158945:0.12079942226409912,156323:0.11937588453292847,161363:0.1174997091293335,147585:0.1146882176399231,167273:0.11205130815505981,146594:0.11156117916107178,154257:0.10262298583984375,159970:0.10145127773284912,110681:0.10142368078231812,140286:0.10073506832122803,136978:0.1000661849975586,103162:0.09991544485092163,145425:0.09977126121520996,144809:0.09888046979904175,159296:0.09877467155456543,175200:0.09861201047897339,139147:0.09813833236694336,147736:0.09693789482116699,161159:0.09646838903427124,134241:0.09453755617141724,161881:0.09361177682876587,143573:0.09261864423751831,144940:0.09254330396652222,170247:0.09181433916091919',\n",
       " '131149:131359,131335,131153,132377,130783|130783:0.47099220752716064,135723:0.2044144868850708,118743:0.20187246799468994,162427:0.17834895849227905,160575:0.17322999238967896,105110:0.16421711444854736,117705:0.15304124355316162,104706:0.14379125833511353,119995:0.13846468925476074,138781:0.13153290748596191,124001:0.12870770692825317,156953:0.12656545639038086,166240:0.12400740385055542,105016:0.1193091869354248,78931:0.11755609512329102,134571:0.11418461799621582,123928:0.10967570543289185,174048:0.10459131002426147,110525:0.10133785009384155,77512:0.1011916995048523,74323:0.09655994176864624,142556:0.096096932888031,136011:0.0932268500328064,103043:0.09253823757171631,173340:0.08901149034500122,152010:0.08596307039260864,131359:0.08262801170349121,121337:0.08062905073165894,121338:0.08062905073165894',\n",
       " '131151:132544,131236|127846:0.5007576942443848,131236:0.4006832242012024,132544:0.3185916543006897,154096:0.30109280347824097,100085:0.2928277850151062,125829:0.2752203345298767,115901:0.266965389251709,130982:0.24491995573043823,110218:0.2192784547805786,110060:0.19507592916488647,123764:0.19344717264175415,151654:0.19248449802398682,150448:0.1916857361793518,150575:0.1856101155281067,123950:0.18315523862838745,103294:0.176108717918396,122332:0.1676884889602661,127306:0.15305936336517334,120761:0.1515728235244751,121397:0.13958150148391724,124564:0.13333696126937866,103751:0.11535024642944336,137499:0.09727203845977783,103772:0.09696674346923828,143307:0.09687429666519165,93205:0.09684371948242188,113235:0.09637302160263062,111578:0.09095382690429688,102846:0.09053897857666016',\n",
       " '131153:131359,131335,131149,132377,130783|162237:0.3803858160972595,141397:0.3300069570541382,164754:0.30566275119781494,132377:0.28824514150619507,163984:0.25693750381469727,170341:0.25508320331573486,170150:0.24072623252868652,106982:0.21406936645507812,142667:0.20630884170532227,142309:0.19895803928375244,142348:0.19895803928375244,130359:0.19403356313705444,126398:0.19365638494491577,150960:0.19339591264724731,161009:0.19322192668914795,170921:0.18918883800506592,169581:0.18422210216522217,149357:0.18282020092010498,145876:0.1814625859260559,145878:0.1814625859260559,143350:0.18094485998153687,126576:0.17980659008026123,162100:0.1778339147567749,172490:0.17778939008712769,170867:0.1754104495048523,163993:0.17149561643600464,144005:0.17119264602661133,172731:0.16986727714538574,127369:0.16894292831420898',\n",
       " '131166:131594|132192:0.3376311659812927,132470:0.31097090244293213,132033:0.29892534017562866,140623:0.2637025713920593,138200:0.24846374988555908,178634:0.2007574439048767,187060:0.1633836030960083,140640:0.15578913688659668,142230:0.13239532709121704,108962:0.13205868005752563,107239:0.12785214185714722,166943:0.12117022275924683,166070:0.12027645111083984,151900:0.11809009313583374,100619:0.11763668060302734,100621:0.11763668060302734,100623:0.11763668060302734,150863:0.10972708463668823,194489:0.1096651554107666,154752:0.10946398973464966,139048:0.10861903429031372,121363:0.10777539014816284,161076:0.10616153478622437,137947:0.10268747806549072,162718:0.10242193937301636,136365:0.10131913423538208,174821:0.10059380531311035,162086:0.09942150115966797,99311:0.09732234477996826',\n",
       " '131170:127369,125195,113747,143350|168286:0.23301410675048828,127369:0.21642667055130005,150248:0.20536822080612183,145473:0.16534346342086792,155550:0.16124862432479858,168858:0.16095280647277832,107893:0.1571282148361206,96097:0.15494978427886963,103481:0.14828306436538696,151433:0.13662445545196533,158533:0.12351477146148682,75748:0.11799800395965576,158982:0.11524391174316406,161786:0.11358946561813354,105532:0.11334902048110962,168795:0.10808414220809937,133740:0.10358136892318726,128640:0.10165882110595703,140876:0.1001976728439331,158644:0.09376668930053711,126430:0.0933641791343689,112255:0.09119576215744019,164865:0.08599931001663208,149563:0.08410650491714478,169721:0.08302253484725952,170476:0.08302253484725952,171846:0.08302253484725952,140172:0.08233124017715454,98506:0.08045399188995361',\n",
       " '131180:129163|124473:0.4759778380393982,127010:0.4203185439109802,130863:0.3895084857940674,128722:0.38582348823547363,129163:0.36787188053131104,169246:0.36652618646621704,127095:0.3354990482330322,123602:0.3283824920654297,158096:0.32431191205978394,175469:0.3100774884223938,114315:0.30771130323410034,144217:0.30071574449539185,108942:0.29910022020339966,149234:0.2985905408859253,116315:0.2762044668197632,134984:0.2714182734489441,134452:0.270957350730896,114909:0.2679727077484131,116598:0.26621532440185547,134114:0.26567643880844116,131524:0.2642213702201843,125946:0.260492742061615,128726:0.25878244638442993,129652:0.2582487463951111,74385:0.25812816619873047,108335:0.2560200095176697,150379:0.25225830078125,137247:0.25068914890289307,112141:0.24779343605041504',\n",
       " '131192:133635,131782,131306,131759,130869,130934,131293|133635:0.36358916759490967,163073:0.35940277576446533,132037:0.35010361671447754,97671:0.350024938583374,145780:0.336239218711853,165125:0.32532477378845215,131293:0.32508111000061035,137994:0.3246117830276489,150660:0.322584867477417,162154:0.30807167291641235,157531:0.3051210045814514,132612:0.3020535707473755,83540:0.29961180686950684,132609:0.27910149097442627,132035:0.27784478664398193,151686:0.2755911350250244,135176:0.27009642124176025,155985:0.26870137453079224,150215:0.2685887813568115,144112:0.26612865924835205,152756:0.26478397846221924,157022:0.2542714476585388,151466:0.2511664032936096,115716:0.24894797801971436,158587:0.2365698218345642,154147:0.23388797044754028,129828:0.22856754064559937,119423:0.223097562789917,111055:0.22181099653244019',\n",
       " '131197:119617,131970,127169,123908,155137,126563,120745,167562,176395,126057,122219,120880,32755,136212,126260,187290,170110|126563:0.4103139638900757,126057:0.407515287399292,116102:0.38214439153671265,127169:0.3818563222885132,117855:0.376492977142334,131970:0.36702960729599,130329:0.3481820821762085,112808:0.3422461748123169,169490:0.3348093628883362,172877:0.33414608240127563,126260:0.3298841714859009,105313:0.32891547679901123,164796:0.32553911209106445,123423:0.31838130950927734,118941:0.3179256319999695,56401:0.30765682458877563,119831:0.3063076138496399,152688:0.30440467596054077,116926:0.3024231791496277,120745:0.2995861768722534,117558:0.2985891103744507,114628:0.2972974181175232,116415:0.2972069978713989,147290:0.2937402129173279,117564:0.2927250266075134,148565:0.28761202096939087,142239:0.28423595428466797,121222:0.28166258335113525,165175:0.2791522145271301',\n",
       " '131209:177304|93558:0.4229978322982788,113099:0.33202970027923584,93387:0.3254263997077942,93566:0.3237069845199585,93317:0.31906306743621826,93545:0.31398922204971313,93539:0.31242698431015015,93319:0.29574841260910034,115451:0.295485258102417,106754:0.2751285433769226,107258:0.2576136589050293,140509:0.25710874795913696,121603:0.24907314777374268,175779:0.24495631456375122,122040:0.24489718675613403,106414:0.24451088905334473,93541:0.24210160970687866,108202:0.24178177118301392,101925:0.2387576699256897,135505:0.23568105697631836,164499:0.23397880792617798,106920:0.2318909764289856,132554:0.23091351985931396,107957:0.22733139991760254,121538:0.22516918182373047,151827:0.22506564855575562,102097:0.22499984502792358,100822:0.21946334838867188,105825:0.21925872564315796']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of queries: 17002\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of queries:\", len(retrieval.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_FEATURE.replace('@number_of_epochs@', str(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26036"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exported_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_RANK_PATH = os.path.join(DIR, 'exported_rank_{}.txt'.format(METHOD))\n",
    "EXPORT_RANK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_RANK_PATH, 'w') as file_out:\n",
    "    for row in exported_rank:\n",
    "        file_out.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 - recall_at_5': 0.56,\n",
       " '2 - recall_at_10': 0.62,\n",
       " '3 - recall_at_15': 0.65,\n",
       " '4 - recall_at_20': 0.67,\n",
       " '5 - recall_at_25': 0.69}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = experiment.evaluation.evaluate(EXPORT_RANK_PATH)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[baseline] Bug triage with Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
